{
  
    
        "post0": {
            "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
            "content": "The Hugging Face Hub has become the de facto place to share machine learning models and datasets. As the number of models and datasets grow the challenge of finding the right model or dataset for your needs may become more challenging. There are various ways in which we can try and make it easier for people to find relevant models and datasets. One of these is by associating metadata with datasets and models. This blog post will (very briefly) begin to explore metadata on the Hugging Face Hub. Often you&#39;ll want to explore models and datasets via the Hub website but this isn&#39;t the only way to explore the Hub. As part of the process of exploring metadata on the Hugging Face Hub we&#39;ll briefly look at how we can use the huggingface_hub library to programmatically interact with the Hub. . Library imports . For this post we&#39;ll need a few libraries, pandas, requests and matplotlib are likely old friends (or foes...). The huggingface_hub library might be new to you but will soon become a good friend too! The rich library is fantastically useful for quickly getting familiar with a library (i.e. avoiding reading all the docs!) so we&#39;ll import that too. . import requests from huggingface_hub import hf_api import pandas as pd import matplotlib.pyplot as plt import rich . %matplotlib inline plt.style.use(&quot;ggplot&quot;) . We&#39;ll instantiate an instance of the HfApi class. . api = hf_api.HfApi() . We can use rich inspect to get a better sense of what a function or class instance is all about. Let&#39;s see what methods the api has. . rich.inspect(api, methods=True) . ╭──────────────────────────────────── &lt;class &#39;huggingface_hub.hf_api.HfApi&#39;&gt; ─────────────────────────────────────╮ │ ╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────╮ │ │ │ &lt;huggingface_hub.hf_api.HfApi object at 0x136a2ce80&gt; │ │ │ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ │ │ │ │ endpoint = &#39;https://huggingface.co&#39; │ │ token = None │ │ change_discussion_status = def change_discussion_status(repo_id: str, discussion_num: int, new_status: │ │ Literal[&#39;open&#39;, &#39;closed&#39;], *, token: Optional[str] = None, comment: Optional[str] = │ │ None, repo_type: Optional[str] = None) -&gt; │ │ huggingface_hub.community.DiscussionStatusChange: Closes or re-opens a Discussion or │ │ Pull Request. │ │ comment_discussion = def comment_discussion(repo_id: str, discussion_num: int, comment: str, *, token: │ │ Optional[str] = None, repo_type: Optional[str] = None) -&gt; │ │ huggingface_hub.community.DiscussionComment: Creates a new comment on the given │ │ Discussion. │ │ create_branch = def create_branch(repo_id: str, *, branch: str, token: Optional[str] = None, │ │ repo_type: Optional[str] = None) -&gt; None: Create a new branch from `main` on a repo │ │ on the Hub. │ │ create_commit = def create_commit(repo_id: str, operations: │ │ Iterable[Union[huggingface_hub._commit_api.CommitOperationAdd, │ │ huggingface_hub._commit_api.CommitOperationDelete]], *, commit_message: str, │ │ commit_description: Optional[str] = None, token: Optional[str] = None, repo_type: │ │ Optional[str] = None, revision: Optional[str] = None, create_pr: Optional[bool] = │ │ None, num_threads: int = 5, parent_commit: Optional[str] = None) -&gt; │ │ huggingface_hub.hf_api.CommitInfo: Creates a commit in the given repo, deleting &amp; │ │ uploading files as needed. │ │ create_discussion = def create_discussion(repo_id: str, title: str, *, token: Optional[str] = None, │ │ description: Optional[str] = None, repo_type: Optional[str] = None, pull_request: │ │ bool = False) -&gt; huggingface_hub.community.DiscussionWithDetails: Creates a │ │ Discussion or Pull Request. │ │ create_pull_request = def create_pull_request(repo_id: str, title: str, *, token: Optional[str] = None, │ │ description: Optional[str] = None, repo_type: Optional[str] = None) -&gt; │ │ huggingface_hub.community.DiscussionWithDetails: Creates a Pull Request . Pull │ │ Requests created programmatically will be in `&quot;draft&quot;` status. │ │ create_repo = def create_repo(repo_id: str, *, token: Optional[str] = None, private: bool = False, │ │ repo_type: Optional[str] = None, exist_ok: bool = False, space_sdk: Optional[str] = │ │ None) -&gt; str: Create an empty repo on the HuggingFace Hub. │ │ create_tag = def create_tag(repo_id: str, *, tag: str, tag_message: Optional[str] = None, │ │ revision: Optional[str] = None, token: Optional[str] = None, repo_type: │ │ Optional[str] = None) -&gt; None: Tag a given commit of a repo on the Hub. │ │ dataset_info = def dataset_info(repo_id: str, *, revision: Optional[str] = None, timeout: │ │ Optional[float] = None, files_metadata: bool = False, token: Union[bool, str, │ │ NoneType] = None) -&gt; huggingface_hub.hf_api.DatasetInfo: Get info on one specific │ │ dataset on huggingface.co. │ │ delete_branch = def delete_branch(repo_id: str, *, branch: str, token: Optional[str] = None, │ │ repo_type: Optional[str] = None) -&gt; None: Delete a branch from a repo on the Hub. │ │ delete_file = def delete_file(path_in_repo: str, repo_id: str, *, token: Optional[str] = None, │ │ repo_type: Optional[str] = None, revision: Optional[str] = None, commit_message: │ │ Optional[str] = None, commit_description: Optional[str] = None, create_pr: │ │ Optional[bool] = None, parent_commit: Optional[str] = None) -&gt; │ │ huggingface_hub.hf_api.CommitInfo: Deletes a file in the given repo. │ │ delete_folder = def delete_folder(path_in_repo: str, repo_id: str, *, token: Optional[str] = None, │ │ repo_type: Optional[str] = None, revision: Optional[str] = None, commit_message: │ │ Optional[str] = None, commit_description: Optional[str] = None, create_pr: │ │ Optional[bool] = None, parent_commit: Optional[str] = None) -&gt; │ │ huggingface_hub.hf_api.CommitInfo: Deletes a folder in the given repo. │ │ delete_repo = def delete_repo(repo_id: str, *, token: Optional[str] = None, repo_type: │ │ Optional[str] = None): Delete a repo from the HuggingFace Hub. CAUTION: this is │ │ irreversible. │ │ delete_tag = def delete_tag(repo_id: str, *, tag: str, token: Optional[str] = None, repo_type: │ │ Optional[str] = None) -&gt; None: Delete a tag from a repo on the Hub. │ │ edit_discussion_comment = def edit_discussion_comment(repo_id: str, discussion_num: int, comment_id: str, │ │ new_content: str, *, token: Optional[str] = None, repo_type: Optional[str] = None) │ │ -&gt; huggingface_hub.community.DiscussionComment: Edits a comment on a Discussion / │ │ Pull Request. │ │ get_dataset_tags = def get_dataset_tags() -&gt; huggingface_hub.utils.endpoint_helpers.DatasetTags: Gets │ │ all valid dataset tags as a nested namespace object. │ │ get_discussion_details = def get_discussion_details(repo_id: str, discussion_num: int, *, repo_type: │ │ Optional[str] = None, token: Optional[str] = None) -&gt; │ │ huggingface_hub.community.DiscussionWithDetails: Fetches a Discussion&#39;s / Pull │ │ Request &#39;s details from the Hub. │ │ get_full_repo_name = def get_full_repo_name(model_id: str, *, organization: Optional[str] = None, token: │ │ Union[bool, str, NoneType] = None): │ │ Returns the repository name for a given model ID and optional │ │ organization. │ │ get_model_tags = def get_model_tags() -&gt; huggingface_hub.utils.endpoint_helpers.ModelTags: Gets all │ │ valid model tags as a nested namespace object │ │ get_repo_discussions = def get_repo_discussions(repo_id: str, *, repo_type: Optional[str] = None, token: │ │ Optional[str] = None) -&gt; Iterator[huggingface_hub.community.Discussion]: Fetches │ │ Discussions and Pull Requests for the given repo. │ │ hide_discussion_comment = def hide_discussion_comment(repo_id: str, discussion_num: int, comment_id: str, *, │ │ token: Optional[str] = None, repo_type: Optional[str] = None) -&gt; │ │ huggingface_hub.community.DiscussionComment: Hides a comment on a Discussion / Pull │ │ Request. │ │ list_datasets = def list_datasets(*, filter: │ │ Union[huggingface_hub.utils.endpoint_helpers.DatasetFilter, str, Iterable[str], │ │ NoneType] = None, author: Optional[str] = None, search: Optional[str] = None, sort: │ │ Union[Literal[&#39;lastModified&#39;], str, NoneType] = None, direction: │ │ Optional[Literal[-1]] = None, limit: Optional[int] = None, cardData: Optional[bool] │ │ = None, full: Optional[bool] = None, token: Optional[str] = None) -&gt; │ │ List[huggingface_hub.hf_api.DatasetInfo]: Get the list of all the datasets on │ │ huggingface.co │ │ list_metrics = def list_metrics() -&gt; List[huggingface_hub.hf_api.MetricInfo]: Get the public list │ │ of all the metrics on huggingface.co │ │ list_models = def list_models(*, filter: Union[huggingface_hub.utils.endpoint_helpers.ModelFilter, │ │ str, Iterable[str], NoneType] = None, author: Optional[str] = None, search: │ │ Optional[str] = None, emissions_thresholds: Optional[Tuple[float, float]] = None, │ │ sort: Union[Literal[&#39;lastModified&#39;], str, NoneType] = None, direction: │ │ Optional[Literal[-1]] = None, limit: Optional[int] = None, full: Optional[bool] = │ │ None, cardData: bool = False, fetch_config: bool = False, token: Union[bool, str, │ │ NoneType] = None) -&gt; List[huggingface_hub.hf_api.ModelInfo]: Get the list of all the │ │ models on huggingface.co │ │ list_repo_files = def list_repo_files(repo_id: str, *, revision: Optional[str] = None, repo_type: │ │ Optional[str] = None, timeout: Optional[float] = None, token: Union[bool, str, │ │ NoneType] = None) -&gt; List[str]: Get the list of files in a given repo. │ │ list_spaces = def list_spaces(*, filter: Union[str, Iterable[str], NoneType] = None, author: │ │ Optional[str] = None, search: Optional[str] = None, sort: │ │ Union[Literal[&#39;lastModified&#39;], str, NoneType] = None, direction: │ │ Optional[Literal[-1]] = None, limit: Optional[int] = None, datasets: Union[str, │ │ Iterable[str], NoneType] = None, models: Union[str, Iterable[str], NoneType] = None, │ │ linked: bool = False, full: Optional[bool] = None, token: Optional[str] = None) -&gt; │ │ List[huggingface_hub.hf_api.SpaceInfo]: Get the public list of all Spaces on │ │ huggingface.co │ │ merge_pull_request = def merge_pull_request(repo_id: str, discussion_num: int, *, token: Optional[str] = │ │ None, comment: Optional[str] = None, repo_type: Optional[str] = None): Merges a Pull │ │ Request. │ │ model_info = def model_info(repo_id: str, *, revision: Optional[str] = None, timeout: │ │ Optional[float] = None, securityStatus: Optional[bool] = None, files_metadata: bool │ │ = False, token: Union[bool, str, NoneType] = None) -&gt; │ │ huggingface_hub.hf_api.ModelInfo: Get info on one specific model on huggingface.co │ │ move_repo = def move_repo(from_id: str, to_id: str, *, repo_type: Optional[str] = None, token: │ │ Optional[str] = None): Moving a repository from namespace1/repo_name1 to │ │ namespace2/repo_name2 │ │ rename_discussion = def rename_discussion(repo_id: str, discussion_num: int, new_title: str, *, token: │ │ Optional[str] = None, repo_type: Optional[str] = None) -&gt; │ │ huggingface_hub.community.DiscussionTitleChange: Renames a Discussion. │ │ repo_info = def repo_info(repo_id: str, *, revision: Optional[str] = None, repo_type: │ │ Optional[str] = None, timeout: Optional[float] = None, files_metadata: bool = False, │ │ token: Union[bool, str, NoneType] = None) -&gt; Union[huggingface_hub.hf_api.ModelInfo, │ │ huggingface_hub.hf_api.DatasetInfo, huggingface_hub.hf_api.SpaceInfo]: Get the info │ │ object for a given repo of a given type. │ │ set_access_token = def set_access_token(access_token: str): │ │ Saves the passed access token so git can correctly authenticate the │ │ user. │ │ space_info = def space_info(repo_id: str, *, revision: Optional[str] = None, timeout: │ │ Optional[float] = None, files_metadata: bool = False, token: Union[bool, str, │ │ NoneType] = None) -&gt; huggingface_hub.hf_api.SpaceInfo: Get info on one specific │ │ Space on huggingface.co. │ │ unset_access_token = def unset_access_token(): Resets the user&#39;s access token. │ │ update_repo_visibility = def update_repo_visibility(repo_id: str, private: bool = False, *, token: │ │ Optional[str] = None, organization: Optional[str] = None, repo_type: Optional[str] = │ │ None, name: Optional[str] = None) -&gt; Dict[str, bool]: Update the visibility setting │ │ of a repository. │ │ upload_file = def upload_file(*, path_or_fileobj: Union[str, bytes, BinaryIO], path_in_repo: str, │ │ repo_id: str, token: Optional[str] = None, repo_type: Optional[str] = None, │ │ revision: Optional[str] = None, commit_message: Optional[str] = None, │ │ commit_description: Optional[str] = None, create_pr: Optional[bool] = None, │ │ parent_commit: Optional[str] = None) -&gt; str: │ │ Upload a local file (up to 50 GB) to the given repo. The upload is done │ │ through a HTTP post request, and doesn&#39;t require git or git-lfs to be │ │ installed. │ │ upload_folder = def upload_folder(*, repo_id: str, folder_path: Union[str, pathlib.Path], │ │ path_in_repo: Optional[str] = None, commit_message: Optional[str] = None, │ │ commit_description: Optional[str] = None, token: Optional[str] = None, repo_type: │ │ Optional[str] = None, revision: Optional[str] = None, create_pr: Optional[bool] = │ │ None, parent_commit: Optional[str] = None, allow_patterns: Union[List[str], str, │ │ NoneType] = None, ignore_patterns: Union[List[str], str, NoneType] = None): │ │ Upload a local folder to the given repo. The upload is done │ │ through a HTTP requests, and doesn&#39;t require git or git-lfs to be │ │ installed. │ │ whoami = def whoami(token: Optional[str] = None) -&gt; Dict: Call HF API to know &quot;whoami&quot;. │ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ . You&#39;ll see from looking through this there is a bunch of different things we can now do programmatically via the hub. For this post we&#39;re interested in the list_datasets and list_models methods. If we look at one of these we can see it has a bunch of different options we can use when listing datasets or models. . rich.inspect(api.list_models) . ╭─────────── &lt;bound method HfApi.list_models of &lt;huggingface_hub.hf_api.HfApi object at 0x136a2ce80&gt;&gt; ────────────╮ │ def HfApi.list_models(*, filter: Union[huggingface_hub.utils.endpoint_helpers.ModelFilter, str, Iterable[str], │ │ NoneType] = None, author: Optional[str] = None, search: Optional[str] = None, emissions_thresholds: │ │ Optional[Tuple[float, float]] = None, sort: Union[Literal[&#39;lastModified&#39;], str, NoneType] = None, direction: │ │ Optional[Literal[-1]] = None, limit: Optional[int] = None, full: Optional[bool] = None, cardData: bool = False, │ │ fetch_config: bool = False, token: Union[bool, str, NoneType] = None) -&gt; │ │ List[huggingface_hub.hf_api.ModelInfo]: │ │ │ │ Get the list of all the models on huggingface.co │ │ │ │ 28 attribute(s) not shown. Run inspect(inspect) for options. │ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ . For our use case we want everything, so we set limit=None, we don&#39;t want any filters so we set this to None (this is the default behaviour, but we set them explicitly here to make it clearer for our future selves). We also set full=True so we get back more verbose information about our dataset and models. We also wrap the result in iter and list since the behaviour of these methods will change in future versions to support paging. . hub_datasets = list(iter(api.list_datasets(limit=None, filter=None, full=True))) . hub_models = list(iter(api.list_models(limit=None, filter=None, full=True))) . Let&#39;s peek at an example of what we get back . hub_models[0] . ModelInfo: { modelId: albert-base-v1 sha: aeffd769076a5c4f83b2546aea99ca45a15a5da4 lastModified: 2021-01-13T15:08:24.000Z tags: [&#39;pytorch&#39;, &#39;tf&#39;, &#39;albert&#39;, &#39;fill-mask&#39;, &#39;en&#39;, &#39;dataset:bookcorpus&#39;, &#39;dataset:wikipedia&#39;, &#39;arxiv:1909.11942&#39;, &#39;transformers&#39;, &#39;exbert&#39;, &#39;license:apache-2.0&#39;, &#39;autotrain_compatible&#39;, &#39;has_space&#39;] pipeline_tag: fill-mask siblings: [RepoFile(rfilename=&#39;.gitattributes&#39;, size=&#39;None&#39;, blob_id=&#39;None&#39;, lfs=&#39;None&#39;), RepoFile(rfilename=&#39;README.md&#39;, size=&#39;None&#39;, blob_id=&#39;None&#39;, lfs=&#39;None&#39;), RepoFile(rfilename=&#39;config.json&#39;, size=&#39;None&#39;, blob_id=&#39;None&#39;, lfs=&#39;None&#39;), RepoFile(rfilename=&#39;pytorch_model.bin&#39;, size=&#39;None&#39;, blob_id=&#39;None&#39;, lfs=&#39;None&#39;), RepoFile(rfilename=&#39;spiece.model&#39;, size=&#39;None&#39;, blob_id=&#39;None&#39;, lfs=&#39;None&#39;), RepoFile(rfilename=&#39;tf_model.h5&#39;, size=&#39;None&#39;, blob_id=&#39;None&#39;, lfs=&#39;None&#39;), RepoFile(rfilename=&#39;tokenizer.json&#39;, size=&#39;None&#39;, blob_id=&#39;None&#39;, lfs=&#39;None&#39;), RepoFile(rfilename=&#39;with-prefix-tf_model.h5&#39;, size=&#39;None&#39;, blob_id=&#39;None&#39;, lfs=&#39;None&#39;)] private: False author: None config: None securityStatus: None _id: 621ffdc036468d709f174328 id: albert-base-v1 gitalyUid: 4f35551ea371da7a8762caab54319a54ade836044f0ca7690d21e86b159867eb likes: 1 downloads: 75182 library_name: transformers } . hub_datasets[0] . DatasetInfo: { id: acronym_identification sha: 173af1516c409eb4596bc63a69626bdb5584c40c lastModified: 2022-11-18T17:25:49.000Z tags: [&#39;task_categories:token-classification&#39;, &#39;annotations_creators:expert-generated&#39;, &#39;language_creators:found&#39;, &#39;multilinguality:monolingual&#39;, &#39;size_categories:10K&lt;n&lt;100K&#39;, &#39;source_datasets:original&#39;, &#39;language:en&#39;, &#39;license:mit&#39;, &#39;acronym-identification&#39;, &#39;arxiv:2010.14678&#39;] private: False author: None description: Acronym identification training and development sets for the acronym identification task at SDU@AAAI-21. citation: @inproceedings{veyseh-et-al-2020-what, title={{What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation}}, author={Amir Pouran Ben Veyseh and Franck Dernoncourt and Quan Hung Tran and Thien Huu Nguyen}, year={2020}, booktitle={Proceedings of COLING}, link={https://arxiv.org/pdf/2010.14678v1.pdf} } cardData: {&#39;annotations_creators&#39;: [&#39;expert-generated&#39;], &#39;language_creators&#39;: [&#39;found&#39;], &#39;language&#39;: [&#39;en&#39;], &#39;license&#39;: [&#39;mit&#39;], &#39;multilinguality&#39;: [&#39;monolingual&#39;], &#39;size_categories&#39;: [&#39;10K&lt;n&lt;100K&#39;], &#39;source_datasets&#39;: [&#39;original&#39;], &#39;task_categories&#39;: [&#39;token-classification&#39;], &#39;task_ids&#39;: [], &#39;paperswithcode_id&#39;: &#39;acronym-identification&#39;, &#39;pretty_name&#39;: &#39;Acronym Identification Dataset&#39;, &#39;train-eval-index&#39;: [{&#39;config&#39;: &#39;default&#39;, &#39;task&#39;: &#39;token-classification&#39;, &#39;task_id&#39;: &#39;entity_extraction&#39;, &#39;splits&#39;: {&#39;eval_split&#39;: &#39;test&#39;}, &#39;col_mapping&#39;: {&#39;tokens&#39;: &#39;tokens&#39;, &#39;labels&#39;: &#39;tags&#39;}}], &#39;tags&#39;: [&#39;acronym-identification&#39;], &#39;dataset_info&#39;: {&#39;features&#39;: [{&#39;name&#39;: &#39;id&#39;, &#39;dtype&#39;: &#39;string&#39;}, {&#39;name&#39;: &#39;tokens&#39;, &#39;sequence&#39;: &#39;string&#39;}, {&#39;name&#39;: &#39;labels&#39;, &#39;sequence&#39;: {&#39;class_label&#39;: {&#39;names&#39;: {&#39;0&#39;: &#39;B-long&#39;, &#39;1&#39;: &#39;B-short&#39;, &#39;2&#39;: &#39;I-long&#39;, &#39;3&#39;: &#39;I-short&#39;, &#39;4&#39;: &#39;O&#39;}}}}], &#39;splits&#39;: [{&#39;name&#39;: &#39;train&#39;, &#39;num_bytes&#39;: 7792803, &#39;num_examples&#39;: 14006}, {&#39;name&#39;: &#39;validation&#39;, &#39;num_bytes&#39;: 952705, &#39;num_examples&#39;: 1717}, {&#39;name&#39;: &#39;test&#39;, &#39;num_bytes&#39;: 987728, &#39;num_examples&#39;: 1750}], &#39;download_size&#39;: 8556464, &#39;dataset_size&#39;: 9733236}} siblings: [] _id: 621ffdd236468d709f181d58 disabled: False gated: False gitalyUid: 6570517623fa521aa189178e7c7e73d9d88c01b295204edef97f389a15eae144 likes: 9 downloads: 6074 paperswithcode_id: acronym-identification } . Since we want both models and datasets we create a dictionary which stores the types of item i.e. is it a dataset or a model. . hub_data = {&quot;model&quot;: hub_models, &quot;dataset&quot;: hub_datasets} . We&#39;ll be putting our data inside a pandas DataFrame, so we&#39;ll grab the .__dict__ attribute for each hub item, so it&#39;s more pandas friendly. . hub_item_dict = [] for hub_type, hub_item in hub_data.items(): for item in hub_item: data = item.__dict__ data[&quot;type&quot;] = hub_type hub_item_dict.append(data) . df = pd.DataFrame.from_dict(hub_item_dict) . How many hub items do we have? . len(df) . 151343 . What info do we have? . df.columns . Index([&#39;modelId&#39;, &#39;sha&#39;, &#39;lastModified&#39;, &#39;tags&#39;, &#39;pipeline_tag&#39;, &#39;siblings&#39;, &#39;private&#39;, &#39;author&#39;, &#39;config&#39;, &#39;securityStatus&#39;, &#39;_id&#39;, &#39;id&#39;, &#39;gitalyUid&#39;, &#39;likes&#39;, &#39;downloads&#39;, &#39;library_name&#39;, &#39;type&#39;, &#39;description&#39;, &#39;citation&#39;, &#39;cardData&#39;, &#39;disabled&#39;, &#39;gated&#39;, &#39;paperswithcode_id&#39;], dtype=&#39;object&#39;) . Tags . Models and datasets have a bunch of metadata i.e. last modified and number of downloads. We&#39;ll focus on tags here. Let&#39;s start by looking at a single example. . df.loc[30, &quot;tags&quot;] . [&#39;pytorch&#39;, &#39;tf&#39;, &#39;rust&#39;, &#39;safetensors&#39;, &#39;distilbert&#39;, &#39;text-classification&#39;, &#39;en&#39;, &#39;dataset:sst2&#39;, &#39;dataset:glue&#39;, &#39;doi:10.57967/hf/0181&#39;, &#39;transformers&#39;, &#39;license:apache-2.0&#39;, &#39;model-index&#39;, &#39;has_space&#39;] . We can see that tags capture can relate to tasks i.e. text-classification, libraries supported i.e. tf, or the licence associated with a model or dataset. As a starting point for exploring tags we can take a look at how many tags models and datasets have. We&#39;ll add a new column to capture this number. . def calculate_number_of_tags(tags: [str]) -&gt; int: return len(tags) . df[&quot;number_of_tags&quot;] = df[&quot;tags&quot;].apply(lambda x: calculate_number_of_tags(x)) . We can now use describe to see the breakdown of this number. . df.number_of_tags.describe() . count 151343.000000 mean 3.855566 std 6.878613 min 0.000000 25% 0.000000 50% 4.000000 75% 6.000000 max 650.000000 Name: number_of_tags, dtype: float64 . We can see that we have quite a range of tag numbers ranging from 0 to 650! If your brain works anything like mine you probably want to know what this high value is about! . df[df.number_of_tags &gt; 640][[&quot;id&quot;, &quot;tags&quot;]] . id tags . 136372 bible-nlp/biblenlp-corpus | [task_categories:translation, annotations_crea... | . df[df.number_of_tags &gt; 640][&quot;tags&quot;].tolist() . [[&#39;task_categories:translation&#39;, &#39;annotations_creators:no-annotation&#39;, &#39;language_creators:expert-generated&#39;, &#39;multilinguality:translation&#39;, &#39;multilinguality:multilingual&#39;, &#39;size_categories:1M&lt;n&lt;10M&#39;, &#39;source_datasets:original&#39;, &#39;language:aau&#39;, &#39;language:aaz&#39;, &#39;language:abx&#39;, &#39;language:aby&#39;, &#39;language:acf&#39;, &#39;language:acu&#39;, &#39;language:adz&#39;, &#39;language:aey&#39;, &#39;language:agd&#39;, &#39;language:agg&#39;, &#39;language:agm&#39;, &#39;language:agn&#39;, &#39;language:agr&#39;, &#39;language:agu&#39;, &#39;language:aia&#39;, &#39;language:ake&#39;, &#39;language:alp&#39;, &#39;language:alq&#39;, &#39;language:als&#39;, &#39;language:aly&#39;, &#39;language:ame&#39;, &#39;language:amk&#39;, &#39;language:amp&#39;, &#39;language:amr&#39;, &#39;language:amu&#39;, &#39;language:anh&#39;, &#39;language:anv&#39;, &#39;language:aoi&#39;, &#39;language:aoj&#39;, &#39;language:apb&#39;, &#39;language:apn&#39;, &#39;language:apu&#39;, &#39;language:apy&#39;, &#39;language:arb&#39;, &#39;language:arl&#39;, &#39;language:arn&#39;, &#39;language:arp&#39;, &#39;language:aso&#39;, &#39;language:ata&#39;, &#39;language:atb&#39;, &#39;language:atd&#39;, &#39;language:atg&#39;, &#39;language:auc&#39;, &#39;language:aui&#39;, &#39;language:auy&#39;, &#39;language:avt&#39;, &#39;language:awb&#39;, &#39;language:awk&#39;, &#39;language:awx&#39;, &#39;language:azg&#39;, &#39;language:azz&#39;, &#39;language:bao&#39;, &#39;language:bbb&#39;, &#39;language:bbr&#39;, &#39;language:bch&#39;, &#39;language:bco&#39;, &#39;language:bdd&#39;, &#39;language:bea&#39;, &#39;language:bel&#39;, &#39;language:bgs&#39;, &#39;language:bgt&#39;, &#39;language:bhg&#39;, &#39;language:bhl&#39;, &#39;language:big&#39;, &#39;language:bjr&#39;, &#39;language:bjv&#39;, &#39;language:bkd&#39;, &#39;language:bki&#39;, &#39;language:bkq&#39;, &#39;language:bkx&#39;, &#39;language:bla&#39;, &#39;language:blw&#39;, &#39;language:blz&#39;, &#39;language:bmh&#39;, &#39;language:bmk&#39;, &#39;language:bmr&#39;, &#39;language:bnp&#39;, &#39;language:boa&#39;, &#39;language:boj&#39;, &#39;language:bon&#39;, &#39;language:box&#39;, &#39;language:bqc&#39;, &#39;language:bre&#39;, &#39;language:bsn&#39;, &#39;language:bsp&#39;, &#39;language:bss&#39;, &#39;language:buk&#39;, &#39;language:bus&#39;, &#39;language:bvr&#39;, &#39;language:bxh&#39;, &#39;language:byx&#39;, &#39;language:bzd&#39;, &#39;language:bzj&#39;, &#39;language:cab&#39;, &#39;language:caf&#39;, &#39;language:cao&#39;, &#39;language:cap&#39;, &#39;language:car&#39;, &#39;language:cav&#39;, &#39;language:cax&#39;, &#39;language:cbc&#39;, &#39;language:cbi&#39;, &#39;language:cbk&#39;, &#39;language:cbr&#39;, &#39;language:cbs&#39;, &#39;language:cbt&#39;, &#39;language:cbu&#39;, &#39;language:cbv&#39;, &#39;language:cco&#39;, &#39;language:ces&#39;, &#39;language:cgc&#39;, &#39;language:cha&#39;, &#39;language:chd&#39;, &#39;language:chf&#39;, &#39;language:chk&#39;, &#39;language:chq&#39;, &#39;language:chz&#39;, &#39;language:cjo&#39;, &#39;language:cjv&#39;, &#39;language:cle&#39;, &#39;language:clu&#39;, &#39;language:cme&#39;, &#39;language:cmn&#39;, &#39;language:cni&#39;, &#39;language:cnl&#39;, &#39;language:cnt&#39;, &#39;language:cof&#39;, &#39;language:con&#39;, &#39;language:cop&#39;, &#39;language:cot&#39;, &#39;language:cpa&#39;, &#39;language:cpb&#39;, &#39;language:cpc&#39;, &#39;language:cpu&#39;, &#39;language:crn&#39;, &#39;language:crx&#39;, &#39;language:cso&#39;, &#39;language:cta&#39;, &#39;language:ctp&#39;, &#39;language:ctu&#39;, &#39;language:cub&#39;, &#39;language:cuc&#39;, &#39;language:cui&#39;, &#39;language:cut&#39;, &#39;language:cux&#39;, &#39;language:cwe&#39;, &#39;language:daa&#39;, &#39;language:dad&#39;, &#39;language:dah&#39;, &#39;language:ded&#39;, &#39;language:deu&#39;, &#39;language:dgr&#39;, &#39;language:dgz&#39;, &#39;language:dif&#39;, &#39;language:dik&#39;, &#39;language:dji&#39;, &#39;language:djk&#39;, &#39;language:dob&#39;, &#39;language:dwr&#39;, &#39;language:dww&#39;, &#39;language:dwy&#39;, &#39;language:eko&#39;, &#39;language:emi&#39;, &#39;language:emp&#39;, &#39;language:eng&#39;, &#39;language:epo&#39;, &#39;language:eri&#39;, &#39;language:ese&#39;, &#39;language:etr&#39;, &#39;language:faa&#39;, &#39;language:fai&#39;, &#39;language:far&#39;, &#39;language:for&#39;, &#39;language:fra&#39;, &#39;language:fuf&#39;, &#39;language:gai&#39;, &#39;language:gam&#39;, &#39;language:gaw&#39;, &#39;language:gdn&#39;, &#39;language:gdr&#39;, &#39;language:geb&#39;, &#39;language:gfk&#39;, &#39;language:ghs&#39;, &#39;language:gia&#39;, &#39;language:glk&#39;, &#39;language:gmv&#39;, &#39;language:gng&#39;, &#39;language:gnn&#39;, &#39;language:gnw&#39;, &#39;language:gof&#39;, &#39;language:grc&#39;, &#39;language:gub&#39;, &#39;language:guh&#39;, &#39;language:gui&#39;, &#39;language:gul&#39;, &#39;language:gum&#39;, &#39;language:guo&#39;, &#39;language:gvc&#39;, &#39;language:gvf&#39;, &#39;language:gwi&#39;, &#39;language:gym&#39;, &#39;language:gyr&#39;, &#39;language:hat&#39;, &#39;language:haw&#39;, &#39;language:hbo&#39;, &#39;language:hch&#39;, &#39;language:heb&#39;, &#39;language:heg&#39;, &#39;language:hix&#39;, &#39;language:hla&#39;, &#39;language:hlt&#39;, &#39;language:hns&#39;, &#39;language:hop&#39;, &#39;language:hrv&#39;, &#39;language:hub&#39;, &#39;language:hui&#39;, &#39;language:hus&#39;, &#39;language:huu&#39;, &#39;language:huv&#39;, &#39;language:hvn&#39;, &#39;language:ign&#39;, &#39;language:ikk&#39;, &#39;language:ikw&#39;, &#39;language:imo&#39;, &#39;language:inb&#39;, &#39;language:ind&#39;, &#39;language:ino&#39;, &#39;language:iou&#39;, &#39;language:ipi&#39;, &#39;language:ita&#39;, &#39;language:jac&#39;, &#39;language:jao&#39;, &#39;language:jic&#39;, &#39;language:jiv&#39;, &#39;language:jpn&#39;, &#39;language:jvn&#39;, &#39;language:kaq&#39;, &#39;language:kbc&#39;, &#39;language:kbh&#39;, &#39;language:kbm&#39;, &#39;language:kdc&#39;, &#39;language:kde&#39;, &#39;language:kdl&#39;, &#39;language:kek&#39;, &#39;language:ken&#39;, &#39;language:kew&#39;, &#39;language:kgk&#39;, &#39;language:kgp&#39;, &#39;language:khs&#39;, &#39;language:kje&#39;, &#39;language:kjs&#39;, &#39;language:kkc&#39;, &#39;language:kky&#39;, &#39;language:klt&#39;, &#39;language:klv&#39;, &#39;language:kms&#39;, &#39;language:kmu&#39;, &#39;language:kne&#39;, &#39;language:knf&#39;, &#39;language:knj&#39;, &#39;language:kos&#39;, &#39;language:kpf&#39;, &#39;language:kpg&#39;, &#39;language:kpj&#39;, &#39;language:kpw&#39;, &#39;language:kqa&#39;, &#39;language:kqc&#39;, &#39;language:kqf&#39;, &#39;language:kql&#39;, &#39;language:kqw&#39;, &#39;language:ksj&#39;, &#39;language:ksr&#39;, &#39;language:ktm&#39;, &#39;language:kto&#39;, &#39;language:kud&#39;, &#39;language:kue&#39;, &#39;language:kup&#39;, &#39;language:kvn&#39;, &#39;language:kwd&#39;, &#39;language:kwf&#39;, &#39;language:kwi&#39;, &#39;language:kwj&#39;, &#39;language:kyf&#39;, &#39;language:kyg&#39;, &#39;language:kyq&#39;, &#39;language:kyz&#39;, &#39;language:kze&#39;, &#39;language:lac&#39;, &#39;language:lat&#39;, &#39;language:lbb&#39;, &#39;language:leu&#39;, &#39;language:lex&#39;, &#39;language:lgl&#39;, &#39;language:lid&#39;, &#39;language:lif&#39;, &#39;language:lww&#39;, &#39;language:maa&#39;, &#39;language:maj&#39;, &#39;language:maq&#39;, &#39;language:mau&#39;, &#39;language:mav&#39;, &#39;language:maz&#39;, &#39;language:mbb&#39;, &#39;language:mbc&#39;, &#39;language:mbh&#39;, &#39;language:mbl&#39;, &#39;language:mbt&#39;, &#39;language:mca&#39;, &#39;language:mcb&#39;, &#39;language:mcd&#39;, &#39;language:mcf&#39;, &#39;language:mcp&#39;, &#39;language:mdy&#39;, &#39;language:med&#39;, &#39;language:mee&#39;, &#39;language:mek&#39;, &#39;language:meq&#39;, &#39;language:met&#39;, &#39;language:meu&#39;, &#39;language:mgh&#39;, &#39;language:mgw&#39;, &#39;language:mhl&#39;, &#39;language:mib&#39;, &#39;language:mic&#39;, &#39;language:mie&#39;, &#39;language:mig&#39;, &#39;language:mih&#39;, &#39;language:mil&#39;, &#39;language:mio&#39;, &#39;language:mir&#39;, &#39;language:mit&#39;, &#39;language:miz&#39;, &#39;language:mjc&#39;, &#39;language:mkn&#39;, &#39;language:mks&#39;, &#39;language:mlh&#39;, &#39;language:mlp&#39;, &#39;language:mmx&#39;, &#39;language:mna&#39;, &#39;language:mop&#39;, &#39;language:mox&#39;, &#39;language:mph&#39;, &#39;language:mpj&#39;, &#39;language:mpm&#39;, &#39;language:mpp&#39;, &#39;language:mps&#39;, &#39;language:mpx&#39;, &#39;language:mqb&#39;, &#39;language:mqj&#39;, &#39;language:msb&#39;, &#39;language:msc&#39;, &#39;language:msk&#39;, &#39;language:msm&#39;, &#39;language:msy&#39;, &#39;language:mti&#39;, &#39;language:muy&#39;, &#39;language:mva&#39;, &#39;language:mvn&#39;, &#39;language:mwc&#39;, &#39;language:mxb&#39;, &#39;language:mxp&#39;, &#39;language:mxq&#39;, &#39;language:mxt&#39;, &#39;language:myu&#39;, &#39;language:myw&#39;, &#39;language:myy&#39;, &#39;language:mzz&#39;, &#39;language:nab&#39;, &#39;language:naf&#39;, &#39;language:nak&#39;, &#39;language:nay&#39;, &#39;language:nbq&#39;, &#39;language:nca&#39;, &#39;language:nch&#39;, &#39;language:ncj&#39;, &#39;language:ncl&#39;, &#39;language:ncu&#39;, &#39;language:ndj&#39;, &#39;language:nfa&#39;, &#39;language:ngp&#39;, &#39;language:ngu&#39;, &#39;language:nhg&#39;, &#39;language:nhi&#39;, &#39;language:nho&#39;, &#39;language:nhr&#39;, &#39;language:nhu&#39;, &#39;language:nhw&#39;, &#39;language:nhy&#39;, &#39;language:nif&#39;, &#39;language:nin&#39;, &#39;language:nko&#39;, &#39;language:nld&#39;, &#39;language:nlg&#39;, &#39;language:nna&#39;, &#39;language:nnq&#39;, &#39;language:not&#39;, &#39;language:nou&#39;, &#39;language:npl&#39;, &#39;language:nsn&#39;, &#39;language:nss&#39;, &#39;language:ntj&#39;, &#39;language:ntp&#39;, &#39;language:nwi&#39;, &#39;language:nyu&#39;, &#39;language:obo&#39;, &#39;language:ong&#39;, &#39;language:ons&#39;, &#39;language:ood&#39;, &#39;language:opm&#39;, &#39;language:ote&#39;, &#39;language:otm&#39;, &#39;language:otn&#39;, &#39;language:otq&#39;, &#39;language:ots&#39;, &#39;language:pab&#39;, &#39;language:pad&#39;, &#39;language:pah&#39;, &#39;language:pao&#39;, &#39;language:pes&#39;, &#39;language:pib&#39;, &#39;language:pio&#39;, &#39;language:pir&#39;, &#39;language:pjt&#39;, &#39;language:plu&#39;, &#39;language:pma&#39;, &#39;language:poe&#39;, &#39;language:poi&#39;, &#39;language:pon&#39;, &#39;language:poy&#39;, &#39;language:ppo&#39;, &#39;language:prf&#39;, &#39;language:pri&#39;, &#39;language:ptp&#39;, &#39;language:ptu&#39;, &#39;language:pwg&#39;, &#39;language:quc&#39;, &#39;language:quf&#39;, &#39;language:quh&#39;, &#39;language:qul&#39;, &#39;language:qup&#39;, &#39;language:qvc&#39;, &#39;language:qve&#39;, &#39;language:qvh&#39;, &#39;language:qvm&#39;, &#39;language:qvn&#39;, &#39;language:qvs&#39;, &#39;language:qvw&#39;, &#39;language:qvz&#39;, &#39;language:qwh&#39;, &#39;language:qxh&#39;, &#39;language:qxn&#39;, &#39;language:qxo&#39;, &#39;language:rai&#39;, &#39;language:rkb&#39;, &#39;language:rmc&#39;, &#39;language:roo&#39;, &#39;language:rop&#39;, &#39;language:rro&#39;, &#39;language:ruf&#39;, &#39;language:rug&#39;, &#39;language:rus&#39;, &#39;language:sab&#39;, &#39;language:san&#39;, &#39;language:sbe&#39;, &#39;language:seh&#39;, &#39;language:sey&#39;, &#39;language:sgz&#39;, &#39;language:shj&#39;, &#39;language:shp&#39;, &#39;language:sim&#39;, &#39;language:sja&#39;, &#39;language:sll&#39;, &#39;language:smk&#39;, &#39;language:snc&#39;, &#39;language:snn&#39;, &#39;language:sny&#39;, &#39;language:som&#39;, &#39;language:soq&#39;, &#39;language:spa&#39;, &#39;language:spl&#39;, &#39;language:spm&#39;, &#39;language:sps&#39;, &#39;language:spy&#39;, &#39;language:sri&#39;, &#39;language:srm&#39;, &#39;language:srn&#39;, &#39;language:srp&#39;, &#39;language:srq&#39;, &#39;language:ssd&#39;, &#39;language:ssg&#39;, &#39;language:ssx&#39;, &#39;language:stp&#39;, &#39;language:sua&#39;, &#39;language:sue&#39;, &#39;language:sus&#39;, &#39;language:suz&#39;, &#39;language:swe&#39;, &#39;language:swh&#39;, &#39;language:swp&#39;, &#39;language:sxb&#39;, &#39;language:tac&#39;, &#39;language:tav&#39;, &#39;language:tbc&#39;, &#39;language:tbl&#39;, &#39;language:tbo&#39;, &#39;language:tbz&#39;, &#39;language:tca&#39;, &#39;language:tee&#39;, &#39;language:ter&#39;, &#39;language:tew&#39;, &#39;language:tfr&#39;, &#39;language:tgp&#39;, &#39;language:tif&#39;, &#39;language:tim&#39;, &#39;language:tiy&#39;, &#39;language:tke&#39;, &#39;language:tku&#39;, &#39;language:tna&#39;, &#39;language:tnc&#39;, &#39;language:tnn&#39;, &#39;language:tnp&#39;, &#39;language:toc&#39;, &#39;language:tod&#39;, &#39;language:toj&#39;, &#39;language:ton&#39;, &#39;language:too&#39;, &#39;language:top&#39;, &#39;language:tos&#39;, &#39;language:tpt&#39;, &#39;language:trc&#39;, &#39;language:tsw&#39;, &#39;language:ttc&#39;, &#39;language:tue&#39;, &#39;language:tuo&#39;, &#39;language:txu&#39;, &#39;language:ubr&#39;, &#39;language:udu&#39;, &#39;language:ukr&#39;, &#39;language:uli&#39;, &#39;language:ura&#39;, &#39;language:urb&#39;, &#39;language:usa&#39;, &#39;language:usp&#39;, &#39;language:uvl&#39;, &#39;language:vid&#39;, &#39;language:vie&#39;, &#39;language:viv&#39;, &#39;language:vmy&#39;, &#39;language:waj&#39;, &#39;language:wal&#39;, &#39;language:wap&#39;, &#39;language:wat&#39;, &#39;language:wbp&#39;, &#39;language:wed&#39;, &#39;language:wer&#39;, &#39;language:wim&#39;, &#39;language:wmt&#39;, &#39;language:wmw&#39;, &#39;language:wnc&#39;, &#39;language:wnu&#39;, &#39;language:wos&#39;, &#39;language:wrk&#39;, &#39;language:wro&#39;, &#39;language:wsk&#39;, &#39;language:wuv&#39;, &#39;language:xav&#39;, &#39;language:xed&#39;, &#39;language:xla&#39;, &#39;language:xnn&#39;, &#39;language:xon&#39;, &#39;language:xsi&#39;, &#39;language:xtd&#39;, &#39;language:xtm&#39;, &#39;language:yaa&#39;, &#39;language:yad&#39;, &#39;language:yal&#39;, &#39;language:yap&#39;, &#39;language:yaq&#39;, &#39;language:yby&#39;, &#39;language:ycn&#39;, &#39;language:yka&#39;, &#39;language:yml&#39;, &#39;language:yre&#39;, &#39;language:yuj&#39;, &#39;language:yut&#39;, &#39;language:yuw&#39;, &#39;language:yva&#39;, &#39;language:zaa&#39;, &#39;language:zab&#39;, &#39;language:zac&#39;, &#39;language:zad&#39;, &#39;language:zai&#39;, &#39;language:zaj&#39;, &#39;language:zam&#39;, &#39;language:zao&#39;, &#39;language:zar&#39;, &#39;language:zas&#39;, &#39;language:zat&#39;, &#39;language:zav&#39;, &#39;language:zaw&#39;, &#39;language:zca&#39;, &#39;language:zia&#39;, &#39;language:ziw&#39;, &#39;language:zos&#39;, &#39;language:zpc&#39;, &#39;language:zpl&#39;, &#39;language:zpo&#39;, &#39;language:zpq&#39;, &#39;language:zpu&#39;, &#39;language:zpv&#39;, &#39;language:zpz&#39;, &#39;language:zsr&#39;, &#39;language:ztq&#39;, &#39;language:zty&#39;, &#39;language:zyp&#39;, &#39;language:be&#39;, &#39;language:br&#39;, &#39;language:cs&#39;, &#39;language:ch&#39;, &#39;language:zh&#39;, &#39;language:de&#39;, &#39;language:en&#39;, &#39;language:eo&#39;, &#39;language:fr&#39;, &#39;language:ht&#39;, &#39;language:he&#39;, &#39;language:hr&#39;, &#39;language:id&#39;, &#39;language:it&#39;, &#39;language:ja&#39;, &#39;language:la&#39;, &#39;language:nl&#39;, &#39;language:ru&#39;, &#39;language:sa&#39;, &#39;language:so&#39;, &#39;language:es&#39;, &#39;language:sr&#39;, &#39;language:sv&#39;, &#39;language:to&#39;, &#39;language:uk&#39;, &#39;language:vi&#39;, &#39;license:cc-by-4.0&#39;, &#39;license:other&#39;]] . We can see that in this case many of the tags relate to language. Since the dataset is bible related and the bible has been heavily translated this might not be as surprising. . Although these high-level stats are somewhat interesting we probably want to break these numbers down. At a high level we can groupby datasets vs models. . df.groupby(&quot;type&quot;)[&quot;number_of_tags&quot;].describe() . count mean std min 25% 50% 75% max . type . dataset 19576.0 | 2.46935 | 13.137220 | 0.0 | 0.0 | 0.0 | 2.0 | 650.0 | . model 131767.0 | 4.06151 | 5.327066 | 0.0 | 0.0 | 4.0 | 6.0 | 413.0 | . We can see that the mean number of tags for models is higher than datasets. We can also see at the 75% percentile models also have more tags compared to datasets. The possible reasons for this (and whether this is a problem or not) is something we may wish to explore further... . Since the hub hosts models from different libraries we may want to also breakdown by library. First let&#39;s grab only the model part of our DataFrame. . models_df = df[df[&quot;type&quot;] == &quot;model&quot;] . The library_name column contains info about the library. Let&#39;s see how many unique libraries we have. . models_df.library_name.unique().shape . (63,) . This is quite a few! We can do a groupby on this column . models_df.groupby(&quot;library_name&quot;)[&quot;number_of_tags&quot;].describe() . count mean std min 25% 50% 75% max . library_name . BERT 1.0 | 7.0 | NaN | 7.0 | 7.0 | 7.0 | 7.0 | 7.0 | . Doc-UFCN 2.0 | 4.0 | 0.000000 | 4.0 | 4.0 | 4.0 | 4.0 | 4.0 | . EveryDream 2.0 | 7.0 | 0.000000 | 7.0 | 7.0 | 7.0 | 7.0 | 7.0 | . FastAI 1.0 | 1.0 | NaN | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | . JoeyNMT 1.0 | 4.0 | NaN | 4.0 | 4.0 | 4.0 | 4.0 | 4.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . ultralytics 4.0 | 10.0 | 1.414214 | 8.0 | 9.5 | 10.5 | 11.0 | 11.0 | . ultralyticsplus 1.0 | 9.0 | NaN | 9.0 | 9.0 | 9.0 | 9.0 | 9.0 | . yolor 2.0 | 9.0 | 0.000000 | 9.0 | 9.0 | 9.0 | 9.0 | 9.0 | . yolov5 36.0 | 9.0 | 0.000000 | 9.0 | 9.0 | 9.0 | 9.0 | 9.0 | . yolov6detect 1.0 | 10.0 | NaN | 10.0 | 10.0 | 10.0 | 10.0 | 10.0 | . 62 rows × 8 columns . We might find this a bit tricky to look at. We may want to only include the top n libraries since some of these libraries may be less well used. . models_df.library_name.value_counts()[:15] . transformers 63754 stable-baselines3 3183 diffusers 2802 sentence-transformers 1273 ml-agents 763 keras 470 timm 383 espnet 381 spacy 296 sample-factory 273 adapter-transformers 201 sklearn 113 nemo 103 fastai 99 speechbrain 94 Name: library_name, dtype: int64 . top_libraries = models_df.library_name.value_counts()[:9].index.to_list() . top_libraries_df = models_df[models_df.library_name.isin(top_libraries)] . top_libraries_df.groupby(&quot;library_name&quot;)[&quot;number_of_tags&quot;].describe() . count mean std min 25% 50% 75% max . library_name . diffusers 2802.0 | 4.374732 | 2.171226 | 1.0 | 3.0 | 4.0 | 5.0 | 18.0 | . espnet 381.0 | 6.965879 | 0.595060 | 3.0 | 7.0 | 7.0 | 7.0 | 9.0 | . keras 470.0 | 3.842553 | 14.422674 | 1.0 | 1.0 | 2.0 | 5.0 | 311.0 | . ml-agents 763.0 | 6.965924 | 0.273775 | 2.0 | 7.0 | 7.0 | 7.0 | 7.0 | . sentence-transformers 1273.0 | 6.984289 | 3.221840 | 2.0 | 6.0 | 6.0 | 7.0 | 36.0 | . spacy 296.0 | 4.611486 | 0.985180 | 2.0 | 4.0 | 5.0 | 5.0 | 10.0 | . stable-baselines3 3183.0 | 4.997801 | 0.163426 | 3.0 | 5.0 | 5.0 | 5.0 | 8.0 | . timm 383.0 | 3.548303 | 1.315291 | 2.0 | 3.0 | 3.0 | 3.0 | 13.0 | . transformers 63754.0 | 6.912037 | 5.262633 | 1.0 | 5.0 | 6.0 | 8.0 | 240.0 | . Let&#39;s take a quick look at some examples from the library with the highest and lowest number or tags. . top_libraries_df[top_libraries_df.library_name == &quot;sentence-transformers&quot;].sample(15)[ &quot;tags&quot; ] . 6123 [pytorch, gpt_neo, arxiv:2202.08904, sentence-... 2488 [pytorch, distilbert, sentence-transformers, f... 37669 [pytorch, distilbert, sentence-transformers, f... 71483 [pytorch, bert, sentence-transformers, feature... 20710 [pytorch, tf, roberta, ko, sentence-transforme... 27073 [pytorch, tf, jax, roberta, arxiv:1908.10084, ... 92037 [pytorch, mpnet, sentence-transformers, featur... 90320 [pytorch, mpnet, sentence-transformers, featur... 63555 [pytorch, bert, sentence-transformers, feature... 87707 [pytorch, mpnet, sentence-transformers, featur... 80570 [pytorch, bert, sentence-transformers, feature... 111407 [pytorch, bert, sentence-transformers, feature... 82690 [pytorch, mpnet, sentence-transformers, featur... 36217 [pytorch, bert, pl, dataset:Wikipedia, arxiv:1... 100086 [pytorch, roberta, sentence-transformers, feat... Name: tags, dtype: object . top_libraries_df[top_libraries_df.library_name == &quot;timm&quot;].sample(15)[&quot;tags&quot;] . 104432 [pytorch, timm, image-classification] 110296 [pytorch, arxiv:2301.00808, timm, image-classi... 24158 [pytorch, timm, image-classification] 26471 [pytorch, timm, image-classification] 104437 [pytorch, timm, image-classification] 61630 [pytorch, dataset:beans, timm, image-classific... 110298 [pytorch, arxiv:2301.00808, timm, image-classi... 104015 [pytorch, timm, image-classification] 101124 [pytorch, timm, image-classification] 57882 [coreml, onnx, en, dataset:imagenet-1k, arxiv:... 83459 [pytorch, timm, image-classification, vision, ... 99461 [pytorch, timm, image-classification] 104029 [pytorch, timm, image-classification] 84402 [pytorch, timm, image-classification, vision, ... 104428 [pytorch, timm, image-classification] Name: tags, dtype: object . We can see here that some tags for sentence-transformers are very closely tied to that libraries purpose e.g. the sentence-similarity tag. This tag migth be useful when a user is looking for models to do sentence-similarity but might be less useful if you are trying to choose between models for this task i.e. trying to find the setence-transformer model that will be useful for you. We should be careful, therefore, in treating number of tags as a proxy for quality. . Grouping by pipeline tags . We have a column in our dataframe pipeline tag, which refers to the type of task a model is for. We should be careful relying too much on this but we can have a quick look at how often these are used. . models_df[&quot;pipeline_tag&quot;].value_counts() . text-classification 14479 text2text-generation 8102 text-generation 7602 reinforcement-learning 6885 token-classification 6386 automatic-speech-recognition 6238 fill-mask 5447 question-answering 3147 feature-extraction 2661 translation 1837 conversational 1770 image-classification 1760 text-to-image 1604 sentence-similarity 1248 summarization 735 unconditional-image-generation 428 text-to-speech 244 audio-classification 234 multiple-choice 169 object-detection 158 image-segmentation 134 audio-to-audio 130 tabular-classification 97 zero-shot-classification 97 image-to-text 76 zero-shot-image-classification 56 video-classification 50 table-question-answering 47 tabular-regression 44 image-to-image 43 depth-estimation 37 document-question-answering 18 visual-question-answering 13 voice-activity-detection 6 other 4 time-series-forecasting 1 Name: pipeline_tag, dtype: int64 . We may also want to see if there are some type of task that have more tags. . models_df.groupby(&quot;pipeline_tag&quot;)[&quot;number_of_tags&quot;].mean().sort_values().plot.barh() . &lt;AxesSubplot: ylabel=&#39;pipeline_tag&#39;&gt; . We can also look at the breakdown for a particular task . text_classification_df = models_df[models_df[&quot;pipeline_tag&quot;] == &quot;text-classification&quot;] . text_classification_df[&quot;number_of_tags&quot;].describe() . count 14479.000000 mean 5.948822 std 3.718800 min 1.000000 25% 4.000000 50% 5.000000 75% 7.000000 max 240.000000 Name: number_of_tags, dtype: float64 . Again, we have some extreme outliers . text_classification_df[text_classification_df.number_of_tags &gt; 230][[&quot;tags&quot;, &quot;modelId&quot;]] . tags modelId . 22457 [pytorch, tf, roberta, text-classification, mu... | m3hrdadfi/zabanshenas-roberta-base-mix | . 101628 [pytorch, canine, text-classification, ace, af... | SebOchs/canine-c-lang-id | . We see that these mostly seem to relate to language. Let&#39;s remove these outliers and look at the distribution in the number of tags without these. . text_classification_df_no_outliers = text_classification_df[ text_classification_df[&quot;number_of_tags&quot;] &lt;= text_classification_df[&quot;number_of_tags&quot;].quantile(0.95) ] text_classification_df_no_outliers[&quot;number_of_tags&quot;].plot.hist(bins=9) . &lt;AxesSubplot: ylabel=&#39;Frequency&#39;&gt; . Why counting tags might not make sense . I&#39;e already hinted at why looking at raw number of tags might not be a good idea. Let&#39;s close this blog by briefly digging into at least one reason why. We&#39;ll use the toolz library for some of this analysis. . from toolz import concat . First we grab all the tags and put them in a single list. . all_tags = list(concat(df.tags.tolist())) . If we look at some examples, we&#39;ll see some tags are in the form of something:somethingelse. . all_tags[:10] . [&#39;pytorch&#39;, &#39;tf&#39;, &#39;albert&#39;, &#39;fill-mask&#39;, &#39;en&#39;, &#39;dataset:bookcorpus&#39;, &#39;dataset:wikipedia&#39;, &#39;arxiv:1909.11942&#39;, &#39;transformers&#39;, &#39;exbert&#39;] . for example dataset:wikipedia, we should therefore avoid treating all tags as the same since tags can have a particular purpose. i.e. indicating a dataset is associated with a model. . def is_special_tag(tag: str): return &quot;:&quot; in tag . from toolz import countby, valmap . special_tag_vs_normal = countby(is_special_tag, all_tags) . special_tag_vs_normal . {False: 467758, True: 115755} . total = sum(special_tag_vs_normal.values()) valmap(lambda x: x / total, special_tag_vs_normal) . {False: 0.8016239569641121, True: 0.1983760430358878} . We can see that a good chunk of tags are &#39;special&#39; tags. i.e. they have a &#39;type&#39; associated with them. If we want to explore tags on the hub more carefully we&#39;ll need to take this into account... .",
            "url": "https://danielvanstrien.xyz/huggingface/metadata/2024/08/07/_01_16_hub_api_explore.html",
            "relUrl": "/huggingface/metadata/2024/08/07/_01_16_hub_api_explore.html",
            "date": " • Aug 7, 2024"
        }
        
    
  
    
        ,"post1": {
            "title": "How to load a Hugging Face dataset into Qdrant?",
            "content": "%pip install datasets qdrant-client --q . [notice] A new release of pip available: 22.3.1 -&gt; 23.3.1 [notice] To update, run: pip install --upgrade pip Note: you may need to restart the kernel to use updated packages. . Loading our dataset . For this post we&#39;ll use the Cohere/wikipedia-22-12-simple-embeddings dataset which has already had embeddings generated for it. This dataset was created by Cohere and creates embeddings for millions of Wikipedia articles. See this post for more details. . We&#39;ll use the Hugging Face datasets library to load the dataset. . from datasets import load_dataset dataset = load_dataset(&quot;Cohere/wikipedia-22-12-simple-embeddings&quot;, split=&quot;train&quot;) . /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode=&#39;default&#39;. table = cls._concat_blocks(blocks, axis=0) . Let&#39;s take a quick look at the dataset. . dataset . Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;text&#39;, &#39;url&#39;, &#39;wiki_id&#39;, &#39;views&#39;, &#39;paragraph_id&#39;, &#39;langs&#39;, &#39;emb&#39;], num_rows: 485859 }) . We can see the dataset has a emb column which contains the embeddings for each article. Alongside this we see the title and text for the articles alongside some other metadata. Let&#39;s also take a look at the features of the dataset. . Let&#39;s also take a quick look at the features of the dataset. Hugging Face Dataset objects have a features attribute which contains the features of the dataset. We can see that the emb column is a Sequence of float32 values. We also have some other columns with string values, int32 and float32 values. . dataset.features . {&#39;id&#39;: Value(dtype=&#39;int32&#39;, id=None), &#39;title&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;text&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;url&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;wiki_id&#39;: Value(dtype=&#39;int32&#39;, id=None), &#39;views&#39;: Value(dtype=&#39;float32&#39;, id=None), &#39;paragraph_id&#39;: Value(dtype=&#39;int32&#39;, id=None), &#39;langs&#39;: Value(dtype=&#39;int32&#39;, id=None), &#39;emb&#39;: Sequence(feature=Value(dtype=&#39;float32&#39;, id=None), length=-1, id=None)} . Qdrant has support for a pretty varied range of types. All of these types in our dataset are supported by Qdrant so we don&#39;t need to do any conversion. . Creating a Qdrant collection . We&#39;ll use the Qdrant Python client for this post. This client is really nice since it allows you to create a local collection using pure Python i.e. no need to run a Qdrant server. This is great for testing and development. Once you&#39;re ready to deploy your collection you can use the same client to connect to a remote Qdrant server. . from qdrant_client import QdrantClient . We first create a client, in this case using a local path for our DB. . client = QdrantClient(path=&quot;db&quot;) # Persists changes to disk . Configuring our Qdrant collection . Qdrant is very flexible but we need to let Qdrant now a few things about our collection. These include the name, and a config for the vectors we want to store. This config includes the dimensionality of the vectors and the distance metric we want to use. Let&#39;s first check out the dimensionality of our vectors. . vector_size = len(dataset[0][&#39;emb&#39;]) . We&#39;ll also store our collection in a variable so we can use it later. . collection_name = &quot;cohere_wikipedia&quot; . from qdrant_client.models import Distance, VectorParams client.recreate_collection( collection_name=collection_name, vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE), ) . True . Adding our data to Qdrant . Note this code can be heavily optimized but gives an idea of how easy adding data to Qdrant can be. For many datasets this naive approach will work fine. . The approach we&#39;ll take below is to loop through our dataset and yield each row as a PointStruct. This is a Qdrant object that contains the vector and any other data, referred to as the payload, that we want to store. . from qdrant_client.models import PointStruct . def yield_rows(dataset): for idx, row in enumerate(dataset, start=1): vector = row[&quot;emb&quot;] # grab the vector payload = {k: v for k, v in row.items() if k != &quot;emb&quot;} # grab the rest of the fields without the vector yield PointStruct(id=idx, vector=vector, payload=payload) . For this post we&#39;ll use a smallish subset of the dataset. We&#39;ll use the first 100_000 rows. Big enough to be interesting but small enough to play around with quickly. . sample = dataset.select(range(100_000)) . We&#39;ll use the toolz libraries partition_all function to get batches from our yield_rows function. We&#39;ll use tqdm to show a progress bar. . from toolz import partition_all from tqdm.auto import tqdm . %%time bs = 100 for batch in tqdm(partition_all(bs, yield_rows(sample)), total=len(sample) // bs): client.upsert(collection_name=collection_name, points=list(batch), wait=False) . CPU times: user 30.9 s, sys: 35.7 s, total: 1min 6s Wall time: 1min 19s . On my 2021 MacBook Pro with an M1 chip this takes about 90 seconds to run. As mentioned above this can be heavily optimized but this gives an idea of how easy it is to add data to Qdrant from a Hugging Face dataset. . Searching our Qdrant collection . What can we do with our Qdrant collection? We can use our embeddings to find similar wikipedia articles. Let&#39;s see how we can do that. . First we&#39;ll use the get_collection method to see some information about our collection. . from rich import print print(client.get_collection(collection_name)) . CollectionInfo( status=&lt;CollectionStatus.GREEN: &#39;green&#39;&gt;, optimizer_status=&lt;OptimizersStatusOneOf.OK: &#39;ok&#39;&gt;, vectors_count=100000, indexed_vectors_count=0, points_count=100000, segments_count=1, config=CollectionConfig( params=CollectionParams( vectors=VectorParams( size=768, distance=&lt;Distance.COSINE: &#39;Cosine&#39;&gt;, hnsw_config=None, quantization_config=None, on_disk=None ), shard_number=None, replication_factor=None, write_consistency_factor=None, read_fan_out_factor=None, on_disk_payload=None ), hnsw_config=HnswConfig( m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=None, payload_m=None ), optimizer_config=OptimizersConfig( deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1 ), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None ), payload_schema={} ) . We can see a bunch of information about our collection. Including the vector count, the dimensionality of the vectors and the distance metric we&#39;re using. You&#39;ll see that there are plenty of knobs to turn here to optimize your collection but that&#39;s for another post. . We can use the scroll method to get the first vector from our collection . print(client.scroll(collection_name,limit=1)[0][0]) . Record( id=1, payload={ &#39;id&#39;: 0, &#39;title&#39;: &#39;24-hour clock&#39;, &#39;text&#39;: &#39;The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format.&#39;, &#39;url&#39;: &#39;https://simple.wikipedia.org/wiki?curid=9985&#39;, &#39;wiki_id&#39;: 9985, &#39;views&#39;: 2450.62548828125, &#39;paragraph_id&#39;: 0, &#39;langs&#39;: 30 }, vector=None ) . We can also grab items from the payload for each point. . print(client.scroll(&#39;cohere_wikipedia&#39;,limit=1)[0][0].payload[&#39;text&#39;]) . The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format. . We can see this article is about the 24-hour clock system. Let&#39;s see what other pages are similar to this one. We can optionally get the vector for the query point. . vector = client.scroll(&#39;cohere_wikipedia&#39;,limit=1,with_vectors=True)[0][0].vector . We can use our vector as a query to find similar vectors in our collection. We&#39;ll use the search method to do this. . query_vector = client.scroll(collection_name, limit=1, with_vectors=True)[0][0].vector hits = client.search( collection_name=collection_name, query_vector=query_vector, limit=15, # Return 5 closest points ) . Let&#39;s look at some of the results. We can see that the first result is the same article. The rest also seem to be about time/24 hour clock systems! . for hit in hits: print(f&quot;{hit.payload[&#39;title&#39;]} | {hit.payload[&#39;text&#39;]}&quot;) print(&quot;&quot;) . 24-hour clock | The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format. . . 24-hour clock | However, the US military prefers not to say 24:00 - they do not like to have two names for the same thing, so they always say &quot;23:59&quot;, which is one minute before midnight. . . 24-hour clock | 24-hour clock time is used in computers, military, public safety, and transport. In many Asian, European and Latin American countries people use it to write the time. Many European people use it in speaking. . . 24-hour clock | In railway timetables 24:00 means the &quot;end&quot; of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00. . . 24-hour clock | A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under the 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and ends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called 24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you would say &quot;Tuesday at 24:00&quot; and &quot;Wednesday at 00:00&quot; to mean exactly the same time. . . 12-hour clock | The 12-hour clock is a way of dividing the 24 hours of the day into two sections. The two halves are called ante meridiem (a.m.) and post meridiem (p.m.). . . 12-hour clock | Both names are from Latin, and numbered from 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 and 12. Time from midnight to noon is a.m. and from noon to midnight p.m. The table at right shows how it relates to the 24-hour clock. . . Hour | An hour (abbreviation: h or hr) is a unit of measurement used to measure time. An hour is equal to 60 minutes. 24 hours are equal to one day. Unlike the second, the hour is not a base SI unit. . . Midnight | The time period &quot;00:00 - 00:01&quot; is midnight. On computer clocks, the day changes to the next day the minute(s) after midnight. . . Chinese zodiac | In the old days, China and Japan used a 12-hour system to tell the time of day and night (unlike the 24 hour system used today). The 12 hour system divides the day of 24 hours into 12 hours, each of which has a sign of the zodiac: . . Coordinated Universal Time | Note that UTC uses the 24-hour clock. That means there is no &#39;AM&#39; or &#39;PM&#39;. For example, 4:00PM would be 16:00 or 1600. UTC also does not use daylight savings time - that way the time stays consistent the entire year. . . Midnight | In the world, midnight is the start of one day and the end of the last day. It&#39;s the dividing point between two days. . . Noon | Noon is the time exactly halfway through the day (12.00-12:00 in the 24-hour clock and 12:00 PM-12:00 PM in the 12-hour clock). Midday also means noon, although this also means &quot;around&quot; noon, or very early afternoon. . . Coordinated Universal Time | The standard before was Greenwich Mean Time (GMT). UTC and GMT are almost the same. In fact, there is no practical difference which would be noticed by ordinary people. . . Midnight | In the United States and Canada, digital clocks and computers usually show 12 a.m. right at midnight. However, people have to remember that any time is actually an instant. The &quot;a.m.&quot; shown on clock displays means the 12-hour period after the instant of midnight. So when a clock says &quot;12:00 a.m.&quot;, midnight has already passed and a new day has started. In other words, 11:59 p.m. shows until midnight; at the instant of midnight, it changes to 12:00. At the same time, the p.m. changes to a.m., but a.m. does not mean the instant of midnight which separates p.m. and a.m. . . Conclusion . This post showed how it&#39;s possible to easily convert a Hugging Face dataset into a Qdrant collection. We then showed how we can use this collection to find similar articles. . There is a lot of scope for optimization here. For example, we could use a more efficient way to add data to Qdrant. We could also use a more efficient way to search our collection. It would be very cool to directly have a from_hf_datasets method in the Qdrant Python client that would do all of this for us and include some optimizations! . I hope this post has shown how easy it is to use Qdrant with Hugging Face datasets. If you have any questions or comments please let me know on Twitter. .",
            "url": "https://danielvanstrien.xyz/huggingface/datasets/qdrant/vector-search/2023/11/08/datasets_to_qdrant.html",
            "relUrl": "/huggingface/datasets/qdrant/vector-search/2023/11/08/datasets_to_qdrant.html",
            "date": " • Nov 8, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "How do to groupby for Hugging Face datasets",
            "content": "%pip install datasets polars . Collecting datasets Downloading datasets-2.14.5-py3-none-any.whl (519 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 4.8 MB/s eta 0:00:00 Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5) Requirement already satisfied: pyarrow&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0) Collecting dill&lt;0.3.8,&gt;=0.3.0 (from datasets) Downloading dill-0.3.7-py3-none-any.whl (115 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 11.5 MB/s eta 0:00:00 Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3) Requirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0) Requirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1) Collecting xxhash (from datasets) Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 19.2 MB/s eta 0:00:00 Collecting multiprocess (from datasets) Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 13.6 MB/s eta 0:00:00 Requirement already satisfied: fsspec[http]&lt;2023.9.0,&gt;=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0) Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5) Collecting huggingface-hub&lt;1.0.0,&gt;=0.14.0 (from datasets) Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.8/294.8 kB 26.8 MB/s eta 0:00:00 Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1) Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1) Requirement already satisfied: typing_extensions&gt;=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (23.1.0) Requirement already satisfied: charset-normalizer&lt;4.0,&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (3.2.0) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.0.4) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.9.2) Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.4.0) Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1) Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.14.0-&gt;datasets) (3.12.2) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.4) Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2023.7.22) Requirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2.8.2) Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2023.3.post1) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;datasets) (1.16.0) Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.1 multiprocess-0.70.15 xxhash-3.3.0 . from datasets import load_dataset import polars as pl ds = load_dataset(&quot;argilla/databricks-dolly-15k-curated-en&quot;) . ds[&#39;train&#39;][0] . {&#39;id&#39;: &#39;0&#39;, &#39;category&#39;: &#39;closed_qa&#39;, &#39;original-instruction&#39;: &#39;When did Virgin Australia start operating?&#39;, &#39;original-context&#39;: &#34;Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.[3] It suddenly found itself as a major airline in Australia&#39;s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.[4]&#34;, &#39;original-response&#39;: &#39;Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.&#39;, &#39;new-instruction&#39;: {&#39;user_id&#39;: [None], &#39;value&#39;: [&#39;When did Virgin Australia start operating?&#39;], &#39;status&#39;: [&#39;submitted&#39;]}, &#39;new-context&#39;: {&#39;user_id&#39;: [None], &#39;value&#39;: [&#34;Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia&#39;s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.&#34;], &#39;status&#39;: [&#39;submitted&#39;]}, &#39;new-response&#39;: {&#39;user_id&#39;: [None], &#39;value&#39;: [&#39;Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.&#39;], &#39;status&#39;: [&#39;submitted&#39;]}, &#39;external_id&#39;: None} . df = pl.from_arrow(ds[&#39;train&#39;].data.table) . grouped = df.groupby(&#39;category&#39;).agg(pl.col(&#39;original-instruction&#39;).str.lengths().alias(&#39;length&#39;).mean()) grouped . shape: (8, 2)categorylength . str | f64 | . &quot;creative_writi… | 89.729958 | . &quot;brainstorming&quot; | 60.71267 | . &quot;information_ex… | 74.921958 | . &quot;classification… | 123.231273 | . &quot;open_qa&quot; | 45.715868 | . &quot;closed_qa&quot; | 73.166758 | . &quot;general_qa&quot; | 74.157919 | . &quot;summarization&quot; | 56.284244 | . grouped = grouped.sort(&#39;length&#39;) . import matplotlib.pyplot as plt import matplotlib matplotlib.style.use(&#39;fivethirtyeight&#39;) categories = grouped[&#39;category&#39;] lengths = grouped[&#39;length&#39;] plt.figure(figsize=(10, 6)) plt.bar(categories, lengths) plt.xticks(rotation=45, ha=&#39;right&#39;) plt.ylabel(&#39;Mean Length&#39;) plt.title(&#39;original-instruction length for each category&#39;) plt.tight_layout() plt.savefig(&#39;groupby-fig.png&#39;) plt.show() . &lt;Figure size 640x480 with 0 Axes&gt; .",
            "url": "https://danielvanstrien.xyz/huggingface/datasets/2023/09/18/datasets-groupby.html",
            "relUrl": "/huggingface/datasets/2023/09/18/datasets-groupby.html",
            "date": " • Sep 18, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Dynamically updating a Hugging Face hub organization README",
            "content": "tl;dr we can use the huggingface_hub library to auto generate a model card readme for the BigLAM organization. . What are we aiming to do? . The Hugging Face hub allows organizations to create a README card to describe their organization. . . Whilst you can manually create this there might be some content that would be nice to auto populate. For example, for the BigLAM organization, we&#39;re mainly focused on collecting datasets. Since we have many tasks supported by these datasets we might want to create a list of datasets organized by task. Ideally we don&#39;t want to have to manually update this. Let&#39;s see how we can do this! . First we&#39;ll install the huggingface_hub library which allows us to interact with the hub. We&#39;ll install Jinja2 for templating and toolz because toolz makes Python infinitely more delightful! . %pip install huggingface_hub toolz Jinja2 . Requirement already satisfied: huggingface_hub in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (0.11.1) Requirement already satisfied: toolz in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (0.12.0) Requirement already satisfied: Jinja2 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (3.1.2) Requirement already satisfied: pyyaml&gt;=5.1 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (6.0) Requirement already satisfied: packaging&gt;=20.9 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (23.0) Requirement already satisfied: requests in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (2.28.2) Requirement already satisfied: filelock in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (3.9.0) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (4.4.0) Requirement already satisfied: tqdm in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (4.64.1) Requirement already satisfied: MarkupSafe&gt;=2.0 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from Jinja2) (2.1.1) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (1.26.14) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (3.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (2022.12.7) Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (3.0.1) [notice] A new release of pip available: 22.3.1 -&gt; 23.0.1 [notice] To update, run: pip install --upgrade pip Note: you may need to restart the kernel to use updated packages. . import toolz from huggingface_hub import list_datasets . We list all the datasets under this organization . big_lam_datasets = list(iter(list_datasets(author=&quot;biglam&quot;, limit=None, full=True))) . We want to check which tasks our organization currently has. If we look at an example of one dataset: . big_lam_datasets[0] . DatasetInfo: { id: biglam/illustrated_ads sha: 688e7d96e99cd5730a17a5c55b0964d27a486904 lastModified: 2023-01-18T20:38:15.000Z tags: [&#39;task_categories:image-classification&#39;, &#39;task_ids:multi-class-image-classification&#39;, &#39;annotations_creators:expert-generated&#39;, &#39;size_categories:n&lt;1K&#39;, &#39;license:cc0-1.0&#39;, &#39;lam&#39;, &#39;historic newspapers&#39;] private: False author: biglam description: The Dataset contains images derived from the Newspaper Navigator (news-navigator.labs.loc.gov/), a dataset of images drawn from the Library of Congress Chronicling America collection. citation: @dataset{van_strien_daniel_2021_5838410, author = {van Strien, Daniel}, title = {{19th Century United States Newspaper Advert images with &#39;illustrated&#39; or &#39;non illustrated&#39; labels}}, month = oct, year = 2021, publisher = {Zenodo}, version = {0.0.1}, doi = {10.5281/zenodo.5838410}, url = {https://doi.org/10.5281/zenodo.5838410}} cardData: {&#39;annotations_creators&#39;: [&#39;expert-generated&#39;], &#39;language&#39;: [], &#39;language_creators&#39;: [], &#39;license&#39;: [&#39;cc0-1.0&#39;], &#39;multilinguality&#39;: [], &#39;pretty_name&#39;: &#34;19th Century United States Newspaper Advert images with &#39;illustrated&#39; or &#39;non illustrated&#39; labels&#34;, &#39;size_categories&#39;: [&#39;n&lt;1K&#39;], &#39;source_datasets&#39;: [], &#39;tags&#39;: [&#39;lam&#39;, &#39;historic newspapers&#39;], &#39;task_categories&#39;: [&#39;image-classification&#39;], &#39;task_ids&#39;: [&#39;multi-class-image-classification&#39;]} siblings: [] _id: 62b9bb453b3301c319d5b53e disabled: False gated: False gitalyUid: 4a051da032bb27da0bc286b288384bb3362f56546a387b130121cd279db336e1 likes: 3 downloads: 11 } . We can see the cardData attribute contains an item containing the tasks supported by a dataset . big_lam_datasets[0].cardData[&#39;task_categories&#39;] . [&#39;image-classification&#39;] . def get_task_categories(dataset): try: yield from dataset.cardData[&#39;task_categories&#39;] except KeyError: return None . We can use the toolz.frequencies function to get counts of these tasks in our org. . task_frequencies = toolz.frequencies( toolz.concat(map(get_task_categories, big_lam_datasets)) ) task_frequencies . {&#39;image-classification&#39;: 8, &#39;text-classification&#39;: 6, &#39;image-to-text&#39;: 2, &#39;text-generation&#39;: 7, &#39;object-detection&#39;: 5, &#39;fill-mask&#39;: 2, &#39;text-to-image&#39;: 1, &#39;image-to-image&#39;: 1, &#39;token-classification&#39;: 1} . Since we want to organize by task type, let&#39;s grab the names of all the tasks in the BigLAM organization. . tasks = task_frequencies.keys() tasks . dict_keys([&#39;image-classification&#39;, &#39;text-classification&#39;, &#39;image-to-text&#39;, &#39;text-generation&#39;, &#39;object-detection&#39;, &#39;fill-mask&#39;, &#39;text-to-image&#39;, &#39;image-to-image&#39;, &#39;token-classification&#39;]) . We now want to group together datasets by the task(s) they support. We can use a default dict to create a dictionary where the keys are the task and the values are a list of datasets supporting that task. Note some datasets support multiple tasks so may appear under more than one task key. . from collections import defaultdict . datasets_by_task = defaultdict(list) . for dataset in big_lam_datasets: tasks = get_task_categories(dataset) for task in tasks: datasets_by_task[task].append(dataset) . We now have a dictionary which allows us to get all datasets supporting a task, for example fill-mask . datasets_by_task[&quot;fill-mask&quot;] . [DatasetInfo: { id: biglam/berlin_state_library_ocr sha: a890935d5bd754ddc5b85f56b6f34f6d2bb4abba lastModified: 2022-08-05T09:36:24.000Z tags: [&#39;task_categories:fill-mask&#39;, &#39;task_categories:text-generation&#39;, &#39;task_ids:masked-language-modeling&#39;, &#39;task_ids:language-modeling&#39;, &#39;annotations_creators:machine-generated&#39;, &#39;language_creators:expert-generated&#39;, &#39;multilinguality:multilingual&#39;, &#39;size_categories:1M&lt;n&lt;10M&#39;, &#39;language:de&#39;, &#39;language:nl&#39;, &#39;language:en&#39;, &#39;language:fr&#39;, &#39;language:es&#39;, &#39;license:cc-by-4.0&#39;, &#39;ocr&#39;, &#39;library&#39;] private: False author: biglam description: None citation: None cardData: {&#39;annotations_creators&#39;: [&#39;machine-generated&#39;], &#39;language&#39;: [&#39;de&#39;, &#39;nl&#39;, &#39;en&#39;, &#39;fr&#39;, &#39;es&#39;], &#39;language_creators&#39;: [&#39;expert-generated&#39;], &#39;license&#39;: [&#39;cc-by-4.0&#39;], &#39;multilinguality&#39;: [&#39;multilingual&#39;], &#39;pretty_name&#39;: &#39;Berlin State Library OCR&#39;, &#39;size_categories&#39;: [&#39;1M&lt;n&lt;10M&#39;], &#39;source_datasets&#39;: [], &#39;tags&#39;: [&#39;ocr&#39;, &#39;library&#39;], &#39;task_categories&#39;: [&#39;fill-mask&#39;, &#39;text-generation&#39;], &#39;task_ids&#39;: [&#39;masked-language-modeling&#39;, &#39;language-modeling&#39;]} siblings: [] _id: 62e0431281d9ca6484efac31 disabled: False gated: False gitalyUid: 3818ba9c8b624d79f1fcfb0c79bd197fb5b3a3f9de2452aed5028e8b6435f56a likes: 3 downloads: 5 }, DatasetInfo: { id: biglam/bnl_newspapers1841-1879 sha: 588db6c242ecae417b92830d5646121c15726fea lastModified: 2022-11-15T09:25:43.000Z tags: [&#39;task_categories:text-generation&#39;, &#39;task_categories:fill-mask&#39;, &#39;task_ids:language-modeling&#39;, &#39;task_ids:masked-language-modeling&#39;, &#39;annotations_creators:no-annotation&#39;, &#39;language_creators:expert-generated&#39;, &#39;multilinguality:multilingual&#39;, &#39;size_categories:100K&lt;n&lt;1M&#39;, &#39;source_datasets:original&#39;, &#39;language:de&#39;, &#39;language:fr&#39;, &#39;language:lb&#39;, &#39;language:nl&#39;, &#39;language:la&#39;, &#39;language:en&#39;, &#39;license:cc0-1.0&#39;, &#39;newspapers&#39;, &#39;1800-1900&#39;] private: False author: biglam description: None citation: None cardData: {&#39;annotations_creators&#39;: [&#39;no-annotation&#39;], &#39;language&#39;: [&#39;de&#39;, &#39;fr&#39;, &#39;lb&#39;, &#39;nl&#39;, &#39;la&#39;, &#39;en&#39;], &#39;language_creators&#39;: [&#39;expert-generated&#39;], &#39;license&#39;: [&#39;cc0-1.0&#39;], &#39;multilinguality&#39;: [&#39;multilingual&#39;], &#39;pretty_name&#39;: &#39;BnL Newspapers 1841-1879&#39;, &#39;size_categories&#39;: [&#39;100K&lt;n&lt;1M&#39;], &#39;source_datasets&#39;: [&#39;original&#39;], &#39;tags&#39;: [&#39;newspapers&#39;, &#39;1800-1900&#39;], &#39;task_categories&#39;: [&#39;text-generation&#39;, &#39;fill-mask&#39;], &#39;task_ids&#39;: [&#39;language-modeling&#39;, &#39;masked-language-modeling&#39;]} siblings: [] _id: 6372286ce8891da06b2a5d2f disabled: False gated: False gitalyUid: 039f217af964cfa1317f03d58c367ba6f0e415721b107a298cd4e75cbad50e8b likes: 2 downloads: 3 }] . How can we create a README that dynamically updates . We now have our datasets organized by task. However, at the moment, this is in the form of a Python dictionary. It would be much nicer to render it a more pleasing format. This is where a templating engine can help. In this case we&#39;ll use Jinja. . A templating engine allows us to create a template which can dynamically be updated based on values we pass in. We won&#39;t go in depth to templating engines/Jinja in this blog post because I&#39;m not an expert in templating engines. This Real Python article is a nice introduction to Jinja. . from jinja2 import Environment, FileSystemLoader . We can start by taking a look at our template. Since a lot of the template I created doesn&#39;t update, we&#39;ll use tail to look at the bottom of the template which is dynamically updating. . !tail -n 12 templates/readme.jinja . An overview of datasets currently made available via BigLam organised by task type. {% for task_type, datasets in task_dictionary.items() %} &lt;details&gt; &lt;summary&gt;{{ task_type }}&lt;/summary&gt; {% for dataset in datasets %} - [{{dataset.cardData[&#39;pretty_name&#39;]}}](https://huggingface.co/datasets/biglam/{{ dataset.id }}) {%- endfor %} &lt;/details&gt; {% endfor %} . Even if you aren&#39;t familiar with templating engines, you can probably see roughly what this does. We look through all the keys and values in our dictionary, create a section for that task based on the dictionary key. We next loop through the dictionary values (which in this case is a list) and create a link for that dataset. Since we&#39;re looping through DatasetInfo objects in the list we can grab things like the pretty_name for the dataset and dynamically create a URL link. . We can load this template as follows . environment = Environment(loader=FileSystemLoader(&quot;templates/&quot;)) template = environment.get_template(&quot;readme.jinja&quot;) . Create a context dictionary which we use to pass through our dictionary . context = { &quot;task_dictionary&quot;: datasets_by_task, } . We can now render this and see how it looks . print(template.render(context)) . title: README emoji: 📚 colorFrom: pink colorTo: gray sdk: static pinned: false BigScience 🌸 is an open scientific collaboration of nearly 600 researchers from 50 countries and 250 institutions who collaborate on various projects within the natural language processing (NLP) space to broaden the accessibility of language datasets while working on challenging scientific questions around training language models. BigLAM started as a [datasets hackathon](https://github.com/bigscience-workshop/lam) focused on making data from Libraries, Archives, and Museums (LAMS) with potential machine-learning applications accessible via the Hugging Face Hub. We are continuing to work on making more datasets available via the Hugging Face hub to help make these datasets more discoverable, open them up to new audiences, and help ensure that machine-learning datasets more closely reflect the richness of human culture. ## Dataset Overview An overview of datasets currently made available via BigLam organised by task type. &lt;details&gt; &lt;summary&gt;image-classification&lt;/summary&gt; - [19th Century United States Newspaper Advert images with &#39;illustrated&#39; or &#39;non illustrated&#39; labels](https://huggingface.co/datasets/biglam/biglam/illustrated_ads) - [Brill Iconclass AI Test Set ](https://huggingface.co/datasets/biglam/biglam/brill_iconclass) - [National Library of Scotland Chapbook Illustrations](https://huggingface.co/datasets/biglam/biglam/nls_chapbook_illustrations) - [Encyclopaedia Britannica Illustrated](https://huggingface.co/datasets/biglam/biglam/encyclopaedia_britannica_illustrated) - [V4Design Europeana style dataset](https://huggingface.co/datasets/biglam/biglam/v4design_europeana_style_dataset) - [Early Printed Books Font Detection Dataset](https://huggingface.co/datasets/biglam/biglam/early_printed_books_font_detection) - [Dataset of Pages from Early Printed Books with Multiple Font Groups](https://huggingface.co/datasets/biglam/biglam/early_printed_books_with_multiple_font_groups) - [DEArt: Dataset of European Art](https://huggingface.co/datasets/biglam/biglam/european_art) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;text-classification&lt;/summary&gt; - [Annotated dataset to assess the accuracy of the textual description of cultural heritage records](https://huggingface.co/datasets/biglam/biglam/cultural_heritage_metadata_accuracy) - [Atypical Animacy](https://huggingface.co/datasets/biglam/biglam/atypical_animacy) - [Old Bailey Proceedings](https://huggingface.co/datasets/biglam/biglam/old_bailey_proceedings) - [Lampeter Corpus](https://huggingface.co/datasets/biglam/biglam/lampeter_corpus) - [Hansard Speeches](https://huggingface.co/datasets/biglam/biglam/hansard_speech) - [Contentious Contexts Corpus](https://huggingface.co/datasets/biglam/biglam/contentious_contexts) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;image-to-text&lt;/summary&gt; - [Brill Iconclass AI Test Set ](https://huggingface.co/datasets/biglam/biglam/brill_iconclass) - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;text-generation&lt;/summary&gt; - [Old Bailey Proceedings](https://huggingface.co/datasets/biglam/biglam/old_bailey_proceedings) - [Hansard Speeches](https://huggingface.co/datasets/biglam/biglam/hansard_speech) - [Berlin State Library OCR](https://huggingface.co/datasets/biglam/biglam/berlin_state_library_ocr) - [Literary fictions of Gallica](https://huggingface.co/datasets/biglam/biglam/gallica_literary_fictions) - [Europeana Newspapers ](https://huggingface.co/datasets/biglam/biglam/europeana_newspapers) - [Gutenberg Poetry Corpus](https://huggingface.co/datasets/biglam/biglam/gutenberg-poetry-corpus) - [BnL Newspapers 1841-1879](https://huggingface.co/datasets/biglam/biglam/bnl_newspapers1841-1879) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;object-detection&lt;/summary&gt; - [National Library of Scotland Chapbook Illustrations](https://huggingface.co/datasets/biglam/biglam/nls_chapbook_illustrations) - [YALTAi Tabular Dataset](https://huggingface.co/datasets/biglam/biglam/yalta_ai_tabular_dataset) - [YALTAi Tabular Dataset](https://huggingface.co/datasets/biglam/biglam/yalta_ai_segmonto_manuscript_dataset) - [Beyond Words](https://huggingface.co/datasets/biglam/biglam/loc_beyond_words) - [DEArt: Dataset of European Art](https://huggingface.co/datasets/biglam/biglam/european_art) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;fill-mask&lt;/summary&gt; - [Berlin State Library OCR](https://huggingface.co/datasets/biglam/biglam/berlin_state_library_ocr) - [BnL Newspapers 1841-1879](https://huggingface.co/datasets/biglam/biglam/bnl_newspapers1841-1879) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;text-to-image&lt;/summary&gt; - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;image-to-image&lt;/summary&gt; - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations) &lt;/details&gt; &lt;details&gt; &lt;summary&gt;token-classification&lt;/summary&gt; - [Unsilencing Colonial Archives via Automated Entity Recognition](https://huggingface.co/datasets/biglam/biglam/unsilence_voc) &lt;/details&gt; . with open(&#39;/tmp/README.md&#39;,&#39;w&#39;) as f: f.write(template.render(context)) . Updating the README on the Hugging Face Hub . This looks pretty good! It would be nice to also update the org README without having to manually edit the file. The huggingface_hub library helps us out here once again. Since the organization README is actually a special type of Hugging Face Space, we can interact with it in the same way we could for models or datasets. . from huggingface_hub import HfApi from huggingface_hub import notebook_login . We&#39;ll create a HFApi instance. . api = HfApi() . Since we&#39;re planning to write to a repo we&#39;ll need to login to the hub. . notebook_login() . We can now upload the rendered README file we created above to our biglam/README space. . api.upload_file( path_or_fileobj=&quot;/tmp/readme.md&quot;, path_in_repo=&quot;README.md&quot;, repo_id=&quot;biglam/README&quot;, repo_type=&quot;space&quot;, ) . &#39;https://huggingface.co/spaces/biglam/README/blob/main/README.md&#39; . If we look at our updated README, we&#39;ll see we now have some nice collapsible sections for each task type containing the datasets for that task . . Next steps, whilst this was already quite useful, at the moment we still have to run this code when we want to regenerate our README. Webhooks make it possible to make this fully automated by creating a webhook that monitors any changes to repos under the BigLAM org. Would love to hear from anyone who tries this out! .",
            "url": "https://danielvanstrien.xyz/metadata/huggingface/2023/03/07/readme-template.html",
            "relUrl": "/metadata/huggingface/2023/03/07/readme-template.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
            "content": "Introduction . There are many potential uses of computer vision in GLAM (Galleries, Libraries, Archives and Museums). These uses include: . image similarity search, i.e., given an image, find similar images | text search of images, i.e., given a text string “a picture of a dog eating an ice cream,” return relevant images | page layout recognition, i.e., pull out semantically important parts of a document (articles, photos, titles, etc.) | Optical Character Recognition (OCR) | . All of these use cases require some technical work to implement or use. Usually, they need some programming knowledge too. However, there are many tasks in GLAM where computer vision could be helpful to, requiring less technical work to implement. In particular, many uses of computer vision can be framed as an image classification task (putting images into categories). . Last year, Kaspar Beelen, Melvin Wevers, Thomas Smits, Katherine McDonough, and I shared a two-part Programming Historian lesson, Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification. . This lesson aimed to provide an introduction to how computer vision can be leveraged to work with images ‘at scale’ – in particular for research applications. While we tried hard to make the lesson (particularly part one) accessible, there are still barriers to getting started: . You need some Python knowledge: while we tried to keep the Python code simple (helped massively by the fastai library we use in the lesson), knowing how to code is still required. I couldn’t find a good citation for this, but most estimates for the number of people who know how to program are around 0.5-1% of the global population. Of this percentage, fewer will know Python. | Need to have access to a GPU: whilst you can train deep learning models (the type of machine learning model introduced in our Programming Historian tutorial), it is a lot slower without them. However, setting up access to a GPU can be annoying. While ‘free’ access is possible, this can also come with constraints. | The costs involved in training deep learning models can be hard to predict. You can usually get started for free, but often at some point, you need to invest some money in cloud computing. However, it can be difficult to know before you start training a model(s) how much it will cost. | . Beyond this, there is also a bigger question of how much energy you might want to invest in all of the above stuff involved in getting machine learning set up. This is especially true if you don’t want to become a machine learning engineer and want to do something practical with machine learning. . Training a machine learning model is the boring part . Many machine learning engineers will grimace at the title of this section. However, many use cases of machine learning exist where an existing machine learning architecture will work well. Training a model is not what would benefit most from human intervention. . For novel applications of machine learning or situations where you want to ensure a model is well suited to your domain, you may need to spend time creating training data. After training your model, there is also a step where you need to decide how to integrate machine learning into existing or new workflows. This is partially a technical question but often involves considerations beyond how I set up an API to serve my model. . Hand-training models can eat up a lot of time. Sometimes this time might be warranted but other times you might wish you could make some of this process less hands-on. . Can we approach this in another way? . AutoTrain is a tool that allow us to train machine learning models without needing to use Python, setup compute infrastructure or deal with unpredictable costs for training our models. In the rest of this blog post we’ll go through the steps to using AutoTrain for a semi-realistic computer vision problem. . The dataset . For this project we’ll use a dataset created by the Internet Archive as part of a request for help to judge a book by its cover. The blog post presents a use case for wanting to know if an image of a book cover is ‘useful’ or ‘not useful’. They provide some examples . Useful image example: . Not useful image example: . . Essentially the task is to decide whether an image of a digitized book cover is ‘useful’ or ‘not useful,’ i.e. whether showing this cover to Internet Archive users would give them useful information or not. The Internet Archive shared a dataset along with this blog post which contains examples for each category. . What type of machine learning task is this? . If we look at the dataset shared by the Internet Archive, we have a directory structure that looks like this: . . ├── year-1923-not-very-useful-covers └── year-1923-useful-covers . We have two folders containing images. Each folder contains examples of image belonging to the name of each folder. Essentially, we want a model that learns which image belongs in each folder (based on the examples) and can put new images into the correct folder/category. This is known as an image classification task (as was mentioned in the introduction). The Hugging Face tasks page for this gives an excellent overview: https://huggingface.co/tasks/image-classification . What are the steps involved? . How do we go from the dataset we started with to a trained model that we can begin to explore? For this particular example, the steps are as follows: . Download our data | Prepare our data | choose our autotrain task | Upload our data to autotrain | Train our models | Evaluate our models | . Download our data . This step will depend on where your data is and how it’s arranged, but in this example, we can download the dataset from the Internet Archive. Three folders are provided in this case covering useful/not-useful for 1923 and for the year 2000 useful. Since the types of cover will have changed a fair bit in this time period we’ll just download the folders for 1923. . . Preparing our data . There isn’t much prep we need to do for our data; however, we can provide data to AutoTrain in a few different ways for our image classification task. In this case we’ll use the imagefolder format. This is essentially what we have already (folders containing examples of the labels we’re interested in). We’ll create a top-level directory for our image data cover, which contains two subfolders with our example images. . . Resize our images (optional) . This step isn’t strictly necessary, but it’ll save time when uploading our dataset to AutoTrain. Most machine learning models expect training images to be relatively small (often 224x224 or 512x512 pixels). You can do this from the command line, but most operating systems have inbuilt tools for bulk resizing images, e.g., https://www.makeuseof.com/tag/batch-convert-resize-images-mac/ . Setup AutoTrain . From the projects page, we can create a new project. . . Here we give our project a name and choose a task (image classification). We can also specify for AutoTrain to use a particular model. If you don’t have a solid reason to select a model you can leave this decision to AutoTrain 🤗. . Once you’ve created your project, you’ll need to upload your data. There are different ways of doing this depending on the task. For image classification, we can use pre-arranged folders with a CSV/JSONL file with the labels or upload a dataset hosted on the Hugging Face hub. . . We already have an organized folder so we can upload data. . . Once we’ve uploaded our images, we’ll need to wait for the data to be uploaded. How long this takes depends on your internet speed. We can now click on Go to trainings. . . Here you will see that AutoTrain is formatting your uploaded data. . . Once your data has been prepared, you can decide how many models you want AutoTrain to train for you. This decision depends on how much you want to spend on training your models and where you are in your project. If you are getting started and want to know how well a model may do, you may choose a lower number. If you want the best possible chance of getting the best-performing model, you could choose to train a more significant number of models. . . Once you are ready, you can smash the start model training button!🔥 The nice thing is that AutoTrain will ask you to confirm how much model training will cost. Once your models start training, a screen pops up with some randomly named models. Depending on the size of your dataset, it might take a bit longer to start seeing metrics for your model, but after a little while, you will begin to see scores (in this case, accuracy). . . As the models train, you will see some models overtake others in performance. If you are easily amused like me, you will treat this like a fun spectator sport. . . You also have a metrics overview tab for all the models you have trained. This makes it easy to sort by different metrics. . . Each of these models created by AutoTrain is a ‘real’ model hosted in a model repository on the Hugging Face hub. Some AutoTrain solutions hide away the actual artifacts and only allow you to interact with the models through their API. These models are available in the same way as any other model on the hub. By default, the models are made private, but you can decide to make the models openly available for others to use 🤗. . You’ll also see in the screenshot that the models come with the outlines of a model card. . . Why does our model suck? . For this particular dataset, our models don’t do super well (around 92% accuracy). Why is this? . The importance of training data . Start to dig into the training data examples provided. You’ll see that quite a few images might be reasonably classified as belonging to the other category. In particular, quite a few images of the not-useful folder are similar to those in the useful folder. This is going to make it hard for our model to learn what we’re after. . This also shows the importance of focusing on data and not over-focusing on model training. In this case, fixing our data will likely yield much better results than messing around with how we train the models. Using a tool like AutoTrain can quickly help you spot these issues early on so you can iterate on your training data. . How can we fix this?? . Move images between folders!! . There are better ways, but spending 30 mins removing examples you don’t think the fit will make a big difference to the model performance. At some point, you are likely to want to use a proper annotation tool but to start with; you might be able to get quite far by using your operating systems file browser to re-arrange your training data. . Below is an example from another similar dataset where we get models with 99% accuracy. All of this without writing a line of code! . What can I do with this model? . There are various ways in which you can use the model you’ve created. How you want to use it depends largely on your use case. In a follow-up blog post I’ll suggest a few options for how you can continue on the no/low-code journey to creating and using ML tools customised to your needs and data 🤗. . Show me the models! . You can find the best models shown above here: . https://huggingface.co/davanstrien/autotrain-ia-useful-covers-3665397856 | https://huggingface.co/davanstrien/autotrain-encyclopaedia-illustrations-blog-post-3327992158 | .",
            "url": "https://danielvanstrien.xyz/huggingface/autotrain/2023/02/22/autotrain.html",
            "relUrl": "/huggingface/autotrain/2023/02/22/autotrain.html",
            "date": " • Feb 22, 2023"
        }
        
    
  
    
        ,"post5": {
            "title": "Label Studio x Hugging Face datasets hub",
            "content": "Full stack deep learning: annotating data . I&#39;m currently going through the Full Stack Deep Learning course. As part of this we&#39;ve been going through tools for different parts of the machine learning pipeline. This post talks about data annotation, and how we can combine Label Studio and the Hugging Face Datasets hub. I&#39;ll use the example of annotating image data for an image classification task. The details of why I&#39;m annotating this data will wait for a future post! . note this post assumes you already know roughly what the Hugging Face Hub is. If you don&#39;t this is a nice intro. . What is the goal? . We want to have a way of easily moving from different stages of our machine learning project pipeline. For many projects, especially the weird stuff I&#39;m likely to do, you will need to do some of your own annotating. It almost always makes sense to move quickly between annotating a first batch of data, trying to train a model and iterating. This can help: . flag issues with your data | identify if you have ambiguous labels | help you get some sense of how a model might perform on the task you are working on | allow you to deploy a model early so you can begin iterating on the whole pipeline | ... | . from Imgflip Meme Generator . This approach can cause some challenges; how do you keep updating your annotations, how can you version the changes? . A more mundane challenge . In the full stack deep learning course, one of the labs covered using Label Studio to annotate data. Label studio is a great open source tool for annotating data across a range of domains and for a variety of tasks. . Label studio has great support for annotating image data. One challenge we can face, however, is how to load images into label studio. This can be particularly tricky if you only have the images locally since label studio prefers images to be available via a URL. There are various ways around this but we may also be able to tackle this challenge using the datasets hub. . We&#39;ll start by downloading a dataset we want annotate warning this dataset is pretty big ~44GB uncompressed. . %%bash wget https://nlsfoundry.s3.amazonaws.com/data/nls-data-encyclopaediaBritannica.zip unzip *.zip . We&#39;ll import some standard libraries . import pandas as pd from pathlib import Path . Create a new dataset on the Hub . Since we want to upload our data to the Hugging Face hub we&#39;ll create a new dataset on the Hugging Face Hub via the CLI. . huggingface-cli repo create encyclopaedia_britannica --type dataset . Under the hood, Hugging Face hub datasets (and models) are Git repositories. We&#39;ll clone this repo and move the downloaded dataset into this new Git repository. . git clone https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica mv nls-data-encyclopaediaBritannica encyclopaedia_britannica/ . Since the number of examples in this dataset is beyond what we&#39;re likely to annotate we do a bit of deleting of the dataset. You could also take a sample of the original but in this case I&#39;m happy to reclaim some space on my hardrive! . import shutil from tqdm.auto import tqdm . first we get rid of some alto folders that we don&#39;t need for the dataset we&#39;re aiming to create . for directory in tqdm( list( ( Path(&quot;encyclopaedia_britannica/nls-data-encyclopaediaBritannica&quot;).rglob( &quot;*alto&quot; ) ) ) ): shutil.rmtree(directory) . 100%|██████████| 195/195 [00:34&lt;00:00, 5.62it/s] . there are a few other *.xml files in this dataset we also remove . for xml_file in tqdm( list( ( Path(&quot;encyclopaedia_britannica/nls-data-encyclopaediaBritannica&quot;).rglob( &quot;*xml&quot; ) ) ) ): xml_file.unlink() . 100%|██████████| 195/195 [00:00&lt;00:00, 1464.47it/s] . Let&#39;s take a look at how many images we have now . image_files = list( (Path(&quot;encyclopaedia_britannica/nls-data-encyclopaediaBritannica&quot;).rglob(&quot;*jpg&quot;)) ) . len(image_files) . 155388 . We&#39;re not likely to annotate this many images, let&#39;s aim to have a max of 10,000 images. This is also likely to be more than we&#39;ll annotate but we may use a smaller sample for unsupervised pre-training. . num_to_remove = len(image_files) - 10_000 . We&#39;ll now randomly remove the extra images we don&#39;t need beyond our sample . import random . to_remove = random.sample(image_files, num_to_remove) for file in tqdm(to_remove): file.unlink() . 100%|██████████| 90000/90000 [00:33&lt;00:00, 2659.02it/s] . len( list( ( Path(&quot;encyclopaedia_britannica/nls-data-encyclopaediaBritannica&quot;).rglob( &quot;*jpg&quot; ) ) ) ) . 10000 . Uploading our raw data to the hub . We can now upload this data to the Hugging Face Hub. Under the hood the Hub uses Git so everything you love (and hate) about Git should be familiar. The main difference between using the hub and GitHub or another Git hosting platform is that the Hugging Face hub has support for large files. This means we can more easily work with large files (like our images). . cd encyclopaedia_britannica git lfs track &quot;*.jpg&quot; git add .gitattributes git add nls-data-encyclopaediaBritannica git commit -m &quot;add image files&quot; git push . Loading local files and metadata . The particular dataset we&#39;re working with also has a metadata file associated with it. We can grab all of the images so we can put them in a DataFrame and merge this with metadata about these images. We may not use this extra metadata but it&#39;s nice to have this additional metadata about our items alongside our annotations. This can help us debug where our model is performing badly later on. . image_files = list( Path(&quot;encyclopaedia_britannica/nls-data-encyclopaediaBritannica&quot;).rglob(&quot;*.jpg&quot;) ) . df = pd.DataFrame(image_files, columns=[&quot;filename&quot;]) . This dataset also comes with some metadata. We&#39;ll load that in to another DataFrame . metadata_df = pd.read_csv( &quot;encyclopaedia_britannica/nls-data-encyclopaediaBritannica/encyclopaediaBritannica-inventory.csv&quot;, header=None, names=[&quot;id&quot;, &quot;meta&quot;], dtype={&quot;id&quot;: &quot;int64&quot;}, ) . df[&quot;id&quot;] = df.filename.apply(lambda x: x.parts[-3]).astype(&quot;int64&quot;) . df = df.merge(metadata_df, on=&quot;id&quot;) df . filename id meta . 0 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291 | Encyclopaedia Britannica - Third edition, Volu... | . 1 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291 | Encyclopaedia Britannica - Third edition, Volu... | . 2 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291 | Encyclopaedia Britannica - Third edition, Volu... | . 3 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291 | Encyclopaedia Britannica - Third edition, Volu... | . 4 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291 | Encyclopaedia Britannica - Third edition, Volu... | . ... ... | ... | ... | . 9995 encyclopaedia_britannica/nls-data-encyclopaedi... | 193696083 | Encyclopaedia Britannica - Seventh edition, Vo... | . 9996 encyclopaedia_britannica/nls-data-encyclopaedi... | 193696083 | Encyclopaedia Britannica - Seventh edition, Vo... | . 9997 encyclopaedia_britannica/nls-data-encyclopaedi... | 193696083 | Encyclopaedia Britannica - Seventh edition, Vo... | . 9998 encyclopaedia_britannica/nls-data-encyclopaedi... | 193696083 | Encyclopaedia Britannica - Seventh edition, Vo... | . 9999 encyclopaedia_britannica/nls-data-encyclopaedi... | 193696083 | Encyclopaedia Britannica - Seventh edition, Vo... | . 10000 rows × 3 columns . Annotating using label studio . Now we have our images uploaded to the Hugging Face hub, how we go about annotating? As was mentioned already the Hugging Face hub is essentially a Git repo. Since we uploaded our image files individually i.e. not in a compressed folder, we can access each file from that repo. We mentioned before that label studio can load images from URLs. The hub has an API that we can use to interact with our repository. Let&#39;s see how we can use this to get our data ready for label studio. . from huggingface_hub import list_repo_files, hf_hub_url . files = list_repo_files(&quot;davanstrien/encyclopaedia_britannica&quot;, repo_type=&quot;dataset&quot;) files[:2] . [&#39;.gitattributes&#39;, &#39;nls-data-encyclopaediaBritannica/144133901/image/188082865.3.jpg&#39;] . We&#39;ll filter out some data we are not interested in annotating . files = [file for file in files if not file.startswith(&quot;.&quot;)] len(files) . 10002 . hf_hub_url can be used to generate the URL for a particular file . hf_hub_url( &quot;davanstrien/encyclopaedia_britannica&quot;, &quot;192866824.3.jpg&quot;, subfolder=&quot;sample/nls-data-encyclopaediaBritannica/192547788/image&quot;, repo_type=&quot;dataset&quot;, ) . &#39;https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/sample/nls-data-encyclopaediaBritannica/192547788/image/192866824.3.jpg&#39; . We can use this to grab all of the URLs we&#39;re interested in . urls = [] for file in files: file = Path(file) urls.append( hf_hub_url( &quot;davanstrien/encyclopedia_britannica&quot;, file.name, subfolder=file.parents[0], repo_type=&quot;dataset&quot;, ) ) . We can now load these into a DataFrame, and save this to a CSV file. . pd.DataFrame(urls, columns=[&quot;image&quot;]).to_csv(&quot;data.csv&quot;, index=False) . pd.read_csv(&quot;data.csv&quot;) . image . 0 https://huggingface.co/datasets/davanstrien/en... | . 1 https://huggingface.co/datasets/davanstrien/en... | . 2 https://huggingface.co/datasets/davanstrien/en... | . 3 https://huggingface.co/datasets/davanstrien/en... | . 4 https://huggingface.co/datasets/davanstrien/en... | . ... ... | . 9997 https://huggingface.co/datasets/davanstrien/en... | . 9998 https://huggingface.co/datasets/davanstrien/en... | . 9999 https://huggingface.co/datasets/davanstrien/en... | . 10000 https://huggingface.co/datasets/davanstrien/en... | . 10001 https://huggingface.co/datasets/davanstrien/en... | . 10002 rows × 1 columns . Loading annotations into label studio . We can use this file to load our data into label studio . From here, we need to define our annotation task. We can then begin annotating data. . Export annotations . You can either wait until you&#39;ve finished doing all the labels, however, we may have a lot of data to annotate so it&#39;s likely instead that we will want to export once we&#39;ve either hit a reasonable number of labels or get too bored of annotating. There are various different export formats available in this case we&#39;ll use JSON-Min . . Load annotations . Now we have export our annotations lets load them into a new DatafFame. We&#39;ll only select the columns we&#39;re interested in . annotation_dataframe = pd.read_json(&quot;project-3-at-2022-09-08-15-16-4279e901.json&quot;)[ [&quot;image&quot;, &quot;choice&quot;] ] . annotation_dataframe . image choice . 0 https://huggingface.co/datasets/davanstrien/en... | text-only | . 1 https://huggingface.co/datasets/davanstrien/en... | text-only | . 2 https://huggingface.co/datasets/davanstrien/en... | text-only | . 3 https://huggingface.co/datasets/davanstrien/en... | text-only | . 4 https://huggingface.co/datasets/davanstrien/en... | text-only | . ... ... | ... | . 1516 https://huggingface.co/datasets/davanstrien/en... | text-only | . 1517 https://huggingface.co/datasets/davanstrien/en... | text-only | . 1518 https://huggingface.co/datasets/davanstrien/en... | text-only | . 1519 https://huggingface.co/datasets/davanstrien/en... | text-only | . 1520 https://huggingface.co/datasets/davanstrien/en... | text-only | . 1521 rows × 2 columns . If we take a look at the URL for one of the annotations, you&#39;ll see that we still have a nice path that mirrors the folder structure of the original data. This also means we can merge this annotations DataFrame with our previous metadata DataFrame. . annotation_dataframe.loc[0, &quot;image&quot;] . &#39;https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/nls-data-encyclopaediaBritannica/192693396/image/192979378.3.jpg&#39; . annotation_dataframe.loc[0, &quot;image&quot;].split(&quot;/&quot;)[-4:] . [&#39;nls-data-encyclopaediaBritannica&#39;, &#39;192693396&#39;, &#39;image&#39;, &#39;192979378.3.jpg&#39;] . annotation_dataframe[&quot;filename&quot;] = annotation_dataframe[&quot;image&quot;].apply( lambda x: &quot;/&quot;.join(x.split(&quot;/&quot;)[-4:]) ) . annotation_dataframe[&quot;filename&quot;] = annotation_dataframe[&quot;filename&quot;].astype(str) . df = df.merge(annotation_dataframe, on=&quot;filename&quot;, how=&quot;outer&quot;) . df . filename id meta image choice . 0 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291.0 | Encyclopaedia Britannica - Third edition, Volu... | NaN | NaN | . 1 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291.0 | Encyclopaedia Britannica - Third edition, Volu... | NaN | NaN | . 2 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291.0 | Encyclopaedia Britannica - Third edition, Volu... | NaN | NaN | . 3 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291.0 | Encyclopaedia Britannica - Third edition, Volu... | NaN | NaN | . 4 encyclopaedia_britannica/nls-data-encyclopaedi... | 190273291.0 | Encyclopaedia Britannica - Third edition, Volu... | NaN | NaN | . ... ... | ... | ... | ... | ... | . 11516 nls-data-encyclopaediaBritannica/144133901/ima... | NaN | NaN | https://huggingface.co/datasets/davanstrien/en... | text-only | . 11517 nls-data-encyclopaediaBritannica/144133901/ima... | NaN | NaN | https://huggingface.co/datasets/davanstrien/en... | text-only | . 11518 nls-data-encyclopaediaBritannica/144133901/ima... | NaN | NaN | https://huggingface.co/datasets/davanstrien/en... | text-only | . 11519 nls-data-encyclopaediaBritannica/144133901/ima... | NaN | NaN | https://huggingface.co/datasets/davanstrien/en... | text-only | . 11520 nls-data-encyclopaediaBritannica/144133901/ima... | NaN | NaN | https://huggingface.co/datasets/davanstrien/en... | text-only | . 11521 rows × 5 columns . This means we can keep our nice orignal metadata intact but also add our additional metadata where it exists. Let&#39;s check how many annotations we have . df.choice.value_counts() . text-only 1436 illustrated 70 Name: choice, dtype: int64 . We can also see how much of our dataset we have coverage for . len(df[df.choice.notna()]) / len(df) . 0.13071781963371235 . How to use our annotations? . We now have some annoations inside a DataFrame. What should we do we these? We can also use the Hub for storing this. This comes with a few benefits: . we keep our data and annotations in the same place. | since the Hub uses Git under the hood we also get versioning for our dataset. We can use this version information to track for example how different models perform during training as we add more labels. | . Another nice thing about the Hub is that we can create dataset loading scripts to load our data. This script can use this CSV we&#39;ve just created and only load the data we have examples for. . First we&#39;ll save to a CSV file: . df.to_csv(&quot;annotations.csv&quot;, index=None) . We can then copy these into the same repository used to host our dataset. . cp annotations.csv encyclopedia_britannica/ . Once we&#39;ve done this we can commit these and push our annotations to the hub: . cd encyclopedia_britannica/ git add annotations.csv git commit -m &quot;update annotations&quot; git push . What next? . We now have a repository which contains a bunch of images, and a CSV file which contains annotations for some of these images. How do we use this for model training? From this point we can create a dataset loading script inside the same repository. . This dataset loading script will allow us to load the data from the hub using the datasets library. Additionally we can write this script so that it only loads data we have annotations for. . What does this mean: . we have a dataset we can use to train our model | the dataset is hosted on the Hugging Face hub which means it&#39;s easy to share with other people | we can keep adding new annotations to this dataset and pushing our changes to the hub | Since the datasets library has nice caching support it will only download the dataset if there are changes. This change will be triggered by changes to our annotations.csv file. | . Loading the dataset . Once we have our loading script we can load our annotations using the datasets library: . from datasets import load_dataset import datasets . ds = load_dataset(&#39;davanstrien/encyclopedia_britannica&#39;) . Using custom data configuration default Reusing dataset encyclopedia_britannica (/Users/dvanstrien/.cache/huggingface/datasets/davanstrien___encyclopedia_britannica/default/1.1.0/8dd4d7982f31fd11ed71020b79b4b11a0068c8243080066e43b9fe3980934467) . ds[&#39;train&#39;][0] . {&#39;metadata&#39;: &#39;nan&#39;, &#39;image&#39;: &#39;https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/nls-data-encyclopaediaBritannica/192693396/image/192979378.3.jpg&#39;, &#39;label&#39;: 0} . from datasets import load_dataset from datasets.utils.file_utils import get_datasets_user_agent from functools import partial from concurrent.futures import ThreadPoolExecutor import urllib import io import PIL USER_AGENT = get_datasets_user_agent() def fetch_single_image(image_url, timeout=None, retries=0): for _ in range(retries + 1): try: request = urllib.request.Request( image_url, data=None, headers={&quot;user-agent&quot;: USER_AGENT}, ) with urllib.request.urlopen(request, timeout=timeout) as req: image = PIL.Image.open(io.BytesIO(req.read())) break except Exception: image = None return image def fetch_images(batch, num_threads, timeout=1, retries=0): fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries) with ThreadPoolExecutor(max_workers=num_threads) as executor: batch[&quot;image&quot;] = list(executor.map(fetch_single_image_with_args, batch[&quot;image&quot;])) return batch . num_threads = 16 ds = ds.map(fetch_images, batched=True, batch_size=64, fn_kwargs={&quot;num_threads&quot;: num_threads}, writer_batch_size=64) . Loading cached processed dataset at /Users/dvanstrien/.cache/huggingface/datasets/encyclopaedia_britannica/default/1.1.0/f7fb8d1f26daa72fbaf883bb1707e13d304414c1af16f02c00782c985971f87c/cache-fda9502ac5b20332.arrow . ds = ds.cast_column(&#39;image&#39;, datasets.Image()) . ds[&#39;train&#39;][0][&#39;image&#39;] . Where won&#39;t this work? . This workflow is based on the assumption that the dataset you are annotating is public from the start. This is usually possible for the domain I work in (libraries) but could be a major blocker for other people. This workflow might also break if you have lots of people annotating. There are probably ways around this but things could start becoming a bit hacky... . The loading script for loading this dataset does some slightly strange things to avoid loading images that don&#39;t yet have annotations. I think it would make sense to rework this script if you get to a point you are unlikely to do any more annotations. .",
            "url": "https://danielvanstrien.xyz/huggingface/huggingface-datasets/annotation/full%20stack%20deep%20learning%20notes/2022/09/07/label-studio-annotations-hub.html",
            "relUrl": "/huggingface/huggingface-datasets/annotation/full%20stack%20deep%20learning%20notes/2022/09/07/label-studio-annotations-hub.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Training an object detection model using Hugging Face",
            "content": "Training a Detr object detection model using Hugging Face transformers and datasets . The Hugging Face transformers library has increasingly expanded from its original focus on Natural Language Processing tasks to include more models covering a range of computer vision tasks. This blog post will look at how we can train an object detection model using the Hugging Face transformers and datasets libraries. . What is object detection? . Object detection is the task of predicting objects contained within an image. . . Object detection can be helpful in several applications where you want to know not only whether a thing is in an image but where (and how many) of that thing there are. Various approaches have been developed over the years for this task, often relying on various complex hand-crafted features. . As with other areas of computer vision, there has been an increasing adoption of transformer-based solutions to this task. One model using transformers is the Detr architecture. . What is Detr? . Detr (DEtection TRansformer) is a model architecture introduced in the paper End-to-End Object Detection with Transformers. We won&#39;t dig into the architecture in massive detail in this blog since we&#39;re focused on the practical use of this model architecture in this post. One thing that is important to note here is that DETR still uses a CNN backbone. More recently, other models such as YOLOS use a transformer backbone too. Currently, however, these fully transformer-based approaches show some performance gap over more traditional techniques (because this is deep learning, &#39;traditional&#39; refers to stuff from last year, of course). . Using Hugging Face for object detection . There are existing examples for using the Hugging Face transformers library and datasets with the Trainer class to do image classification. There are also example notebooks showing how to fine-tune a Detr model on custom data. However, I didn&#39;t find examples that use the datasets library and the Trainer class to manage training. Training an object detection model using datasets and the transformers library is what this blog post covers. . Why the datasets library? . You may ask why it is helpful to provide an example of using the datasets library for training an object detection model, i.e. why not use PyTorch for the data loading, which already has many examples for training object detection models? . There are a few reasons why trying to use datasets for this can be helpful. A significant one for me is the close integration between the datasets library and the Hugging Face datasets hub. Loading a dataset from the hugging face hub often involves two lines of code (including the imports). . Quickly loading a dataset and then using the same library to prepare the dataset for training an object detection model removes some friction. This becomes especially helpful when you are iterating on the process of creating training data, training a model, and creating more training data. In this iterative process, the hub can be used for storing models and datasets at each stage. Having a clear provenance of these changes (without relying on additional tools) is also a benefit of this workflow. This is the kind of pipeline hugit is intended to support (in this case, for image classification models). . Scope of this blog post . At the moment, this is mainly intended to give a quick overview of the steps involved. It isn&#39;t intended to be a proper tutorial. If I have time later, I may flesh this out (particularly if other projects I&#39;m working on that use object detection progress further). . Enough talk, let&#39;s get started. First we install required libraries. . %%capture !pip install datasets transformers timm wandb rich[jupyter] . I&#39;m a big fan of the rich library so almost always have this extension loaded. . %load_ext rich . The next couple of lines gets us authenticated with the Hugging Face hub. . !git config --global credential.helper store . from huggingface_hub import notebook_login . notebook_login() . Login successful Your token has been saved to /root/.huggingface/token . We&#39;ll use Weights and Biases for tracking our model training. . import wandb . wandb.login() . %env WANDB_PROJECT=chapbooks %env WANDB_ENTITY=davanstrien . Loading the dataset . In this blog post will use a dataset being added to the Hugging Face datasets hub as part of the BigLAM hackathon. This dataset has a configuration for object detection and image classification, so we&#39;ll need to specify which one we want. Since the dataset doesn&#39;t define train/test/valid splits for us, we&#39;ll grab the training split. I won&#39;t provide a full description of the dataset in this blog post since the dataset is still in the process of being documented. The tl;dr summary is that the dataset includes images of digitized books with bounding boxes for illustrations. . from datasets import load_dataset dataset = load_dataset( &quot;biglam/nls_chapbook_illustrations&quot;, &quot;illustration-detection&quot;, split=&quot;train&quot; ) . Reusing dataset nls_chapbook_illustrations (/Users/dvanstrien/.cache/huggingface/datasets/biglam___nls_chapbook_illustrations/illustration-detection/1.0.0/75f355eb0ba564ef120939a78730eb187a4d3eb682e987ed1f682a5bea5466eb) . Let&#39;s take a look at one example from this dataset to get a sense of how the data looks . dataset[0] . { &#39;image_id&#39;: 4, &#39;image&#39;: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x1080 at 0x7FDD6504FAD0&gt;, &#39;width&#39;: 600, &#39;height&#39;: 1080, &#39;url&#39;: None, &#39;date_captured&#39;: &#39;&#39;, &#39;objects&#39;: [ { &#39;category_id&#39;: 0, &#39;image_id&#39;: &#39;4&#39;, &#39;id&#39;: 1, &#39;area&#39;: 110901, &#39;bbox&#39;: [34.529998779296875, 556.8300170898438, 401.44000244140625, 276.260009765625], &#39;segmentation&#39;: [ [ 34.529998779296875, 556.8300170898438, 435.9700012207031, 556.8300170898438, 435.9700012207031, 833.0900268554688, 34.529998779296875, 833.0900268554688 ] ], &#39;iscrowd&#39;: False } ] } . You will see we hav some metadata for the image, the image itself and the field objects contains the annotations themselves. Looking just at an example of the annotations: . { &quot;category_id&quot;: 0, &quot;image_id&quot;: &quot;4&quot;, &quot;id&quot;: 1, &quot;area&quot;: 110901, &quot;bbox&quot;: [ 34.529998779296875, 556.8300170898438, 401.44000244140625, 276.260009765625, ], &quot;segmentation&quot;: [ [ 34.529998779296875, 556.8300170898438, 435.9700012207031, 556.8300170898438, 435.9700012207031, 833.0900268554688, 34.529998779296875, 833.0900268554688, ] ], &quot;iscrowd&quot;: False, } . We see here, that we again have some metadata for each image. We also have a category_id and a bbox. Some of these fields should look familiar to you if you are familiar with the coco format. This will become relevant later, so don&#39;t worry if these aren&#39;t familiar to you. . One issue we can run into when training object detection models is stray bounding boxes (i.e. ones where the bounding boxes stretch beyond the edge of the image). We can check and remove these quite easily. This is some ugly code/there is probably a better way, but this is a quick check, so I&#39;ll forgive myself. . from tqdm.auto import tqdm . remove_idx = [] for idx, row in tqdm(enumerate(dataset)): objects_ = row[&quot;objects&quot;] for ob in objects_: bbox = ob[&quot;bbox&quot;] negative = [box for box in bbox if box &lt; 0] if negative: remove_idx.append(idx) . len(remove_idx) . 1 . keep = [i for i in range(len(dataset)) if i not in remove_idx] len(keep) . 7257 . The above code has given us a list of indexes to keep so we use the select method to grab those. . dataset = dataset.select(keep) . We also create a test split. If we were properly doing this we&#39;d likely want to be a bit more thoughfull about how to do this split. . dataset = dataset.train_test_split(0.1) . Preparing the data . This section of the blog post is the bit which focuses on getting data ready for an object detection model such as detr via the datasets library. This is, therefore, also the section which will differ most from the other examples showing how to train models using PyTorch data loaders. . The Feature Extractor . If you are familiar with Hugging Face for natural language tasks, you are probably familiar with using Tokenizer_for_blah_model when pre-processing text. Often if you are using a pre-trained model, you will use AutoTokenizer.from_pretrained, passing in the ID to the model you want to fine-tune. This tokenizer then ensures that the tokenization matches the approach used for the pre-trained model. . The Feature Extractor performs a similar task. Let&#39;s look at this more closely. We&#39;ll use a pre-trained model for this example and fine-tune it. I also include commented-out code, which shows how you could use the same process with any CNN backbone. This may be useful if you have particular requirements about what backbone to use or if you have a CNN backbone that is already fine-tuned on your domain. . model_checkpoint = &quot;facebook/detr-resnet-50&quot; . from transformers import DetrFeatureExtractor feature_extractor = DetrFeatureExtractor.from_pretrained(model_checkpoint) . If you wanted to use a different CNN backbone as your starting point you would instead define a config. . # from transformers import DetrConfig # from transformers import DetrFeatureExtractor # feature_extractor = DetrFeatureExtractor() . What does the feature extractor do? . To check what feature extractor does we can make use of the handy inspect function . from rich import inspect . inspect(feature_extractor, methods=True, dunder=True) . . ╭─ DetrFeatureExtractor { &quot;do_normalize&quot;: true, &quot;do_resize&quot;: true, &quot;feature_extractor_type&quot;: &quot;DetrFeatureEx─╮ │ def (images: Union[PIL.Image.Image, numpy.ndarray, ForwardRef(&#39;torch.Tensor&#39;), List[PIL.Image.Image], │ │ List[numpy.ndarray], List[ForwardRef(&#39;torch.Tensor&#39;)]], annotations: Union[List[Dict], List[List[Dict]]] = │ │ None, return_segmentation_masks: Union[bool, NoneType] = False, masks_path: Union[pathlib.Path, NoneType] = │ │ None, pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors: Union[str, │ │ transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt; │ │ transformers.feature_extraction_utils.BatchFeature: │ │ │ │ Constructs a DETR feature extractor. │ │ │ │ _auto_class = None │ │ __dict__ = { │ │ &#39;_processor_class&#39;: None, │ │ &#39;feature_extractor_type&#39;: &#39;DetrFeatureExtractor&#39;, │ │ &#39;format&#39;: &#39;coco_detection&#39;, │ │ &#39;do_resize&#39;: True, │ │ &#39;size&#39;: 800, │ │ &#39;max_size&#39;: 1333, │ │ &#39;do_normalize&#39;: True, │ │ &#39;image_mean&#39;: [0.485, 0.456, 0.406], │ │ &#39;image_std&#39;: [0.229, 0.224, 0.225] │ │ } │ │ do_normalize = True │ │ do_resize = True │ │ __doc__ = &#39; n Constructs a DETR feature extractor. n n This feature extractor inherits │ │ from [`FeatureExtractionMixin`] which contains most of the main methods. Users n │ │ should refer to this superclass for more information regarding those │ │ methods. n n n Args: n format (`str`, *optional*, defaults to │ │ `&quot;coco_detection&quot;`): n Data format of the annotations. One of │ │ &quot;coco_detection&quot; or &quot;coco_panoptic&quot;. n do_resize (`bool`, *optional*, │ │ defaults to `True`): n Whether to resize the input to a certain │ │ `size`. n size (`int`, *optional*, defaults to 800): n Resize │ │ the input to the given size. Only has an effect if `do_resize` is set to `True`. │ │ If size is a n sequence like `(width, height)`, output size will be │ │ matched to this. If size is an int, smaller edge of n the image will be │ │ matched to this number. i.e, if `height &gt; width`, then image will be rescaled to │ │ `(size * n height / width, size)`. n max_size (`int`, │ │ *optional*, defaults to `1333`): n The largest size an image dimension │ │ can have (otherwise it &#39;s capped). Only has an effect if `do_resize` is n │ │ set to `True`. n do_normalize (`bool`, *optional*, defaults to `True`): n │ │ Whether or not to normalize the input with mean and standard deviation. n │ │ image_mean (`int`, *optional*, defaults to `[0.485, 0.456, 0.406]`): n │ │ The sequence of means for each channel, to be used when normalizing images. │ │ Defaults to the ImageNet mean. n image_std (`int`, *optional*, defaults to │ │ `[0.229, 0.224, 0.225]`): n The sequence of standard deviations for │ │ each channel, to be used when normalizing images. Defaults to the n │ │ ImageNet std. n &#39; │ │ feature_extractor_type = &#39;DetrFeatureExtractor&#39; │ │ format = &#39;coco_detection&#39; │ │ image_mean = [0.485, 0.456, 0.406] │ │ image_std = [0.229, 0.224, 0.225] │ │ max_size = 1333 │ │ model_input_names = [&#39;pixel_values&#39;, &#39;pixel_mask&#39;] │ │ __module__ = &#39;transformers.models.detr.feature_extraction_detr&#39; │ │ _processor_class = None │ │ size = 800 │ │ __weakref__ = None │ │ __call__ = def __call__(images: Union[PIL.Image.Image, numpy.ndarray, │ │ ForwardRef(&#39;torch.Tensor&#39;), List[PIL.Image.Image], List[numpy.ndarray], │ │ List[ForwardRef(&#39;torch.Tensor&#39;)]], annotations: Union[List[Dict], │ │ List[List[Dict]]] = None, return_segmentation_masks: Union[bool, NoneType] = │ │ False, masks_path: Union[pathlib.Path, NoneType] = None, │ │ pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors: │ │ Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt; │ │ transformers.feature_extraction_utils.BatchFeature: │ │ Main method to prepare for the model one or several image(s) and optional │ │ annotations. Images are by default │ │ padded up to the largest image in a batch, and a pixel mask is created that │ │ indicates which pixels are │ │ real/which are padding. │ │ center_crop = def center_crop(image, size): │ │ Crops `image` to the given size using a center crop. Note that if the image is too │ │ small to be cropped to the │ │ size given, it will be padded (so the returned result has the size asked). │ │ __class__ = class __class__(format=&#39;coco_detection&#39;, do_resize=True, size=800, max_size=1333, │ │ do_normalize=True, image_mean=None, image_std=None, **kwargs): Constructs a DETR │ │ feature extractor. │ │ convert_coco_poly_to_mask = def convert_coco_poly_to_mask(segmentations, height, width): │ │ convert_rgb = def convert_rgb(image): Converts `PIL.Image.Image` to RGB format. │ │ _create_or_get_repo = def _create_or_get_repo(repo_path_or_name: Union[str, NoneType] = None, repo_url: │ │ Union[str, NoneType] = None, organization: Union[str, NoneType] = None, private: │ │ bool = None, use_auth_token: Union[bool, str, NoneType] = None) -&gt; │ │ huggingface_hub.repository.Repository: │ │ __delattr__ = def __delattr__(name, /): Implement delattr(self, name). │ │ __dir__ = def __dir__(): Default dir() implementation. │ │ _ensure_format_supported = def _ensure_format_supported(image): │ │ __eq__ = def __eq__(value, /): Return self==value. │ │ expand_dims = def expand_dims(image): Expands 2-dimensional `image` to 3 dimensions. │ │ __format__ = def __format__(format_spec, /): Default object formatter. │ │ from_dict = def from_dict(feature_extractor_dict: Dict[str, Any], **kwargs) -&gt; │ │ ForwardRef(&#39;SequenceFeatureExtractor&#39;): │ │ Instantiates a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a │ │ Python dictionary of │ │ parameters. │ │ from_json_file = def from_json_file(json_file: Union[str, os.PathLike]) -&gt; │ │ ForwardRef(&#39;SequenceFeatureExtractor&#39;): │ │ Instantiates a feature extractor of type │ │ [`~feature_extraction_utils.FeatureExtractionMixin`] from the path to │ │ a JSON file of parameters. │ │ from_pretrained = def from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], │ │ **kwargs) -&gt; ForwardRef(&#39;SequenceFeatureExtractor&#39;): │ │ Instantiate a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a │ │ feature extractor, *e.g.* a │ │ derived class of [`SequenceFeatureExtractor`]. │ │ __ge__ = def __ge__(value, /): Return self&gt;=value. │ │ get_feature_extractor_dict = def get_feature_extractor_dict(pretrained_model_name_or_path: Union[str, │ │ os.PathLike], **kwargs) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]: │ │ From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to │ │ be used for instantiating a │ │ feature extractor of type [`~feature_extraction_utils.FeatureExtractionMixin`] │ │ using `from_dict`. │ │ _get_repo_url_from_name = def _get_repo_url_from_name(repo_name: str, organization: Union[str, NoneType] = │ │ None, private: bool = None, use_auth_token: Union[bool, str, NoneType] = None) -&gt; │ │ str: │ │ __getattribute__ = def __getattribute__(name, /): Return getattr(self, name). │ │ __gt__ = def __gt__(value, /): Return self&gt;value. │ │ __hash__ = def __hash__(): Return hash(self). │ │ __init__ = def __init__(format=&#39;coco_detection&#39;, do_resize=True, size=800, max_size=1333, │ │ do_normalize=True, image_mean=None, image_std=None, **kwargs): Set elements of │ │ `kwargs` as attributes. │ │ __init_subclass__ = def __init_subclass__(...) This method is called when a class is subclassed. │ │ _is_valid_format = def _is_valid_format(format): │ │ __le__ = def __le__(value, /): Return self&lt;=value. │ │ __lt__ = def __lt__(value, /): Return self&lt;value. │ │ _max_by_axis = def _max_by_axis(the_list): │ │ __ne__ = def __ne__(value, /): Return self!=value. │ │ __new__ = def __new__(*args, **kwargs): Create and return a new object. See help(type) for │ │ accurate signature. │ │ _normalize = def _normalize(image, mean, std, target=None): Normalize the image with a certain │ │ mean and std. │ │ normalize = def normalize(image, mean, std): │ │ Normalizes `image` with `mean` and `std`. Note that this will trigger a conversion │ │ of `image` to a NumPy array │ │ if it&#39;s a PIL Image. │ │ pad_and_create_pixel_mask = def pad_and_create_pixel_mask(pixel_values_list: List[ForwardRef(&#39;torch.Tensor&#39;)], │ │ return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = │ │ None): Pad images up to the largest image in a batch and create a corresponding │ │ `pixel_mask`. │ │ post_process = def post_process(outputs, target_sizes): │ │ Converts the output of [`DetrForObjectDetection`] into the format expected by the │ │ COCO api. Only supports │ │ PyTorch. │ │ post_process_instance = def post_process_instance(results, outputs, orig_target_sizes, max_target_sizes, │ │ threshold=0.5): │ │ Converts the output of [`DetrForSegmentation`] into actual instance segmentation │ │ predictions. Only supports │ │ PyTorch. │ │ post_process_panoptic = def post_process_panoptic(outputs, processed_sizes, target_sizes=None, │ │ is_thing_map=None, threshold=0.85): Converts the output of [`DetrForSegmentation`] │ │ into actual panoptic predictions. Only supports PyTorch. │ │ post_process_segmentation = def post_process_segmentation(outputs, target_sizes, threshold=0.9, │ │ mask_threshold=0.5): Converts the output of [`DetrForSegmentation`] into image │ │ segmentation predictions. Only supports PyTorch. │ │ prepare = def prepare(image, target, return_segmentation_masks=False, masks_path=None): │ │ prepare_coco_detection = def prepare_coco_detection(image, target, return_segmentation_masks=False): │ │ Convert the target in COCO format into the format expected by DETR. │ │ prepare_coco_panoptic = def prepare_coco_panoptic(image, target, masks_path, return_masks=True): │ │ _push_to_hub = def _push_to_hub(repo: huggingface_hub.repository.Repository, commit_message: │ │ Union[str, NoneType] = None) -&gt; str: │ │ push_to_hub = def push_to_hub(repo_path_or_name: Union[str, NoneType] = None, repo_url: │ │ Union[str, NoneType] = None, use_temp_dir: bool = False, commit_message: │ │ Union[str, NoneType] = None, organization: Union[str, NoneType] = None, private: │ │ Union[bool, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None, │ │ **model_card_kwargs) -&gt; str: │ │ Upload the feature extractor file to the 🤗 Model Hub while synchronizing a local │ │ clone of the repo in │ │ `repo_path_or_name`. │ │ __reduce__ = def __reduce__(): Helper for pickle. │ │ __reduce_ex__ = def __reduce_ex__(protocol, /): Helper for pickle. │ │ register_for_auto_class = def register_for_auto_class(auto_class=&#39;AutoFeatureExtractor&#39;): │ │ Register this class with a given auto class. This should only be used for custom │ │ feature extractors as the ones │ │ in the library are already mapped with `AutoFeatureExtractor`. │ │ __repr__ = def __repr__(): Return repr(self). │ │ _resize = def _resize(image, size, target=None, max_size=None): │ │ Resize the image to the given size. Size can be min_size (scalar) or (w, h) tuple. │ │ If size is an int, smaller │ │ edge of the image will be matched to this number. │ │ resize = def resize(image, size, resample=2, default_to_square=True, max_size=None): │ │ Resizes `image`. Enforces conversion of input to PIL.Image. │ │ save_pretrained = def save_pretrained(save_directory: Union[str, os.PathLike], push_to_hub: bool = │ │ False, **kwargs): │ │ Save a feature_extractor object to the directory `save_directory`, so that it can │ │ be re-loaded using the │ │ [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`] class method. │ │ _set_processor_class = def _set_processor_class(processor_class: str): Sets processor class as an │ │ attribute. │ │ __setattr__ = def __setattr__(name, value, /): Implement setattr(self, name, value). │ │ __sizeof__ = def __sizeof__(): Size of object in memory, in bytes. │ │ __str__ = def __str__(): Return str(self). │ │ __subclasshook__ = def __subclasshook__(...) Abstract classes can override this to customize │ │ issubclass(). │ │ to_dict = def to_dict() -&gt; Dict[str, Any]: Serializes this instance to a Python dictionary. │ │ to_json_file = def to_json_file(json_file_path: Union[str, os.PathLike]): Save this instance to a │ │ JSON file. │ │ to_json_string = def to_json_string() -&gt; str: Serializes this instance to a JSON string. │ │ to_numpy_array = def to_numpy_array(image, rescale=None, channel_first=True): │ │ Converts `image` to a numpy array. Optionally rescales it and puts the channel │ │ dimension as the first │ │ dimension. │ │ to_pil_image = def to_pil_image(image, rescale=None): │ │ Converts `image` to a PIL Image. Optionally rescales it and puts the channel │ │ dimension back as the last axis if │ │ needed. │ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ . The output of inspect can be pretty verbose, but I often find this a handy tool for quickly trying to work out a new library of API. . We’ll look at the most critical parts in more detail, but I’ll point out a few things; you’ll see some attributes that will probably sound familiar. . image_mean = [0.485, 0.456, 0.406] image_std = [0.229, 0.224, 0.225] . These are the mean and standard deviation used during the model training. It’s essential when we’re doing inference or fine-tuning to replicate these, and having these all stored inside a feature_extractor means we don’t have to go poking around in papers to try and work out what these values should be. . Another thing to point out is the push_to_hub method. We can store feature_extractors in the hub just as we can store models and tokenizers. Having to track the appropriate pre-processing steps for an image manually is super annoying to do manually. Storing this as we do other model components is much simpler and helps avoid errors resulting from tracing these things by hand. . The __call__ method for the DetrFeatureExtractor is what we&#39;ll use to prepare our images before we pass them into the model, let&#39;s dig more closely into this. . inspect( feature_extractor.__call__, ) . ╭─ &lt;bound method DetrFeatureExtractor.__call__ of DetrFeatureExtractor { &quot;do_normalize&quot;: true, &quot;do_resize&quot;: t─╮ │ def DetrFeatureExtractor.__call__(images: Union[PIL.Image.Image, numpy.ndarray, ForwardRef(&#39;torch.Tensor&#39;), │ │ List[PIL.Image.Image], List[numpy.ndarray], List[ForwardRef(&#39;torch.Tensor&#39;)]], annotations: Union[List[Dict], │ │ List[List[Dict]]] = None, return_segmentation_masks: Union[bool, NoneType] = False, masks_path: │ │ Union[pathlib.Path, NoneType] = None, pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors: │ │ Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt; │ │ transformers.feature_extraction_utils.BatchFeature: │ │ │ │ Main method to prepare for the model one or several image(s) and optional annotations. Images are by default │ │ padded up to the largest image in a batch, and a pixel mask is created that indicates which pixels are │ │ real/which are padding. │ │ │ │ 27 attribute(s) not shown. Run inspect(inspect) for options. │ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ . Understanding what the __call__ method expected, and how to make sure that is what&#39;s delivered by the datasets library is the key thing I needed to work out. What does it expect: . images: this can be a list or a single image (and stored in different formats) | annotations this should be of type Union[List[Dict],││ List[List[Dict]]]. | . The images part is not too tricky to understand. We can either pass in a single image, a NumPy array representing an image or a list of images or NumPy arrays. . The annotations part is where Python type annotations don&#39;t always do us many favours since we only know we&#39;re expecting a list of dictionaries, but we can safely assume those dictionaries probably need to have a particular format. We can try and see what happens if we pass in an image and a list of a random dictionary. . import io import requests from PIL import Image . im = Image.open( io.BytesIO( requests.get( &quot;https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/cute-cat-photos-1593441022.jpg?crop=1.00xw:0.749xh;0,0.154xh&amp;resize=980:*&quot; ).content ) ) im . labels = [ { &quot;bbox&quot;: [ 0.0, 3, 3, 4, ] } ] . feature_extractor(im, labels) . ValueError Traceback (most recent call last) Input In [23], in &lt;cell line: 1&gt;() -&gt; 1 feature_extractor(im, labels) File /usr/local/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/models/detr/feature_extraction_detr.py:524, in DetrFeatureExtractor.__call__(self, images, annotations, return_segmentation_masks, masks_path, pad_and_return_pixel_mask, return_tensors, **kwargs) 521 valid_annotations = True 523 if not valid_annotations: --&gt; 524 raise ValueError( 525 &#34;&#34;&#34; 526 Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object 527 detection, each dictionary should contain the keys &#39;image_id&#39; and &#39;annotations&#39;, with the latter 528 being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary 529 should contain the keys &#39;file_name&#39;, &#39;image_id&#39; and &#39;segments_info&#39;, with the latter being a list 530 of annotations in COCO format. 531 &#34;&#34;&#34; 532 ) 534 # Check that masks_path has a valid type 535 if masks_path is not None: ValueError: Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object detection, each dictionary should contain the keys &#39;image_id&#39; and &#39;annotations&#39;, with the latter being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary should contain the keys &#39;file_name&#39;, &#39;image_id&#39; and &#39;segments_info&#39;, with the latter being a list of annotations in COCO format. . We can see that this raises a ValueError. We also get some more information here that gives us a clue where we went wrong. Specifically we can see that the annotations for a single image should be a Dict or List[Dict] if we&#39;re using a batch of images. We also see that we should pass in this data in the COCO format. Since our data is already in this format we should be able to pass in an example. . image = dataset[&quot;train&quot;][0][&quot;image&quot;] image . annotations = dataset[&quot;train&quot;][0][&quot;objects&quot;] annotations . [ { &#39;category_id&#39;: 0, &#39;image_id&#39;: &#39;8081&#39;, &#39;id&#39;: 646, &#39;area&#39;: 114552, &#39;bbox&#39;: [81.0, 408.0, 387.0, 296.0], &#39;segmentation&#39;: [[81.0, 408.0, 468.0, 408.0, 468.0, 704.0, 81.0, 704.0]], &#39;iscrowd&#39;: False } ] . feature_extractor(images=image, annotations=annotations, return_tensors=&quot;pt&quot;) . ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮ │ /var/folders/tj/54sfzlyj6_573fn82y996grc0000gr/T/ipykernel_47936/1569761185.py:1 in &lt;cell line: │ │ 1&gt; │ │ │ │ [Errno 2] No such file or directory: │ │ &#39;/var/folders/tj/54sfzlyj6_573fn82y996grc0000gr/T/ipykernel_47936/1569761185.py&#39; │ │ │ │ /usr/local/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/models/det │ │ r/feature_extraction_detr.py:524 in __call__ │ │ │ │ 521 │ │ │ │ │ │ │ │ valid_annotations = True │ │ 522 │ │ │ │ │ 523 │ │ │ if not valid_annotations: │ │ ❱ 524 │ │ │ │ raise ValueError( │ │ 525 │ │ │ │ │ &quot;&quot;&quot; │ │ 526 │ │ │ │ │ Annotations must of type `Dict` (single image) or `List[Dict]` (batc │ │ 527 │ │ │ │ │ detection, each dictionary should contain the keys &#39;image_id&#39; and &#39;a │ ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯ ValueError: Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object detection, each dictionary should contain the keys &#39;image_id&#39; and &#39;annotations&#39;, with the latter being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary should contain the keys &#39;file_name&#39;, &#39;image_id&#39; and &#39;segments_info&#39;, with the latter being a list of annotations in COCO format. . Oh no! It still doesn&#39;t work. At this point, it&#39;s we probably either want to dig into the source code to work out what we should be passing to the feature_extractor. The relevant function is def prepare_coco_detection. . We also have another tutorial.ipynb) to consult. In this tutorial we see that the annotations are stored in a dictionary target with the keys image_id and annotations. . target = {&#39;image_id&#39;: image_id, &#39;annotations&#39;: target} encoding = self.feature_extractor(images=img, annotations=target, return_tensors=&quot;pt&quot;) . With a bit more wrangling let&#39;s see if this works. . target = {&quot;image_id&quot;: 4, &quot;annotations&quot;: annotations} . feature_extractor(images=image, annotations=target, return_tensors=&quot;pt&quot;) . { &#39;pixel_values&#39;: tensor([[[[-0.7650, -0.9705, -0.9705, ..., -1.4158, -1.3815, -1.3815], [-0.7822, -0.9020, -0.9020, ..., -1.3815, -1.3644, -1.4500], [-0.8164, -0.9020, -0.9192, ..., -1.3815, -1.3815, -1.4500], ..., [ 1.5297, 1.5639, 1.5810, ..., 1.4612, 1.4612, 1.4783], [ 1.5125, 1.5297, 1.5468, ..., 1.4783, 1.4783, 1.4954], [ 1.4954, 1.5125, 1.5125, ..., 1.5125, 1.5125, 1.5297]], [[-0.6527, -0.8627, -0.8627, ..., -1.3179, -1.2829, -1.2829], [-0.7052, -0.8102, -0.8277, ..., -1.3004, -1.2829, -1.3704], [-0.7402, -0.8102, -0.8277, ..., -1.3529, -1.3529, -1.4405], ..., [ 1.5357, 1.5707, 1.5882, ..., 1.3957, 1.3957, 1.4132], [ 1.5182, 1.5357, 1.5532, ..., 1.4132, 1.4132, 1.4307], [ 1.5007, 1.5182, 1.5182, ..., 1.4482, 1.4482, 1.4657]], [[-0.4275, -0.6367, -0.6367, ..., -1.1073, -1.0898, -1.0898], [-0.4624, -0.5670, -0.5844, ..., -1.1247, -1.1247, -1.2119], [-0.5147, -0.6018, -0.6193, ..., -1.2293, -1.2467, -1.3339], ..., [ 1.4548, 1.4897, 1.5071, ..., 1.3154, 1.3154, 1.3328], [ 1.4374, 1.4548, 1.4722, ..., 1.3328, 1.3328, 1.3502], [ 1.4200, 1.4374, 1.4374, ..., 1.3677, 1.3677, 1.3851]]]]), &#39;pixel_mask&#39;: tensor([[[1, 1, 1, ..., 1, 1, 1], [1, 1, 1, ..., 1, 1, 1], [1, 1, 1, ..., 1, 1, 1], ..., [1, 1, 1, ..., 1, 1, 1], [1, 1, 1, ..., 1, 1, 1], [1, 1, 1, ..., 1, 1, 1]]]), &#39;labels&#39;: [ { &#39;boxes&#39;: tensor([[0.4575, 0.5120, 0.6450, 0.2726]]), &#39;class_labels&#39;: tensor([0]), &#39;image_id&#39;: tensor([4]), &#39;area&#39;: tensor([172346.9688]), &#39;iscrowd&#39;: tensor([0]), &#39;orig_size&#39;: tensor([1086, 600]), &#39;size&#39;: tensor([1332, 736]) } ] } . This is looking more like it! Now we have one example working we can translate this to a function that can prepare a batch into the same format. . Since we get a batch at a time we might need to refactor things slightly. In this example I&#39;ve just grabbed the relevant lists for the images, image_id and annotations. We then use a list compression to store these in the dictionary format expected by the feature_extractor. . def transform(example_batch): images = example_batch[&quot;image&quot;] ids_ = example_batch[&quot;image_id&quot;] objects = example_batch[&quot;objects&quot;] targets = [ {&quot;image_id&quot;: id_, &quot;annotations&quot;: object_} for id_, object_ in zip(ids_, objects) ] return feature_extractor(images=images, annotations=targets, return_tensors=&quot;pt&quot;) . We could apply this to our data using map but it often makes more sense to applay these on the fly using the with_transform method. . dataset[&quot;train&quot;] = dataset[&quot;train&quot;].with_transform(transform) . Let&#39;s take a look at an example . dataset[&quot;train&quot;][0] . { &#39;pixel_values&#39;: tensor([[[-0.7650, -0.9705, -0.9705, ..., -1.4158, -1.3815, -1.3815], [-0.7822, -0.9020, -0.9020, ..., -1.3815, -1.3644, -1.4500], [-0.8164, -0.9020, -0.9192, ..., -1.3815, -1.3815, -1.4500], ..., [ 1.5297, 1.5639, 1.5810, ..., 1.4612, 1.4612, 1.4783], [ 1.5125, 1.5297, 1.5468, ..., 1.4783, 1.4783, 1.4954], [ 1.4954, 1.5125, 1.5125, ..., 1.5125, 1.5125, 1.5297]], [[-0.6527, -0.8627, -0.8627, ..., -1.3179, -1.2829, -1.2829], [-0.7052, -0.8102, -0.8277, ..., -1.3004, -1.2829, -1.3704], [-0.7402, -0.8102, -0.8277, ..., -1.3529, -1.3529, -1.4405], ..., [ 1.5357, 1.5707, 1.5882, ..., 1.3957, 1.3957, 1.4132], [ 1.5182, 1.5357, 1.5532, ..., 1.4132, 1.4132, 1.4307], [ 1.5007, 1.5182, 1.5182, ..., 1.4482, 1.4482, 1.4657]], [[-0.4275, -0.6367, -0.6367, ..., -1.1073, -1.0898, -1.0898], [-0.4624, -0.5670, -0.5844, ..., -1.1247, -1.1247, -1.2119], [-0.5147, -0.6018, -0.6193, ..., -1.2293, -1.2467, -1.3339], ..., [ 1.4548, 1.4897, 1.5071, ..., 1.3154, 1.3154, 1.3328], [ 1.4374, 1.4548, 1.4722, ..., 1.3328, 1.3328, 1.3502], [ 1.4200, 1.4374, 1.4374, ..., 1.3677, 1.3677, 1.3851]]]), &#39;pixel_mask&#39;: tensor([[1, 1, 1, ..., 1, 1, 1], [1, 1, 1, ..., 1, 1, 1], [1, 1, 1, ..., 1, 1, 1], ..., [1, 1, 1, ..., 1, 1, 1], [1, 1, 1, ..., 1, 1, 1], [1, 1, 1, ..., 1, 1, 1]]), &#39;labels&#39;: { &#39;boxes&#39;: tensor([[0.4575, 0.5120, 0.6450, 0.2726]]), &#39;class_labels&#39;: tensor([0]), &#39;image_id&#39;: tensor([8081]), &#39;area&#39;: tensor([172346.9688]), &#39;iscrowd&#39;: tensor([0]), &#39;orig_size&#39;: tensor([1086, 600]), &#39;size&#39;: tensor([1332, 736]) } } . The next thing we need to take care of is a collate function. &#39;Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.&#39; source. . def collate_fn(batch): pixel_values = [item[&quot;pixel_values&quot;] for item in batch] encoding = feature_extractor.pad_and_create_pixel_mask( pixel_values, return_tensors=&quot;pt&quot; ) labels = [item[&quot;labels&quot;] for item in batch] batch = {} # collated batch batch[&#39;pixel_values&#39;] = encoding[&#39;pixel_values&#39;] batch[&quot;pixel_mask&quot;] = encoding[&quot;pixel_mask&quot;] batch[&quot;labels&quot;] = labels return batch . Creating a detr model . Avoiding ambiguous labels . We&#39;re almost at the point where we can start training the model. We just do a little bit of housekeeping to make sure our model knows what our encoded labels are. It&#39;s super annoying when you are trying a model out on the Hugging Face Hub and you get back labels, 0 or 3 with no clue what these labels refer to. We can avoid this by telling our model what labels we have. This mapping will then be bundled with the model when we push it to the hub. . id2label = dict(enumerate(dataset[&quot;train&quot;].features[&quot;objects&quot;][0][&quot;category_id&quot;].names)) label2id = {v: k for k, v in id2label.items()} label2id . {&#39;early_printed_illustration&#39;: 0} . Now we can create the DetrForObjectDetection model. This should all look familiar if you&#39;ve used transformers for other tasks. . from transformers import DetrForObjectDetection model = DetrForObjectDetection.from_pretrained( model_checkpoint, num_labels=1, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True, ) . Downloading: &#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50_a1_0-14fe96d1.pth Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match: - class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated - class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([2]) in the model instantiated You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . If you wanted to use another backbone you could do something like: . # from transformers import DetrForObjectDetection # config = DetrConfig(backbone=&#39;regnetz_e8&#39;,id2label=id2label, label2id=label2id) # model = DetrForObjectDetection(config) . Training the detr model . We now specify our TrainingArguments . from transformers import TrainingArguments training_args = TrainingArguments( output_dir=&quot;detr-resnet-50_fine_tuned_nls_chapbooks&quot;, per_device_train_batch_size=8, num_train_epochs=10, fp16=False, save_steps=200, logging_steps=50, learning_rate=1e-4, save_total_limit=2, remove_unused_columns=False, push_to_hub=True, hub_model_id=&quot;davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks&quot;, ) . and create our Trainer . from transformers import Trainer trainer = Trainer( model=model, args=training_args, data_collator=collate_fn, train_dataset=dataset[&quot;train&quot;], tokenizer=feature_extractor, ) . Cloning https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks into local empty directory. . trainer.train() . . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 6531 Num Epochs = 10 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 8170 Automatic Weights &amp; Biases logging enabled, to disable set os.environ[&#34;WANDB_DISABLED&#34;] = &#34;true&#34; . Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to the W&amp;B docs. wandb: Currently logged in as: davanstrien (flyswot). Use `wandb login --relogin` to force relogin . Tracking run with wandb version 0.12.21 Run data is saved locally in /content/wandb/run-20220724_144321-sgzdksxm Syncing run detr-resnet-50_fine_tuned_nls_chapbooks to Weights &amp; Biases (docs) . [8170/8170 5:15:38, Epoch 10/10] Step Training Loss . 50 | 1.975700 | . 100 | 3.254000 | . 150 | 1.563300 | . 200 | 1.103100 | . 250 | 1.468000 | . 300 | 1.169700 | . 350 | 1.326600 | . 400 | 1.413800 | . 450 | 1.101600 | . 500 | 1.054500 | . 550 | 0.946000 | . 600 | 0.871600 | . 650 | 0.723600 | . 700 | 0.866800 | . 750 | 0.740400 | . 800 | 0.753300 | . 850 | 0.748900 | . 900 | 0.919600 | . 950 | 0.805800 | . 1000 | 0.902200 | . 1050 | 0.788800 | . 1100 | 0.734400 | . 1150 | 0.635700 | . 1200 | 0.769000 | . 1250 | 0.673000 | . 1300 | 0.766200 | . 1350 | 0.664800 | . 1400 | 0.653700 | . 1450 | 0.589500 | . 1500 | 0.580900 | . 1550 | 0.583200 | . 1600 | 0.736000 | . 1650 | 0.594900 | . 1700 | 0.701400 | . 1750 | 0.600300 | . 1800 | 0.470900 | . 1850 | 0.522800 | . 1900 | 0.590300 | . 1950 | 0.566300 | . 2000 | 0.586800 | . 2050 | 0.623800 | . 2100 | 0.523400 | . 2150 | 0.562500 | . 2200 | 0.604100 | . 2250 | 0.518000 | . 2300 | 0.525100 | . 2350 | 0.499100 | . 2400 | 0.564900 | . 2450 | 0.455100 | . 2500 | 0.465000 | . 2550 | 0.533200 | . 2600 | 0.512500 | . 2650 | 0.465100 | . 2700 | 0.521800 | . 2750 | 0.519500 | . 2800 | 0.456800 | . 2850 | 0.444400 | . 2900 | 0.429600 | . 2950 | 0.445400 | . 3000 | 0.425600 | . 3050 | 0.439600 | . 3100 | 0.468000 | . 3150 | 0.426500 | . 3200 | 0.433500 | . 3250 | 0.479400 | . 3300 | 0.421800 | . 3350 | 0.449500 | . 3400 | 0.399300 | . 3450 | 0.424500 | . 3500 | 0.447700 | . 3550 | 0.428900 | . 3600 | 0.403800 | . 3650 | 0.448300 | . 3700 | 0.424300 | . 3750 | 0.396600 | . 3800 | 0.405900 | . 3850 | 0.436300 | . 3900 | 0.371500 | . 3950 | 0.412300 | . 4000 | 0.389200 | . 4050 | 0.391900 | . 4100 | 0.403200 | . 4150 | 0.386800 | . 4200 | 0.382500 | . 4250 | 0.402000 | . 4300 | 0.374400 | . 4350 | 0.355900 | . 4400 | 0.390800 | . 4450 | 0.402600 | . 4500 | 0.397100 | . 4550 | 0.399700 | . 4600 | 0.363900 | . 4650 | 0.373600 | . 4700 | 0.391600 | . 4750 | 0.339200 | . 4800 | 0.351900 | . 4850 | 0.381800 | . 4900 | 0.381800 | . 4950 | 0.326000 | . 5000 | 0.388300 | . 5050 | 0.359100 | . 5100 | 0.380500 | . 5150 | 0.357100 | . 5200 | 0.389500 | . 5250 | 0.386200 | . 5300 | 0.373000 | . 5350 | 0.340000 | . 5400 | 0.337100 | . 5450 | 0.357500 | . 5500 | 0.338900 | . 5550 | 0.334500 | . 5600 | 0.362000 | . 5650 | 0.426100 | . 5700 | 0.329500 | . 5750 | 0.321500 | . 5800 | 0.328800 | . 5850 | 0.322400 | . 5900 | 0.385900 | . 5950 | 0.373800 | . 6000 | 0.326000 | . 6050 | 0.335200 | . 6100 | 0.341600 | . 6150 | 0.309600 | . 6200 | 0.295700 | . 6250 | 0.338600 | . 6300 | 0.326800 | . 6350 | 0.305600 | . 6400 | 0.287200 | . 6450 | 0.307700 | . 6500 | 0.297000 | . 6550 | 0.296700 | . 6600 | 0.292700 | . 6650 | 0.305300 | . 6700 | 0.289300 | . 6750 | 0.290600 | . 6800 | 0.277100 | . 6850 | 0.296500 | . 6900 | 0.291800 | . 6950 | 0.285800 | . 7000 | 0.291400 | . 7050 | 0.282500 | . 7100 | 0.271500 | . 7150 | 0.278300 | . 7200 | 0.272100 | . 7250 | 0.273800 | . 7300 | 0.313500 | . 7350 | 0.288600 | . 7400 | 0.258700 | . 7450 | 0.275700 | . 7500 | 0.248200 | . 7550 | 0.280800 | . 7600 | 0.268500 | . 7650 | 0.258700 | . 7700 | 0.302800 | . 7750 | 0.288700 | . 7800 | 0.278400 | . 7850 | 0.260700 | . 7900 | 0.271200 | . 7950 | 0.247900 | . 8000 | 0.234700 | . 8050 | 0.263900 | . 8100 | 0.251900 | . 8150 | 0.246900 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/preprocessor_config.json Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/preprocessor_config.json Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/preprocessor_config.json Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400] due to args.save_total_limit Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000 Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/preprocessor_config.json Deleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600] due to args.save_total_limit Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput( global_step=8170, training_loss=0.5031144771902768, metrics={ &#39;train_runtime&#39;: 18954.8014, &#39;train_samples_per_second&#39;: 3.446, &#39;train_steps_per_second&#39;: 0.431, &#39;total_flos&#39;: 5.190134584879058e+19, &#39;train_loss&#39;: 0.5031144771902768, &#39;epoch&#39;: 10.0 } ) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Once we&#39;ve finished training we can use push_to_hub to share our model on the hub. . trainer.push_to_hub(&quot;finished training&quot;) . Saving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks Configuration saved in detr-resnet-50_fine_tuned_nls_chapbooks/config.json Model weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/pytorch_model.bin Feature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/preprocessor_config.json Several commits (2) will be pushed upstream. The progress bars may be unreliable. To https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks c94bb78..5c7b9d8 main -&gt; main Dropping the following result as it does not have all the necessary fields: {&#39;task&#39;: {&#39;name&#39;: &#39;Object Detection&#39;, &#39;type&#39;: &#39;object-detection&#39;}, &#39;dataset&#39;: {&#39;name&#39;: &#39;nls_chapbook_illustrations&#39;, &#39;type&#39;: &#39;nls_chapbook_illustrations&#39;, &#39;args&#39;: &#39;illustration_detection&#39;}} To https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks 5c7b9d8..2ece586 main -&gt; main . &#39;https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks/commit/5c7b9d8907981c7ee0005334a3d96d5a8d623957&#39; . Conclusion . We&#39;ve seen how we can use the datasets library to perform object detection. The main thing we need to work out when we&#39;re trying to use datasets for a computer vision task in the transfortmer libraryy is how to ensure we can create a transform that gets the images and annotations into a format understood by the relevant feature_extractor. Once we&#39;ve done this for one example we need to replicate the same thing with a batch of examples so we can use it as a transform. . One this is in place many of the same approaches to defining the model and training arguments should look very familiar. I didn&#39;t spend much time on the training process in this post. I&#39;ll dig into tha in a fguure post as well as covering the process of using/evaluating the model. . &lt;/div&gt; .",
            "url": "https://danielvanstrien.xyz/huggingface/huggingface-datasets/transformers/2022/08/16/detr-object-detection.html",
            "relUrl": "/huggingface/huggingface-datasets/transformers/2022/08/16/detr-object-detection.html",
            "date": " • Aug 16, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Searching for machine learning models using semantic search",
            "content": "The Hugging Face model hub has (at the time of the last checking) 60,509 models publicly available. Some of these models are useful as base models for further fine-tuning; these include your classics like bert-base-uncased. . The hub also has more obscure indie hits that might already do a good job on your desired downstream task or be a closer start. For example, if one wanted to classify the genre of 18th Century books, it might make sense to start with a model for classifying 19th Century books. . Finding candidate models . Ideally, we&#39;d like a quick way to identify if a model might already do close to what we want. From there, we would likely want to review a bunch of other info about the model before deciding if it might be helpful for us or not. . Unfortunately, finding suitable models on the hub isn&#39;t always that easy. Even knowing that models for genre classification exist on the hub, we don&#39;t find any results. . . It&#39;s not documented exactly how the search on the hub works, but it seems to be based mainly on the model&#39;s name rather than the README or other information. In this blog post, I will continue some previous experiments with embeddings to see if there might be different ways in which we could identify potential models. . This will be a very rough experiment and is more about establishing whether this is an avenue worth exploring rather than a fully fleshed-out approach. . First install some libraries we&#39;ll use: . import torch . deps = [&quot;datasets&quot; ,&quot;sentence-transformers&quot;, &quot;rich[&#39;jupyter&#39;]&quot;, &quot;requests&quot;] if torch.cuda.is_available(): deps.append(&quot;faiss-gpu&quot;) else: deps.append(&quot;faise-cpu&quot;) . %%capture !pip install {&quot; &quot;.join(deps)} --upgrade . !git config --global credential.helper store . These days I almost always have the rich extension loaded! . %load_ext rich . Using the huggingface_hub API to download some model metadata . Our goal is to see if we might be able to find suitable models more efficiently using some form of semantic search (i.e. using embeddings). To do this, we should grab some model data from the hub. The easiest way to do this is using the hub API. . from huggingface_hub import hf_api import re from rich import print . api = hf_api.HfApi() . api . &lt;huggingface_hub.hf_api.HfApi object at 0x7f63832ff810&gt; . We can take a look at some example models . all_models = api.list_models() all_models[:3] . [ ModelInfo: { modelId: hfl/chinese-macbert-base sha: None lastModified: None tags: [] pipeline_tag: fill-mask siblings: None private: False author: None config: None id: hfl/chinese-macbert-base }, ModelInfo: { modelId: bert-base-uncased sha: None lastModified: None tags: [] pipeline_tag: fill-mask siblings: None private: False author: None config: None id: bert-base-uncased }, ModelInfo: { modelId: microsoft/deberta-base sha: None lastModified: None tags: [] pipeline_tag: None siblings: None private: False author: None config: None id: microsoft/deberta-base } ] . For a particular model we can also see what files there are. . files = api.list_repo_files(all_models[0].modelId) . files . [ &#39;.gitattributes&#39;, &#39;README.md&#39;, &#39;added_tokens.json&#39;, &#39;config.json&#39;, &#39;flax_model.msgpack&#39;, &#39;pytorch_model.bin&#39;, &#39;special_tokens_map.json&#39;, &#39;tf_model.h5&#39;, &#39;tokenizer.json&#39;, &#39;tokenizer_config.json&#39;, &#39;vocab.txt&#39; ] . Filtering . To limit the scope of this blog post, we&#39;ll focus only on Pytorch models and &#39;text classification&#39; models. The metadata about the model type is likely usually pretty reliable. The model task metadata, on the other hand, is not always reliable in my experience. This means we probably have some models that aren&#39;t text-classification models and don&#39;t include some actual text classification models in our dataset. For now, we won&#39;t worry too much about this. . from huggingface_hub import ModelSearchArguments . model_args = ModelSearchArguments() . from huggingface_hub import ModelFilter model_filter = ModelFilter( task=model_args.pipeline_tag.TextClassification, library=model_args.library.PyTorch ) api.list_models(filter=model_filter)[0] . ModelInfo: { modelId: distilbert-base-uncased-finetuned-sst-2-english sha: 00c3f1ef306e837efb641eaca05d24d161d9513c lastModified: 2022-07-22T08:00:55.000Z tags: [&#39;pytorch&#39;, &#39;tf&#39;, &#39;rust&#39;, &#39;distilbert&#39;, &#39;text-classification&#39;, &#39;en&#39;, &#39;dataset:sst2&#39;, &#39;dataset:glue&#39;, &#39;transformers&#39;, &#39;license:apache-2.0&#39;, &#39;model-index&#39;] pipeline_tag: text-classification siblings: [RepoFile(rfilename=&#39;.gitattributes&#39;), RepoFile(rfilename=&#39;README.md&#39;), RepoFile(rfilename=&#39;config.json&#39;), RepoFile(rfilename=&#39;map.jpeg&#39;), RepoFile(rfilename=&#39;pytorch_model.bin&#39;), RepoFile(rfilename=&#39;rust_model.ot&#39;), RepoFile(rfilename=&#39;tf_model.h5&#39;), RepoFile(rfilename=&#39;tokenizer_config.json&#39;), RepoFile(rfilename=&#39;vocab.txt&#39;)] private: False author: None config: None id: distilbert-base-uncased-finetuned-sst-2-english downloads: 5185721 likes: 76 library_name: transformers } . Now we have a filter we&#39;ll use that to grab all the models that match this filter. . all_models = api.list_models(filter=model_filter) . all_models[0] . ModelInfo: { modelId: distilbert-base-uncased-finetuned-sst-2-english sha: 00c3f1ef306e837efb641eaca05d24d161d9513c lastModified: 2022-07-22T08:00:55.000Z tags: [&#39;pytorch&#39;, &#39;tf&#39;, &#39;rust&#39;, &#39;distilbert&#39;, &#39;text-classification&#39;, &#39;en&#39;, &#39;dataset:sst2&#39;, &#39;dataset:glue&#39;, &#39;transformers&#39;, &#39;license:apache-2.0&#39;, &#39;model-index&#39;] pipeline_tag: text-classification siblings: [RepoFile(rfilename=&#39;.gitattributes&#39;), RepoFile(rfilename=&#39;README.md&#39;), RepoFile(rfilename=&#39;config.json&#39;), RepoFile(rfilename=&#39;map.jpeg&#39;), RepoFile(rfilename=&#39;pytorch_model.bin&#39;), RepoFile(rfilename=&#39;rust_model.ot&#39;), RepoFile(rfilename=&#39;tf_model.h5&#39;), RepoFile(rfilename=&#39;tokenizer_config.json&#39;), RepoFile(rfilename=&#39;vocab.txt&#39;)] private: False author: None config: None id: distilbert-base-uncased-finetuned-sst-2-english downloads: 5185721 likes: 76 library_name: transformers } . Let&#39;s see how many models that gives us. . len(all_models) . 6860 . Later on, in this blog, we&#39;ll want to work with the config.json files (we&#39;ll get back to why later!), so we&#39;ll quickly check that all our models have this. . def has_config(model): has_config = False files = model.siblings for file in files: if &quot;config.json&quot; in file.rfilename: has_config = True return has_config else: continue . has_config(all_models[0]) . True . has_config = [model for model in all_models if has_config(model)] . Let&#39;s check how many we have now . len(has_config) . 6858 . We can also download a particular file from the hub . from huggingface_hub import hf_hub_download file = hf_hub_download(repo_id=all_models[0].modelId, filename=&quot;config.json&quot;) . file . &#39;/root/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/00c3f1ef306e837efb641eaca05d24d161d9513c/config.json&#39; . import json with open(file) as f: data = json.load(f) . data . { &#39;activation&#39;: &#39;gelu&#39;, &#39;architectures&#39;: [&#39;DistilBertForSequenceClassification&#39;], &#39;attention_dropout&#39;: 0.1, &#39;dim&#39;: 768, &#39;dropout&#39;: 0.1, &#39;finetuning_task&#39;: &#39;sst-2&#39;, &#39;hidden_dim&#39;: 3072, &#39;id2label&#39;: {&#39;0&#39;: &#39;NEGATIVE&#39;, &#39;1&#39;: &#39;POSITIVE&#39;}, &#39;initializer_range&#39;: 0.02, &#39;label2id&#39;: {&#39;NEGATIVE&#39;: 0, &#39;POSITIVE&#39;: 1}, &#39;max_position_embeddings&#39;: 512, &#39;model_type&#39;: &#39;distilbert&#39;, &#39;n_heads&#39;: 12, &#39;n_layers&#39;: 6, &#39;output_past&#39;: True, &#39;pad_token_id&#39;: 0, &#39;qa_dropout&#39;: 0.1, &#39;seq_classif_dropout&#39;: 0.2, &#39;sinusoidal_pos_embds&#39;: False, &#39;tie_weights_&#39;: True, &#39;vocab_size&#39;: 30522 } . We can also check if the model has a README.md . def has_file_in_repo(model,file_name): has_file = False files = model.siblings for file in files: if file_name in file.rfilename: has_file = True return has_file else: continue . has_file_in_repo(has_config[0],&#39;README.md&#39;) . True . has_readme = [model for model in has_config if has_file_in_repo(model,&quot;README.md&quot;)] . We can see that there are more configs than READMEs . len(has_readme) . 3482 . len(has_config) . 6858 . We now write some functions to grab both the README.md and config.json files from the hub. . from requests.exceptions import JSONDecodeError import concurrent.futures . @lru_cache(maxsize=None) def get_model_labels(model): try: url = hf_hub_url(repo_id=model.modelId, filename=&quot;config.json&quot;) return model.modelId, list(requests.get(url).json()[&#39;label2id&#39;].keys()) except (KeyError, JSONDecodeError, AttributeError): return model.modelId, None . get_model_labels(has_config[0]) . (&#39;distilbert-base-uncased-finetuned-sst-2-english&#39;, [&#39;NEGATIVE&#39;, &#39;POSITIVE&#39;]) . def get_model_readme(model): url = hf_hub_url(repo_id=model.modelId, filename=&quot;README.md&quot;) return requests.get(url).text . def get_data(model): readme = get_model_readme(model) _, labels = get_model_labels(model) return model.modelId, labels, readme . Since this takes a little while we make a progress bar and do this using multiple threads . from tqdm.auto import tqdm . with tqdm(total=len(has_config)) as progress: with concurrent.futures.ThreadPoolExecutor() as e: tasks = [] for model in has_config: future = e.submit(get_data, model) future.add_done_callback(lambda p: progress.update()) tasks.append(future) results = [task.result() for task in tasks] . Load our data using Pandas. . import pandas as pd . df = pd.DataFrame(results,columns=[&#39;modelId&#39;,&#39;label&#39;,&#39;readme&#39;]) . df . modelId label readme . 0 distilbert-base-uncased-finetuned-sst-2-english | [NEGATIVE, POSITIVE] | nlanguage: en nlicense: apache-2.0 ndatase... | . 1 cross-encoder/ms-marco-MiniLM-L-12-v2 | [LABEL_0] | nlicense: apache-2.0 n n# Cross-Encoder... | . 2 cardiffnlp/twitter-xlm-roberta-base-sentiment | [Negative, Neutral, Positive] | nlanguage: multilingual nwidget: n- text: ... | . 3 facebook/bart-large-mnli | [contradiction, entailment, neutral] | nlicense: mit nthumbnail: https://huggingf... | . 4 ProsusAI/finbert | [positive, negative, neutral] | nlanguage: &quot;en&quot; ntags: n- financial-sentim... | . ... ... | ... | ... | . 6845 jinwooChoi/SKKU_AP_SA_KBT6 | [LABEL_0, LABEL_1, LABEL_2] | Entry not found | . 6846 jinwooChoi/SKKU_AP_SA_KBT7 | [LABEL_0, LABEL_1, LABEL_2] | Entry not found | . 6847 naem1023/electra-phrase-clause-classification-... | None | Entry not found | . 6848 naem1023/electra-phrase-clause-classification-... | None | nlicense: apache-2.0 n n | . 6849 YYAH/Bert_Roman_Urdu | [LABEL_0, LABEL_1, LABEL_2, LABEL_3] | nlicense: unknown n n | . 6850 rows × 3 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; You can see we now have a DataFrame containing the modelID, the model labels and the README.md for each model (where it exists). . Since the README.md (the model card) is the obvious source of information about a model we&#39;ll start here. One question we may have is how long our the README.md is. Some models have very detailed model cards whilst others have very little information in the model card. We can get a bit of a sense of this by looking at the range of README.md lenghts: . df[&#39;readme&#39;].apply(len).describe() . count 6850.000000 mean 1009.164818 std 1750.509155 min 0.000000 25% 15.000000 50% 20.500000 75% 1736.000000 max 56172.000000 Name: readme, dtype: float64 . We might want to filter on the length of the README so we&#39;ll store that info in a new column. . df[&#39;readme_len&#39;] = df[&#39;readme&#39;].apply(len) . Since we might want to work with this data again, let&#39;s load it into a datasets Dataset and use push_to_hub to store a copy. . from datasets import Dataset . ds = Dataset.from_pandas(df) ds . Dataset({ features: [&#39;modelId&#39;, &#39;label&#39;, &#39;readme&#39;, &#39;readme_len&#39;], num_rows: 6850 }) . from huggingface_hub import notebook_login . notebook_login() . Login successful Your token has been saved to /root/.huggingface/token . ds.push_to_hub(&#39;davanstrien/hf_model_metadata&#39;) . /usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:1951: FutureWarning: `identical_ok` has no effect and is deprecated. It will be removed in 0.11.0. FutureWarning, . We can now load it again using load_dataset. . from datasets import load_dataset . ds = load_dataset(&#39;davanstrien/hf_model_metadata&#39;, split=&#39;train&#39;) . Using custom data configuration davanstrien--hf_model_metadata-019f1ad4bdf705b5 . Downloading and preparing dataset None/None (download: 3.71 MiB, generated: 10.64 MiB, post-processed: Unknown size, total: 14.35 MiB) to /root/.cache/huggingface/datasets/davanstrien___parquet/davanstrien--hf_model_metadata-019f1ad4bdf705b5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec... Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/davanstrien___parquet/davanstrien--hf_model_metadata-019f1ad4bdf705b5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data. . Clean up some memory... . del df . Semantic search of model cards . We now get to the main point of all of this. Can we use semantic search to try and find models of interest? For this, we&#39;ll use the sentence-transformers library. This blog won&#39;t cover all the background of this library. The docs give a helpful overview and some tutorials. . To start, we&#39;ll see if we can search using the information in the README.md. This should, in theory, contain data that might be similar to the kinds of things we want to search for when finding candidate models. We might prefer to use semantic search over an exact match because the terms we use might be different, or there is a related concept/model that might be close enough to make it worthwhile for fine-tuning. . First, we import the SentenceTransformer class and some util functions. . from sentence_transformers import SentenceTransformer, util . We&#39;ll now download an embedding model. There are many we could choose from but since we&#39;re just trying things out at the moment we won&#39;t stress about the particular model we use here. . model = SentenceTransformer(&#39;all-MiniLM-L6-v2&#39;) . Let&#39;s start on longer README&#39;s, here i mean a long readme that is just not super short... . ds_longer_readmes = ds.filter(lambda x: x[&#39;readme_len&#39;]&gt;100) . We now create embeddings for the readme column and store this in a new embedding column . def encode_readme(readme): return model.encode(readme,device=&#39;cuda&#39;) . ds_with_embeddings = ds_longer_readmes.map(lambda example: {&quot;embedding&quot;:encode_readme(example[&#39;readme&#39;])},batched=True, batch_size=16) . ds_with_embeddings . Dataset({ features: [&#39;modelId&#39;, &#39;label&#39;, &#39;readme&#39;, &#39;readme_len&#39;, &#39;embedding&#39;], num_rows: 3284 }) . We can now use the add_fais_index to create an index which allows us to efficiently query these embeddings . ds_with_embeddings.add_faiss_index(column=&#39;embedding&#39;) . Dataset({ features: [&#39;modelId&#39;, &#39;label&#39;, &#39;readme&#39;, &#39;readme_len&#39;, &#39;embedding&#39;], num_rows: 3284 }) . Similar models . To start, we&#39;ll take a readme for a model and see how well the model performs on finding similar models. . query_readme = ds_with_embeddings[35][&#39;readme&#39;] . print(query_readme) . # Twitter-roBERTa-base for Irony Detection This is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark. - Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). - Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval). ## Example of classification python from transformers import AutoModelForSequenceClassification from transformers import TFAutoModelForSequenceClassification from transformers import AutoTokenizer import numpy as np from scipy.special import softmax import csv import urllib.request # Preprocess text (username and link placeholders) def preprocess(text): new_text = [ ] for t in text.split(&quot; &quot;): t = &#39;@user&#39; if t.startswith(&#39;@&#39;) and len(t) &gt; 1 else t t = &#39;http&#39; if t.startswith(&#39;http&#39;) else t new_text.append(t) return &quot; &quot;.join(new_text) # Tasks: # emoji, emotion, hate, irony, offensive, sentiment # stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary task=&#39;irony&#39; MODEL = f&quot;cardiffnlp/twitter-roberta-base-{task}&quot; tokenizer = AutoTokenizer.from_pretrained(MODEL) # download label mapping labels=[] mapping_link = f&quot;https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt&quot; with urllib.request.urlopen(mapping_link) as f: html = f.read().decode(&#39;utf-8&#39;).split(&quot; n&quot;) csvreader = csv.reader(html, delimiter=&#39; t&#39;) labels = [row[1] for row in csvreader if len(row) &gt; 1] # PT model = AutoModelForSequenceClassification.from_pretrained(MODEL) model.save_pretrained(MODEL) text = &quot;Great, it broke the first day...&quot; text = preprocess(text) encoded_input = tokenizer(text, return_tensors=&#39;pt&#39;) output = model(**encoded_input) scores = output[0][0].detach().numpy() scores = softmax(scores) # # TF # model = TFAutoModelForSequenceClassification.from_pretrained(MODEL) # model.save_pretrained(MODEL) # text = &quot;Great, it broke the first day...&quot; # encoded_input = tokenizer(text, return_tensors=&#39;tf&#39;) # output = model(encoded_input) # scores = output[0][0].numpy() # scores = softmax(scores) ranking = np.argsort(scores) ranking = ranking[::-1] for i in range(scores.shape[0]): l = labels[ranking] s = scores[ranking] print(f&quot;{i+1}) {l} {np.round(float(s), 4)}&quot;) Output: 1) irony 0.914 2) non_irony 0.086 . We pass this README into the model we used to create our embedding. This creates a query embedding for this README. . q = model.encode(query_readme) . We can use get_nearest_examples to look for the most similar results to this query. . scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(&#39;embedding&#39;, q, k=10) . Let&#39;s take a look at the first result . print(retrieved_examples[&#39;modelId&#39;][0]) . cardiffnlp/twitter-roberta-base-irony . print(retrieved_examples[&quot;readme&quot;][0]) . # Twitter-roBERTa-base for Irony Detection This is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark. - Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). - Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval). ## Example of classification python from transformers import AutoModelForSequenceClassification from transformers import TFAutoModelForSequenceClassification from transformers import AutoTokenizer import numpy as np from scipy.special import softmax import csv import urllib.request # Preprocess text (username and link placeholders) def preprocess(text): new_text = [ ] for t in text.split(&quot; &quot;): t = &#39;@user&#39; if t.startswith(&#39;@&#39;) and len(t) &gt; 1 else t t = &#39;http&#39; if t.startswith(&#39;http&#39;) else t new_text.append(t) return &quot; &quot;.join(new_text) # Tasks: # emoji, emotion, hate, irony, offensive, sentiment # stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary task=&#39;irony&#39; MODEL = f&quot;cardiffnlp/twitter-roberta-base-{task}&quot; tokenizer = AutoTokenizer.from_pretrained(MODEL) # download label mapping labels=[] mapping_link = f&quot;https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt&quot; with urllib.request.urlopen(mapping_link) as f: html = f.read().decode(&#39;utf-8&#39;).split(&quot; n&quot;) csvreader = csv.reader(html, delimiter=&#39; t&#39;) labels = [row[1] for row in csvreader if len(row) &gt; 1] # PT model = AutoModelForSequenceClassification.from_pretrained(MODEL) model.save_pretrained(MODEL) text = &quot;Great, it broke the first day...&quot; text = preprocess(text) encoded_input = tokenizer(text, return_tensors=&#39;pt&#39;) output = model(**encoded_input) scores = output[0][0].detach().numpy() scores = softmax(scores) # # TF # model = TFAutoModelForSequenceClassification.from_pretrained(MODEL) # model.save_pretrained(MODEL) # text = &quot;Great, it broke the first day...&quot; # encoded_input = tokenizer(text, return_tensors=&#39;tf&#39;) # output = model(encoded_input) # scores = output[0][0].numpy() # scores = softmax(scores) ranking = np.argsort(scores) ranking = ranking[::-1] for i in range(scores.shape[0]): l = labels[ranking] s = scores[ranking] print(f&quot;{i+1}) {l} {np.round(float(s), 4)}&quot;) Output: 1) irony 0.914 2) non_irony 0.086 . and a lower similarity result . print(retrieved_examples[&quot;readme&quot;][9]) . language: &quot;en&quot; tags: - roberta - sentiment - twitter widget: - text: &quot;Oh no. This is bad..&quot; - text: &quot;To be or not to be.&quot; - text: &quot;Oh Happy Day&quot; This RoBERTa-based model can classify the sentiment of English language text in 3 classes: - positive 😀 - neutral 😐 - negative 🙁 The model was fine-tuned on 5,304 manually annotated social media posts. The hold-out accuracy is 86.1%. For details on the training approach see Web Appendix F in Hartmann et al. (2021). # Application python from transformers import pipeline classifier = pipeline(&quot;text-classification&quot;, model=&quot;j-hartmann/sentiment-roberta-large-english-3-classes&quot;, return_all_scores=True) classifier(&quot;This is so nice!&quot;) python Output: [[{&#39;label&#39;: &#39;negative&#39;, &#39;score&#39;: 0.00016451838018838316}, {&#39;label&#39;: &#39;neutral&#39;, &#39;score&#39;: 0.000174045650055632}, {&#39;label&#39;: &#39;positive&#39;, &#39;score&#39;: 0.9996614456176758}]] # Reference Please cite (https://journals.sagepub.com/doi/full/10.1177/00222437211037258) when you use our model. Feel free to reach out to (mailto:j.p.hartmann@rug.nl) with any questions or feedback you may have. @article{hartmann2021, title={The Power of Brand Selfies}, author={Hartmann, Jochen and Heitmann, Mark and Schamp, Christina and Netzer, Oded}, journal={Journal of Marketing Research} year={2021} } . The results seem pretty reasonable; the first result appears to be a duplicate. The lower result is for a slightly different task using social media data. . Searching . Being able to find similar model cards is a start but we actually wanted to be able to search directly. Let&#39;s see how our results do if we instead search for some terms we might use to try and find suitable models. . q = model.encode(&quot;fake news&quot;) . scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(&#39;embedding&#39;, q, k=10) . print(retrieved_examples[&quot;readme&quot;][0]) . This model is fined tuned for the Fake news classifier: Train a text classification model to detect fake news articles. Base on the Kaggle dataset(https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset). . print(retrieved_examples[&quot;readme&quot;][1]) . Fake news classifier This model trains a text classification model to detect fake news articles, it uses distilbert-base-uncased-finetuned-sst-2-english pretrained model to work on fake and real news dataset from kaggle (https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset) . print(retrieved_examples[&quot;readme&quot;][2]) . license: mit # Fake and real news classification task Model : [DistilRoBERTa base model](https://huggingface.co/distilroberta-base) Dataset : [Fake and real news dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset) . Not a bad start. Let&#39;s try another one . q = model.encode(&quot;financial sentiment&quot;) scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(&#39;embedding&#39;, q, k=10) print(retrieved_examples[&quot;readme&quot;][0]) . language: en tags: - financial-sentiment-analysis - sentiment-analysis datasets: - financial_phrasebank widget: - text: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales. - text: Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR 4,000. - text: Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in the corresponding period of 2008. ### FinancialBERT for Sentiment Analysis [*FinancialBERT*](https://huggingface.co/ahmedrachid/FinancialBERT) is a BERT model pre-trained on a large corpora of financial texts. The purpose is to enhance financial NLP research and practice in financial domain, hoping that financial practitioners and researchers can benefit from this model without the necessity of the significant computational resources required to train the model. The model was fine-tuned for Sentiment Analysis task on _Financial PhraseBank_ dataset. Experiments show that this model outperforms the general BERT and other financial domain-specific models. More details on `FinancialBERT`&#39;s pre-training process can be found at: https://www.researchgate.net/publication/358284785_FinancialBERT_-_A_Pretrained_Language_Model_for_Financial_Text_M ining ### Training data FinancialBERT model was fine-tuned on [Financial PhraseBank](https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10), a dataset consisting of 4840 Financial News categorised by sentiment (negative, neutral, positive). ### Fine-tuning hyper-parameters - learning_rate = 2e-5 - batch_size = 32 - max_seq_length = 512 - num_train_epochs = 5 ### Evaluation metrics The evaluation metrics used are: Precision, Recall and F1-score. The following is the classification report on the test set. | sentiment | precision | recall | f1-score | support | | - |:-:|:-:|:-:| --:| | negative | 0.96 | 0.97 | 0.97 | 58 | | neutral | 0.98 | 0.99 | 0.98 | 279 | | positive | 0.98 | 0.97 | 0.97 | 148 | | macro avg | 0.97 | 0.98 | 0.98 | 485 | | weighted avg | 0.98 | 0.98 | 0.98 | 485 | ### How to use The model can be used thanks to Transformers pipeline for sentiment analysis. python from transformers import BertTokenizer, BertForSequenceClassification from transformers import pipeline model = BertForSequenceClassification.from_pretrained(&quot;ahmedrachid/FinancialBERT-Sentiment-Analysis&quot;,num_labels=3) tokenizer = BertTokenizer.from_pretrained(&quot;ahmedrachid/FinancialBERT-Sentiment-Analysis&quot;) nlp = pipeline(&quot;sentiment-analysis&quot;, model=model, tokenizer=tokenizer) sentences = [&quot;Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales.&quot;, &quot;Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR 4,000.&quot;, &quot;Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in the corresponding period of 2008.&quot;, ] results = nlp(sentences) print(results) [{&#39;label&#39;: &#39;positive&#39;, &#39;score&#39;: 0.9998133778572083}, {&#39;label&#39;: &#39;neutral&#39;, &#39;score&#39;: 0.9997822642326355}, {&#39;label&#39;: &#39;negative&#39;, &#39;score&#39;: 0.9877365231513977}] &gt; Created by [Ahmed Rachid Hazourli](https://www.linkedin.com/in/ahmed-rachid/) . print(retrieved_examples[&quot;readme&quot;][1]) . language: &quot;en&quot; tags: - financial-sentiment-analysis - sentiment-analysis widget: - text: &quot;Stocks rallied and the British pound gained.&quot; FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientation s_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related (https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium. The model will give softmax outputs for three labels: positive, negative or neutral. About Prosus Prosus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com. Contact information Please contact Dogu Araci dogu.araciprosuscom and Zulkuf Genc zulkuf.gencprosuscom about any FinBERT related issues and questions. . print(retrieved_examples[&quot;readme&quot;][9]) . license: apache-2.0 tags: - Finance-sentiment-analysis - generated_from_trainer metrics: - f1 - accuracy - precision - recall model-index: - name: bert-base-finance-sentiment-noisy-search results: [] widget: - text: &quot;Third quarter reported revenues were $10.9 billion, up 5 percent compared to prior year and up 8 percent on a currency-neutral basis&quot; example_title: &quot;Positive&quot; - text: &quot;The London-listed website for businesses reported a pretax loss of $26.6 million compared with a loss of $12.9 million the previous year&quot; example_title: &quot;Negative&quot; - text: &quot;Microsoft updates Outlook, Teams, and PowerPoint to be hybrid work ready&quot; example_title: &quot;Neutral&quot; &lt;!-- This model card has been generated automatically according to the information the Trainer had access to. You should probably proofread and complete it, then remove this comment. --&gt; # bert-base-finance-sentiment-noisy-search This model is a fine-tuned version of (https://huggingface.co/bert-base-uncased) on Kaggle finance news sentiment analysis with data enhancement using noisy search. The process is explained below: 1. First &quot;bert-base-uncased&quot; was fine-tuned on Kaggle&#39;s finance news sentiment analysis https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news dataset achieving accuracy of about 88% 2. We then used a logistic-regression classifier on the same data. Here we looked at coefficients that contributed the most to the &quot;Positive&quot; and &quot;Negative&quot; classes by inspecting only bi-grams. 3. Using the top 25 bi-grams per class (i.e. &quot;Positive&quot; / &quot;Negative&quot;) we invoked Bing news search with those bi-grams and retrieved up to 50 news items per bi-gram phrase. 4. We called it &quot;noisy-search&quot; because it is assumed the positive bi-grams (e.g. &quot;profit rose&quot; , &quot;growth net&quot;) give rise to positive examples whereas negative bi-grams (e.g. &quot;loss increase&quot;, &quot;share loss&quot;) result in negative examples but note that we didn&#39;t test for the validity of this assumption (hence: noisy-search) 5. For each article we kept the title + excerpt and labeled it according to pre-assumptions on class associations. 6. We then trained the same model on the noisy data and apply it to an held-out test set from the original data set split. 7. Training with couple of thousands noisy &quot;positives&quot; and &quot;negatives&quot; examples yielded a test set accuracy of about 95%. 8. It shows that by automatically collecting noisy examples using search we can boost accuracy performance from about 88% to more than 95%. Accuracy results for Logistic Regression (LR) and BERT (base-cased) are shown in the attached pdf: https://drive.google.com/file/d/1MI9gRdppactVZ_XvhCwvoaOV1aRfprrd/view?usp=sharing ## Model description BERT model trained on noisy data from search results. See PDF for more details. ## Intended uses &amp; limitations Intended for use on finance news sentiment analysis with 3 options: &quot;Positive&quot;, &quot;Neutral&quot; and &quot;Negative&quot; To get the best results feed the classifier with the title and either the 1st paragraph or a short news summarization e.g. of up to 64 tokens. ### Training hyperparameters The following hyperparameters were used during training: - learning_rate: 5e-05 - train_batch_size: 8 - eval_batch_size: 8 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - num_epochs: 5 ### Framework versions - Transformers 4.16.2 - Pytorch 1.10.0+cu111 - Datasets 1.18.3 - Tokenizers 0.11.0 . These seem like a good starting point. However, we have a few issues relying on model cards alone. Firstly a lot of models don&#39;t include them and the quality of them can be mixed. It&#39;s maybe a question if we want to use a model that has no model card at all but it is possible that despite a good model card we don&#39;t capture everything we&#39;d need for searching in the README. . Can we search using model labels? . We&#39;re only working with classification models in this case. For most Pytorch models on the hub, we have a config file. This config usually contains the model&#39;s labels. For example, &#39;positive&#39;, &#39;negative&#39;. . Maybe instead of relying only on the metadata, we can search &#39;inside&#39; the model. The labels will often be a helpful reflection of what we&#39;re looking for. For example, we want to find a sentiment classification model that roughly puts text into positive or negative sentiment. Again, relying on exact label matches may not work well, but maybe embeddings get around this problem. Let&#39;s try it out! . Let&#39;s look at an example label. . ds[0][&#39;label&#39;] . [&#39;NEGATIVE&#39;, &#39;POSITIVE&#39;] . Since we&#39;re expecting labels to match this format lets filter out any that don&#39;t fit this structure. . ds = ds.filter(lambda example: isinstance(example[&#39;label&#39;],list)) . How to create embeddings for our labels? . How should we encode our labels? At the moment, we have a list of labels. One option would be to create an embedding for every single label, which will require us to query multiple embeddings to check for a match. We may also prefer intuatively to have an embedding for the combination of labels. This is because we probably know more about the model type from all its labels rather than looking at one label at a time. We&#39;ll deal with the labels very crudely by joining them on , and creating a single string out of all the labels. I&#39;m sure this isn&#39;t the best possible approach, but it might be a good place to start testing this idea. . ds = ds.map(lambda example: {&quot;string_label&quot;: &quot;,&quot;.join(example[&#39;label&#39;])}) . ds . Dataset({ features: [&#39;modelId&#39;, &#39;label&#39;, &#39;readme&#39;, &#39;readme_len&#39;, &#39;string_label&#39;], num_rows: 4175 }) . ds_with_embeddings = ds.map(lambda example: {&quot;label_embedding&quot;:encode_readme(example[&#39;string_label&#39;])},batched=True, batch_size=16) . ds_with_embeddings . Dataset({ features: [&#39;modelId&#39;, &#39;label&#39;, &#39;readme&#39;, &#39;readme_len&#39;, &#39;string_label&#39;, &#39;label_embedding&#39;], num_rows: 4175 }) . Searching with labels . Now we have some embeddings for the labels, let&#39;s try searching. Let&#39;s start with an existing set of labels to see how well we can match those. . ds_with_embeddings[0][&#39;string_label&#39;] . &#39;NEGATIVE,POSITIVE&#39; . q = model.encode(&quot;negative&quot;) . ds_with_embeddings.add_faiss_index(column=&#39;label_embedding&#39;) . Dataset({ features: [&#39;modelId&#39;, &#39;label&#39;, &#39;readme&#39;, &#39;readme_len&#39;, &#39;string_label&#39;, &#39;label_embedding&#39;], num_rows: 4175 }) . scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(&#39;label_embedding&#39;, q, k=10) . retrieved_examples[&#39;label&#39;][:10] . [ [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;], [&#39;negative&#39;, &#39;positive&#39;] ] . So far, these results look pretty good, although we haven&#39;t done anything we couldn&#39;t do with simple string matching. Let&#39;s see what happens if we use a slightly more abstract search. . q = model.encode(&quot;music&quot;) . scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(&#39;label_embedding&#39;, q, k=10) . retrieved_examples[&#39;label&#39;][:10] . [ [&#39;Dance&#39;, &#39;Heavy Metal&#39;, &#39;Hip Hop&#39;, &#39;Indie&#39;, &#39;Pop&#39;, &#39;Rock&#39;], [&#39;Dance&#39;, &#39;Heavy Metal&#39;, &#39;Hip Hop&#39;, &#39;Indie&#39;, &#39;Pop&#39;, &#39;Rock&#39;], [&#39;Dance&#39;, &#39;Heavy Metal&#39;, &#39;Hip Hop&#39;, &#39;Indie&#39;, &#39;Pop&#39;, &#39;Rock&#39;], [ &#39;Alternative&#39;, &#39;Country&#39;, &#39;Eletronic Music&#39;, &#39;Gospel and Worship Songs&#39;, &#39;Hip-Hop&#39;, &#39;Jazz/Blues&#39;, &#39;Pop&#39;, &#39;R&amp;B/Soul&#39;, &#39;Reggae&#39;, &#39;Rock&#39; ], [&#39;business&#39;, &#39;entertainment&#39;, &#39;sports&#39;], [&#39;_silence_&#39;, &#39;_unknown_&#39;, &#39;down&#39;, &#39;go&#39;, &#39;left&#39;, &#39;no&#39;, &#39;off&#39;, &#39;on&#39;, &#39;right&#39;, &#39;stop&#39;, &#39;up&#39;, &#39;yes&#39;], [&#39;angry&#39;, &#39;happy&#39;, &#39;others&#39;, &#39;sad&#39;], [&#39;Feeling&#39;, &#39;Thinking&#39;], [ &#39;am_thuc&#39;, &#39;bong_da&#39;, &#39;cho_thue&#39;, &#39;doi_song&#39;, &#39;dong_vat&#39;, &#39;mua_ban&#39;, &#39;nhac&#39;, &#39;phim&#39;, &#39;phu_kien&#39;, &#39;sach&#39;, &#39;showbiz&#39;, &#39;the_thao&#39;, &#39;thoi_trang_nam&#39;, &#39;thoi_trang_nu&#39;, &#39;thuc_vat&#39;, &#39;tin_bds&#39;, &#39;tin_tuc&#39;, &#39;tri_thuc&#39; ], [&#39;intimacy&#39;] ] . We can see that we get back labels related to music genre: [&#39;Dance&#39;, &#39;Heavy Metal&#39;, &#39;Hip Hop&#39;, &#39;Indie&#39;, &#39;Pop&#39;, &#39;Rock&#39;], for our first four results. After that, we get back [&#39;business&#39;, &#39;entertainment&#39;, &#39;sports&#39;], which might not be too far off what we want if we searched for music. . How about another search term . q = model.encode(&quot;hateful&quot;) . scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(&#39;label_embedding&#39;, q, k=10) . retrieved_examples[&#39;label&#39;][:10] . [ [&#39;Hateful&#39;, &#39;Not hateful&#39;], [&#39;Hateful&#39;, &#39;Not hateful&#39;], [&#39;hateful&#39;, &#39;non-hateful&#39;], [&#39;hateful&#39;, &#39;non-hateful&#39;], [&#39;hateful&#39;, &#39;non-hateful&#39;], [&#39;HATE&#39;, &#39;NOT_HATE&#39;], [&#39;NON_HATE&#39;, &#39;HATE&#39;], [&#39;NON_HATE&#39;, &#39;HATE&#39;], [&#39;NON_HATE&#39;, &#39;HATE&#39;], [&#39;NON_HATE&#39;, &#39;HATE&#39;] ] . Again here we have something quite close to what we&#39;d get with string matching, but we have a bit more flexibility in how we spell/define our labels which might help surface more possible results. . We&#39;ll try a bunch more things... . def query_labels(query:str): q = model.encode(query) scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(&#39;label_embedding&#39;, q, k=10) print(f&quot;results for: {query}&quot;) print(list(zip(retrieved_examples[&#39;label&#39;][:10],retrieved_examples[&#39;modelId&#39;][:10]))) . query_labels(&quot;politics&quot;) . results for: politics . [ ([&#39;Democrat&#39;, &#39;Republican&#39;], &#39;m-newhauser/distilbert-political-tweets&#39;), ([&#39;Geopolitical&#39;, &#39;Personal&#39;, &#39;Political&#39;, &#39;Religious&#39;], &#39;dee4hf/autotrain-deephate2-1093539673&#39;), ([&#39;None&#39;, &#39;Environmental&#39;, &#39;Social&#39;, &#39;Governance&#39;], &#39;yiyanghkust/finbert-esg&#39;), ([&#39;business&#39;, &#39;entertainment&#39;, &#39;sports&#39;], &#39;bipin/malayalam-news-classifier&#39;), ( [&#39;CRIME&#39;, &#39;ENTERTAINMENT&#39;, &#39;Finance&#39;, &#39;POLITICS&#39;, &#39;SPORTS&#39;, &#39;Terrorism&#39;], &#39;Yarn007/autotrain-Napkin-872827783&#39; ), ([&#39;business&#39;, &#39;entertainment&#39;, &#39;politics&#39;, &#39;sport&#39;, &#39;tech&#39;], &#39;abhishek/autonlp-bbc-roberta-37249301&#39;), ( [&#39;business&#39;, &#39;entertainment&#39;, &#39;politics&#39;, &#39;sport&#39;, &#39;tech&#39;], &#39;abhishek/autonlp-bbc-news-classification-37229289&#39; ), ([&#39;business&#39;, &#39;entertainment&#39;, &#39;politics&#39;, &#39;sport&#39;, &#39;tech&#39;], &#39;Yarn/autotrain-Traimn-853827191&#39;), ([&#39;Neutral&#39;, &#39;Propaganda&#39;], &#39;Real29/my-model-proppy&#39;), ([&#39;Neutral&#39;, &#39;Propaganda&#39;], &#39;Real29/my-model-ptc&#39;) ] . query_labels(&quot;fiction, non_fiction&quot;) . results for: fiction, non_fiction . [ ( [&#39;action&#39;, &#39;drama&#39;, &#39;horror&#39;, &#39;sci_fi&#39;, &#39;superhero&#39;, &#39;thriller&#39;], &#39;Tejas3/distillbert_110_uncased_movie_genre&#39; ), ([&#39;action&#39;, &#39;drama&#39;, &#39;horror&#39;, &#39;sci_fi&#39;, &#39;superhero&#39;, &#39;thriller&#39;], &#39;Tejas3/distillbert_110_uncased_v1&#39;), ( [&#39;action&#39;, &#39;animation&#39;, &#39;comedy&#39;, &#39;drama&#39;, &#39;romance&#39;, &#39;thriller&#39;], &#39;langfab/distilbert-base-uncased-finetuned-movie-genre&#39; ), ([&#39;HATE&#39;, &#39;NON_HATE&#39;], &#39;anthonny/dehatebert-mono-spanish-finetuned-sentiments_reviews_politicos&#39;), ([&#39;NON_HATE&#39;, &#39;HATE&#39;], &#39;Hate-speech-CNERG/dehatebert-mono-english&#39;), ([&#39;NON_HATE&#39;, &#39;HATE&#39;], &#39;Hate-speech-CNERG/dehatebert-mono-german&#39;), ([&#39;NON_HATE&#39;, &#39;HATE&#39;], &#39;Hate-speech-CNERG/dehatebert-mono-italian&#39;), ([&#39;NON_HATE&#39;, &#39;HATE&#39;], &#39;Hate-speech-CNERG/dehatebert-mono-spanish&#39;), ([&#39;NON_HATE&#39;, &#39;HATE&#39;], &#39;Hate-speech-CNERG/dehatebert-mono-portugese&#39;), ([&#39;NON_HATE&#39;, &#39;HATE&#39;], &#39;Hate-speech-CNERG/dehatebert-mono-polish&#39;) ] . Let&#39;s try the set of emotions one should feel everyday. . query_labels(&quot;worry, disgust, anxiety, fear&quot;) . results for: worry, disgust, anxiety, fear . [ ([&#39;anger&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;guilt&#39;, &#39;joy&#39;, &#39;sadness&#39;, &#39;shame&#39;], &#39;crcb/isear_bert&#39;), ( [&#39;anger&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;others&#39;, &#39;sadness&#39;, &#39;surprise&#39;], &#39;pysentimiento/robertuito-emotion-analysis&#39; ), ( [&#39;anger&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;others&#39;, &#39;sadness&#39;, &#39;surprise&#39;], &#39;daveni/twitter-xlm-roberta-emotion-es&#39; ), ( [&#39;anger&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;others&#39;, &#39;sadness&#39;, &#39;surprise&#39;], &#39;finiteautomata/beto-emotion-analysis&#39; ), ( [&#39;anger&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;others&#39;, &#39;sadness&#39;, &#39;surprise&#39;], &#39;finiteautomata/bertweet-base-emotion-analysis&#39; ), ([&#39;ANGER&#39;, &#39;DISGUST&#39;, &#39;FEAR&#39;, &#39;HAPPINESS&#39;, &#39;NEUTRALITY&#39;, &#39;SADNESS&#39;, &#39;SURPRISED&#39;], &#39;Gunulhona/tbecmodel&#39;), ( [&#39;anger&#39;, &#39;anticipation&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;sadness&#39;, &#39;surprise&#39;, &#39;trust&#39;], &#39;Yuetian/bert-base-uncased-finetuned-plutchik-emotion&#39; ), ([&#39;anger&#39;, &#39;fear&#39;, &#39;happy&#39;, &#39;love&#39;, &#39;sadness&#39;], &#39;jasonpratamas7/Thesis-Model-1&#39;), ([&#39;anger&#39;, &#39;fear&#39;, &#39;happy&#39;, &#39;love&#39;, &#39;sadness&#39;], &#39;jasonpratamas7/Thesis-Model1&#39;), ([&#39;anger&#39;, &#39;fear&#39;, &#39;happy&#39;, &#39;love&#39;, &#39;sadness&#39;], &#39;StevenLimcorn/indonesian-roberta-base-emotion-classifier&#39;) ] . This example of searching for a set of labels might be a better approach in general since the query will better match the format of the intitial search. . Conclusion . It seems like there is some merit in exploring some of these ideas further. There are a lot of improvements that could be made: . how the embeddings are created | removing some &#39;noise&#39; from the README, for example, by first parsing the Markdown | improving how the embeddings are created for the labels | combining the embeddings in some way either upfront or when queryig | a bunch of other things... | . If I find some spare time, I plan to dig into these topics a bit further. This is also a nice excuse to play with one of the new open source embedding databases that have popped up in the last couple of years. .",
            "url": "https://danielvanstrien.xyz/huggingface/huggingface-datasets/semantic-search/2022/07/26/semantic-search-ml-models.html",
            "relUrl": "/huggingface/huggingface-datasets/semantic-search/2022/07/26/semantic-search-ml-models.html",
            "date": " • Jul 26, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Combining Hugging Face datasets with dask",
            "content": "Hugging Face datasets is a super useful library for loading, processing and sharing datasets with other people. . For many pre-processing steps it works beautifully. The one area where it can be a bit trickier to use is for EDA style analysis. This column-wise EDA is often important as an early step in working with some data or for preparing a data card. . Fortunately combining datasets and another data library, dask works pretty smoothly. This isn&#39;t intended to be a full intro to either datasets or dask but hopefully gives you a sense of how both libaries work and how they can complement each other. . First, make sure we have the required libraries. Rich is there for a little added visual flair ✨ . %%capture !pip install datasets toolz rich[jupyter] dask . %load_ext rich . Load some data . For this example we will use a the blbooksgenre dataset that contains metadata about some digitised books from the British Library. This collection also includes some annotations for the genre of the book which we could use to train a machine learning model. . We can load a dataset hosted on the Hugging Face hub by using the load_dataset function. . from datasets import load_dataset . ds = load_dataset(&quot;blbooksgenre&quot;, &quot;annotated_raw&quot;, split=&quot;train&quot;) . Reusing dataset bl_books_genre (/Users/dvanstrien/.cache/huggingface/datasets/bl_books_genre/annotated_raw/1.1.0/1e01f82403b3d9344121c3b81e5ad7c130338b250bf95dad4c6ab342c642dbe8) . Since we requested only the train split we get back a Dataset . ds . Dataset({ features: [&#39;BL record ID&#39;, &#39;Name&#39;, &#39;Dates associated with name&#39;, &#39;Type of name&#39;, &#39;Role&#39;, &#39;All names&#39;, &#39;Title&#39;, &#39;Variant titles&#39;, &#39;Series title&#39;, &#39;Number within series&#39;, &#39;Country of publication&#39;, &#39;Place of publication&#39;, &#39;Publisher&#39;, &#39;Date of publication&#39;, &#39;Edition&#39;, &#39;Physical description&#39;, &#39;Dewey classification&#39;, &#39;BL shelfmark&#39;, &#39;Topics&#39;, &#39;Genre&#39;, &#39;Languages&#39;, &#39;Notes&#39;, &#39;BL record ID for physical resource&#39;, &#39;classification_id&#39;, &#39;user_id&#39;, &#39;subject_ids&#39;, &#39;annotator_date_pub&#39;, &#39;annotator_normalised_date_pub&#39;, &#39;annotator_edition_statement&#39;, &#39;annotator_FAST_genre_terms&#39;, &#39;annotator_FAST_subject_terms&#39;, &#39;annotator_comments&#39;, &#39;annotator_main_language&#39;, &#39;annotator_other_languages_summaries&#39;, &#39;annotator_summaries_language&#39;, &#39;annotator_translation&#39;, &#39;annotator_original_language&#39;, &#39;annotator_publisher&#39;, &#39;annotator_place_pub&#39;, &#39;annotator_country&#39;, &#39;annotator_title&#39;, &#39;Link to digitised book&#39;, &#39;annotated&#39;, &#39;Type of resource&#39;, &#39;created_at&#39;, &#39;annotator_genre&#39;], num_rows: 4398 }) . We can see this has a bunch of columns. One that is of interest is the Data of publication column. Since we could use this dataset to train some type of classifier we may want to check whether we have enough examples across different time periods in the dataset. . ds[0][&quot;Date of publication&quot;] . &#39;1879&#39; . Using toolz to calculate frequencies for a column . One quick way we can get the frequency count for a column is using the wonderful toolz library . If our data fits in memory, we can simply pass in a column containing a categorical value to a frequency function to get a frequency count. . from toolz import frequencies, topk . dates = ds[&quot;Date of publication&quot;] . frequencies(dates) . . { &#39;1879&#39;: 99, &#39;1774&#39;: 5, &#39;1765&#39;: 5, &#39;1877&#39;: 69, &#39;1893&#39;: 222, &#39;1891&#39;: 148, &#39;1827&#39;: 29, &#39;1868&#39;: 42, &#39;1878&#39;: 72, &#39;1895&#39;: 189, &#39;1897&#39;: 120, &#39;1899&#39;: 104, &#39;1896&#39;: 174, &#39;1876&#39;: 48, &#39;1812&#39;: 13, &#39;1799&#39;: 8, &#39;1830&#39;: 32, &#39;1870&#39;: 42, &#39;1894&#39;: 155, &#39;1864&#39;: 28, &#39;1855&#39;: 42, &#39;1871&#39;: 42, &#39;1836&#39;: 37, &#39;1883&#39;: 51, &#39;1880&#39;: 111, &#39;1884&#39;: 69, &#39;1822&#39;: 16, &#39;1856&#39;: 38, &#39;1872&#39;: 42, &#39;1875&#39;: 57, &#39;1844&#39;: 35, &#39;1890&#39;: 134, &#39;1886&#39;: 43, &#39;1840&#39;: 15, &#39;1888&#39;: 109, &#39;1858&#39;: 43, &#39;1867&#39;: 53, &#39;1826&#39;: 24, &#39;1800&#39;: 3, &#39;1851&#39;: 43, &#39;1838&#39;: 14, &#39;1824&#39;: 20, &#39;1887&#39;: 58, &#39;1874&#39;: 42, &#39;1857&#39;: 44, &#39;1873&#39;: 34, &#39;1837&#39;: 16, &#39;1846&#39;: 32, &#39;1881&#39;: 55, &#39;1898&#39;: 104, &#39;1906&#39;: 4, &#39;1892&#39;: 134, &#39;1869&#39;: 25, &#39;1885&#39;: 69, &#39;1882&#39;: 71, &#39;1863&#39;: 55, &#39;1865&#39;: 53, &#39;1635&#39;: 3, &#39;1859&#39;: 39, &#39;1818&#39;: 17, &#39;1845&#39;: 28, &#39;1852&#39;: 43, &#39;1841&#39;: 23, &#39;1842&#39;: 29, &#39;1848&#39;: 28, &#39;1828&#39;: 23, &#39;1850&#39;: 38, &#39;1860&#39;: 45, &#39;1889&#39;: 140, &#39;1815&#39;: 5, &#39;1861&#39;: 28, &#39;1814&#39;: 13, &#39;1843&#39;: 28, &#39;1817&#39;: 12, &#39;1819&#39;: 16, &#39;1853&#39;: 34, &#39;1833&#39;: 5, &#39;1854&#39;: 36, &#39;1839&#39;: 33, &#39;1803&#39;: 7, &#39;1835&#39;: 14, &#39;1813&#39;: 8, &#39;1695&#39;: 4, &#39;1809-1811&#39;: 5, &#39;1832&#39;: 9, &#39;1823&#39;: 17, &#39;1847&#39;: 28, &#39;1816&#39;: 8, &#39;1806&#39;: 5, &#39;1866&#39;: 26, &#39;1829&#39;: 13, &#39;1791&#39;: 5, &#39;1637&#39;: 5, &#39;1821&#39;: 4, &#39;1807&#39;: 14, &#39;1862&#39;: 22, &#39;1795&#39;: 5, &#39;1834&#39;: 12, &#39;1831&#39;: 10, &#39;1849&#39;: 13, &#39;1811&#39;: 1, &#39;1825&#39;: 1, &#39;1809&#39;: 3, &#39;1905&#39;: 1, &#39;1808&#39;: 1, &#39;1900&#39;: 5, &#39;1892-1912&#39;: 1, &#39;1804&#39;: 4, &#39;1769&#39;: 5, &#39;1910&#39;: 1, &#39;1805&#39;: 5, &#39;1802&#39;: 3, &#39;1871-&#39;: 1, &#39;1901&#39;: 5, &#39;1884-1909&#39;: 1, &#39;1873-1887&#39;: 1, &#39;1979&#39;: 1, &#39;1852-1941&#39;: 1, &#39;1903&#39;: 1, &#39;1871-1873&#39;: 1, &#39;1810&#39;: 3, &#39;1907&#39;: 1, &#39;1820&#39;: 5, &#39;1789&#39;: 5 } . Make it parallel! . If our data doesn&#39;t fit in memory or we want to do things in parallel we might want to use a slightly different approach. This is where dask can play a role. . Dask offers a number of different collection abstractions that make it easier to do things in parallel. This includes dask bag. . First we&#39;ll create a dask client here, I won&#39;t dig into the details of this here but you can get a good overview in the getting started pages. . from distributed import Client . client = Client() . Since we don&#39;t want to load all of our data into memory we can great a generator that will yield one row at a time. In this case we&#39;ll start by exploring the Title column . def yield_titles(): for row in ds: yield row[&quot;Title&quot;] . We can see that this returns a generator . yield_titles() . &lt;generator object yield_titles at 0x7ffc28fdc040&gt; . next(iter(yield_titles())) . &#39;The Canadian farmer. A missionary incident [Signed: W. J. H. Y, i.e. William J. H. Yates.]&#39; . We can store this in a titles variable. . titles = yield_titles() . We&#39;ll now import dask bag. . import dask.bag as db . We can create a dask bag object using the from_sequence method. . bag = db.from_sequence(titles) . bag . dask.bag&lt;from_sequence, npartitions=1&gt; . We can look at an example using the take method . bag.take(1) . ( [ &#39;The&#39;, &#39;Canadian&#39;, &#39;farmer.&#39;, &#39;A&#39;, &#39;missionary&#39;, &#39;incident&#39;, &#39;[Signed:&#39;, &#39;W.&#39;, &#39;J.&#39;, &#39;H.&#39;, &#39;Y,&#39;, &#39;i.e.&#39;, &#39;William&#39;, &#39;J.&#39;, &#39;H.&#39;, &#39;Yates.]&#39; ], ) . dask bag has a bunch of handy methods for processing data (some of these we could also do in 🤗 datasets but others are not available as specific methods in datasets). . For example we can make sure we only have unique titles using the distinct method. . unique_titles = bag.distinct() . unique_titles.take(4) . ( &#39;The Canadian farmer. A missionary incident [Signed: W. J. H. Y, i.e. William J. H. Yates.]&#39;, &#39;A new musical Interlude, called the Election [By M. P. Andrews.]&#39;, &#39;An Elegy written among the ruins of an Abbey. By the author of the Nun [E. Jerningham]&#39;, &quot;The Baron&#39;s Daughter. A ballad by the author of Poetical Recreations [i.e. William C. Hazlitt] . F.P&quot; ) . Similar to 🤗 datasets we have a map method that we can use to apply a function to all of our examples. In this case we split the title text into individual words. . title_words_split = unique_titles.map(lambda x: x.split(&quot; &quot;)) . title_words_split.take(2) . ( [ &#39;The&#39;, &#39;Canadian&#39;, &#39;farmer.&#39;, &#39;A&#39;, &#39;missionary&#39;, &#39;incident&#39;, &#39;[Signed:&#39;, &#39;W.&#39;, &#39;J.&#39;, &#39;H.&#39;, &#39;Y,&#39;, &#39;i.e.&#39;, &#39;William&#39;, &#39;J.&#39;, &#39;H.&#39;, &#39;Yates.]&#39; ], [ &#39;A&#39;, &#39;new&#39;, &#39;musical&#39;, &#39;Interlude,&#39;, &#39;called&#39;, &#39;the&#39;, &#39;Election&#39;, &#39;[By&#39;, &#39;M.&#39;, &#39;P.&#39;, &#39;Andrews.]&#39; ] ) . We can see we now have all our words in a list. Helpfully dask bag has a flatten method. This will consume our lists and put all the words in a single sequence. . flattend_title_words = title_words_split.flatten() . flattend_title_words.take(2) . (&#39;The&#39;, &#39;Canadian&#39;) . We could now use the frequencies method to get the top words. . freqs = flattend_title_words.frequencies(sort=True) . freqs . dask.bag&lt;sorted, npartitions=1&gt; . Since dask bag methods are lazy by default nothing has actually been calculated yet. We could just grab the top 10 words. . top_10_words = freqs.topk(10, key=1) . If we want the results of something we call compute which will call all of the chained methods on our bag. . top_10_words.compute() . [ (&#39;of&#39;, 808), (&#39;the&#39;, 674), (&#39;and&#39;, 550), (&#39;...&#39;, 518), (&#39;in&#39;, 402), (&#39;van&#39;, 306), (&#39;etc&#39;, 301), (&#39;de&#39;, 258), (&#39;en&#39;, 258), (&#39;a&#39;, 231) ] . We could also do the same with lowered version . lowered_title_words = flattend_title_words.map(lambda x: x.lower()) . freqs = lowered_title_words.frequencies(sort=True) . The visualize method gives you some insights into how the computation is managed by dask. . freqs.visualize(engine=&quot;cytoscape&quot;, optimize_graph=True) . Moving from datasets to a dask dataframe . For some operations, dask bag is super easy to use. Sometimes though you will hurt your brain trying to crow bar your problem into the dask bag API 😵‍💫 This is where dask dataframes come in! Using parquet, we can easily save our 🤗 dataset as a parquet file. . ds.to_parquet(&quot;genre.parquet&quot;) . 3583138 . import dask.dataframe as dd . and load from this file . ddf = dd.read_parquet(&quot;genre.parquet&quot;) . As dask dataframe works quite similar to a pandas dataframe. It is lazy by default so if we just print it out . ddf . Dask DataFrame Structure: BL record ID Name Dates associated with name Type of name Role All names Title Variant titles Series title Number within series Country of publication Place of publication Publisher Date of publication Edition Physical description Dewey classification BL shelfmark Topics Genre Languages Notes BL record ID for physical resource classification_id user_id subject_ids annotator_date_pub annotator_normalised_date_pub annotator_edition_statement annotator_FAST_genre_terms annotator_FAST_subject_terms annotator_comments annotator_main_language annotator_other_languages_summaries annotator_summaries_language annotator_translation annotator_original_language annotator_publisher annotator_place_pub annotator_country annotator_title Link to digitised book annotated Type of resource created_at annotator_genre . npartitions=1 . object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | object | bool | int64 | datetime64[ns] | int64 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Dask Name: read-parquet, 1 tasks You&#39;ll see we don&#39;t actually get back any data. If we use head we get the number of examples we ask for. . ddf.head(3) . BL record ID Name Dates associated with name Type of name Role All names Title Variant titles Series title Number within series ... annotator_original_language annotator_publisher annotator_place_pub annotator_country annotator_title Link to digitised book annotated Type of resource created_at annotator_genre . 0 014603046 | Yates, William Joseph H. | | person | | [Yates, William Joseph H. [person] , Y, W. J.... | The Canadian farmer. A missionary incident [Si... | | | | ... | | NONE | London | enk | The Canadian farmer. A missionary incident [Si... | http://access.bl.uk/item/viewer/ark:/81055/vdc... | True | 0 | 2020-08-11 14:30:33 | 0 | . 1 014603046 | Yates, William Joseph H. | | person | | [Yates, William Joseph H. [person] , Y, W. J.... | The Canadian farmer. A missionary incident [Si... | | | | ... | | NONE | London | enk | The Canadian farmer. A missionary incident [Si... | http://access.bl.uk/item/viewer/ark:/81055/vdc... | True | 0 | 2021-04-15 09:53:23 | 0 | . 2 014603046 | Yates, William Joseph H. | | person | | [Yates, William Joseph H. [person] , Y, W. J.... | The Canadian farmer. A missionary incident [Si... | | | | ... | | NONE | London | enk | The Canadian farmer. A missionary incident [Si... | http://access.bl.uk/item/viewer/ark:/81055/vdc... | True | 0 | 2020-09-24 14:27:54 | 0 | . 3 rows × 46 columns . We have some familiar methods from pandas available to us . ddf = ddf.drop_duplicates(subset=&quot;Title&quot;) . As an example of something that would be a bit tricky in datasets, we can see how to groupby the mean title length by year of publication. First we create a new column for title length . ddf[&quot;title_len&quot;] = ddf[&quot;Title&quot;].map(lambda x: len(x)) . We can then groupby the date of publication . grouped = ddf.groupby(&quot;Date of publication&quot;) . and then calculate the mean title_len . mean_title_len = grouped[&quot;title_len&quot;].mean() . To actually compute this value we call the compute method . mean_title_len.compute() . Date of publication 1635 248.0 1637 67.0 1695 63.0 1765 86.0 1769 20.0 ... 1905 141.0 1906 225.0 1907 142.0 1910 65.0 1979 43.0 Name: title_len, Length: 124, dtype: float64 . We can also create a plot in the usual way . mean_title_len.compute().plot() . &lt;AxesSubplot:xlabel=&#39;Date of publication&#39;&gt; . &lt;Figure size 432x288 with 1 Axes&gt; . This was a very quick overview. The dask docs go into much more detail as do the Hugging Face datasets docs. .",
            "url": "https://danielvanstrien.xyz/huggingface/huggingface-datasets/dask/2022/06/20/dask-and-datasets.html",
            "relUrl": "/huggingface/huggingface-datasets/dask/2022/06/20/dask-and-datasets.html",
            "date": " • Jun 20, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Using 🤗 datasets for image search",
            "content": "tl;dr it&#39;s really easy to use the huggingface datasets library to create an image search application but it might not be suitable for sharing. update an updated version of this post is on the 🤗 blog! . 🤗 Datasets is a library for easily accessing and sharing datasets, and evaluation metrics for Natural Language Processing (NLP), computer vision, and audio tasks. source . When datasets was first launched it was more usually associated with text data and nlp. However, datasets has got support for images. In particular there is now a datasets feature type for images. In this blog post I try and play around with this new datatype, in combination with some other nice features of the library to make an image search app. . To start lets take a look at the image feature. We can use the wonderful rich libary to poke around python objects (functions, classes etc.) . from rich import inspect from datasets.features import features . inspect(features.Image, help=True) . ╭──────────────────── &lt;class &#39;datasets.features.image.Image&#39;&gt; ─────────────────────╮ │ def Image(id: Union[str, NoneType] = None) -&gt; None: │ │ │ │ Image feature to read image data from an image file. │ │ │ │ Input: The Image feature accepts as input: │ │ - A :obj:`str`: Absolute path to the image file (i.e. random access is allowed). │ │ - A :obj:`dict` with the keys: │ │ │ │ - path: String with relative path of the image file to the archive file. │ │ - bytes: Bytes of the image file. │ │ │ │ This is useful for archived files with sequential access. │ │ │ │ - An :obj:`np.ndarray`: NumPy array representing an image. │ │ - A :obj:`PIL.Image.Image`: PIL image object. │ │ │ │ dtype = &#39;dict&#39; │ │ id = None │ │ pa_type = None │ ╰──────────────────────────────────────────────────────────────────────────────────╯ . We can see there a few different ways in which we can pass in our images. We&#39;ll come back to this in a little while. . A really nice feature of the datasets library (beyond the functionality for processing data, memory mapping etc.) is that you get some nice things for free. One of these is the ability to add a faiss index. faiss is a &quot;library for efficient similarity search and clustering of dense vectors&quot;. . The datasets docs show and example of using faiss for text retrieval. What I&#39;m curious about doing is using the faiss index to search for images. This can be super useful for a number of reasons but also comes with some potential issues. . The dataset: &quot;Digitised Books - Images identified as Embellishments. c. 1510 - c. 1900. JPG&quot; . This is a dataset of images which have been pulled from a collection of digitised books from the British Library. These images come from books across a wide time period and from a broad range of domains. These images were extracted using information in the OCR output for each book. As a result it&#39;s known which book the images came from but not necessarily anything else about that image i.e. what it is of. . Some attempts to help overcome this have included uploading the images to flickr. This allows people to tag the images or put them into various different categories. . There have also been projects to tag the dataset using machine learning. This work already makes it possible to search by tags but we might want a &#39;richer&#39; ability to search. For this particular experiment I will work with a subset of the collections which contain &quot;embellishments&quot;. This dataset is a bit smaller so will be better for experimenting with. We can get the data from the BL repository: https://doi.org/10.21250/db17 . !aria2c -x8 -o dig19cbooks-embellishments.zip &quot;https://bl.iro.bl.uk/downloads/ba1d1d12-b1bd-4a43-9696-7b29b56cdd20?locale=en&quot; . !unzip -q dig19cbooks-embellishments.zip . Install required packages . There are a few packages we&#39;ll need for this work. To start with we&#39;ll need the datasets library. . import sys !{sys.executable} -m pip install datasets . Now we have the data downloaded we&#39;ll try and load it into datasets. There are various ways of doing this. To start with we can grab all of the files we need. . from pathlib import Path . files = list(Path(&#39;embellishments/&#39;).rglob(&quot;*.jpg&quot;)) . Since the file path encodes the year of publication for the book the image came from let&#39;s create a function to grab that. . def get_parts(f:Path): _,year,fname = f.parts return year, fname . &#128248; Loading the images . The images are fairly large, since this is an experiment we&#39;ll resize them a little using the thumbnail method (this makes sure we keep the same aspect ratio for our images) . from PIL import Image import io . def load_image(path): with open(path, &#39;rb&#39;) as f: im = Image.open(io.BytesIO(f.read())) im.thumbnail((224,224)) return im . im = load_image(files[0]) im . Where is the image &#129300; . You may have noticed that the load_image function doesn&#39;t load the filepath into pillow directly. Often we would do Image.open(filepath.jpg). This is done deliberately. If we load it this way when we inspect the resulting image you&#39;ll see that the filepath attribute is empty. . inspect(im) . ╭─────────────────────── &lt;class &#39;PIL.JpegImagePlugin.JpegImageFile&#39;&gt; ───────────────────────╮ │ ╭───────────────────────────────────────────────────────────────────────────────────────╮ │ │ │ &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=200x224 at 0x7FBBB392D040&gt; │ │ │ ╰───────────────────────────────────────────────────────────────────────────────────────╯ │ │ │ │ app = {&#39;APP0&#39;: b&#39;JFIF x00 x01 x01 x00 x00 x01 x00 x01 x00 x00&#39;} │ │ applist = [(&#39;APP0&#39;, b&#39;JFIF x00 x01 x01 x00 x00 x01 x00 x01 x00 x00&#39;)] │ │ bits = 8 │ │ custom_mimetype = None │ │ decoderconfig = (2, 0) │ │ decodermaxblock = 65536 │ │ encoderconfig = (False, -1, -1, b&#39;&#39;) │ │ encoderinfo = {} │ │ filename = &#39;&#39; │ │ format = &#39;JPEG&#39; │ │ format_description = &#39;JPEG (ISO 10918)&#39; │ │ fp = None │ │ height = 224 │ │ huffman_ac = {} │ │ huffman_dc = {} │ │ icclist = [] │ │ im = &lt;ImagingCore object at 0x7fbba120dc10&gt; │ │ info = { │ │ &#39;jfif&#39;: 257, │ │ &#39;jfif_version&#39;: (1, 1), │ │ &#39;jfif_unit&#39;: 0, │ │ &#39;jfif_density&#39;: (1, 1) │ │ } │ │ layer = [(1, 2, 2, 0), (2, 1, 1, 1), (3, 1, 1, 1)] │ │ layers = 3 │ │ map = None │ │ mode = &#39;RGB&#39; │ │ palette = None │ │ pyaccess = None │ │ quantization = { │ │ 0: [ │ │ 2, │ │ 2, │ │ 1, │ │ 2, │ │ 3, │ │ 6, │ │ 7, │ │ 9, │ │ 2, │ │ 2, │ │ 2, │ │ 3, │ │ 4, │ │ 8, │ │ 8, │ │ 8, │ │ 2, │ │ 2, │ │ 2, │ │ 3, │ │ 6, │ │ 8, │ │ 10, │ │ 8, │ │ 2, │ │ 2, │ │ 3, │ │ 4, │ │ 7, │ │ 12, │ │ 11, │ │ 9, │ │ 3, │ │ 3, │ │ 5, │ │ 8, │ │ 10, │ │ 15, │ │ 14, │ │ 11, │ │ 3, │ │ 5, │ │ 8, │ │ 9, │ │ 11, │ │ 15, │ │ 16, │ │ 13, │ │ 7, │ │ 9, │ │ 11, │ │ 12, │ │ 14, │ │ 17, │ │ 17, │ │ 14, │ │ 10, │ │ 13, │ │ 13, │ │ 14, │ │ 16, │ │ 14, │ │ 14, │ │ 14 │ │ ], │ │ 1: [ │ │ 2, │ │ 3, │ │ 3, │ │ 7, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 3, │ │ 3, │ │ 4, │ │ 9, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 3, │ │ 4, │ │ 8, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 7, │ │ 9, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14, │ │ 14 │ │ ] │ │ } │ │ readonly = 0 │ │ size = (200, 224) │ │ tile = [] │ │ width = 200 │ ╰───────────────────────────────────────────────────────────────────────────────────────────╯ . . You can also directly see this . im.filename . &#39;&#39; . Pillow usually loads images in a lazy way i.e. it only opens them when they are needed. The filepath is used to access the image. We can see the filename attribute is present if we open it from the filepath . im_file = Image.open(files[0]) im_file.filename . &#39;/Users/dvanstrien/Documents/daniel/blog/_notebooks/embellishments/1855/000811462_05_000205_1_The Pictorial History of England being a history of the people as well as a hi_1855.jpg&#39; . The reason I don&#39;t want the filename attribute present here is because not only do I want to use datasets to process our images but also store the images. If we pass a Pillow object with the filename attribute datasets will also use this for loading the images. This is often what we&#39;d want but we don&#39;t want this here for reasons we&#39;ll see shortly. . Preparing images for datasets . We can now load our images. What we&#39;ll do is is loop through all our images and then load the information for each image into a dictionary. . from collections import defaultdict . data = defaultdict(list) . from tqdm import tqdm . for file in tqdm(files): try: #load_image(file) year, fname = get_parts(file) data[&#39;fname&#39;].append(fname) data[&#39;year&#39;].append(year) data[&#39;path&#39;].append(str(file)) except: Image.UnidentifiedImageError pass . 100%|████████████████████████████████████████████████████████████████████████████████████████████| 416944/416944 [00:05&lt;00:00, 77169.45it/s] . We can now load the from_dict method to create a new dataset. . from datasets import Dataset . dataset = Dataset.from_dict(data) . We can look at one example to see what this looks like. . dataset[0] . {&#39;fname&#39;: &#39;000811462_05_000205_1_The Pictorial History of England being a history of the people as well as a hi_1855.jpg&#39;, &#39;year&#39;: &#39;1855&#39;, &#39;path&#39;: &#39;embellishments/1855/000811462_05_000205_1_The Pictorial History of England being a history of the people as well as a hi_1855.jpg&#39;} . Loading our images . At the moment our dataset has the filename and full path for each image. However, we want to have an actual image loaded into our dataset. We already have a load_image function. This gets us most of the way there but we might also want to add some ability to deal with image errors. The datasets library has gained increased uspport for handling None types- this includes support for None types for images see pull request 3195. . We&#39;ll wrap our load_image function in a try block, catch a Image.UnidentifiedImageError error and return None if we can&#39;t load the image. . def try_load_image(filename): try: image = load_image(filename) if isinstance(image, Image.Image): return image except Image.UnidentifiedImageError: return None . %%time dataset = dataset.map(lambda example: {&quot;img&quot;: try_load_image(example[&#39;path&#39;])},writer_batch_size=50) . CPU times: user 51min 42s, sys: 4min 31s, total: 56min 13s Wall time: 1h 10min 31s . Let&#39;s see what this looks like . dataset . Dataset({ features: [&#39;fname&#39;, &#39;year&#39;, &#39;path&#39;, &#39;img&#39;], num_rows: 416944 }) . We have an image column, let&#39;s check the type of all our features . dataset.features . {&#39;fname&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;year&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;path&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;img&#39;: Image(id=None)} . This is looking great already. Since we might have some None types for images let&#39;s get rid of these. . dataset = dataset.filter(lambda example: example[&#39;img&#39;] is not None) . dataset . Dataset({ features: [&#39;fname&#39;, &#39;year&#39;, &#39;path&#39;, &#39;img&#39;], num_rows: 416935 }) . You&#39;ll see we lost a few rows by doing this filtering. We should now just have images which are successfully loaded. . If we access an example and index into the img column we&#39;ll see our image 😃 . dataset[10][&#39;img&#39;] . Push all the things to the hub! . One of the super awesome things about the huggingface ecosystem is the huggingface hub. We can use the hub to access models and datasets. Often this is used for sharing work with others but it can also be a useful tool for work in progress. The datasets library recently added a push_to_hub method that allows you to push a dataset to the hub with minimal fuss. This can be really helpful by allowing you to pass around a dataset with all the transformers etc. already done. . When I started playing around with this feature I was also keen to see if it could be used as a way of &#39;bundling&#39; everything together. This is where I noticed that if you push a dataset containing images which have been loaded in from filepaths by pillow the version on the hub won&#39;t have the images attached. If you always have the image files in the same place when you work with the dataset then this doesn&#39;t matter. If you want to have the images stored in the parquet file(s) associated with the dataset we need to load it without the filename attribute present (there might be another way of ensuring that datasets doesn&#39;t rely on the image file being on the file system -- if you of this I&#39;d love to hear about it). . Since we loaded our images this way when we download the dataset from the hub onto a different machine we have the images already there 🤗 . For now we&#39;ll push the dataset to the hub and keep them private initially. . dataset.push_to_hub(&#39;davanstrien/embellishments&#39;, private=True) . The repository already exists: the `private` keyword argument will be ignored. . Switching machines . At this point I&#39;ve created a dataset and moved it to the huggingface hub. This means it is possible to pickup the work/dataset elsewhere. . In this particular example, having access to a GPU is important. So the next parts of this notebook are run on Colab instead of locally on my laptop. . We&#39;ll need to login since the dataset is currently private. . !huggingface-cli login . _| _| _| _| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _|_|_|_| _|_| _|_|_| _|_|_|_| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _|_|_|_| _| _| _| _|_| _| _|_| _| _| _| _| _| _|_| _|_|_| _|_|_|_| _| _|_|_| _| _| _| _| _| _| _| _| _| _| _|_| _| _| _| _| _| _| _| _| _| _|_| _|_|_| _|_|_| _|_|_| _| _| _|_|_| _| _| _| _|_|_| _|_|_|_| To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token. (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C. Token: Login successful Your token has been saved to /root/.huggingface/token Authenticated through git-credential store but this isn&#39;t the helper defined on your machine. You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default git config --global credential.helper store . Once we&#39;ve done this we can load our dataset . from datasets import load_dataset dataset = load_dataset(&quot;davanstrien/embellishments&quot;, use_auth_token=True) . Using custom data configuration davanstrien--embellishments-543da8e15e8f0242 . Downloading and preparing dataset None/None (download: 2.38 GiB, generated: 2.50 GiB, post-processed: Unknown size, total: 4.88 GiB) to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-543da8e15e8f0242/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121... Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-543da8e15e8f0242/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121. Subsequent calls will reuse this data. . Creating embeddings &#128376; . We now have a dataset with a bunch of images in it. To begin creating our image search app we need to create some embeddings for these images. There are various ways in which we can try and do this but one possible way is to use the clip models via the sentence_transformers library. The clip model from OpenAI learns a joint representation for both images and text which is very useful for what we want to do since we want to be able to input text and get back an image. We can download the model using the SentenceTransformer class. . from sentence_transformers import SentenceTransformer, util model = SentenceTransformer(&#39;clip-ViT-B-32&#39;) . ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy. . This model will encode either an image or some text returning an embedding. We can use the map method to encode all our images. . ds_with_embeddings = dataset.map( lambda example: {&#39;embeddings&#39;:model.encode(example[&#39;img&#39;],device=&#39;cuda&#39;)}, batch_size=32) . We can &quot;save&quot; our work by pushing back to the hub . ds_with_embeddings.push_to_hub(&#39;davanstrien/embellishments&#39;, private=True) . Pushing split train to the Hub. The repository already exists: the `private` keyword argument will be ignored. . If we were to move to a different machine we could grab our work again by loading it from the hub 😃 . from datasets import load_dataset ds_with_embeddings = load_dataset(&quot;davanstrien/embellishments&quot;, use_auth_token=True) . Using custom data configuration davanstrien--embellishments-c2c1f142f272db02 . Downloading and preparing dataset None/None (download: 3.19 GiB, generated: 3.30 GiB, post-processed: Unknown size, total: 6.49 GiB) to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-c2c1f142f272db02/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121... Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-c2c1f142f272db02/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121. Subsequent calls will reuse this data. . We now have a new column which contains the embeddings for our images. We could manually search through these and compare it to some input embedding but datasets has an add_faiss_index method. This uses the faiss library to create an efficient index for searching embeddings. For more background on this library you can watch this youtube video . ds_with_embeddings[&#39;train&#39;].add_faiss_index(column=&#39;embeddings&#39;) . Dataset({ features: [&#39;fname&#39;, &#39;year&#39;, &#39;path&#39;, &#39;img&#39;, &#39;embeddings&#39;], num_rows: 416935 }) . Image search . We now have everything we need to create a simple image search. We can use the same model we used to encode our images to encode some input text. This will act as the prompt we try and find close examples for. Let&#39;s start with &#39;a steam engine&#39;. . prompt = model.encode(&quot;A steam engine&quot;) . We can see what this looks like . prompt . array([-4.68399227e-02, -1.25237599e-01, 1.25164926e-01, 1.23583399e-01, 5.36684394e-02, -2.80560672e-01, 2.89631691e-02, -9.63450074e-01, -1.52872965e-01, -3.83016393e-02, 9.01967064e-02, -5.84575422e-02, 1.04646191e-01, 2.44443744e-01, 1.38233244e-01, -3.97525132e-02, 4.35137331e-01, -4.26820181e-02, -8.48560631e-02, -6.94137365e-02, 6.25562131e-01, 3.68572891e-01, 3.34365219e-01, -3.37864846e-01, -2.53632635e-01, -3.01467925e-01, -1.56484097e-01, 2.94869483e-01, -1.89204350e-01, -1.13111593e-01, -1.46938376e-02, 2.97405511e-01, -2.92487741e-01, 3.56931627e-01, 1.44009173e-01, 7.53008351e-02, -1.02462962e-01, 2.26309776e-01, -3.77506733e-01, 4.75439876e-02, -8.52131844e-03, 7.40285963e-03, -1.36876494e-01, 1.12041593e-01, 3.65501434e-01, -9.37360153e-02, 1.00782610e-01, -3.86462212e-01, -1.39045209e-01, -2.31989667e-01, -2.62126565e-01, 8.75059143e-02, -9.87479314e-02, 7.31039718e-02, -5.99793851e-01, -5.31058311e-01, 1.46116272e-01, 1.58094600e-01, -5.31955510e-02, 1.91384852e-01, 1.16943717e-01, -4.84316409e-01, -1.38332695e-01, 1.76510841e-01, -2.17938051e-01, -1.00890748e-01, -4.45051998e-01, 2.71521568e-01, -1.12926617e-01, -3.37198257e-01, -1.34169117e-01, -7.38745630e-02, -1.23398125e-01, 3.62316787e-01, 9.09636840e-02, -3.20305794e-01, 5.82561374e-01, -3.51719618e-01, -1.05368085e-02, -3.90766770e-01, -3.18382740e-01, 5.37567735e-02, -6.24650240e-01, 2.18755245e-01, 3.51645321e-01, -3.01214606e-02, -8.49011913e-02, -3.29971045e-01, 2.13861912e-01, -1.10820271e-02, -3.52595486e-02, -3.70746814e-02, -1.35805202e+00, 3.35692495e-01, -2.83742435e-02, -1.39813796e-01, 3.66676860e-02, 2.62957454e-01, 2.52151459e-01, -6.14355244e-02, 2.01516539e-01, -4.14117992e-01, -2.58466527e-02, 1.06067717e-01, 3.14981639e-02, -1.45749748e-02, -5.94865866e-02, 2.55122900e-01, -3.30369681e-01, 6.39781356e-04, 1.65513411e-01, 7.37893358e-02, -4.69729975e-02, 3.36943477e-01, 4.38236594e-02, -4.21047479e-01, -1.14590853e-01, 1.49240956e-01, 1.34405270e-01, 3.97198983e-02, -1.20852023e-01, -7.22009778e-01, 1.17442548e-01, -7.35135227e-02, 5.45979321e-01, 1.76602621e-02, 6.59747049e-02, 8.00846070e-02, 3.87920737e-01, -3.57501693e-02, 1.19425125e-01, -2.89906412e-01, -2.84183323e-02, 5.73142338e+00, 1.24172762e-01, -1.59575850e-01, -5.33452034e-02, -1.77120879e-01, 2.14188576e-01, -3.49292234e-02, -4.76958305e-02, -1.05941862e-01, -1.58911452e-01, 1.87136307e-02, -2.16531213e-02, 1.37230158e-01, 4.62583750e-02, 2.19857365e-01, 3.41235586e-02, -3.29913348e-02, 9.88523886e-02, -1.30611554e-01, -1.53349772e-01, 2.20886514e-01, 1.53534949e-01, -4.27889526e-01, -4.12531018e-01, 2.70397663e-01, 1.88448757e-01, 4.66853082e-02, 2.63707846e-01, -9.56512764e-02, -3.26435685e-01, -1.24463499e-01, 4.49354291e-01, -4.17843968e-01, -5.27932420e-02, -1.28314078e-01, -1.19249836e-01, -1.19294032e-01, 3.73742878e-01, 2.07954675e-01, -1.41953439e-01, 3.89361024e-01, -1.99988037e-01, 3.62350583e-01, -8.77851099e-02, -1.08132876e-01, -9.82177258e-03, 1.80039972e-01, 1.35815665e-02, 3.20201695e-01, -1.74580999e-02, -1.08204901e-01, -2.29793668e-01, -2.09628209e-01, 4.13929313e-01, -1.73814282e-01, -4.10574347e-01, -1.59104809e-01, -6.01581074e-02, 6.22577034e-02, -3.67693931e-01, 1.85215116e-01, -2.03229636e-01, -8.92911255e-02, -4.25831258e-01, -1.45366028e-01, 2.45514482e-01, -1.65927559e-01, -2.54413635e-02, -2.91361034e-01, -8.33243579e-02, -4.79405448e-02, 6.35769814e-02, 8.04642588e-02, 5.31384498e-02, 2.50850171e-02, -8.98692310e-02, 4.97757077e-01, 6.37893498e-01, -2.58815974e-01, 4.14507166e-02, 9.45882648e-02, -9.01474580e-02, -9.18833911e-02, -2.48883665e-01, 9.16991904e-02, -2.93194801e-01, -1.49350330e-01, 7.20755905e-02, -9.76985693e-03, -4.70465049e-02, -2.78597653e-01, -7.63949528e-02, -3.14843357e-01, 3.18657011e-01, -3.06758255e-01, -2.06573829e-01, -2.20574200e-01, 1.81351285e-02, 2.57636189e-01, 2.39799708e-01, -2.31798366e-01, -8.34087562e-03, 6.13241374e-01, -2.10393399e-01, 2.52263397e-01, 1.66839644e-01, -2.71174073e-01, 2.31348664e-01, 1.15150154e-01, 2.23357946e-01, 1.37287825e-01, -8.56669843e-02, 3.43877286e-01, -1.09687179e-01, 3.24211985e-01, -4.53893900e-01, -2.30711773e-01, -2.48840563e-02, 1.80964172e-01, 4.73472506e-01, 5.22104502e-01, 9.96741354e-02, 1.87694326e-01, 2.41730541e-01, -2.78556377e-01, 7.48419687e-02, 2.80560136e-01, -1.25464931e-01, 1.51028201e-01, 1.39490321e-01, 5.16689643e-02, 5.30310348e-02, 1.61938250e-01, 3.72225225e-01, -4.49403644e-01, 1.19608052e-01, 2.43661910e-01, 9.89501849e-02, 2.74168640e-01, 4.84039634e-02, -1.19901955e-01, -1.57916725e-01, -2.20868304e-01, 1.03498720e-01, 3.99750322e-01, 1.03758566e-01, 8.08660090e-02, 1.68566346e-01, -3.42532575e-01, 2.51480471e-02, 1.23976640e-01, -2.10433707e-01, 2.81242996e-01, 2.39082754e-01, 2.01786831e-02, 4.61297363e-01, 5.62884361e-02, 2.15039015e-01, -1.65275872e-01, 1.01690084e-01, -4.50959802e-03, -4.46137577e-01, 4.31368239e-02, -4.51804757e-01, -2.26415813e-01, 1.31732523e-01, -2.00945437e-02, 1.77461311e-01, -1.64631978e-02, 4.40553159e-01, 1.41214132e-01, 3.42677176e-01, -2.23303795e-01, -2.10693538e-01, 1.94943929e-03, -2.33348235e-01, 4.64889407e-03, 5.71020804e-02, 1.99669391e-01, 5.72273111e+00, -2.95036316e-01, -5.13455391e-01, 1.87334672e-01, 4.09545094e-01, -7.09135592e-01, 1.89325869e-01, -6.14660345e-02, 3.29098284e-01, 2.82059342e-01, 3.48631829e-01, -9.74263549e-02, -4.83064592e-01, -1.35906041e-04, 3.44773471e-01, -3.56532484e-01, 5.36619090e-02, -1.85481656e+00, 3.87955368e-01, -1.83132842e-01, -1.34021699e-01, -1.84214741e-01, 6.85371086e-02, 1.10808179e-01, -6.64586425e-02, 6.85550272e-02, 1.81145087e-01, -2.15605676e-01, -1.09192222e-01, -7.09795505e-02, 1.77813157e-01, -2.76037157e-01, 2.19184965e-01, -3.35977226e-01, 1.01434961e-01, 4.24576849e-02, 6.37579709e-04, -1.23296835e-01, -6.84914351e-01, 5.02923191e-01, 2.19384342e-01, 4.92008686e-01, -1.94621727e-01, -2.48740703e-01, -1.32586688e-01, -1.77171156e-02, -4.71081585e-03, 1.58246011e-01, -3.27363521e-01, -3.30681592e-01, -2.68038437e-02, -1.85811728e-01, -1.84623767e-02, -3.22798610e-01, 3.07092518e-01, 1.06014945e-01, 3.20541680e-01, -2.55453944e-01, -2.30755419e-01, -1.19963072e-01, -2.04865620e-01, 4.02828932e-01, -3.01321566e-01, 4.01021272e-01, -3.02002877e-01, 1.42853945e-01, 2.94484437e-01, -2.06042349e-01, -3.03069353e-01, -2.83185482e-01, -1.03388466e-02, -1.03018671e-01, 4.25990820e-02, -2.94244856e-01, 3.19168091e-01, 3.89839858e-02, -1.95185751e-01, -9.88216847e-02, -4.01682496e-01, 4.60841119e-01, 1.40236557e-01, 1.49914265e-01, -4.25037295e-01, 2.63067722e-01, 1.31706342e-01, 3.21884871e-01, -2.39963964e-01, 4.01636630e-01, -2.55293436e-02, -7.36447945e-02, -8.34826380e-03, 1.11923724e-01, -2.71807779e-02, -3.35412771e-02, 2.33933121e-01, 3.33954431e-02, 3.56481314e-01, -8.09433609e-02, -1.82573602e-01, 1.75429478e-01, -3.23554099e-01, 9.15928558e-03, 1.54344559e-01, 2.50909716e-01, 1.45193070e-01, 2.48686507e-01, -9.65276286e-02, -2.73654372e-01, 5.46456315e-02, 1.83476061e-02, -1.61773548e-01, -2.97708124e-01, -1.74462572e-01, -1.14246726e-01, 2.32043359e-02, 1.98346555e-01, 2.31929243e-01, -9.74937603e-02, -2.26448864e-01, -6.31427839e-02, 2.23113708e-02, -3.72859359e-01, 2.47197479e-01, -3.65516663e-01, -3.24409932e-01, 1.83964625e-01, -3.17104161e-03, -2.66632497e-01, -1.86478943e-01, 1.11006252e-01, -3.93829793e-02, -3.11926544e-01, 2.88751245e-01, 2.66543150e-01, -1.74334750e-01, -4.89967108e-01, 3.38638097e-01, 2.47487854e-02, -3.66539627e-01, 5.78703731e-03, 1.11349493e-01, -2.60909855e-01, -4.34429348e-02, -4.47440267e-01, 2.80311018e-01, -6.46181554e-02, -2.93976814e-02, -3.02857161e-01, 2.10391358e-03, -3.70345414e-02, 7.15476647e-02, 4.39802915e-01, 2.11817563e-01, 6.87709302e-02, 5.68117499e-01, -2.40518659e-01, 2.59056687e-01, -1.32284269e-01, 1.28509507e-01, -1.94875181e-01, -2.68568173e-02, -7.85035193e-02, -2.49556839e-01, 1.44016743e-01, -2.98127495e-02, -1.41643599e-01, 1.77106410e-02, 1.83453292e-01, -1.39113069e-02, -1.97993904e-01, 3.07995021e-01, 3.31339300e-01, 2.07652867e-01, 1.27762616e-01, 2.26422980e-01, 1.94940835e-01, -4.90801185e-02, -5.35061479e-01, -2.99495637e-01, 3.68627608e-02, -4.15636569e-01, 6.44698918e-01, -4.50457260e-02, 7.05935210e-02, -1.11036956e-01, -1.42384216e-01, -7.05560222e-02, 2.86495592e-03, 3.45641613e-01, -5.66974521e-01, -1.34682715e-01, -2.59017110e-01, 3.27597320e-01, 1.04890786e-01, -3.11988890e-01, -2.32627541e-01, 3.14653963e-02, 2.76591361e-01, 1.66302443e-01, -2.39517853e-01], dtype=float32) . . We can use another method from the datasets library get_nearest_examples to get images which have an embedding close to our input prompt embedding. We can pass in a number of results we want to get back. . scores, retrieved_examples = ds_with_embeddings[&#39;train&#39;].get_nearest_examples(&#39;embeddings&#39;, prompt,k=9) . We can index into the first example this retrieves: . retrieved_examples[&#39;img&#39;][0] . This isn&#39;t quite a steam engine but it&#39;s also not a completely weird result. We can plot the other results to see what was returned. . import matplotlib.pyplot as plt . plt.figure(figsize=(20, 20)) columns = 3 for i in range(9): image = retrieved_examples[&#39;img&#39;][i] plt.subplot(9 / columns + 1, columns, i + 1) plt.imshow(image) . Some of these results look fairly close to our input prompt. We can wrap this in a function so can more easily play around with different prompts . def get_image_from_text(text_prompt, number_to_retrieve=9): prompt = model.encode(text_prompt) scores, retrieved_examples = ds_with_embeddings[&#39;train&#39;].get_nearest_examples(&#39;embeddings&#39;, prompt,k=number_to_retrieve) plt.figure(figsize=(20, 20)) columns = 3 for i in range(9): image = retrieved_examples[&#39;img&#39;][i] plt.title(text_prompt) plt.subplot(9 / columns + 1, columns, i + 1) plt.imshow(image) . get_image_from_text(&quot;An illustration of the sun behind a mountain&quot;) . Trying a bunch of prompts &#10024; . Now we have a function for getting a few results we can try a bunch of different prompts: . For some of these I&#39;ll choose prompts which are a broad &#39;category&#39; i.e. &#39;a musical instrument&#39; or &#39;an animal&#39;, others are specific i.e. &#39;a guitar&#39;. . | Out of interest I also tried a boolean operator: &quot;An illustration of a cat or a dog&quot;. . | Finally I tried something a little more abstract: &quot;an empty abyss&quot; . | . prompts = [&quot;A musical instrument&quot;, &quot;A guitar&quot;, &quot;An animal&quot;, &quot;An illustration of a cat or a dog&quot;, &quot;an empty abyss&quot;] . for prompt in prompts: get_image_from_text(prompt) . We can see these results aren&#39;t always right but they are usually some reasonable results in there. It already seems like this could be useful for searching for a the semantic content of an image in this dataset. However we might hold off on sharing this as is... . Creating a huggingface space? &#129335;&#127996; . One obvious next step for this kind of project is to create a hugginface spaces demo. This is what I&#39;ve done for other models . It was a fairly simple process to get a Gradio app setup from the point we got to here. Here is a screenshot of this app. . . However, I&#39;m a little bit vary about making this public straightaway. Looking at the model card for the CLIP model we can look at the primary intended uses: . Primary intended uses . We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models. source This is fairly close to what we are interested in here. Particularly we might be interested in how well the model deals with the kinds of images in our dataset (illustrations from mostly 19th century books). The images in our dataset are (probably) fairly different from the training data. The fact that some of the images also contain text might help CLIP since it displays some OCR ability. . However, looking at the out-of-scope use cases in the model card: . Out-of-Scope Use Cases . Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. source suggests that &#39;deployment&#39; is not a good idea. Whilst the results I got are interesting I haven&#39;t played around with the model enough yet (and haven&#39;t done anything more systematic to evaluate its performance and biases). Another additional consideration is the target dataset itself. The images are drawn from books covering a variety of subjects and time periods. There are plenty of books which represent colonial attitudes and as a result some of the images included may represent certain groups of people in a negative way. This could potentially be a bad combo with a tool which allows any arbitrary text input to be encoded as a prompt. . There may be ways around this issue but this will require a bit more thought. . Conclusion . Although I don&#39;t have a nice demo to show for it I did get to work out a few more details of how datasets handles images. I&#39;ve already used it to train some classification models and everything seems to be working smoothly. The ability to push images around on the hub will be super useful for many use cases too. . I plan to spend a bit more time thinking about whether there is a better way of sharing a clip powered image search for the BL book images or not... . 1. If you aren&#39;t familiar with datasets. A feature represents the datatype for different data you can have inside a dataset. For example you my have int32, timestamps and strings. You can read more about how features work in the docs↩ .",
            "url": "https://danielvanstrien.xyz/metadata/deployment/huggingface/ethics/huggingface-datasets/faiss/2022/01/13/image_search.html",
            "relUrl": "/metadata/deployment/huggingface/ethics/huggingface-datasets/faiss/2022/01/13/image_search.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Using the 🤗 Hub for model storage",
            "content": "Intro . Part of what the flyswot should take care of is handle machine learning models. The flyswot tool is essentially just a command-line wrapper for the machine learning model. However, these two things the command-line tool, and the model, are kept separate for a number of reasons: . the flyswot tool might have changes separate from the model changing i.e. some new functionality is added or some bug fixed | we might want to update the model based on new training data or a change in the labels used | . We want to be able to release a new model without having to create a new release of the flyswot tool and vice-versa. As a result of this, both of these things are versioned separately. . We might want to keep our model separate because flyswot is made available as a Python Package. Since a computer vision model can be pretty large, we probably don&#39;t want this to be included as a part of the Python package. . How is this currently being done . Currently models are stored in a separate GitHub repository separate from the repository used to store the flyswot code. flyswot has some functionality for checking against this GitHub repository to see if a more recent remote model has superseded a local model. If there is a more recent model available (and a CLI flag indicates that the latest model should be used), then flyswot downloads the new model and stores it in a new directory. . What is wrong with this . Whilst this approach does work okay there is quite a surprising amount of code that is needed to take care of some of this. Currently the option to pass a specific snapshot of a model doesn&#39;t exist. . On the storage side although GitHub is great for storing code there are some limitations to it for storing large files. I&#39;ve created a GitHub action to create a release when new pull requests to update the model are made. This then creates a new release with date information in the filename. Again, this works okay, but there might be a better way... . &#129303; hub to the rescue? . I have already been using the huggingface hub when using other peoples models and uploading fine-tuned transformer models. However, digging around the docs, it seemed like there are a few things in this ecosystem that could be useful for flyswot. . What is the &#129303; hub? . If you haven&#39;t come across the 🤗 hub before, it is essentially a place where models can be uploaded and explored by others. So, for example, if I want a language model trained on Arabic text, I might find it in the hub. The goal is to help avoid duplication of effort and allow other people to use or adapt existing work. . This video gives a helpful overview of navigating the hub and finding models you might be interested in using. . Part of the aim of sharing the flyswot models on GitHub (or the 🤗 hub) is to make them available to other people to use. The 🤗 hub well supports this use case. We can easily share models (including large ones) because of the underpinning use of git-lfs. However, our interest is not only in sharing a model for others to use but also in grabbing the correct model for the flyswot CLI tool easier. Some other components might help here. . The hub vs huggingface_hub library . The 🤗 hub already provides a place to store the model. You can interact with this model using the web interface only but what we want is also to download models using our CLI from the hub. We already have a way to do this with GitHub, so ideally, we want something that works better than our current approach. . This is where the huggingface_hub Python Library might come in. This Python library provides us with various ways of interacting with the hub. This could give us enough ways of interacting with the hub that we can delete some of the code that currently does this with GitHub (and there is nothing nicer than deleting code 😃) . I&#39;ll use the remainder of this blog to see if we can use the 🤗 hub and the [huggingface_hub](https://pypi.org/project/huggingface-hub/) library for this purpose as a replacement for the current approach. . We&#39;ll start by installing the huggingface_hub library . !pip install huggingface_hub . Getting information about a model . One of the things we need to be able to do is get the latest version of the model. One way we could try and do this is by grabbing metadata about the model. This is the current approach taken by flyswot. We can import model_info to do this: . from huggingface_hub import model_info . info = model_info(&quot;distilbert-base-cased&quot;) info . ModelInfo: { modelId: distilbert-base-cased sha: 935ac13b473164bb9d578640e33d9f21144c365e lastModified: 2020-12-11T21:23:53.000Z tags: [&#39;pytorch&#39;, &#39;tf&#39;, &#39;distilbert&#39;, &#39;en&#39;, &#39;dataset:bookcorpus&#39;, &#39;dataset:wikipedia&#39;, &#39;arxiv:1910.01108&#39;, &#39;transformers&#39;, &#39;license:apache-2.0&#39;, &#39;infinity_compatible&#39;] pipeline_tag: None siblings: [ModelFile(rfilename=&#39;.gitattributes&#39;), ModelFile(rfilename=&#39;README.md&#39;), ModelFile(rfilename=&#39;config.json&#39;), ModelFile(rfilename=&#39;pytorch_model.bin&#39;), ModelFile(rfilename=&#39;tf_model.h5&#39;), ModelFile(rfilename=&#39;tokenizer.json&#39;), ModelFile(rfilename=&#39;tokenizer_config.json&#39;), ModelFile(rfilename=&#39;vocab.txt&#39;)] config: {&#39;model_type&#39;: &#39;distilbert&#39;} id: distilbert-base-cased private: False downloads: 3556770 library_name: transformers mask_token: [MASK] likes: 4 model-index: None cardData: {&#39;language&#39;: &#39;en&#39;, &#39;license&#39;: &#39;apache-2.0&#39;, &#39;datasets&#39;: [&#39;bookcorpus&#39;, &#39;wikipedia&#39;]} } . type(info) . huggingface_hub.hf_api.ModelInfo . You can see this gives us back a bunch of information about the model. We could for example grab the date the model was changed: . info.lastModified . &#39;2020-12-11T21:23:53.000Z&#39; . This already gives us what we need for checking if a model is updated in comparison to a local model already downloaded by the flyswot CLI. However we might be able to cut out some of this checking work. . Lets see if there are other ways we can do this in the library. Since huggingface_hub requires git-lfs lets start by installing this. . !apt install git-lfs . Reading package lists... Done Building dependency tree Reading state information... Done git-lfs is already the newest version (2.3.4-1). 0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded. . We also need to make sure we have git-lfs setup . !git init &amp;&amp; git lfs install . hint: Using &#39;master&#39; as the name for the initial branch. This default branch name hint: is subject to change. To configure the initial branch name to use in all hint: of your new repositories, which will suppress this warning, call: hint: hint: git config --global init.defaultBranch &lt;name&gt; hint: hint: Names commonly chosen instead of &#39;master&#39; are &#39;main&#39;, &#39;trunk&#39; and hint: &#39;development&#39;. The just-created branch can be renamed via this command: hint: hint: git branch -m &lt;name&gt; Initialized empty Git repository in /Users/dvanstrien/Documents/daniel/blog/_notebooks/.git/ Updated git hooks. Git LFS initialized. . Downloading files from the hub . We can use hf_hub_url to get the url for a specific file from a repository . from huggingface_hub import hf_hub_url . onnx_model_url = hf_hub_url(&quot;davanstrien/flyswot-test&quot;, &quot;2021-09-22.onnx&quot;) onnx_model_url . &#39;https://huggingface.co/davanstrien/flyswot-test/resolve/main/2021-09-22.onnx&#39; . We can pass this url to cached_download, this will download the file for us if we don&#39;t have the latest version, we can also specify a place to download the file. This is important so we can make sure we put the file somewhere flyswot can find. . from huggingface_hub import cached_download . cached_download(onnx_model_url, cache_dir=&quot;.&quot;) . &#39;./de9d2ce054e3e410e3fc61b5c2ad55da7861d30e3b90aa018615b7d902e6e51e.1300a5792e44de2c59f4d25c4f7efd447ef91d69971a121e6b4df8b95047ad7c&#39; . If we try and download this again it won&#39;t download, and will instead return the path to the model . path = cached_download(onnx_model_url, cache_dir=&quot;.&quot;) path . &#39;./de9d2ce054e3e410e3fc61b5c2ad55da7861d30e3b90aa018615b7d902e6e51e.1300a5792e44de2c59f4d25c4f7efd447ef91d69971a121e6b4df8b95047ad7c&#39; . Downloading all files from the hub . This is quite close to what we want our current approach requires us to get a bunch of different files in a folder. To replicate this we can instead use snapshot_download . from huggingface_hub import snapshot_download . Let&#39;s see what this does . ?snapshot_download . Signature: snapshot_download( repo_id: str, revision: Union[str, NoneType] = None, cache_dir: Union[str, pathlib.Path, NoneType] = None, library_name: Union[str, NoneType] = None, library_version: Union[str, NoneType] = None, user_agent: Union[Dict, str, NoneType] = None, proxies=None, etag_timeout=10, resume_download=False, use_auth_token: Union[bool, str, NoneType] = None, local_files_only=False, ) -&gt; str Docstring: Downloads a whole snapshot of a repo&#39;s files at the specified revision. This is useful when you want all files from a repo, because you don&#39;t know which ones you will need a priori. All files are nested inside a folder in order to keep their actual filename relative to that folder. An alternative would be to just clone a repo but this would require that the user always has git and git-lfs installed, and properly configured. Note: at some point maybe this format of storage should actually replace the flat storage structure we&#39;ve used so far (initially from allennlp if I remember correctly). Return: Local folder path (string) of repo snapshot File: ~/miniconda3/envs/blog/lib/python3.8/site-packages/huggingface_hub/snapshot_download.py Type: function . This will do something similar to cached_download but will instead do it for a whole model repository. If we pass our repository it will download the directory if we don&#39;t have the latest version of the files, if for example, the model has been updated. . model = snapshot_download(&quot;davanstrien/flyswot-test&quot;, cache_dir=&quot;.&quot;) . model . &#39;./davanstrien__flyswot-test.main.e54a7421f5e5eb240783452ab734288f252bb402&#39; . If we look inside this directory we can see we have the files from the repository. . !ls {model} . 2021-09-22.onnx README.md modelcard.md vocab.txt . If we try and download it again we just get back the directory path without having to download the files again. . model = snapshot_download(&quot;davanstrien/flyswot-test&quot;, cache_dir=&quot;.&quot;) model . &#39;./davanstrien__flyswot-test.main.e54a7421f5e5eb240783452ab734288f252bb402&#39; . This gives a replication of what we currently have setup for flyswot in terms of downloading models. There are a few extra things we might want though to be able to make flyswot more flexible. First though let&#39;s look at how we can upload to the model hub. . Uploading to the hub . At the moment flyswot models are uploaded to a GitHub repository which then creates a release. It would be nice to simplify this and upload directly at the end of model training. For this we can use the Repository class. . from huggingface_hub import Repository . ?Repository . Init signature: Repository( local_dir: str, clone_from: Union[str, NoneType] = None, repo_type: Union[str, NoneType] = None, use_auth_token: Union[bool, str] = True, git_user: Union[str, NoneType] = None, git_email: Union[str, NoneType] = None, revision: Union[str, NoneType] = None, private: bool = False, skip_lfs_files: bool = False, ) Docstring: Helper class to wrap the git and git-lfs commands. The aim is to facilitate interacting with huggingface.co hosted model or dataset repos, though not a lot here (if any) is actually specific to huggingface.co. Init docstring: Instantiate a local clone of a git repo. If specifying a `clone_from`: will clone an existing remote repository, for instance one that was previously created using ``HfApi().create_repo(name=repo_name)``. ``Repository`` uses the local git credentials by default, but if required, the ``huggingface_token`` as well as the git ``user`` and the ``email`` can be explicitly specified. If `clone_from` is used, and the repository is being instantiated into a non-empty directory, e.g. a directory with your trained model files, it will automatically merge them. Args: local_dir (``str``): path (e.g. ``&#39;my_trained_model/&#39;``) to the local directory, where the ``Repository`` will be initalized. clone_from (``str``, `optional`): repository url (e.g. ``&#39;https://huggingface.co/philschmid/playground-tests&#39;``). repo_type (``str``, `optional`): To set when creating a repo: et to &#34;dataset&#34; or &#34;space&#34; if creating a dataset or space, default is model. use_auth_token (``str`` or ``bool``, `optional`, defaults to ``True``): huggingface_token can be extract from ``HfApi().login(username, password)`` and is used to authenticate against the hub (useful from Google Colab for instance). git_user (``str``, `optional`): will override the ``git config user.name`` for committing and pushing files to the hub. git_email (``str``, `optional`): will override the ``git config user.email`` for committing and pushing files to the hub. revision (``str``, `optional`): Revision to checkout after initializing the repository. If the revision doesn&#39;t exist, a branch will be created with that revision name from the default branch&#39;s current HEAD. private (``bool``, `optional`, defaults to ``False``): whether the repository is private or not. skip_lfs_files (``bool``, `optional`, defaults to ``False``): whether to skip git-LFS files or not. File: ~/miniconda3/envs/blog/lib/python3.8/site-packages/huggingface_hub/repository.py Type: type Subclasses: . I&#39;ll use flyswot-test as a way of playing around with this. To start with we can use Repository to clone the current version of the model. . repo = Repository(local_dir=&quot;flyswot-models&quot;, clone_from=&quot;davanstrien/flyswot-test&quot;) . Cloning https://huggingface.co/davanstrien/flyswot-test into local empty directory. . repo . &lt;huggingface_hub.repository.Repository at 0x7fb10cccbc10&gt; . We&#39;ll need to be logged in to push changes . from huggingface_hub import notebook_login notebook_login() . To start with let&#39;s mock making a change to some of the repo files and seeing how we can upload these changes. We can use the Repository class as a context manager to make changes and have them committed to our model repository. Here we update the vocab file to add a new label. . with Repository( local_dir=&quot;flyswot-models&quot;, clone_from=&quot;davanstrien/flyswot-test&quot;, git_user=&quot;Daniel van Strien&quot;, ).commit(&quot;update model&quot;): with open(&quot;vocab.txt&quot;, &quot;a&quot;) as f: f.write(&quot;new label&quot;) . /Users/dvanstrien/Documents/daniel/blog/_notebooks/flyswot-models is already a clone of https://huggingface.co/davanstrien/flyswot-test. Make sure you pull the latest changes with `repo.git_pull()`. Pulling changes ... To https://huggingface.co/davanstrien/flyswot-test e54a742..18d149e main -&gt; main . This could already be used at the end of our training script. Currently I have some util files that package up the model vocab, convert Pytorch to ONNX etc. This could easily be adapted to also push the updated model to the hub. There is only one thing we might still want to add. . Adding more metadata: creating revision branches . Currently flyswot uses filenames to capture metadata about the model version. The models are versioned using calendar versioning. This works okay but we might be able to manage this in a slightly better way. One of the nice features that hf_hub (the Python library) offers that flyswot currently doesn&#39;t support well is being able to pass in a specific revision when using snapshot_download. This would then allow someone to run a specific older version of the model. This might be useful for various different scenarios. To do this we&#39;ll create a revision branch for the date the model was created. All that we&#39;ll do now is pass in a formatted date as the revision. . from datetime import datetime . date_now = datetime.now() now = date_now.strftime(&quot;%Y-%m-%d&quot;) now . &#39;2021-12-30&#39; . with Repository( local_dir=&quot;flyswot-models&quot;, clone_from=&quot;davanstrien/flyswot-test&quot;, git_user=&quot;Daniel van Strien&quot;, revision=now, ).commit(f&quot;update model {now}&quot;): for model in Path(&quot;.&quot;).glob(&quot;.onnx&quot;): model.rename(f&quot;{now}-model.onnx&quot;) . /Users/dvanstrien/Documents/daniel/blog/_notebooks/flyswot-models is already a clone of https://huggingface.co/davanstrien/flyswot-test. Make sure you pull the latest changes with `repo.git_pull()`. Checked out 2021-12-30 from 2021-12-30. Your branch is up to date with &#39;origin/2021-12-30&#39;. Pulling changes ... Several commits (2) will be pushed upstream. The progress bars may be unreliable. Everything up-to-date . This creates a new revision branch for the current date. Since I also want to have the default branch be the current model we would also push the same model to the default branch. This would then mean that we end up with a bunch of different branches with model snapshots that could be passed in but for the default behavior we can easily grab the latest model by not specifying a revision. .",
            "url": "https://danielvanstrien.xyz/metadata/deployment/huggingface/flyswot/2021/12/30/hf-hub-model-storage.html",
            "relUrl": "/metadata/deployment/huggingface/flyswot/2021/12/30/hf-hub-model-storage.html",
            "date": " • Dec 30, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "flyswot",
            "content": "My previous series of posts/notes following the full stack course ended slightly abruptly. However, I didn’t give up on the dream of trying to get more experience of ‘deploying’ machine learning in a GLAM setting! One of the things I have been focusing on recently is a project which gave me the chance to use machine learning in somewhat of a production context. . This blog post will be a very short tl;dr on this project. . Detecting fake flysheets using computer vision . The Library has ‘legacy’ digitised content of manuscripts. Due to some limitations of the legacy system on which these manuscripts were shared many of the images have incorrect metadata. The metadata for these images is partially stored in the filename for the image. In particular many images which didn’t fit into an available category were given ‘end flysheet’ labels (this basically means part of the filename contains the string fse). These images may actually be other things like a frontcover, a scroll image, a box etc. . . The library is moving to a new platform which won’t have these restrictions on possible page types. As a result there is a need/desire to find all of the images which have been given a ‘fake’ flysheet label and correct this label with the correct label. . This is a task where computer vision seems like it might be helpful. . The desired outcome . The desire of this project is to be able to use a computer vision model to check a bunch of image directories and see if there are any ‘fake’ flysheets. There are some additional constraints on the project: . $$$ this isn’t a funded project so can’t involve spending a bunch of money | related to the above, the approach to annotation has to be pragmatic - no Mechanical Turk here | the machine learning should fit into existing workflows (this is something we have/are spending a lot of time on) | . Since this is intended to be a tl;dr I won’t go into more detail here about all of these requirements here. . flyswot . The approach we ended up with is to deploy a model using a command line tool that we’ve called flyswot. This tool can be pointed at a directory and it will recursively check for images which contain the fse pattern in the filename. These images are then checked using a computer vision model that looks check whether an image is a ‘real’ flysheet or a ‘fake’ flysheet. . What I have learned (so far) . This project has been a great way of turning some of the theory of ‘production’ ML into practice. In particular I have learned: . I’m super paranoid about domain drift. | (some) of how to use ONNX | More robust testing approaches | DVC (data version control) | and a bunch more things… | . Most of these things are being documented elsewhere and will be available to share at some point in 2022. However, I will try and use this blog to document small things I’ve learned along the way too. These notes are mainly for myself. There are a lot of little things I’ve picked up from doing this project that I will forget if I don’t spend a bit of time writting up. .",
            "url": "https://danielvanstrien.xyz/deployment/glam/flyswot/2021/12/22/flyswot.html",
            "relUrl": "/deployment/glam/flyswot/2021/12/22/flyswot.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Full Stack Deep Learning notes on troubleshooting and debugging",
            "content": "Introduction to my notes . These are my notes from the full-stack deep learning course. . They have a focus on the GLAM setting, which might have different requirements, incentives and resources compared to business deployments of ML projects. These notes are for my use primarily. I am posting them to make myself more accountable for making semi-coherent notes. . These are my notes, written in a personal capacity - my employer denounces all my views . Why talk about debugging? . 80-90% of time spent on debugging? Useful to have some approaches for doing this work. . Why is deep learning debugging hard? . Small implementation details . Often you may get an error that results in weird results but the code still runs. You don’t have an exception to help you debug. Often need to manually dig around to find the cause of the error. Example: . images = glob.glob(&#39;path/to/images/*&#39;) labels = glob.glob(&#39;path/to/labels/*&#39;) . Will not return images and labels in the correct order because of how glob is implemented in python. This doesn’t throw an error but the model won’t learn anything. . Hyperparamers . Small changes to hyperparameters can make the difference between model training well and not training at all. . Data/Model fit . Performance on model on one set of images i.e. imagenet might not translate to the data you are working with. Transfer learning will often help but it might be unclear exactly how the model/new tasks translate to new types of data/problems. # . Models vs Dataset construction . In an academic setting, a lot of thought is given to choosing models/algorithms. Often this is less of a focus when deploying machine learning in a production setting where there will be many more challenges around constructing a dataset. . Notes: I think there is a shift here is going from training n a validation/training set to having in the back of your head that predictions will be made ‘in the wild’. . There can be a fair bit of variation in GLAM datasets which also potentially makes data drift hard to track . Architecture Selection . It can be overwhelming to pick between all the different types of model architectures. Suggestion to start out with a few different flavour and change if needed: . Type Model Maybe move to? . Images | LeNet-like model | ResNet | . Sequences | LSTM with one hidden layer | Attention | . other | Fully connected neural net with one hidden layer | Problem dependent | . Sensible defaults . version zero of your model could start with: . optimizer: Adam with learning rate 3e-4 | activations: relu (FC and Convultional models), tanh (LSTMS) | Intitilization: He et al. normal(relu), GLorot normal (tanh) | Regulirization: none | data normalization: none | . Consider simplifying the problem . start with a subset of the training data | use a fixed number of objects, classes etc. | create a simpler synthetic training set | . Model evaluation . tl;dr apply bias-variance decomposition . . In statistics and machine learning, the bias-variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters. The bias-variance dilemma or bias-variance problem is the conflict in trying to simultaneously minimise these two sources of error that prevent supervised learning algorithms from generalising beyond their training set:[1][2] The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff . Test error = irreducible error + bias + variance + val overfitting | . Here we try and see the difference in error between our training data and some kind of baseline, i.e. human performance, to see how much we are underfitting. We also compare validation to training to see how much overfitting. . This assumes training, validation, and test come from the same distribution. . Notes: tracking the impact of distribution shift on model performance helpful here. . Example . Error source value . Goal | 1% | . Train error | 20% | . Validation error | 27% | . Test error | 28% | . The difference between our goal and our train error shows underfitting | The difference between train and validation also shows we’re also overfitting | test and validation error difference is okay | . 🤷‍♂️ Both overfitting and underfitting - how to decide what to do next… . prioritizing improvements . It seems hard to both deal with under and overfitting. The suggested process is to follow the following steps: . 1) address underfitting 2) address overfitting 3) address distribution shift 4) re-balance data (if applicable) . How to address under-fitting (i.e. reduce bias) . These suggestions are listed in order of what to try first. . Make the model bigger (i.e. resnet34, to resnet50) | reduce regulirization | error analysis | choose different model architecture (i.e. bump from LeNet to ResNet). This can introduce new problems, so be careful here | add features 😬 sometimes this can really help even in deep-learning | . Addressing over-fitting . add more training data (if possible) | add normalisation | data augmentation | regularisation | error analysis | choose better model architecture | tune hyperparameters | early stopping | remove features | reduce model size | . Addressing distribution shift . a) analyse test-val set errors - why might these errors be there in the validation set. b) analyse test-val set errors and synthesise more data c) domain adaptation . Error analysis . Try and find the different sources of error in test/val and see how easily they can be fixed + how much they contribute to the error. . This could be particularly important when domain expertise might help track down why some of these errors occur. This might also be used to reevaluate labels being targeted if they often cause confusion. Fastai has some nice methods for this most_confused to show which labels are often confused. If this confusion is reasonable, you may a) either not care too much about these mistakes b) collapse two labels into one label. . Domain adaptation . supervised: fine-tune pre-trained model | un-supervised: more in the research domain at the moment? | . Tuning Hypermaters . Model and optimiser choices? . how many layers | kernel size | etc. | . How to choose what to tune? some hyperparameters are more essential but its often hard to know which are going to be more important . rules of thumb of what is worth tuning . Hyperparamter likely senstivity . Learning rate | high | . Learning rate schedule | high | . Optimiser choice | low | . other optimiser parameters | low | . batch size | low (what fits onto GPU) | . weight initialisation | medium | . loss function | high | . model depth | medium | . layer size | high | . layer params | medium | . Weight of regularisation | medium | . nonlinearity | low | . Approaches to hyperparameters . Manual: . focus on what is likely to make a difference conceptually | train and evaluate model | guess a better parameters | can be good as a starting point + combined with other approaches | . Grid search . easy to implement | but expensive and you need to have a good sense of sensible range points | . Random search . often better than grid search for the same number of runs | not very easy to interpret | . Bayesian approaches . There are nice frameworks for doing this | generally most efficient t | can be hard to implement | . Overarching notes . personally, I think some of these suggestions might be better suited in settings where ml will be the product/service. In GLAMS, this might sometimes be the case, but there is also low hanging fruit using ml as a ‘tool’. In this case, I think it might make more sense to start with the proper implementation of ResNet as a starting point rather than coding your own net from scratch. . The discussion on error analysis was excellent and is a valuable framework for working out how to prioritise improvements. .",
            "url": "https://danielvanstrien.xyz/deployment/glam/full%20stack%20deep%20learning%20notes/2021/03/13/full-stack-deep-learning-lesson-7-debugging.html",
            "relUrl": "/deployment/glam/full%20stack%20deep%20learning%20notes/2021/03/13/full-stack-deep-learning-lesson-7-debugging.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Full Stack Deep Learning notes on machine learning infrastructure and tools",
            "content": "Introduction to my notes . These are my notes from the full-stack deep learning course. . They have a focus on the GLAM setting, which might have different requirements, incentives and resources compared to business deployments of ML projects. These notes are for my use primarily. I am posting them to make myself more accountable for making semi-coherent notes. . These are my notes, written in a personal capacity - my employer denounces all my views . The Dream . The dream is to build a system that has a tight feedback loop between data -&gt; model -&gt; inference -&gt; more data -&gt; better model -&gt; better predictions … . One note here: this is a nice dream for some ml applications, but others might be more ‘one-off tools’ where this might be less important. For example, tools developed to help work with the ingest of a new collection. . Infrastructure buckets . A bunch of tools that help get closer to this dream broadly fall into three categories . Data Training Deployment . Data Sources | Compute | CI/CD | . Databases | Resource management | Testing | . Data Version control | Frameworks for distributed training | Tooling for edge deployment | . Labelling tools | Experiment management | Web deployment | . Feature stores | Hyperparmater tuning | monitoring | . There is a slightly overwhelming range of tools here. I think some of these only make sense at a certain level of scale, i.e. auto-scaling inference across multiple regions is probably not a scenario many (any?) glam institutions are dealing with at the moment for their ml pipelines. There are probably some notable exceptions here; internet archive, Wikipedia etc. . Editors . I fast forward through this bit as I’m mostly happy with my choices here! . Compute needs . Two primary compute needs: development and training/evaluation. . The development one intended to make experimentation easy. Do basic training etc. . Training compute: multi GPUs to do parameter sweeps etc. I think financial constraints often make this part less possible in a GLAM setting. There may also be some benefits to restricting compute slightly. Since GLAMs might (maybe should) be also considering how to share models approaches it migth not be helpful to immediately jump to models which require multiple GPUs to fine-train when a model with much more constrained resources might only be slightly worse. . Cloud vs local . Nvidia default for GPU | New architecture every year: Kepler -&gt; pascal -&gt; volta -&gt; turing -&gt; ampere | Server and then ‘enthusiast’, then consumer versions of the cards | Business only supposed to use primarily server cards. I don’t know what the definition of business is here, but it seems this is reguarly ignored | . A note here; I think a lot of use cases for deep learnings in GLAMs could probably be addressed with a dev box with one 3090 card and a bunch of storage. This may also be slightly less annoying for IT compared to going into the cloud. . The cost of cloud gets expensive compared to local deep learning rig. . Spot instances can make things cheaper. Can be fine for doing experiments in parallel and time saving is important. . On prem scaling can be challenging at some points. For a Glam lab setting having one/two GPUs still seems prefferable for ‘local work’ there might also be a desire to facilate access to collections for researhcers etc which may work better through the cloud. . Resource management . Managing environments, resources . could use simple scripts to manage this | slurm for scheduling jobs | docker - will come up later in the course | ‘all in one solutions’ - possibly overkill | . Frameworks . Deep learning frameworks put on a chart with good for production and good for development chart. . . I think in the context of using deep learning in a GLAM setting, there should be a lot of emphasis placed on using frameworks that focus on making things easier to understand for a broader audience. I think GLAMs should be trying to upskill a wide range of staff with some of the skills associated with ml and a core of staff with the skills to develop all/some/most things themselves as domain experts. For this, to work, I there needs to be an emphasis placed on the accessibility of model development. i.e. prefer to use a framework training loop (fastai/Keras/lightning) rather than roll a custom loop since this probably won’t add much but will often make code harder for others to understand. . Experiment tracking . Deep learning usually involves at least some experimentation with different model architectures, data augmentations, hyper parameters etc. Keeping track of this can be a pain to do manually. . Various options are available. Weights and Biases seem to be the most liked commercial offering. There are a bunch of open source projects that do this too. . Hyperparameter Optimization . Various options for doing this, including local/open source offerings. . Increasingly a move towards ‘end-to-end’ platforms for helping across the ml lifecycle. . My take away from this lecture is that there can be an overwhelming range of tools available. Probably these tools should be introduced slowly into a workflow. From my perspective, the experiment tracking components should be a priority. .",
            "url": "https://danielvanstrien.xyz/deployment/ethics/glam/full%20stack%20deep%20learning%20notes/2021/03/13/full-stack-deep-learning-lesson-6-tooling.html",
            "relUrl": "/deployment/ethics/glam/full%20stack%20deep%20learning%20notes/2021/03/13/full-stack-deep-learning-lesson-6-tooling.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Full Stack Deep Learning notes on machine learning projects",
            "content": "Introduction to my notes . These are my notes from the full-stack deep learning course. . They have a focus on the GLAM setting, which might have different requirements, incentives and resources compared to business deployments of ML projects. These notes are for my use primarily. I am posting them to make myself more accountable for making semi-coherent notes. . These are my notes, written in a personal capacity - my employer denounces all my views . Lecture 5: Machine learning projects . This is the first ’proper’ lesson of the course with the previous ones focused on a review of deep learning approaches. This lecture focused on an overview of machine learning projects (beyond training a model) . 85% of machine learning projects fail . This stat might be a bit questionable in general but likely has some truth to it. How does this differ in a GLAM setting? It might be more evident in a business setting if a project is successful or not based on a business need. In a GLAM setting, a project that is never “deployed” might still by deemed successful. It is also my impression that there is a much closer overlap between academic/research projects and “business” projects in a GLAM setting. This might mean that the successful outcome is a published paper rather than a deployed model solving a “business” problem. . Lifecycle of a machine learning project . Covered in this lecture: . How to think about all of the activities in an ML project | Prioritising projects: what is feasible? | Archetypes: categories of projects and the implications for project management | Metrics: how to pick a single number to optimise | Baselines: how to know if a model is performing well | . Stages in the lifecycle are not linear but loop back and forth. i.e. you go back to project planning once you’ve started collecting data/labels because you realise the labels you collected aren’t feasible to collect. This might be more important in a GLAM setting? Data collection usually takes place in a different way; for example, I don’t know of any GLAMS that have extensively used paid crowdsource workers to collect training labels. This means that the label collection options might have to factor in more considerations around what is easy enough/fun enough. . Managing trade-offs between, e.g. accuracy and model latency . Cross-project infrastructure and staffing . Should there be a separate team? Where should they sit in an organisation? . Domain expertise is particularly important in a GLAM setting, so there might be risks with creating a team that does ML without this subject/domain expertise. On this point, I’m persuaded a lot by what fastai suggest about making deep learning accessible to domain experts. GLAMs should probably have some people who can support these efforts, but having a bunch of people hired as “the data science people” might be a bad idea if they don’t work closely with others across an institution. Maybe I’m naive here that GLAMs would be willing to invest in broad training of many staff rather than hiring a few people. 🤷‍♂️ . Prioritising machine learning projects . How to decide what a suitable candidate for an ML project is? . A (general) framework for prioritising projects . One way to identify projects is to try and find things that have a high impact and relatively low cost. . Mental models for high-impact ML projects . Four mental models introduced for thinking about impact: . Where can you take advantage of cheap prediction | Where is there friction in your product? | Where can you automate complicated manual processes? | What are other people doing? . | Cheap predictions (1) | For number 1, the idea of “cheap predictions” is useful even if the economics are different. Cheap predictions might be more about unlocking tasks that wouldn’t otherwise be possible to do. Rather than using ML to replace something, it might make it feasible to do something which otherwise would be too expensive for the relative benefit. . Where is there friction in your product? (2) | This objective seems important, but my examples of where GLAMS could make things easier for users would often not be simple projects to implement. They may also sit outside of what would be simple for a single institution to do on their own, or they may have already outsourced these systems. For example, cataloguing/discovery systems. . Where can you automate complicated manual processes? (3) | For number 3, I would adjust this slightly to “where can you automate simple/boring manual processes” I think this is an area where there is a bunch of low hanging fruit for GLAMs. It is tempting to jump to more jazzy end-to-end models that will fully revolutionise cataloguing etc., but there are loads of tasks that GLAM staff have to do that: . might be boring | are probably difficult to solve using “traditional” programming i.e. writing a rules-based system would be difficult | which has enough ’volume’ that it’s worth automating. | these are often also “staff facing” I think this reduced the potential risks of things going wrong versus something user-facing, for example, showing users a bunch of automatically generated tags for collection items? | . What is ML good at? . I think this is a lovely consideration. I also believe this is something where energy shoud be spent in spreading the gospel of what ML is good at to staff working in GLAMS. This will make it much easier for people to self diagnose that something they are working with could be a good candidate for ml to help with and also helps demystify. . The mental model of software 2.0 is a bit of a meme, but I think there is a lot of truth in there. The rules you would need to write to solve, for example, a relatively simple computer vision classification task, could be super complicated. Going back to my domain experts should use ml point: I think that there are places where ml could be used as a “tool” in a glam setting in a much more mundane way for much more modest things. The actual coding ability level is potentially lower for this than creating complex rules-based approaches to the same task. . | What are other people doing? . | This should be an obvious one and ideally this will become more feasible as GLAMS get better at collaborating on sharing code/data (particularly labelled training data). . An interesting point here about how to find out what others are doing. I think GLAMs are good at shouting about ML stuff at the moment because it’s super trendy. Still, there might be a lack of shouting about unsuccessful projects that others could learn from and/or shouting about more boring applications of ml that might be high impact but are not as obviously glamorous (pardon the pun). . Feasability . What drives the feasibility of a project? . Data availability . how much data do you need? | how stable is the data - if data updates in the real world all the time this will need to be replicated in the training data. This might be an argument for starting with more contained ’collections’ even if these collections are slightly arbitrary. | how secure does the data need to be. Copyright might be a challenge. GLAMs should push Text and Data Mining Exemptions more forcefully for machine learning projects | . | Accuracy requirements . how costly are wrong predictions? This obviously depends on the problem, but it might be an argument for starting with “staff facing” or “stand alone” projects where there is either more human checking involved or the system’s limitations can be made more explicit. | how frequently does the system need to be correct to be useful? | ethical implications of the model’s accuracy - what is the harm of a model getting things wrong. Particularly important if the accuracy is not evenly distributed. For example, using Named Entity Recognition across a multi-language dataset with accuracy much higher for English compared to other results. | pushing up accuracy tends to push up project costs because of more training data required, better label quality. | . | Feasibility of the method . What is still hard for ML? One rule of thumb suggested for what is possible with ml given: | . “Pretty much anything that a normal (sic) person can do in &lt;1 sec, we can now automate with ai” . Leaving aside the loaded “normal person” this rule doesn’t seem to work in many cases, as is pointed out in the lecture. There are a bunch of things that are easy for many people to do but aren’t easy at all for an ML system. Jokes, sarcasm etc. are all challenigng for ml. . Still challenging: . Pure unsupervised learning difficult | Reinforcement learning (outside of academia) | . | How to run a feasibility assessment? . Are you sure you need ML at all? Many problems can still be solved with regex… | Put in work up-front to define success criteria with all of the stakeholders: think this one is super important. It could be tempting to be a bit hazy on this and make the success criteria “we did a cool ml project”. This might be okay if the output is a paper/blog post but probably not if you’re going to deploy something into ’production’ | Consider the ethics: consider what weird/bad results commercial black box APIs might give with heritage data. Consider whether you are pilling onto existing biases in your collection. Some of the labels/metadata GALMS use/have used is shitty, so maybe don’t aim to get ml to produce even more of these… | Do a literature review | Try to rapidly build a labelled benchmark dataset | build a minimal product to see if the problem seems viable (not using ml) | . Machine learning product archetypes . Software 2.0 . examples: code completion, recomender systems etc. In a GLAM setting I gues this would incldue catlogues, disvoery systems, document summerization etc. . . Human-in-the-loop . Examples: turn sketches into slides, email auto-completion. In a GLAM setting, there are many potential examples, ML suggestions for cataloguing, document processing/classification/segmentation with some human review. . Autonomous systems . Examples: self driving cars etc. My 2 c is that most (maybe all) GLAM institutions don’t have sufficient experience with ML to really consider this. Exceptions may include robotics systems for some processes, but these are unlikely to be done “in house”. Maybe I am missing something here? . Data flywheel . sometimes you can get a positive feedback loop where you get more data -&gt; better model -&gt; more users - more data… . Things to consider. Where can this loop fail? The “more users” criteria might not translate in a glam setting, but perhaps building useful models/datasets that can then be extended on by other institutions is a similar positive feedback loop you can see. Transkribus is an example of this; they built a platform that got more data, which improved models that drove more people to consider it an option. . Product design . Principles from apple . How can you learn from your users? &lt;- This is super relevant! . Explicit feedback (suggest less pop music) | implicit feedback (like this song)+ calibration during setup (scan your face for FaceID) | corrections (fix model mistakes) &lt;- I think this is particularly important for successfully integrating into GLAM workflows and building trust that ML isn’t going to be making decisions without possibilities for correction | . How should your app handle mistakes? . limitations (let users) know where to expect the model to perform well/badly | corrections (let users succeed even if the model fails), i.e. don’t suggest labels that can’t be ignored | attribution (help users understand where suggestions come from) | confidence (help users gauge the quality of results) | . Metrics . How to pick a single number to optimise? . The real world is messy, and you often care about lots of metrics. If you need to compare models/approaches, its very helpful to have a single number to optimise. This can be done by combining different metrics. the optimisation can change as things improve. This single metric mindset is good for the model training part but should be balanced against other considerations as you move beyond this stage. . Common metrics to pick from . accuracy | precision | recall | . Don’t think we can generalise to saying one metric is more critical for glams, it really depends on the application. . Possible combinations weighted average between precision/recall. . More common in real-world is to use threshold n-1 metrics, evaluate to the nth. The idea here is that you pick one metric like accuracy to focus on but then try and improve that without making other things like the latency of the model much worse. This will be discussed more in future lectures. An example: you need a model that will fit into the memory of a particular edge device, so you start with something that will do this and optimise from there. . Choosing which metrics to the threshold . What should you optimise vs what should threshold? . Choosing metrics to threshold: . domain judgement: what can you engineer around? For example, maybe you can deal with a lower latency model by caching to get away with worse performance. In a glam setting, this will depend on the application, but if you are ’batch processing’ an existing collection you might not care about latency as much as if you are trying to use something which interacts with people in real-time, i.e. a cataloguing system. | which metrics are least sensitive to model choice? | which metrics are closest to desirable values. | . Choosing threshold values: . domain judgement - what is an acceptable tolerance downstream? | how well does a baseline model do? | how well does the model do already - if your model barely works, then its not going to be worth spending time trying to make it go faster… | . Once you have this baseline, you can eliminate models by successive metrics thresholds. i.e. first drop models with recall below n, then choose models which do well on precision given this minimal recall value. . It is important to try and be principled about which metric/combo you are going to use upfront as it will lead to different decisions being made about model choice etc. This might change, but its important to have something to drive decisions. . Baselines . Why? . Give you a lower bound for expected model performance | the tighter the lower bound, the more useful | . Having this baseline gives you some sense of how well/or crappy you are doing—otherwise its hard to make a judgement about next steps when training a model. . Where to look for baselines? | published results, make sure these are fair comparisons. | Scripted baselines e.g. rule-based approaches | could also use a simpler machine learning model as a baseline | or even take some kind of average of the dataset. sklearn has a bunch of dummy estimators useful for getting a baseline https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation | . How to create good human baselines random people quality might be low | might require domain expertise | trainign people on the task can improve quality | ensembling of labels | . | Conclusion . ML lifecycle not linear | projects that are high impact and wrong predictions not so bad | secret sauce is to try and create automated data flywheels getting feedback from users of the model. much of this is about good product design | metrics: having a central metric is important as a starting point | baselines: help direct effort to know how the model is doing and where to direct energy | . suggested resources . https://stanford-cs329s.github.io/syllabus.html | https://developer.apple.com/design/human-interface-guidelines/machine-learning/overview/introduction/ | https://developers.google.com/machine-learning/guides/rules-of-ml/ | .",
            "url": "https://danielvanstrien.xyz/deployment/ethics/glam/full%20stack%20deep%20learning%20notes/2021/03/06/full-stack-deep-learning-glam-notes-5.html",
            "relUrl": "/deployment/ethics/glam/full%20stack%20deep%20learning%20notes/2021/03/06/full-stack-deep-learning-glam-notes-5.html",
            "date": " • Mar 6, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Writing a fastai callback",
            "content": "Intro . I sometimes feel like the process of me writing working code is analogous to the thing about monkeys writing Shakespeare, although in my case the end result of probably more like a Donald Trump tweet. My process for debugging or trying to work out how to implement something I&#39;m not completely familiar with seems to not always be efficient. This is something I&#39;m keen to work on. . This blog post is an attempt to try and do this &#39;in public&#39;. What I&#39;ve tried to do here is to use this blog post (which is also a notebook) as a place to record the steps I tried on the process of trying to implement a new callback in fastai. In particular I try and record my though process and the steps I took. This probably makes for an unreadable mess but I thought it might be useful as a record for myself. . This blog post isn&#39;t intended to be a guide on how to best solve a problem. It&#39;s more a cry for help 😂 . If anyone does read this, I would be really grateful to get any suggestions on: . whether you have suggestions to tackling a problem like this | if you think the &#39;come back to it later&#39; approach is sensible or not | . A callback to generate a rule of thumb about how much more data will help . I&#39;ve recently been doing a lot more annotation of data for various computer vision projects. Since I&#39;m doing all of the annotating myself I often want to have some sense of whether it is worth trying to get some more data. . Prodigy/Prodigy has a &#39;recipe&#39; for doing something that tries to answer this question. train_curve trains: . a model component with different portions of the training examples and print the accuracy figures and accuracy improvements with more data. This recipe takes pretty much the same arguments as train. --n-samples sets the number of sample models to train at different stages. For instance, 10 will train models for 10% of the examples, 20%, 30% and so on. This recipe is useful to determine the quality of the collected annotations, and whether more training examples will improve the accuracy. As a rule of thumb, if accuracy improves within the last 25%, training with more examples will likely result in better accuracy. Source There is also a nice example of this in action in this youtube video . . I wanted to implement something similar in fastai. My stretch goal was to make something that would be useful enough and be implemented cleanly enough that I could make a pull request to fastai proposing this. I am not sure if this will actually get there but I wanted to have this in mind as an end goal so that I: . force myself to not take hacky shortcuts | force myself to be exposed to more of the inner functionality of fastai | learn about the callbacks system in a lot more detail | . Regarding the second point, I am particularly keen to do this &#39;inside&#39; fastai rather than &#39;wrapping&#39; something around fastai. If you are familiar with callbacks already this distinction will probably be clear already. If not, hopefully it will make sense later. . Part 1... . This part goes through most of the steps I took, some of the notebook has been tidied so it&#39;s not even longer than this will already be... . from fastai.vision.all import * from fastai.callback import * from fastai.test_utils import * import pandas as pd . Dynamically altering the training data size . What I want to be able to do is to dynamically alter the training data. i.e. modify the data attached to a learner dynamically rather than recreating the dataloaders outside of the training loop. Try to do this with example data . #hide_show data = untar_data(URLs.IMAGEWANG_160) dls = ImageDataLoaders.from_folder(data/&#39;train&#39;, valid_pct=0.7, item_tfms=[Resize(64)]) len(dls.dataset) . I think train_ds is probably the the thing I want to modify . dls.train_ds . (#4401) [(PILImage mode=RGB size=213x160, TensorCategory(8)),(PILImage mode=RGB size=213x160, TensorCategory(16)),(PILImage mode=RGB size=213x160, TensorCategory(6)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=202x160, TensorCategory(14)),(PILImage mode=RGB size=240x160, TensorCategory(18)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=160x457, TensorCategory(17)),(PILImage mode=RGB size=240x160, TensorCategory(19)),(PILImage mode=RGB size=160x240, TensorCategory(8))...] . learn = cnn_learner(dls, squeezenet1_0, metrics=accuracy, pretrained=False) learn.fit(1) . epoch train_loss valid_loss accuracy time . 0 | 3.584931 | 3.637918 | 0.131671 | 01:05 | . I think it will probably be safer to make a deepcopy of things. . import copy . . Note: I&#8217;m not quite sure whether this is the best way of handling this, it seems to be very hacky. . d_original = copy.deepcopy(learn.dls.dataset) d_original . (#4401) [(PILImage mode=RGB size=213x160, TensorCategory(8)),(PILImage mode=RGB size=213x160, TensorCategory(16)),(PILImage mode=RGB size=213x160, TensorCategory(6)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=202x160, TensorCategory(14)),(PILImage mode=RGB size=240x160, TensorCategory(18)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=160x457, TensorCategory(17)),(PILImage mode=RGB size=240x160, TensorCategory(19)),(PILImage mode=RGB size=160x240, TensorCategory(8))...] . Grab a smaller part of the dataset . len(dls.dataset) * 0.5 . 2200.5 . Pass this back in. . learn.dls.dataset = L(dls.dataset[:100]) . learn.dls.dataset . (#100) [(PILImage mode=RGB size=213x160, TensorCategory(8)),(PILImage mode=RGB size=213x160, TensorCategory(16)),(PILImage mode=RGB size=213x160, TensorCategory(6)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=202x160, TensorCategory(14)),(PILImage mode=RGB size=240x160, TensorCategory(18)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=160x457, TensorCategory(17)),(PILImage mode=RGB size=240x160, TensorCategory(19)),(PILImage mode=RGB size=160x240, TensorCategory(8))...] . learn.fit(5) . epoch train_loss valid_loss accuracy time . 0 | 3.035343 | 2.480581 | 0.215037 | 01:04 | . 1 | 2.913207 | 2.479565 | 0.212115 | 01:01 | . 2 | 2.817269 | 2.409176 | 0.240066 | 01:05 | . 3 | 2.696751 | 2.442177 | 0.211044 | 01:02 | . 4 | 2.664056 | 3.162136 | 0.254967 | 01:04 | . check our data again, (I&#39;m paranoid) . learn.dls.dataset . (#100) [(PILImage mode=RGB size=213x160, TensorCategory(8)),(PILImage mode=RGB size=213x160, TensorCategory(16)),(PILImage mode=RGB size=213x160, TensorCategory(6)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=202x160, TensorCategory(14)),(PILImage mode=RGB size=240x160, TensorCategory(18)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=160x457, TensorCategory(17)),(PILImage mode=RGB size=240x160, TensorCategory(19)),(PILImage mode=RGB size=160x240, TensorCategory(8))...] . Now try and shove in the original data. . learn.dls.dataset = d_original . learn.dls.dataset . (#4401) [(PILImage mode=RGB size=213x160, TensorCategory(8)),(PILImage mode=RGB size=213x160, TensorCategory(16)),(PILImage mode=RGB size=213x160, TensorCategory(6)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=202x160, TensorCategory(14)),(PILImage mode=RGB size=240x160, TensorCategory(18)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=160x457, TensorCategory(17)),(PILImage mode=RGB size=240x160, TensorCategory(19)),(PILImage mode=RGB size=160x240, TensorCategory(8))...] . check that this fit still . learn.fit(1) . epoch train_loss valid_loss accuracy time . 0 | 2.512241 | 2.247095 | 0.284184 | 01:02 | . learn.dls.dataset . (#4401) [(PILImage mode=RGB size=213x160, TensorCategory(8)),(PILImage mode=RGB size=213x160, TensorCategory(16)),(PILImage mode=RGB size=213x160, TensorCategory(6)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=202x160, TensorCategory(14)),(PILImage mode=RGB size=240x160, TensorCategory(18)),(PILImage mode=RGB size=213x160, TensorCategory(15)),(PILImage mode=RGB size=160x457, TensorCategory(17)),(PILImage mode=RGB size=240x160, TensorCategory(19)),(PILImage mode=RGB size=160x240, TensorCategory(8))...] . This seemed to work with a few attempts but it seemed like a bad idea. I asked if anyone could see if this was an issue in the fastai discord . I kind of felt unsure about how best to approach the data issue so for now I&#39;ll move on to playing around with the callbacks. . . Warning: this seemed like a bad idea (spoiler alert it was. I&#8217;ll come back to this later... . Using callbacks . Since I&#39;m not completely sure about how to approach the dynamic resizing of data I&#39;ll move on to getting stuck into the callbacks and coming back to this issue later. . I&#39;m not sure if moving on when I get stuck is the best approach. I feel like I sometimes end up trying to &#39;brute force&#39; a solution at some point rather thanking thinking it through carefully. I feel like coming back to it later sometimes helps but it could also be kicking the can down the road... . How to use callbacks basics . First when can where can a call back be used? The docs for the callbacks gave me a good starting point. One of the things I need to work out is where callbacks can be called, the second where I would want to use callbacks in this example. . The callbacks can helpfully be found under event . [e for e in dir(event) if not e.startswith(&#39;__&#39;)] . [&#39;after_backward&#39;, &#39;after_batch&#39;, &#39;after_cancel_batch&#39;, &#39;after_cancel_epoch&#39;, &#39;after_cancel_fit&#39;, &#39;after_cancel_train&#39;, &#39;after_cancel_validate&#39;, &#39;after_create&#39;, &#39;after_epoch&#39;, &#39;after_fit&#39;, &#39;after_loss&#39;, &#39;after_pred&#39;, &#39;after_step&#39;, &#39;after_train&#39;, &#39;after_validate&#39;, &#39;before_backward&#39;, &#39;before_batch&#39;, &#39;before_epoch&#39;, &#39;before_fit&#39;, &#39;before_train&#39;, &#39;before_validate&#39;] . Maybe we want to do our setup before fit? . My first though is that it was likely that I would want to do some stuff before fitting so I wanted to start here. The other things which the docs page helped is in understanding the attributes available using callbacks. To get more familiar I&#39;ll try printing out some of these attributes. This will hopefully also clarify they are what they think I are. . class ShowTrainInfo(Callback): def before_fit(self): print(f&quot;Number of Epochs:{self.n_epoch}&quot;) . This (hopefully) will print out the number of epochs before the model fit . learn.fit(1, cbs=ShowTrainInfo()) . Number of Epochs:1 . epoch train_loss valid_loss accuracy time . 0 | 2.478273 | 2.763863 | 0.145014 | 01:04 | . This seems to work! Try to do something a bit closer to what I want to achieve... . One thing that I need to be able to do is manipulate the number of epochs. I want to basically run the epochs passed by the user multiple times with different subsets of the data. This will simulate running the training loop multiple times inside the training loop. . Maybe it&#39;s easier to call after_create so that instead of having to try and capture all of the info about the learner and recreating it we do the following: . after create: . multiply the number of epochs by the number of &#39;trials&#39; (maybe with some added options later e.g. early stopping, terminate on NaN) | create first trial using (len(fulldata))/number of trials i.e. for 4 trials get 25% of data | pass first trial data to model | train model | if epoch number % epochs per trial==0: record the information about best result, last result etc. | reset the model weights | create new updated size of training data i.e. next step for 4 trials = 50% | ? not sure how to best record that this is a different trial and record other info -&gt; maybe store in an attribute of the callback? | ? not sure about what the log/print at the end, will do some baby print statements for now, this is obviously where I want to sink loads of time into choosing emojis to print... | . | . Try out the manipulation of the number of epochs. I think it should be possible to just update this value. I&#39;ll do it in before_fit for now. . class ShowTrainInfo(Callback): def before_fit(self): print(self.n_epoch) self.learn.n_epoch *=4 . learn.fit(1, cbs=ShowTrainInfo()) . 1 . . 100.00% [1/1 01:01&lt;00:00] epoch train_loss valid_loss accuracy time . 0 | 2.445697 | 2.376423 | 0.281457 | 01:01 | . . 100.00% [161/161 00:27&lt;00:00 2.3999] . 200.00% [2/1 02:06&lt;-1:58:57] epoch train_loss valid_loss accuracy time . 0 | 2.445697 | 2.376423 | 0.281457 | 01:01 | . 1 | 2.399856 | 2.251869 | 0.281457 | 01:04 | . . 100.00% [161/161 00:27&lt;00:00 2.3335] epoch train_loss valid_loss accuracy time . 0 | 2.445697 | 2.376423 | 0.281457 | 01:01 | . 1 | 2.399856 | 2.251869 | 0.281457 | 01:04 | . 2 | 2.333455 | 2.175574 | 0.308142 | 01:01 | . 3 | 2.291610 | 2.286107 | 0.277854 | 01:04 | . Is this actually training four times? The progress bar makes things a little tricky to read. Let&#39;s print out the epochs to check, and get rid of the progress bar for now so it&#39;s easier to see what&#39;s going on . class ShowTrainInfo(Callback): def before_fit(self): print(f&quot;number epochs passed by user:{self.n_epoch}&quot;) self.learn.n_epoch *=4 def after_epoch(self): print(f&quot;just finished epoch:{self.epoch}&quot;) . with learn.no_bar(): learn.fit(1, cbs=ShowTrainInfo()) . number epochs passed by user:1 [0, 2.2561960220336914, 2.186687469482422, 0.31038177013397217, &#39;01:02&#39;] just finished epoch:0 [1, 2.263982057571411, 3.9907848834991455, 0.2859368920326233, &#39;01:03&#39;] just finished epoch:1 [2, 2.234278917312622, 2.299966812133789, 0.3186599016189575, &#39;01:04&#39;] just finished epoch:2 [3, 2.1652183532714844, 2.047651529312134, 0.36112192273139954, &#39;01:26&#39;] just finished epoch:3 . That seems to be okay, we have 4 total epochs. I&#39;ll get back to the logging issue later... . Let&#39;s try the idea of breaking for each point in the trial i.e in this case after each epoch. Maybe should also add an init to the callback and give it a more sensible name. I will also now add something that will &#39;reset&#39; the model after each trial. i.e. after the original epochs passed by the user. For now, I&#39;ll just add some print statements to see where things are being called. I&#39;m hoping this will let me confirm I&#39;m planning to execute callbacks in the right place. . class DSetSizeTrials(Callback): def __init__(self, n_trials=4): self.n_trials = n_trials def before_fit(self): self.epoch_per_trial = self.n_epoch print(f&quot;number epochs passed by user:{self.n_epoch}&quot;) self.learn.n_epoch *=self.n_trials def after_epoch(self): print(f&quot;just finished epoch:{self.epoch}&quot;) if (self.epoch+1) % self.epoch_per_trial==0: print(&#39;reset model&#39;) . with learn.no_bar(): learn.fit(3, cbs=DSetSizeTrials(2)) . number epochs passed by user:3 [0, 2.108391761779785, 3.002286672592163, 0.30103233456611633, &#39;01:22&#39;] just finished epoch:0 [1, 2.0375003814697266, 2.582606792449951, 0.2694779932498932, &#39;01:09&#39;] just finished epoch:1 [2, 2.0219476222991943, 2.2244386672973633, 0.3190494775772095, &#39;01:15&#39;] just finished epoch:2 reset model [3, 1.9379000663757324, 2.5143325328826904, 0.37748345732688904, &#39;01:04&#39;] just finished epoch:3 [4, 1.939262866973877, 2.207705020904541, 0.33180755376815796, &#39;01:07&#39;] just finished epoch:4 [5, 1.894189715385437, 10.37093448638916, 0.3005453944206238, &#39;01:04&#39;] just finished epoch:5 reset model . I&#39;m not limited by my own brain... . Getting back to the issue of whether passing in a new subset of data by doing learn.dls.dataset = L(dls.dataset[:100]) was a bad idea. It turns out it was. Although this worked without issue for the first dataset I worked with when I tried with data loaded via a DataFrame I got errors even at the indexing stage. Since I had asked the question in the discord I thought it was worth responding: . . My initial response wasn&#39;t super helpful in hindsight. Saying something is a bad idea isn&#39;t very useful for anyone else. This is often super obvious when you see someone else do it but I think it&#39;s easy to forget. One of the things I love about fastai is the community around it which is super focused on helping people out. I&#39;m glad Zach asked followed up here since this exchange also ended up being super useful. My follow up reply: . For context, what I am trying to work out is how to dynamically change my training data size during training, i.e. first use 25%, then 50% of the training data. I thought this might be possible by indexing the dls.dataset attribute and updating it to a slice of that. This worked out when the original dataloaders was defined via a from_folder. When I just tried the same thing using a dls originally created via a DataFrame I get a FileNotFoundError. I&#39;m still trying to wrap my head around exactly why one works but not the other, but I&#39;m assuming that the dataset attribute doesn&#39;t contain all the information that is needed to get to an item in all cases? . . Tip: Say both what you are trying to do and why you are trying to do something. This gives a much better insight for people who might want to help. . Although my response doesn&#39;t say exactly what is going wrong it offer my best guess and also gives some insight into my motivations. In this case since it seems like a slightly weird thing without the wider context/motivations outlined above. . . This led to a super helpful exchange with a few people in the discord which honed in on a few possible solutions to this problem. . . I don&#39;t want to reproduce the whole exchange but what was super nice is that: . a bunch of people tried to help. Even if nothing else it gives you warm fuzzies that people are happy to help you with this kind of thing | people offered a bunch of different approaches all of which offered potential things for me to follow up | this fast tracked my progress, particularly since it might have got super annoying to be stuck on this problem on my own for ages. | . . Tip: other people are happy to help but you should try and make it easy for them to help you (and I should reciprocate where I can) . Following this input from other people I followed up on shuffle_fn as a way to achieve what I&#39;m trying to do (inside of a callback). Again, I&#39;ll try modifying this outside of a callback to see how it works . dls = ImageDataLoaders.from_folder(data/&#39;train&#39;, valid_pct=0.3, item_tfms=[Resize(64)]) . dls.train_ds . (#10269) [(PILImage mode=RGB size=239x160, TensorCategory(19)),(PILImage mode=RGB size=160x240, TensorCategory(19)),(PILImage mode=RGB size=160x357, TensorCategory(17)),(PILImage mode=RGB size=213x160, TensorCategory(19)),(PILImage mode=RGB size=290x160, TensorCategory(0)),(PILImage mode=RGB size=213x160, TensorCategory(17)),(PILImage mode=RGB size=160x213, TensorCategory(17)),(PILImage mode=RGB size=160x263, TensorCategory(9)),(PILImage mode=RGB size=213x160, TensorCategory(14)),(PILImage mode=RGB size=213x160, TensorCategory(18))...] . Looking at what it does at the moment . ??dls.train.shuffle_fn . Signature: dls.train.shuffle_fn(idxs) Docstring: Returns a random permutation of `idxs`. Source: def shuffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs)) File: /usr/local/anaconda3/envs/blog/lib/python3.8/site-packages/fastai/data/load.py Type: method . We can easily patch the functionality, in this case we&#39;ll just return 128 items . @patch_to(DataLoader) def shuffle_fn(self, idxs): return self.rng.sample(idxs, 128) . Check this has changes as expected . ??dls.train.shuffle_fn . Signature: dls.train.shuffle_fn(idxs) Docstring: &lt;no docstring&gt; Source: @patch_to(DataLoader) def shuffle_fn(self, idxs): return self.rng.sample(idxs, 128) File: ~/Documents/daniel/blog/_notebooks/&lt;ipython-input-29-99a68b7fe249&gt; Type: method . Now try training... . learn = cnn_learner(dls, squeezenet1_0) learn.fit(1) . epoch train_loss valid_loss time . 0 | 4.615644 | 6.038196 | 00:17 | . This seemed to do the training step very quickly so it&#39;s probably only getting 128 items but I don&#39;t know that for sure. If only there was some way of getting access to the training loop in fastai 😜 . class PrintItems(Callback): def __init__(self): self.items_done = 0 def after_batch(self): print(self.iter) self.items_done += (max(self.iter,1)* self.learn.dls.bs) def before_validate(self): print(self.items_done) raise CancelFitException(&#39;stopped before valid&#39;) . learn.fit(1, cbs=PrintItems()) . epoch train_loss valid_loss time . 0 | 4.113161 | None | 00:00 | . 0 1 128 . With some more guidance I update this function to accept an input parameter that we can then easily update during training. . @patch def my_shuffle_fn(self:DataLoader, idxs, size=128): return self.rng.sample(idxs, size) . from functools import partial . dls.train.shuffle_fn = dls.train.my_shuffle_fn . ??dls.train.shuffle_fn . Signature: dls.train.shuffle_fn(idxs, size=128) Docstring: &lt;no docstring&gt; Source: @patch def my_shuffle_fn(self:DataLoader, idxs, size=128): return self.rng.sample(idxs, size) File: ~/Documents/daniel/blog/_notebooks/&lt;ipython-input-34-95358dde961e&gt; Type: method . Defining roughly what I want to do with some print statements again... . class PrintItems(Callback): def __init__(self): self.items_done = 0 def before_train(self): print(&#39;calculating partial data sizes...&#39;) print(&#39;updating shuffle_fn....&#39;) self.learn.dls.train.shuffle_fn = partial(self.learn.dls.train.my_shuffle_fn, size=128) def after_batch(self): print(self.iter) self.items_done += (max(self.iter,1)* self.learn.dls.bs) def before_validate(self): print(self.items_done) raise CancelFitException(&#39;stopped before valid&#39;) . learn.fit(1, cbs=PrintItems()) . epoch train_loss valid_loss time . 0 | 2.966318 | None | 00:00 | . calculating partial data sizes... updating shuffle_fn.... 0 1 128 . To be continued... . To avoid this becoming even longer I&#39;ll pick up the next steps of trying to get this callback working in another blog post. . Summary so far . How this doing this help with my original goals. . I&#39;ve definitely got a much better grasp on the callback system in fastai. Even though the specific callback I&#39;m working on is still a work in progress, I now know much better where the entry points for callbacks are and what can be accessed/modified. I would feel more confident implementing callbacks than before. . | my &#39;process&#39; is still a work in progress but having in mind that I should try and record the steps was actually super helpful and something I&#39;ll try and do better next time. . | Asking other people for help can move progress forward quickly. I think there is a skill in asking questions in the best way possible and this is something I&#39;ll continue to work on... . | .",
            "url": "https://danielvanstrien.xyz/fastai/callbacks/learning/2021/03/04/callbacks-and-debugging-part-one.html",
            "relUrl": "/fastai/callbacks/learning/2021/03/04/callbacks-and-debugging-part-one.html",
            "date": " • Mar 4, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Power, Ethics and AI for GLAMs",
            "content": "This is a very quick rambling post on ethics/power and ai in a GLAM setting. . On a recent(ish) call for the fastai4fglams study group we discussed Lecture 5 of the fast.ai course which focuses on Ethics. . A question we discussed in the call was about the more specific ethical issues that could emerge when using AI in a library context. . Some of the topics we touched on in the discussion included: . The Provenance of collection items | How this provenance is represented (or not) in metadata | The use of Black Box commercial solutions for working with GLAM data | The alternatives (or lack of) to using machine learning for some tasks | How much to document the provenance of a label produced by ml | . How to present labels produced via machine learning . I should preface this discussion by saying I am not an expert in cataloging or the associated literature. . Instead of trying to cover all of these issues I will focus on one particular question for GLAMs using machine learning: how to display the labels produced by these models in library systems i.e. catalogues? . One of the main uses cases of Machine Learning models is to produce labels for collections items. These labels could either seek to ‘augment’ or ‘replace’ existing metadata records fields. A question for either of these is how to display, store and manage labels produced via machine learning models in library systems. For example a model which is trained to predict the year field for items in a particular collection. . The model . Knowing the model which produced a label including: . the architecture | the year in which the model was trained/inference made | version of the model | … | . There is a danger of replicated the granularity of tools like Weights and Biases. Whilst this information is useful for training and provenance for people familiar with machine learning methods, some of the information in these systems is going to be less relevant for a typical library catalog user e.g. GPU memory consumption during training. There are potentially some other fields which will be more relevant to a broader number of people. . Possible labels . The first question for this type of task might be to know how the original model task was defined. Predicting labels for ‘year’ could be treated as a classification task. For example if you have a collection where you a certain that all items in that collection will have been produced between certain dates the task may be to predict classify whether the item belong to the 80s or 50s decade. In this case the model has a restricted range of possible outputs i.e. each decade in the original training data. . Another approach would be to instead make this a ‘regression’ task, where the model predicts a continuous value. In this case the model is not bound by a particular set of possible years. . The distinction between a classification and a regression model, and the possible bound of values for a label might give the end-user of that label a better sense of how it should be ‘read’. Using this information will also provide some way of contextualizing the confidence of the label, and what this might mean. An F1 score will mean different things if a model had to choose between one five possible decades for a year label or choose one of a 100 possible years. . Alongside knowing the potential labels, it may be useful for a user of a catalog to know something about the distribution of these labels i.e. how many times does a certain label appear both during the inference and training steps. . Training data . Having at least a minimal way of interrogating the original training data could allow for more interrogation of models even by those who aren’t experts in machine learning and in fact domain experts might pick up on important features of a training dataset that might have been missed during it’s construction. Including some information about: . number of training examples | label distribution | min, max values | etc. | . Ideally this dataset would be available for open use by others but this might not always be possible for collections which aren’t fully open. . How to present this information? . To be continued… .",
            "url": "https://danielvanstrien.xyz/fastai%20course/ethics/2020/11/05/power-ethics-and-ai-in-glams.html",
            "relUrl": "/fastai%20course/ethics/2020/11/05/power-ethics-and-ai-in-glams.html",
            "date": " • Nov 5, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Image labeling vs classification models",
            "content": "The &#39;hello world&#39; example for introducing deep learning based computer vision often involves classifying images as 🐶 or 🐱. An alternative approach to classifying images is to instead apply labels. This is usually introduced in the context of multi-label classification i.e. where an image can have more than one label. In this blog post I discuss some of the differences between these two approaches, specifically the difference in loss functions, and how these two approaches might work better depending on the application. The post starts with a conceptual overview of the differences between these two approaches, before showing the different loss functions and then moving to a practical example of training these two different types of model. . Image Classification vs Image Labeling . In a classification model, an input can have only one label. This could be one of a few or one of a hundred, regardless of the number of potential classes, it is assumed that the input only belongs to one of these. With a model that applies labels this is not true an input can have one, multiple or no labels. . Sorting through family photos . We can use an analogy to illustrate the difference between these two approaches. Let&#39;s say you were sorting through some old family photographs. You might &quot;classify&quot; the photos into one (and only one) of two photo albums, depending on whether they are black-and-white or colour. This would be comparable to using a classification model since each photo will go into exactly one of these two albums - a photo cannot be both simultaneously colour and black-and-white, and it cannot be neither colour nor black-and-white. . You may at the same time also want to make it easier to find photos of particular people in your family. You could do this by assigning labels to each photo, indicating or &quot;tagging&quot; the family members who appear in the photo. In this case, a photo may have one label (a photo of your sister), more than one label (a photo of your sister and aunt), or it may have no labels (a photograph of a landscape taken on a holiday). This would be analogous to a multi-label classification model. . The choice between using a model which performs classification or a model which assigns labels should be considered in relation to the role your model has. It is also useful to look a little bit more closely as how these different types of models work under the hood. . CrossEntropyLoss vs BCEWithLogitsLoss . When we create a model which does classifications or applies labels, the distinction, if using the same data is that they use different loss functions. . A classification model will use a variant of Cross Entropy Loss whilst the label model will use a BCE with Logits Loss. We&#39;ll see how this is inferred by fastai below but fore now take my word for it... . Let&#39;s take a look at a snippet of the Pytorch docs for each of these loss functions . CrossEntropyLoss . This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. Read more . BCEWithLogitsLoss . This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. Read more . Let&#39;s see what these do to some activations. First we&#39;ll import required packages . import torch.nn as nn import numpy as np import torch . Exploring CrossEntropyLoss . We can create some fake activations. To start we&#39;ll just consider one output with three classes. We&#39;ll start with one to keep things simple for now. . one_act = torch.randn((1, 3)) * 1 one_act . tensor([[ 0.9924, 0.8698, -0.0100]]) . We can think of these activations as probabilities for one of three classes. Let&#39;s see what these sum to. . one_act.sum() . tensor(1.0875) . We can see that these activations don&#39;t sum to 1. If we want our image input to belong to only one class, then the labels are not mutually exclusive of each other i.e. if one label probability is higher, another needs to be lower i.e. the probabilities need to add up to 1. Going back to the Pytorch explanation of CrossEntropyLoss we see that one component is nn.LogSoftmax(). What is particularly relevant here is that &#39;softmax&#39; part. Let&#39;s see what this does to our activation . softmax_acts = torch.softmax(one_act, dim=1) softmax_acts . tensor([[0.4525, 0.0381, 0.5093]]) . You can probably already see how this has changed the nature of these activations. Let&#39;s call sum on these outputs again. . softmax_acts.sum() . tensor(1.) . We now have a sum of 1! We can now treat this as the probability of an input image belonging to a particular class. We could then call argmax to find out which class the model is most confident about and use that as our prediction. . softmax_acts.argmax(dim=1) . tensor([2]) . One of the potential issues that was mentioned about using a classification model was that it doesn&#39;t account for ambiguities in the labels very well. . What is softmax doing? . Digging into what softmax does in a little bit more detail will show what is going on here. . First lets see what softmax actually does, I&#39;ll skip the LaTeX formula from Wikepedia because it makes is look much scarier than the Python code example: . a = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0] np.exp(a) / np.sum(np.exp(a)) . array([0.02364054, 0.06426166, 0.1746813 , 0.474833 , 0.02364054, 0.06426166, 0.1746813 ]) . This is much easier for me to parse compared to the Greek. Let&#39;s look at the different parts. Working with one set of activations again: . one_act . tensor([[ 1.1479, -1.3265, 1.2661]]) . Starting from np.exp(a) we can do this in Pytorch like: . one_act.exp() . tensor([[3.1515, 0.2654, 3.5471]]) . We can convert the rest of the numpy code as follows . one_act.exp().sum(dim=1) . tensor([6.9641]) . Putting it all together we get . (one_act.exp() /one_act.exp().sum(dim=1)).sum(dim=1) . tensor([1.]) . This seems to work as expected, i.e. we get the probabilities to sum to 1. To make it clearer what&#39;s going on though, it&#39;s useful to look a little more closely at the difference using exp makes. Let&#39;s import the standard python version of exp and check the docs. . from math import exp doc(exp) . exp[source] . exp(x) . Return e raised to the power of x. . What difference does using the exponent make? We&#39;ll use a simple array of values to keep things simple . x = np.array([1,2,4,1]) x . array([1, 2, 4, 1]) . Now if we want these to be converted to probabilities for different classes we need them to sum to 1. We could just do this by dividing each element by the sum. . x/x.sum() . array([0.125, 0.25 , 0.5 , 0.125]) . We can confirm this add to 1 . (x/x.sum()).sum() . 1.0 . Now this seems to work to get us probabilities for each class. Let&#39;s compare doing the same thing but using exp to create exponents of the inputs . np.exp(x)/np.sum(np.exp(x)) . array([0.04031637, 0.10959126, 0.80977599, 0.04031637]) . Again we get an array of probabilities, let&#39;s confirm they add to one. . one_act.exp()/one_act.exp().sum(dim=1) . tensor([[0.4441, 0.3929, 0.1630]]) . So what is different here? . Let&#39;s put the two arrays next to each other so we can compare the values for each index . np.exp(x)/ np.sum(np.exp(x)), (x/ x.sum()) . (array([0.04031637, 0.10959126, 0.80977599, 0.04031637]), array([0.125, 0.25 , 0.5 , 0.125])) . Other than the difference in decimals, you will probably notice that when we use exponent, some labels for a class have been pushed much higher. Index 2 is 0.80 when we use exp and only 0.5 when we don&#39;t use the exponent. This is an important difference here. By using the magic properties of $e$ we &#39;push&#39; one probability to be higher than the others. . This property is useful when we have a clear distinction between classes. If we were predicting handwritten digits there (should) only be one correct answer. In this case having one class prediction being pushed much higher would be a good thing. . If however, we have labels which are more ambiguous, this would be less of a desirable property. Even if we try and capture ambiguity by using the raw probabilities of the labels, rather than taking the argmax value, the numerical properties of the softmax function mean that it likely that one label value will be pushed higher than the others. . We&#39;ll look at a practical example later on to illustrate this. Let&#39;s now quickly compare our other loss function . Exploring BCEWithLogitsLoss . As a reminder . This loss combines a Sigmoid layer and the BCELoss in one single class. . The part here that we are particularly interested in is the Sigmoid. Let&#39;s use one_acts again . one_act . tensor([[ 1.1479, -1.3265, 1.2661]]) . As a reminder sigmoid function can be plotted as . You&#39;ll probably be familiar with sigmoid as one of the potential activations functions you can use in the a neural network. The property we care about is that it squishes inputs into a value between 0 and 1. Let&#39;s do this for our activations . torch.sigmoid(one_act) . tensor([[0.7591, 0.2097, 0.7801]]) . We can see that all our values have been pushed between 0 and 1. However, we can also see they don&#39;t sum to 1. . torch.sigmoid(one_act).sum() . tensor(1.7489) . What we have here is a probability for each label which is independent of the probability of the other labels. The sigmoid function makes sure the activations for each label becomes a probability but it doesn&#39;t make sure that all of the labels probabilities sum to 1. Looking at a practical example using fastai might illustrate this difference. . We&#39;ll work with some images taken from 19th Century books, the specific images in this case don&#39;t matter to do much . We&#39;ll import fastai and then put images from two folders &#39;building&#39; and &#39;coat&#39; into a Pandas DataFrame. . from fastai.vision.all import * . files = get_image_files(&#39;data/cv_workshop_exercise_data/&#39;, folders=[&#39;building&#39;, &#39;coat&#39;]) df = pd.DataFrame(files.items, columns=[&#39;fname&#39;]) df[&#39;class_label&#39;] = df[&#39;fname&#39;].apply(lambda x: x.parts[2]) df[&#39;class_label&#39;].value_counts() . building 44 coat 26 Name: class_label, dtype: int64 . We can see we have two possible classes building and coat. First we&#39;ll load these into fastai as a classification model. . dls_classification = ImageDataLoaders.from_df(df,fn_col=&#39;fname&#39;,valid_pct=0.4, label_col=&#39;class_label&#39;, item_tfms=Resize(128, ResizeMethod.Squish), bs=8,num_workers=0) . dls_classification.show_batch() . You&#39;ll see that building refers to a building, whilst a coat refers to a coat of arms. Let&#39;s now load this data into fastai . learn = cnn_learner(dls_classification, resnet18, metrics=[accuracy, F1Score()]) . Often if we pass fastai a dataloader it will be able to infer the correct loss function based on this data. we can access this using the loss_func attribute. . learn.loss_func . FlattenedLoss of CrossEntropyLoss() . As promised this is a variant on the CrossEntropyLoss we saw earlier. Let&#39;s now fit it for a bit. . learn.fit(5) . epoch train_loss valid_loss accuracy f1_score time . 0 | 1.023169 | 0.786303 | 0.785714 | 0.769231 | 00:03 | . 1 | 0.721281 | 0.576258 | 0.821429 | 0.814815 | 00:03 | . 2 | 0.477446 | 0.339626 | 0.821429 | 0.782609 | 00:04 | . 3 | 0.423173 | 0.331097 | 0.821429 | 0.782609 | 00:03 | . 4 | 0.351390 | 0.239433 | 0.857143 | 0.818182 | 00:03 | . Now we have a model, we&#39;ll grab the predictions . acts, _ = learn.get_preds() acts . tensor([[9.9795e-01, 2.0519e-03], [9.9811e-01, 1.8889e-03], [9.9911e-01, 8.8577e-04], [9.9680e-01, 3.2038e-03], [6.5879e-01, 3.4121e-01], [1.2512e-04, 9.9987e-01], [9.9734e-01, 2.6599e-03], [9.8866e-01, 1.1341e-02], [9.2739e-01, 7.2608e-02], [9.8336e-01, 1.6643e-02], [1.7059e-01, 8.2941e-01], [9.9899e-01, 1.0067e-03], [5.2081e-01, 4.7919e-01], [4.9184e-03, 9.9508e-01], [9.9930e-01, 7.0161e-04], [1.0109e-04, 9.9990e-01], [9.9533e-01, 4.6670e-03], [3.6834e-02, 9.6317e-01], [5.7022e-06, 9.9999e-01], [9.8635e-01, 1.3647e-02], [2.1610e-01, 7.8390e-01], [2.3512e-02, 9.7649e-01], [2.9994e-01, 7.0006e-01], [4.2728e-02, 9.5727e-01], [9.8494e-01, 1.5062e-02], [1.4194e-01, 8.5806e-01], [6.8620e-01, 3.1380e-01], [7.3493e-01, 2.6507e-01]]) . These are the predictions for each class, let&#39;s confirm these all sum to 1. . acts.sum(dim=1) . tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]) . If we look at the max for each probability we&#39;ll see they tend to be high. . acts.max(dim=1)[0] . tensor([0.9979, 0.9981, 0.9991, 0.9968, 0.6588, 0.9999, 0.9973, 0.9887, 0.9274, 0.9834, 0.8294, 0.9990, 0.5208, 0.9951, 0.9993, 0.9999, 0.9953, 0.9632, 1.0000, 0.9864, 0.7839, 0.9765, 0.7001, 0.9573, 0.9849, 0.8581, 0.6862, 0.7349]) . Looking at the mean, max and min: . acts.max(dim=1)[0].mean(), acts.max(dim=1)[0].max(), acts.max(dim=1)[0].min(), . (tensor(0.9113), tensor(1.0000), tensor(0.5208)) . This is desirable if the input we are trying to label does neatly fit the categories but if we are trying to label something which is more ambiguous then this might be less useful. A particular case where this certainty might not be so helpful is when your model may possibly face out of domain images, i.e. see things it hasn&#39;t seen before and for which none of the classes it is trying to predict should apply. Let&#39;s load a new dataset of images of people. . people = get_image_files(&#39;data/cv_workshop_exercise_data/&#39;, folders=&#39;people&#39;) people . (#38) [Path(&#39;data/cv_workshop_exercise_data/people/000001929_03_000249_2_De Aardbol Magazijn van hedendaagsche land en volkenkunde Met platen en kaarten [Deel 4 9 by P H W ]_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/000194796_0_000133_1_Historical Collections relating to the history and antiquities of every town in Massachusetts with geographical descriptions [With illustrations ]_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/000194796_0_000140_1_Historical Collections relating to the history and antiquities of every town in Massachusetts with geographical descriptions [With illustrations ]_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/000001929_03_000058_1_De Aardbol Magazijn van hedendaagsche land en volkenkunde Met platen en kaarten [Deel 4 9 by P H W ]_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/001099118_02_000168_1_The Victories of the British Armies with anecdotes illustrative of modern warfare By the author of Stories of Waterloo [i e William Hamilton Maxwell] etc [With plates ]_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/000001929_08_000107_1_De Aardbol Magazijn van hedendaagsche land en volkenkunde Met platen en kaarten [Deel 4 9 by P H W ]_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/000001929_06_000006_1_De Aardbol Magazijn van hedendaagsche land en volkenkunde Met platen en kaarten [Deel 4 9 by P H W ]_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/000979699_0_000368_1_Indian Captivities being a collection of the most remarkable narratives of persons taken captive by the North American Indians To which are added notes historical biographical etc_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/000001929_06_000007_1_De Aardbol Magazijn van hedendaagsche land en volkenkunde Met platen en kaarten [Deel 4 9 by P H W ]_1839.jpg&#39;),Path(&#39;data/cv_workshop_exercise_data/people/000001929_08_000106_1_De Aardbol Magazijn van hedendaagsche land en volkenkunde Met platen en kaarten [Deel 4 9 by P H W ]_1839.jpg&#39;)...] . PILImage.create(people[5]) . What happens if we predict one of these: . learn.predict(PILImage.create(people[5])) . (&#39;building&#39;, tensor(0), tensor([0.9916, 0.0084])) . It&#39;s predict that Torstenson is a building with a probability of 99% certainty! Let&#39;s look at some more . preds, _ = learn.get_preds(dl=learn.dls.test_dl(people)) . now we have a bunch of predictions let&#39;s get the max value. i.e. the probability for the label it predicted and see what the min, max and median is: . preds.max(dim=1)[0].min(), preds.max(dim=1)[0].max(), preds.max(dim=1)[0].median() . (tensor(0.5288), tensor(1.0000), tensor(0.9765)) . Although the min is fairly low, the median value is pretty confidently predicting a wrong label. Let&#39;s see how this differs if we instead use a &#39;label model&#39;. fastai expects labels to be inside a list, we can create a new column which puts our classes inside a list. . df[&#39;label&#39;] = df[&#39;class_label&#39;].apply(lambda x: [x]) . We&#39;ll now load in the data. The only difference here is that we specify a y_block, this forces fastai to choose the correct loss function. . dls_label = ImageDataLoaders.from_df(df,fn_col=&#39;fname&#39;, valid_pct=0.4, label_col=&#39;label&#39;, y_block=MultiCategoryBlock, item_tfms=Resize(128, ResizeMethod.Squish), bs=8, num_workers=0) dls_label.show_batch() . If we now create the learner, we&#39;ll see a different loss function . label_learn = cnn_learner(dls_label, resnet18, metrics=[F1ScoreMulti()]) label_learn.loss_func . FlattenedLoss of BCEWithLogitsLoss() . Again we&#39;ll fit for a while . label_learn.fit(5) . epoch train_loss valid_loss f1_score time . 0 | 0.979886 | 1.107939 | 0.520422 | 00:03 | . 1 | 0.742499 | 0.530855 | 0.749681 | 00:03 | . 2 | 0.540384 | 0.294314 | 0.858553 | 00:03 | . 3 | 0.428714 | 0.197953 | 0.874603 | 00:03 | . 4 | 0.358649 | 0.157847 | 0.973684 | 00:03 | . Now we&#39;ll grab some predictions again . preds, _ = label_learn.get_preds() preds . tensor([[9.8592e-01, 2.2629e-02], [5.0721e-01, 1.0558e-01], [9.9954e-01, 1.4539e-02], [2.6342e-01, 9.7471e-01], [9.9732e-01, 3.8691e-04], [1.0064e-02, 9.9507e-01], [1.5311e-02, 9.7020e-01], [2.2675e-03, 9.9944e-01], [9.7902e-01, 2.7719e-02], [6.0582e-01, 1.2015e-01], [7.7181e-02, 9.9448e-01], [9.8096e-01, 5.7364e-03], [8.2864e-01, 4.2955e-01], [9.0980e-01, 1.1982e-02], [9.6249e-01, 1.3159e-02], [2.3728e-01, 6.0858e-01], [9.9327e-01, 4.9904e-03], [9.1160e-04, 9.8218e-01], [9.7016e-01, 2.2057e-03], [9.8055e-01, 1.9247e-02], [8.3900e-01, 2.7438e-01], [4.3518e-01, 1.6118e-01], [6.8165e-01, 1.7120e-01], [7.7239e-01, 5.4064e-02], [9.9350e-01, 7.2269e-02], [6.3511e-01, 1.7830e-02], [1.3994e-01, 8.5564e-01], [2.6746e-02, 6.9240e-01]]) . Let&#39;s see what these add up to . preds.sum(dim=1) . tensor([1.0085, 0.6128, 1.0141, 1.2381, 0.9977, 1.0051, 0.9855, 1.0017, 1.0067, 0.7260, 1.0717, 0.9867, 1.2582, 0.9218, 0.9756, 0.8459, 0.9983, 0.9831, 0.9724, 0.9998, 1.1134, 0.5964, 0.8528, 0.8265, 1.0658, 0.6529, 0.9956, 0.7191]) . Not 1! Again this is because our labels are now independent of each other. We can see that if we now grab the max for each possible lab and take the min, max and median we get quite different results . preds.max(dim=1)[0].min(), preds.max(dim=1)[0].max(), preds.max(dim=1)[0].median() . (tensor(0.4352), tensor(0.9995), tensor(0.9702)) . Since the labels are now independent these probabilities have a much wider range. The lowest value is lower than would be possible when we use a classification model with two classes. This might be useful when we are trying to capture labels which are not tightly defined and therefore we might want our model to have more &#39;flexibility&#39; in the predictions it makes. Let&#39;s see what happens if we predict the same image of Torstenson we had earlier . label_learn.predict(PILImage.create(people[5])) . ((#1) [&#39;building&#39;], tensor([ True, False]), tensor([0.9992, 0.0055])) . Oh dear, this seems to have the same problem as before. However, we have the option to set a threshold for predictions. If we set a threshold and train again... . label_learn = cnn_learner(dls_label, resnet18, metrics=[F1ScoreMulti()],loss_func=BCEWithLogitsLossFlat(thresh=0.9)) . label_learn.fit(5) . epoch train_loss valid_loss f1_score time . 0 | 0.746691 | 0.450183 | 0.655556 | 00:03 | . 1 | 0.572374 | 0.352565 | 0.843939 | 00:03 | . 2 | 0.477375 | 0.328633 | 0.856250 | 00:03 | . 3 | 0.359104 | 0.320807 | 0.841642 | 00:03 | . 4 | 0.306263 | 0.327382 | 0.860795 | 00:03 | . If we now predict the same image . label_learn.predict(PILImage.create(people[5])) . ((#0) [], tensor([False, False]), tensor([0.8369, 0.4833])) . This time we don&#39;t get a prediction! The flexibility of being able to set a threshold is a very nice feature of using this type of loss function since it gives you some more options for deciding how confident you want a model to be. . Discussion . The aim of this blog post was to explore some of the implications of doing &#39;classification&#39; vs &#39;labeling&#39;. Although label models are often only considered in relation to models with multiple labels, they can also be applied to models with only one possible label per image. The key distinction between these two approaches is the loss functions. There are implications of choosing between these two loss functions. . Because of the Softmax component, a classification model will always have probabilities for each class which add to one. Beyond this thought the use of the exponent tends to push one class probability higher than the others. . In contrast the loss function for a labeling model pushes each individual labels probability between 0 and 1, but it doesn&#39;t require all of label probabilities to add to 1. . Labeling in a Digital Humanities/GLAM context . When you have clear labels which are distinct from each other, it is useful to have one label be &#39;pushed to the top&#39;. Often in a humanities or GLAM context labels may not be as clear cut. . This might be because the concepts which you are trying to capture in the labels have fuzzy borders, or because the source material contains some complexities. For example, working with ORC&#39;d text of varying quality. In these situations the fact that softmax will be likely to lead to one prediction being much stronger may not be desirable. . Although you can work with the raw probabilities predicted by the model to capture some potential ambiguity, because one class will tend to be pushed higher (because of the exponent in softmax) this doesn&#39;t fully address this issue. . A preference for one or another approach, will depend on the task at hand but even when you only have one single possible label per input, it might still be helpful to consider using a labeling model i.e. BCELoss instead of a classification model using CrossEntropyLoss. . There are of course other solutions to changing out the loss function you used to train the model. I&#39;m hoping to explore some of these soon 🤓 .",
            "url": "https://danielvanstrien.xyz/models/labels/loss%20functions/2020/10/12/labelling_vs_classification_models.html",
            "relUrl": "/models/labels/loss%20functions/2020/10/12/labelling_vs_classification_models.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Virtual memory",
            "content": "Conda . Package installation . Use Mambda instead of conda to install packages. We&#39;ve done what we do at @QuantStack: making things run faster. Conda has been getting slower with @condaforge&#39;s growing package registry -- we&#39;re fighting it with raw C++ power in the #mamba package!Try now:conda install mamba -c conda-forge/label/mamba-alpha -c conda-forge pic.twitter.com/tnlVQKAbv4 . &mdash; Wolf Vollprecht (@wuoulf) March 25, 2019 . Pandas . import pandas as pd . Drop rows if frequency of a class id below n . sometimes it can be useful to drop rows in a dataset which appear to few times, in the most extreme cases datasets might have only one observation for a particular class. . rows = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;2&#39;,&#39;1&#39;]; df = pd.DataFrame({&#39;a&#39;:rows, &#39;b&#39;: rows}) . df . a b . 0 1 | 1 | . 1 2 | 2 | . 2 3 | 3 | . 3 2 | 2 | . 4 1 | 1 | . df[df.groupby(&#39;a&#39;)[&#39;a&#39;].transform(&#39;count&#39;).ge(2)] . a b . 0 1 | 1 | . 1 2 | 2 | . 3 2 | 2 | . 4 1 | 1 | . fastai . Some things related to fastai (version 2) . Inference . Sources of information: . https://forums.fast.ai/t/doing-predictions-and-showing-results-with-v2-questions-best-practice-thread/62915/6 | . from fastai.vision.all import * . Train a learner so we can do some inference . path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) def label_func(f): return f[0].isupper() dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(32), num_workers=0) . dls.show_batch() . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.999297 | 0.718680 | 0.324763 | 01:26 | . epoch train_loss valid_loss error_rate time . 0 | 0.722174 | 0.548298 | 0.272666 | 01:41 | . Making predictions . preds = learn.get_preds() . Decoding predictions . To decode the output from get_preds. Can decode a label using the dls. Accessing the categorize.decode attribute to get a human readable value. . dls.categorize.decode(0) . &#39;False&#39; . If we have the predictions tensors i.e. the predictions for each class: . preds[0][0] . tensor([0.2110, 0.7890]) . numpy argmax can be used to get the index of the most likely predictions` . dls.categorize.decode(np.argmax(preds[0][0])) . &#39;True&#39; . Can also access the confidence for the maximum prediction directly; . max(preds[0][0]) . tensor(0.7890) . Test time augmentation . dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(32), batch_tfms=[*aug_transforms()],num_workers=0) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fit(1) . epoch train_loss valid_loss error_rate time . 0 | 0.898633 | 0.700572 | 0.299729 | 01:11 | . doc(learn.tta) . Learner.tta[source] . Learner.tta(ds_idx=1, dl=None, n=4, item_tfms=None, batch_tfms=None, beta=0.25, use_max=False) . Return predictions on the ds_idx dataset or dl using Test Time Augmentation . Show in docs . preds, targs = learn.tta() . . error_rate(preds, targs).item() . 0.28890395164489746 . Dataloaders . Accessing input data . Accessing the input from the data loaders can be done through items. The L here is just to automagically limit the number of items displayed in the notebook . L(dls.items) . (#5912) [Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/Bengal_76.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/shiba_inu_8.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/newfoundland_87.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_11.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/pug_50.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/chihuahua_58.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/British_Shorthair_74.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/pomeranian_75.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/Russian_Blue_104.jpg&#39;),Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/Bengal_77.jpg&#39;)...] . Indexing dataloader items . Items can be indexed in the usual way which can be useful to get back to orginal input based on an index . dls.items[0] . Path(&#39;/Users/dvanstrien/.fastai/data/oxford-iiit-pet/images/german_shorthaired_54.jpg&#39;) . Git . Filtering repositories . If you end up with a big repository which you want to split off into a new repo but maintain the history for the stuff in that folder. Use https://github.com/newren/git-filter-repo/ to filter the repo itself. To do this I pulled the old repo into a new folder to make sure I didn&#39;t destroy anything by mistake. You filter out a repo based on a folder and other potential filters. Once this repo has been filtered you can push it to a new repo. .",
            "url": "https://danielvanstrien.xyz/fastai/conda/pandas/2020/08/29/code_snippets.html",
            "relUrl": "/fastai/conda/pandas/2020/08/29/code_snippets.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Hyperparameter Optimization for Transfer Learning",
            "content": "tl;dr . This post covers: . the motivations for &#39;pragmatic hyperparameters optimization&#39; | how to do this using Optuna (with an example applied to the fastai2 library) | . Optimizing hyperparameters? . Deep learning models have a range of Hyperparameters. These include the basic building blocks of a model like the number of layers used or the size of embedding layers, and the parameters for the training of models such as learning rate. Changing some of these parameters will improve the performance of a model. There is therefore a potential win from finding the right values for these parameters. . Auto ML vs pragmatic hyperparameters optimization . As a way of framing &#39;pragmatic search&#39;, it is useful to contrast it to Auto ML. If you haven&#39;t come across it before: . The term AutoML has traditionally been used to describe automated methods for model selection and/or hyperparameter optimization. - 1. . In particular what is termed Auto ML often includes a search across model and Hyperparameters but can also refer to &#39;Neural Architecture Search&#39; in which the objective is to piece together a new model type for a specific problem or dataset. An underlying assumption of some of this Auto ML approach is that each problem or dataset requires a unique model architecture. . In contrast a more &#39;pragmatic&#39; approach uses an existing model architectures which have been shown to work across a range of datasets and tasks, and utilise transfer learning and other &#39;tricks&#39; like cyclical learning rates and data augmentation. In a heritage context, it is likely that there are going to be bigger issues with imbalanced classes, noisy labels etc, and focusing on designing a custom architecture is probably going to lead to modest improvements in the performance of the model. . So what remains to be optimized? . In contrast to Auto ML which can involve looking at huge range of potential architectures and parameters we could instead limit our focus to smaller set of things which may have a large impact on the performance of your model. . As an example use case for hyperparameters optimization I&#39;ll use two datasets which contain transcripts of trials from the Old Bailey online and which are classified into various categories (theft, deception, etc). One of the datasets is drawn the decade 1830 the other one 1730. The approach taken to classifying these trials will be to follow the &quot;Universal Language Model Fine-tuning for Text Classification&quot; approach. 2. . I won&#39;t give an in depth summary of the approach here but idea is that: . A language model - in this case a LSTM based model - is trained on a Wikipedia text. This provides a &quot;general&quot; language model that learns to &quot;understand&quot; general features of a language, in this case English | this language model is then fine-tuned on a target dataset, in the orginal paper this is IMDB movie reviews. | one this language model has been fine-tuned on the target dataset this fine-tuned language model is used as input for a classifier | . The intuition here is that by utilising a pre-trained language model the Wikipedia part, and the fine-tuning part we get the benefits of a massive training set (Wikipedia) whilst also being able to &#39;focus&#39; the language model on a target corpus which will use language differently. This makes a lot of intuitive sense, but a question in this use case is how much to fine-tune the language model on our target datasets. A reasonable assumption might be that since language will be more different in 1730 compared to 1830 we may want to fine tune the language model trained on Wikipedia more on the 1730 dataset. . We could of course test through some trial and error experiments, but this is a question which may benefit from some more systematic searching for appropriate hyperparameters. Before we get into this example in more depth I&#39;ll discuss the library I&#39;m working with for doing this hyperparameter searching. . Optuna: A hyperparameter optimization framework . In this post I will be using Optuna &quot;an automatic hyperparameter optimization software framework, particularly designed for machine learning&quot;. 3. . There are some really nice features in Optuna which I&#39;ll cover in this post as I explore the question of language model fine-tuning, so hopefully even if you don&#39;t care about the specific use case it might still provide a useful overview of Optuna. . In this blog post my examples will use version two of the fastai library but there really isn&#39;t anything that won&#39;t translate to other frameworks. Optuna has integrations for a number of libraries (including version 1 of fastai) but for this blog I won&#39;t use this integration. . A simple optimization example . To show the approach used in Optuna I&#39;ll use a simple image classification example. In this case using a toy example of classifying people vs cats in images taken from 19th Century books. . Optuna has two main concepts to understand: study and trial. A study is the overarching process of optimization based on some objective function. A trial is a single test/execution of the objective function. We&#39;ll return to this in more detail. For now lets look at a simple example. . For our first example we&#39;ll just use Optuna to test whether to use a pre-trained model or not. If the option is True then the ResNet18 model we use will use weights from pre-training on ImageNet, if False the model will start with random weights. . Looking at the high level steps of using Optuna (I&#39;ll go into more detail later). We create an objective function: . !wget -q https://zenodo.org/record/3689444/files/humancats.zip?download=1 !unzip -q *humancats.zip* -d data/ . . def objective(trial): is_pretrained = trial.suggest_categorical(&#39;pre_trained&#39;, [True, False]) dls = ImageDataLoaders.from_folder(&#39;data/human_vs_cats/&#39;, valid_pct=0.4, item_tfms=Resize(64)) learn = cnn_learner(dls, resnet18, pretrained=is_pretrained, metrics=[accuracy]) learn.fit(1) acc = learn.recorder.values[-1][-1] return acc . Most of this will look familiar if you are have used fastai before. Once we have this we create a study: . study = optuna.create_study(direction=&#39;maximize&#39;) . and then optimize this study: . study.optimize(objective, n_trials=2) . epoch train_loss valid_loss accuracy time . 0 | 1.503035 | 0.710954 | 0.555556 | 00:06 | . [I 2020-06-04 16:58:49,862] Finished trial#0 with value: 0.5555555820465088 with parameters: {&#39;pre_trained&#39;: False}. Best is trial#0 with value: 0.5555555820465088. . epoch train_loss valid_loss accuracy time . 0 | 1.691165 | 1.218440 | 0.555556 | 00:05 | . [I 2020-06-04 16:58:56,272] Finished trial#1 with value: 0.5555555820465088 with parameters: {&#39;pre_trained&#39;: False}. Best is trial#0 with value: 0.5555555820465088. . Once we&#39;ve run some trials we can inspect the study object for the best value we&#39;re optimizing for. In this case this is the accuracy but it will be whatever is returned by our function. We can also see the parameters which led to this value. . study.best_value, study.best_params . (0.5555555820465088, {&#39;pre_trained&#39;: False}) . This toy example wasn&#39;t particularly useful (it just confirmed we probably want to use a pre-trained model) but going through the steps provides an overview of the main things required by Optuna. Starting with defining a function objective . def objective(trial): . this is the function we want to optimize. We could call it something else but following the convention in the Optuna docs the function we&#39;ll call it objective. This function takes &#39;trial&#39; as an argument. . is_pretrained = trial.suggest_categorical(&#39;pre_trained&#39;, [True, False]) . here we use trial to &quot;suggest&quot; a categorical in this case one of two options (whether pre trained is set to true or false). We do this using trial.suggest_categorical and pass it the potential options (in this case True or False). . trial.suggest_blah defines the paramater &quot;search space&quot; for Optuna. We&#39;ll look at all of the options for this later on. The final step in defining our objective function i.e. the thing we want to optimize: . return acc . This return value is objective value that Optuna will optimize. Because this is just the return value of a function there is a lot of flexibility in what this can be. In this example it is accuracy but it could be training or validation loss, or another training metrics. Later on we&#39;ll look at this in more detail. . Now let&#39;s look at the study part: . study = optuna.create_study(direction=&#39;maximize&#39;) . This is the most simple way of creating a study. This creates a study object, again, we&#39;ll look at more options as we go along. The one option we pass here is the direction. This refers to to whether Optuna should try to increase the return value of our optimization function or decrease it. This depends on what you a tracking i.e. you&#39;d want to minimize error or validation loss but increase accuracy or F1 score. . Looking at the overview provided in the Optuna docs we have three main building blocks: . Trial: A single call of the objective function . | Study: An optimization session, which is a set of trials . | Parameter: A variable whose value is to be optimized . | . Parameter search space . Borrowing once more from the docs: . The difficulty of optimization increases roughly exponentially with regard to the number of parameters. That is, the number of necessary trials increases exponentially when you increase the number of parameters, so it is recommended to not add unimportant parameters . This is a crucial point. Particularly if we want to use optimization in a pragmatic way. When we have existing knowledge or evidence about what works well for a particular problem, we should use that rather than asking Optuna to find this out for us. There are some extra tricks to make our search for the best parameters more efficient which will be explored below but for now let&#39;s get back to the example use case. . Fine-tuning a language model . df_1830 = pd.read_csv(&#39;https://gist.githubusercontent.com/davanstrien/4bc85d8a4127a2791732280ffaa43293/raw/cd1a3cc53674b64c8f130edbcb34e835afa665fb/1830trial.csv&#39;) df_1730 = pd.read_csv(&#39;https://gist.githubusercontent.com/davanstrien/4bc85d8a4127a2791732280ffaa43293/raw/cd1a3cc53674b64c8f130edbcb34e835afa665fb/1730trial.csv&#39;) . . For the sake of brevity I won&#39;t cover the steps to generate this dataset the instructions for doing so for the 1830s trials can be found here (and can be easily adapted for the 1730s trial). . Unnamed: 0 Unnamed: 0.1 0 file broad narrow text . 0 14463.0 | t18361128-57a | theft-housebreaking | t18361128-57a.txt | theft | housebreaking | n n n n n57. n n n n nJOHN BYE n the younger and n n n n nFREDERICK BYE n were indicted for n n feloniously breaking and entering the dwelling-house of n n n nJohn Bye, on the n21st of November, at nSt. Giles-in-the-Fields, and stealing therein 12 apples, value 9d.; 1 box, value 1d.; 24 pence, and 1 twopenny-piece; the goods and monies of n n n nMary Byrne. n n n n n n nMARY BYRNE n. I sell fruit; I live in Titchbourne-court, Holborn. On the 21st of November I went out at one o&#39;clock, and locked my door?I left 2s. worth of penny-pieces in my drawer, and two dozen large apples?I came... | . 1 19021.0 | t18380917-2214 | theft-pocketpicking | t18380917-2214.txt | theft | pocketpicking | n n n n2214. n n n n nMARY SMITH n was indicted n n for stealing, on the n16th of September, 1 purse, value 2d.; 3 half-crowns, and twopence; the goods and monies of n n n nGeorge Sainsbury, from his person. n n n n n n nGEORGE SAINSBURY n. Between twelve and one o&#39;clock, on the 16th of September, I went to sleep in the fields, at Barnsbury-park, Islington, I had three half-crowns, and twopence, in my pocket?I was awoke, and missed my money?I went to the prisoner, and charged her with it?she said she had not got it?I followed her, and saw her drop ray purse down, it had two penny piece... | . We load the data using fastai2 TextDataLoaders . def load_lm_data(df): data_lm = TextDataLoaders.from_df( df.sample(frac=0.5), text_col=&quot;text&quot;, is_lm=True, bs=128 ) return data_lm # Classification data def load_class_data(df, data_lm): data_class = TextDataLoaders.from_df( df.sample(frac=0.5), text_col=&quot;text&quot;, label_col=&quot;broad&quot;, valid_pct=0.3, bs=128, text_vocab=data_lm.vocab, ) return data_class . . data_lm = load_lm_data(df_1830) data_class = load_class_data(df_1830, data_lm) . Create the language model learner and classifier learner: . def create_lm(): return language_model_learner(data_lm, AWD_LSTM, pretrained=True).to_fp16() def create_class_learn(): return text_classifier_learner( data_class, AWD_LSTM, metrics=[accuracy, F1Score(average=&quot;weighted&quot;)] ).to_fp16() . . Optuna trial suggest . In the example above trial.suggest_categorical was used to define the potential parameter. Optuna has five kinds of parameters which can be optimized. These all work through the trial.suggest method. . Categorical . This can be used for models, optimizers, and for True/False flags. . optimizer = trial.suggest_categorical(&#39;optimizer&#39;, [&#39;MomentumSGD&#39;, &#39;Adam&#39;]) . Integer . n_epochs = trial.suggest_int(&#39;num_epochs&#39;, 1, 3) . Uniform . max_zoom = trial.suggest_uniform(&#39;max_zoom&#39;, 0.0, 1.0) . Loguniform . learning_rate = trial.suggest_loguniform(&#39;learning_rate&#39;, 1e-4, 1e-2) . Discrete-uniform . drop_path_rate = trial.suggest_discrete_uniform(&#39;drop_path_rate&#39;, 0.0, 1.0) . The string value provides a key for the parameters which is used to access these parameters later, it&#39;s therefore important to give them a sensible name. . Limiting parameters? . Adding additional trial.suggest to your optimization function increases the search space for Optuna to optimize over so you should avoid adding additional parameters if they are not necessary. . The other way in which the search space can be constrained is to limit the range of the search i.e. for learning rate . learning_rate = trial.suggest_loguniform(&#39;learning_rate&#39;, 1e-4, 1e-2) . is preferable over . learning_rate = trial.suggest_loguniform(&#39;learning_rate&#39;, 1e-10, 1e-1) . if it&#39;s not likely the optimal learning rate will sit outside of this range. . How many parameters you include will also depend on the type of model you are trying to train. In the use case of fine-tuning a language model we will want to limit the options more since language models are generally quite slow to train. If, on the other hand, we were trying to improve an image classification model which only takes minutes to train then searching through a larger parameter space would become more feasible. . Objective function for fine-tuning a language model . The objective function below has two stages; train a language model, use the encoder from this language model for a classifier. . The parameters we&#39;re trying to optimize in this case are: . learning rate for the frozen language model | number of epochs to train only the final layers of the language model | learning rate for the unfrozen language model | number of epochs for training the whole language model | . We use lm_learn.no_bar() as a context manager to reduce the amount of logging. . def objective(trial): lm_learn = create_lm() lr_frozen = trial.suggest_loguniform(&quot;learning_rate_frozen&quot;, 1e-4, 1e-1) head_epochs = trial.suggest_int(&quot;head_epochs&quot;, 1, 5) with lm_learn.no_bar(): lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen) # Unfrozen Language model lr_unfreeze = trial.suggest_loguniform(&quot;learning_rate_unfrozen&quot;, 1e-7, 1e-1) body_epochs = trial.suggest_int(&quot;lm_body_epochs&quot;, 1, 5) lm_learn.unfreeze() with lm_learn.no_bar(): lm_learn.fit_one_cycle(body_epochs, lr_unfreeze) lm_learn.save_encoder(&quot;finetuned&quot;) # Classification cl_learn = create_class_learn() cl_learn.load_encoder(&quot;finetuned&quot;) cl_learn.fit_one_cycle(3) f1 = cl_learn.recorder.values[-1][-1] return f1 . We can give our study a name and also store it in a database. This allows for resuming previous trials later and accessing the history of previous trials. There are various options for database backends outlined in the documentation. . Creating the study . study_name = &quot;tunelm1830&quot; study = optuna.create_study( study_name=study_name, direction=&quot;maximize&quot;, storage=f&quot;sqlite:///{out_path}/optuma/example.db&quot;, ) . [I 2020-06-05 15:09:05,470] A new study created with name: tunelm1830 . Optimize . Now we&#39;ll run 3 trials and use show_progress_bar=True to give an ETA on when the trials will finish. . study.optimize(objective, n_trials=3, show_progress_bar=True) . /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future. . (#4) [0,5.382655620574951,4.875850200653076,&#39;00:24&#39;] (#4) [1,5.292355537414551,4.737764835357666,&#39;00:24&#39;] (#4) [2,5.183778285980225,4.647550106048584,&#39;00:24&#39;] (#4) [3,5.11093282699585,4.608272552490234,&#39;00:24&#39;] (#4) [4,5.072442054748535,4.601930618286133,&#39;00:24&#39;] (#4) [0,4.7495622634887695,4.241390228271484,&#39;00:27&#39;] . epoch train_loss valid_loss accuracy f1_score time . 0 | 2.326032 | 2.070412 | 0.020000 | 0.017034 | 00:10 | . 1 | 2.302230 | 2.136864 | 0.023333 | 0.003590 | 00:10 | . 2 | 2.269061 | 2.180663 | 0.016667 | 0.004408 | 00:10 | . [I 2020-06-05 15:12:20,128] Finished trial#0 with value: 0.00440805109922757 with parameters: {&#39;learning_rate_frozen&#39;: 0.00014124685078723662, &#39;head_epochs&#39;: 5, &#39;learning_rate_unfrozen&#39;: 0.00010276862511970148, &#39;lm_body_epochs&#39;: 1}. Best is trial#0 with value: 0.00440805109922757. (#4) [0,4.713407516479492,3.7350399494171143,&#39;00:24&#39;] (#4) [1,3.998744249343872,3.3055806159973145,&#39;00:24&#39;] (#4) [2,3.6486754417419434,3.192685842514038,&#39;00:24&#39;] (#4) [3,3.4996860027313232,3.1756556034088135,&#39;00:24&#39;] (#4) [0,3.4227023124694824,3.163315534591675,&#39;00:27&#39;] (#4) [1,3.3954737186431885,3.140226364135742,&#39;00:27&#39;] (#4) [2,3.3778774738311768,3.125929117202759,&#39;00:27&#39;] (#4) [3,3.357388973236084,3.119621753692627,&#39;00:27&#39;] (#4) [4,3.3542206287384033,3.1186859607696533,&#39;00:27&#39;] . epoch train_loss valid_loss accuracy f1_score time . 0 | 2.368984 | 2.121307 | 0.013333 | 0.000759 | 00:11 | . 1 | 2.335033 | 2.022853 | 0.250000 | 0.368652 | 00:10 | . 2 | 2.296630 | 1.948786 | 0.313333 | 0.452365 | 00:10 | . [I 2020-06-05 15:16:49,562] Finished trial#1 with value: 0.45236502121696065 with parameters: {&#39;learning_rate_frozen&#39;: 0.0060643425219262335, &#39;head_epochs&#39;: 4, &#39;learning_rate_unfrozen&#39;: 2.734844423029637e-05, &#39;lm_body_epochs&#39;: 5}. Best is trial#1 with value: 0.45236502121696065. (#4) [0,5.3748459815979,4.851675987243652,&#39;00:24&#39;] (#4) [1,5.247058868408203,4.672318935394287,&#39;00:24&#39;] (#4) [2,5.111597061157227,4.559732437133789,&#39;00:24&#39;] (#4) [3,5.026832103729248,4.512131690979004,&#39;00:24&#39;] (#4) [4,4.982809066772461,4.5044732093811035,&#39;00:24&#39;] (#4) [0,4.915407657623291,4.423311233520508,&#39;00:27&#39;] (#4) [1,4.857243061065674,4.394893646240234,&#39;00:27&#39;] . epoch train_loss valid_loss accuracy f1_score time . 0 | 2.368439 | 2.036706 | 0.240000 | 0.360355 | 00:10 | . 1 | 2.359790 | 2.093103 | 0.033333 | 0.045878 | 00:09 | . 2 | 2.331945 | 2.140194 | 0.016667 | 0.013589 | 00:10 | . [I 2020-06-05 15:20:20,119] Finished trial#2 with value: 0.013588651008106425 with parameters: {&#39;learning_rate_frozen&#39;: 0.0001971120155925954, &#39;head_epochs&#39;: 5, &#39;learning_rate_unfrozen&#39;: 1.0649951798153689e-05, &#39;lm_body_epochs&#39;: 2}. Best is trial#1 with value: 0.45236502121696065. . Results . You can see how trials are peforming in the logs with the last part of the log reporting the best trial so far. We can now access the best value and best_params. . study.best_value, study.best_params . (0.45236502121696065, {&#39;head_epochs&#39;: 4, &#39;learning_rate_frozen&#39;: 0.0060643425219262335, &#39;learning_rate_unfrozen&#39;: 2.734844423029637e-05, &#39;lm_body_epochs&#39;: 5}) . Where to start the search? . As I mentioned at the start I think it&#39;s worth trying to think pragmatically about how to use hyper-parameter optimizations. I already mentioned limiting the number of parameters and limiting the potential options in these parameters. However we can also intervene more directly in how Optuna runs a trial. . Suggesting a learning rate . One of the yummiest features in fastai which has also made it into other deep-learning libraries is the learning rate finer lr_find(). As a reminder: . the LR Finder trains the model with exp onentially growing learning rates from start_lr to end_lr for num_it and stops in case of divergence (unless stop_div=False) then plots the losses vs the learning rates with a log scale. . Since the Learning rate finder often gives a good learning rate we should see if we can use this as a starting point for our trials. . Enqueue trial . Using enqueue_trial you can queue up trials with specied paramters. This can be for all of the parameters or just a subset. We can use lr_find to suggest a learning rate for the language model and then que a trial with this learning rate. . lm_learn = create_lm() lm_learn.unfreeze() . lr_min,lr_steep = lm_learn.lr_find(suggestions=True) . lr_min, lr_steep . (0.014454397559165954, 0.033113110810518265) . study.enqueue_trial({&#39;learning_rate_unfrozen&#39;: lr_steep}) study.optimize(objective, n_trials=1) . /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future. . (#4) [0,5.322241306304932,4.736147403717041,&#39;00:24&#39;] (#4) [1,5.095097541809082,4.474568843841553,&#39;00:24&#39;] (#4) [2,4.91882848739624,4.365659713745117,&#39;00:24&#39;] (#4) [3,4.820737838745117,4.348053455352783,&#39;00:24&#39;] (#4) [0,3.5270116329193115,3.0885186195373535,&#39;00:27&#39;] (#4) [1,3.1028788089752197,2.8053553104400635,&#39;00:27&#39;] (#4) [2,2.7882776260375977,2.611638069152832,&#39;00:27&#39;] (#4) [3,2.49800705909729,2.539992094039917,&#39;00:27&#39;] . epoch train_loss valid_loss accuracy f1_score time . 0 | 2.325723 | 2.102552 | 0.010000 | 0.006709 | 00:10 | . 1 | 2.293266 | 2.006258 | 0.216667 | 0.332841 | 00:10 | . 2 | 2.258634 | 1.928858 | 0.566667 | 0.686662 | 00:10 | . [I 2020-06-05 15:28:58,707] Finished trial#3 with value: 0.6866621960133127 with parameters: {&#39;head_epochs&#39;: 4, &#39;learning_rate_frozen&#39;: 0.0003841551576945897, &#39;learning_rate_unfrozen&#39;: 0.033113110810518265, &#39;lm_body_epochs&#39;: 4}. Best is trial#3 with value: 0.6866621960133127. . Using the learning rate from the LR_finder gives us our best trial so far. This is likely to be because learning rate is a particularly important hyper-parameter. The suggested learning rate from lr_find may not always be the best but using either the suggested one or picking one based on the plot as a starting point for the trial may help Optuna to start from sensible starting point while still giving the freedom for optuna to diverge away from this in later trials if helps the objective function. . Pruning trials . The next feature of Optuna which helps make parameter searching more efficient is pruning. Pruning is a process for stopping bad trials early. . For example if we have the following three trials: . Trial 1 - epoch 1: 87% accuracy | Trial 2 - epoch 1: 85% accuracy | Trial 3 - epoch 1: 60% accuracy | . probably it&#39;s not worth continuing with trial 3. Pruning trials helps focus computational resources on trials which are likely to improve on previous trials. The likely here is important. It is possible that some trials may be pruned early which actually would have done better in the end. Optuna offers a number of different pruning algorithms, I won&#39;t cover these here but the documentation gives a good overview and includes links to the papers which propose the implemented pruning algorithms. . How to do pruning in Optuna? . Optuna has intergrations with various machine learning libraries. These intergrations can help with the pruning but setting up pruning manually is also pretty straight forward to do. . The two things we need to do is report the value and the stage in the training porcess: . trial.report(metric, step) . then we call: . if trial.should_prune(): raise optuna.exceptions.TrialPruned() . Depending on your objective function this will be put in different places. In the example of fine-tuning the language model, because we&#39;re trying to optimize the classification part it, it means the pruning step can only be called quite late in the traing loop. Ideally it would be called earlier but we still save a little bit of time on unpromising trials. . The new objective function with pruning: . def objective(trial): lm_learn = create_lm() lr_frozen = trial.suggest_loguniform(&quot;learning_rate_frozen&quot;, 1e-4, 1e-1) head_epochs = trial.suggest_int(&quot;head_epochs&quot;, 1, 5) with lm_learn.no_bar(): lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen) # Unfrozen Language model lr_unfreeze = trial.suggest_loguniform(&quot;learning_rate_unfrozen&quot;, 1e-7, 1e-1) body_epochs = trial.suggest_int(&quot;lm_body_epochs&quot;, 1, 5) lm_learn.unfreeze() with lm_learn.no_bar(): lm_learn.fit_one_cycle(body_epochs, lr_unfreeze) lm_learn.save_encoder(&quot;finetuned&quot;) # Classification cl_learn = create_class_learn() cl_learn.load_encoder(&quot;finetuned&quot;) for step in range(3): cl_learn.fit(1) # Pruning intermediate_f1 = cl_learn.recorder.values[-1][ -1 ] # get f1 score for current step trial.report(intermediate_f1, step) # report f1 if trial.should_prune(): # let optuna decide whether to prune raise optuna.exceptions.TrialPruned() f1 = cl_learn.recorder.values[-1][-1] return f1 . We can load the same study as before using the python load_if_exists flag. . study_name = &quot;tunelm1830&quot; study = optuna.create_study( study_name=study_name, direction=&quot;maximize&quot;, storage=f&quot;sqlite:///{out_path}/optuma/example.db&quot;, load_if_exists=True, pruner=optuna.pruners.SuccessiveHalvingPruner(), ) . [I 2020-06-06 14:30:47,724] Using an existing study with name &#39;tunelm1830&#39; instead of creating a new one. . We can now run some more trials. Instead of specifying the number of trials we can also specify how long optuma should search for. . study.enqueue_trial({&#39;learning_rate_unfrozen&#39;: lr_steep}) study.optimize(objective, timeout=60*60*0.5) . and get the best trial: . study.best_trial . FrozenTrial(number=13, value=0.8657462002717475, datetime_start=datetime.datetime(2020, 6, 5, 15, 59, 26, 230967), datetime_complete=datetime.datetime(2020, 6, 5, 16, 3, 26, 392390), params={&#39;head_epochs&#39;: 4, &#39;learning_rate_frozen&#39;: 0.0012866609022148768, &#39;learning_rate_unfrozen&#39;: 1.3302852136460371e-06, &#39;lm_body_epochs&#39;: 4}, distributions={&#39;head_epochs&#39;: IntUniformDistribution(high=5, low=1, step=1), &#39;learning_rate_frozen&#39;: LogUniformDistribution(high=0.1, low=0.0001), &#39;learning_rate_unfrozen&#39;: LogUniformDistribution(high=0.1, low=1e-07), &#39;lm_body_epochs&#39;: IntUniformDistribution(high=5, low=1, step=1)}, user_attrs={}, system_attrs={&#39;completed_rung_0&#39;: 0.8156506309537317}, intermediate_values={0: 0.251088767516275, 1: 0.8156506309537317, 2: 0.8657462002717475}, trial_id=14, state=TrialState.COMPLETE) . and best value and pararms: . study.best_value, study.best_params . (0.8657462002717475, {&#39;head_epochs&#39;: 4, &#39;learning_rate_frozen&#39;: 0.0012866609022148768, &#39;learning_rate_unfrozen&#39;: 1.3302852136460371e-06, &#39;lm_body_epochs&#39;: 4}) . Paramters for the 1730s trials data . We can do the same process with the 1730s trials, starting with a suggested learning rate. . data_lm = load_lm_data(df_1730) data_class = load_class_data(df_1730, data_lm) lm_learn = create_lm() lm_learn.unfreeze() lr_min,lr_steep = lm_learn.lr_find(suggestions=True) . . def objective(trial): lm_learn = create_lm() lr_frozen = trial.suggest_loguniform(&quot;learning_rate_frozen&quot;, 1e-4, 1e-1) head_epochs = trial.suggest_int(&quot;head_epochs&quot;, 1, 5) with lm_learn.no_bar(): lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen) # Unfrozen Language model lr_unfreeze = trial.suggest_loguniform(&quot;learning_rate_unfrozen&quot;, 1e-7, 1e-1) body_epochs = trial.suggest_int(&quot;lm_body_epochs&quot;, 1, 5) lm_learn.unfreeze() with lm_learn.no_bar(): lm_learn.fit_one_cycle(body_epochs, lr_unfreeze) lm_learn.save_encoder(&quot;finetuned&quot;) # Classification cl_learn = create_class_learn() cl_learn.load_encoder(&quot;finetuned&quot;) for step in range(3): cl_learn.fit(1) intermediate_f1 = cl_learn.recorder.values[-1][-1] trial.report(intermediate_f1, step) if trial.should_prune(): raise optuna.exceptions.TrialPruned() f1 = cl_learn.recorder.values[-1][-1] return f1 . . study_name = &quot;tunelm1730&quot; study = optuna.create_study( study_name=study_name, direction=&quot;maximize&quot;, storage=f&quot;sqlite:///{out_path}/optuma/example.db&quot;, load_if_exists=True, pruner=optuna.pruners.SuccessiveHalvingPruner(), ) . . [I 2020-06-08 15:06:54,474] Using an existing study with name &#39;tunelm1730&#39; instead of creating a new one. . study.enqueue_trial({&#39;learning_rate_unfrozen&#39;: lr_steep}) study.optimize(objective, timeout=60*60*0.5) . Trials can be accssed as part of the study object. Running trials for 30 mins with early pruning results in 20 trials . len(study.trials) . 20 . We can also see which was the best trial. . study.best_trial.number . 2 . The number of trials run depends mainly on how long your model takes to train, the size of the paramter search space and your patience. If trials are failing to improve better scores for a long time it&#39;s probably better to actively think about how to improve your approach to the problem (better data, more data, chaning model design etc.) rather than hoping hyperaparmet tuning will fix the problem. . Comparing language model parameters . Previous trials can be loaded using load_study . study1830 = optuna.load_study(&#39;tunelm1830&#39;, storage=f&#39;sqlite:///{out_path}/optuma/example.db&#39;) study1730 = optuna.load_study(&#39;tunelm1730&#39;, storage=f&#39;sqlite:///{out_path}/optuma/example.db&#39;) . First comparing the best f1 values for both datasets: . print(f&#39;Best 1830 value was: {study1830.best_value:.3}&#39;) print(f&#39;Best 1730 value was: {study1730.best_value:.3}&#39;) . Best 1830 value was: 0.866 Best 1730 value was: 0.781 . The paramters used to get the best value: . 1830 parameters . {&#39;head_epochs&#39;: 4, &#39;learning_rate_frozen&#39;: 0.0012866609022148768, &#39;learning_rate_unfrozen&#39;: 1.3302852136460371e-06, &#39;lm_body_epochs&#39;: 4} . 1730 parameters . {&#39;head_epochs&#39;: 3, &#39;learning_rate_frozen&#39;: 0.002145480897071231, &#39;learning_rate_unfrozen&#39;: 9.889236991663078e-06, &#39;lm_body_epochs&#39;: 1} . Specific parameters can also be accessed . study1830.best_params[&#39;learning_rate_unfrozen&#39;], study1730.best_params[&#39;learning_rate_unfrozen&#39;] . (1.3302852136460371e-06, 9.889236991663078e-06) . Visualizations . Optuna has a variety of visulizations, I will only briefly show a few of these here. . plot_intermediate_values shows the intermediate values. This can be useful for getting a sense of how trials progress and also help give a sense of whether some trials are being pruned prematurely . optuna.visualization.plot_intermediate_values(study1830) . . . plot_parallel_coordinate plots parameters choices in relation to values. It can be hard to read these plots but they can also be helpful for giving a sense of which choices for parameters work best. . optuna.visualization.plot_parallel_coordinate(study1830) . . . Parameter importance . Optuna has experimental support for getting parameter importance. . optuna.importance.get_param_importances(study1730) . /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: get_param_importances is experimental (supported from v1.3.0). The interface can change in the future. /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning: MeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future. . OrderedDict([(&#39;learning_rate_frozen&#39;, 0.43423246892923717), (&#39;learning_rate_unfrozen&#39;, 0.2904735896601219), (&#39;head_epochs&#39;, 0.2433021650269149), (&#39;lm_body_epochs&#39;, 0.031991776383726155)]) . optuna.importance.get_param_importances(study1830) . /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: get_param_importances is experimental (supported from v1.3.0). The interface can change in the future. /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning: MeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future. . OrderedDict([(&#39;learning_rate_unfrozen&#39;, 0.35548906967729954), (&#39;learning_rate_frozen&#39;, 0.33998779901100146), (&#39;head_epochs&#39;, 0.21196438930810765), (&#39;lm_body_epochs&#39;, 0.09255874200359132)]) . These are broadly similar although learning rate frozen/unfrozen are in different places for the 1730 and 1830 trials. . Multi objective . Optuna has experimental support for multi-objective optimization. This might be useful if you don&#39;t want to optimize for only one metrics. . An alternative to using this approach is to report other things you care about during the trial but don&#39;t directly want to optimize for. As an example, you might mostly care about the accuracy of a model but also care a bit about how long it takes to do inference. . One approach is to use a multi-objective trial. An alternative is to instead log inference time as part of the trial and continue to optimize for other metrics. You can then later on balance the accuracy of different trials with the inference time. It may turn out later that a slightly slower inference time can be dealt with by scaling vertically. Not prematurely optimizing for multi-objectives can therefore give you more flexibility. To show this in practice I&#39;ll use an image classification dataset. . The data . The data is images of maps and other things from historic newspapers. The aim is to classify whether the image is a map or something else. . dls = ImageDataLoaders.from_folder( &quot;data/1905_maps/&quot;, valid_pct=0.3, item_tfms=Resize(256) ) dls.show_batch() . learn.unfreeze() lr_min,unfrozen_lr_steep = learn.lr_find(suggestions=True) . Excessive model parameter search . Since the time to train the model is more reasonable we can add a more parameters to the search space. In practice this is pretty overkill but is useful as an example of working with the outputs of trials with many parameters. . def objective(trial): apply_tfms = trial.suggest_categorical(&quot;apply_tfms&quot;, [True, False]) if apply_tfms: aug_tfms = aug_transforms( mult=trial.suggest_uniform(&quot;mult&quot;, 0.0, 1.0), do_flip=trial.suggest_categorical(&quot;do_flip&quot;, [True, False]), flip_vert=trial.suggest_categorical(&quot;flip_vert&quot;, [True, False]), max_rotate=trial.suggest_uniform(&quot;max_rotate&quot;, 0, 180), max_zoom=trial.suggest_uniform(&quot;max_zoom&quot;, 0, 3.0), max_lighting=trial.suggest_uniform(&quot;max_lighting&quot;, 0.0, 1.0), ) else: aug_tfms = None dls = ImageDataLoaders.from_folder( &quot;data/1905_maps/&quot;, valid_pct=0.3, item_tfms=Resize(256), aug_transforms=aug_tfms ) model = trial.suggest_categorical( &quot;model&quot;, [&quot;resnet18&quot;, &quot;resnet50&quot;, &quot;xresnet50&quot;, &quot;squeezenet1_0&quot;, &quot;densenet121&quot;] ) learn = cnn_learner( dls, arch=eval(model), pretrained=True, metrics=[F1Score(average=&quot;weighted&quot;)] ).to_fp16() epochs = trial.suggest_int(&quot;epochs&quot;, 1, 10) for step in range(epochs): with learn.no_bar(): learn.fit_one_cycle( 1, base_lr=trial.suggest_loguniform(&quot;learning_rate&quot;, 1e-5, 1e-1) ) unfrozen_epochs = trial.suggest_int(&quot;unfrozen_epochs&quot;, 1, 10) unfrozen_lr = trial.suggest_loguniform(&quot;unfrozen_learning_rate&quot;, 1e-10, 1e-1) learn.unfreeze() for step in range(unfrozen_epochs): with learn.no_bar(): learn.fit_one_cycle(1, lr_max=unfrozen_lr) int_f1 = learn.recorder.values[-1][-1] trial.report(int_f1, step) if trial.should_prune(): raise optuna.exceptions.TrialPruned() t0 = time.time() learn.validate() t1 = time.time() execute_time = t1 - t0 trial.set_user_attr(&quot;execute_time&quot;, execute_time) f1 = learn.recorder.values[-1][-1] return f1 . Create the study . study_name = &quot;mapsmegastudyXL&quot; # Unique identifier of the study. study = optuna.create_study( direction=&quot;maximize&quot;, load_if_exists=True, study_name=study_name, storage=f&quot;sqlite:///{out_path}/optuma/blog.db&quot;, pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=2), ) . [I 2020-06-07 15:03:24,138] Using an existing study with name &#39;mapsmegastudyXL&#39; instead of creating a new one. . Queue up with some parameters . study.enqueue_trial( { &quot;pre_trained&quot;: True, &quot;apply_tfms&quot;: True, &quot;epochs&quot;: 5, &quot;learning_rate&quot;: lr_steep, &quot;model&quot;: &quot;resnet50&quot;, &quot;unfrozen_learning_rate&quot;: unfrozen_lr_steep, } ) study.optimize(objective, n_trials=1, show_progress_bar=True) . /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future. /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future. . (#5) [0,1.162647008895874,1.0643662214279175,0.38594077225581136,&#39;00:04&#39;] (#5) [0,1.1730060577392578,0.8583190441131592,0.45458674870439575,&#39;00:02&#39;] (#5) [0,0.7940309047698975,0.40110471844673157,0.8101934029975151,&#39;00:02&#39;] (#5) [0,0.3774714767932892,0.3251221776008606,0.8738329238329239,&#39;00:02&#39;] (#5) [0,0.20592834055423737,0.304998517036438,0.8914149443561209,&#39;00:02&#39;] (#5) [0,0.14400754868984222,0.3399428725242615,0.9003332765709003,&#39;00:03&#39;] (#5) [0,0.11649172753095627,0.3571062982082367,0.8729641116526362,&#39;00:03&#39;] . [I 2020-06-07 15:03:57,405] Finished trial#0 with value: 0.8729641116526362 with parameters: {&#39;apply_tfms&#39;: True, &#39;do_flip&#39;: False, &#39;epochs&#39;: 5, &#39;flip_vert&#39;: True, &#39;learning_rate&#39;: 0.00015848931798245758, &#39;max_lighting&#39;: 0.5155363265412508, &#39;max_rotate&#39;: 93.50185801538605, &#39;max_zoom&#39;: 2.5014402368129147, &#39;model&#39;: &#39;resnet50&#39;, &#39;mult&#39;: 0.7973732804273224, &#39;unfrozen_epochs&#39;: 2, &#39;unfrozen_learning_rate&#39;: 1.4454397387453355e-05}. Best is trial#0 with value: 0.8729641116526362. . queue up with some less sensible defaults . study.enqueue_trial( {&quot;pre_trained&quot;: False, &quot;apply_tfms&quot;: False, &quot;epochs&quot;: 1, &quot;unfrozen_epochs&quot;: 1} ) study.optimize(objective, n_trials=1, show_progress_bar=True) . /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: enqueue_trial is experimental (supported from v1.2.0). The interface can change in the future. /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future. . (#5) [0,1.1455873250961304,1.7333940267562866,0.3123010228273386,&#39;00:01&#39;] (#5) [0,1.0485259294509888,1.4432364702224731,0.4545249081834448,&#39;00:01&#39;] . [I 2020-06-07 15:04:18,823] Finished trial#1 with value: 0.4545249081834448 with parameters: {&#39;apply_tfms&#39;: False, &#39;epochs&#39;: 1, &#39;learning_rate&#39;: 1.4039901997074766e-05, &#39;model&#39;: &#39;resnet18&#39;, &#39;unfrozen_epochs&#39;: 1, &#39;unfrozen_learning_rate&#39;: 4.041607859100835e-07}. Best is trial#0 with value: 0.8729641116526362. . Now optimize for 500 trials . study.optimize(objective, n_trials=500,show_progress_bar=True) . study = optuna.load_study(&#39;mapsmegastudyXL&#39;, storage=f&#39;sqlite:///{out_path}/optuma/blog.db&#39;) . The best finishing values and parameters: . study.best_value, study.best_params . (0.963975663975664, {&#39;apply_tfms&#39;: True, &#39;do_flip&#39;: True, &#39;epochs&#39;: 10, &#39;flip_vert&#39;: False, &#39;learning_rate&#39;: 0.0785689562916925, &#39;max_lighting&#39;: 0.5064203068969654, &#39;max_rotate&#39;: 168.972217754609, &#39;max_zoom&#39;: 1.6141746329756919, &#39;model&#39;: &#39;densenet121&#39;, &#39;mult&#39;: 0.6087267126078458, &#39;unfrozen_epochs&#39;: 4, &#39;unfrozen_learning_rate&#39;: 7.6080876225791396e-06}) . Visualization . Taking a look at parallel_coordinate in this case gives some sense of which options work best. . optuna.visualization.plot_parallel_coordinate(study) . . . Importance . optuna.importance.get_param_importances(study) . /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning: get_param_importances is experimental (supported from v1.3.0). The interface can change in the future. /usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning: MeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future. . OrderedDict([(&#39;unfrozen_learning_rate&#39;, 0.6945560978629778), (&#39;epochs&#39;, 0.13207296757949719), (&#39;model&#39;, 0.07996254760084977), (&#39;unfrozen_epochs&#39;, 0.04455237119259635), (&#39;learning_rate&#39;, 0.04014544684326522), (&#39;apply_tfms&#39;, 0.008710568920813712)]) . Learning rate is by far the most important learning rate, again this suggests that using learning rate finder makes a lot of sense as a starting point. . Working with Optuna trial data . There are now ~500 trials which are stored in the study. Each of these trials contains the parameters used, metadata about the trial, the value of the thing being optimized, and importantly for this example the user attribute which stores the validation time. Optuna makes it very easy to export this information to a dataframe. . df = study.trials_dataframe() df.head(3) . number value datetime_start datetime_complete duration params_apply_tfms params_do_flip params_epochs params_flip_vert params_learning_rate params_max_lighting params_max_rotate params_max_zoom params_model params_mult params_unfrozen_epochs params_unfrozen_learning_rate user_attrs_execute_time system_attrs_completed_rung_0 system_attrs_completed_rung_1 system_attrs_fixed_params state . 0 0 | 0.872964 | 2020-06-07 15:03:29.911841 | 2020-06-07 15:03:57.151460 | 00:00:27.239619 | True | False | 5.0 | True | 0.000158 | 0.515536 | 93.501858 | 2.50144 | resnet50 | 0.797373 | 2.0 | 1.445440e-05 | 0.82319 | NaN | NaN | {&#39;pre_trained&#39;: True, &#39;apply_tfms&#39;: True, &#39;epochs&#39;: 5, &#39;learning_rate&#39;: 0.00015848931798245758, &#39;model&#39;: &#39;resnet50&#39;, &#39;unfrozen_learning_rate&#39;: 1.4454397387453355e-05} | COMPLETE | . 1 1 | 0.454525 | 2020-06-07 15:04:11.520248 | 2020-06-07 15:04:18.419082 | 00:00:06.898834 | False | NaN | 1.0 | NaN | 0.000014 | NaN | NaN | NaN | resnet18 | NaN | 1.0 | 4.041608e-07 | 0.67698 | NaN | NaN | {&#39;pre_trained&#39;: False, &#39;apply_tfms&#39;: False, &#39;epochs&#39;: 1, &#39;unfrozen_epochs&#39;: 1} | COMPLETE | . 2 2 | NaN | 2020-06-07 15:04:32.047588 | NaT | NaT | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | RUNNING | . We can now easily work with the trial data using pandas. Lets start by getting the best two values . df.sort_values([&#39;value&#39;], ascending=False).head(2) . Unnamed: 0 number value datetime_start datetime_complete duration params_apply_tfms params_do_flip params_epochs params_flip_vert ... params_max_zoom params_model params_mult params_unfrozen_epochs params_unfrozen_learning_rate user_attrs_execute_time system_attrs_completed_rung_0 system_attrs_completed_rung_1 system_attrs_fixed_params state . 177 177 | 177 | 0.963976 | 2020-06-07 16:48:36.232551 | 2020-06-07 16:49:21.393454 | 0 days 00:00:45.160903000 | True | True | 10.0 | False | ... | 1.614175 | densenet121 | 0.608727 | 4.0 | 7.608088e-06 | 0.880459 | 0.954955 | NaN | NaN | COMPLETE | . 302 302 | 302 | 0.955064 | 2020-06-07 18:11:00.667449 | 2020-06-07 18:11:45.658241 | 0 days 00:00:44.990792000 | True | True | 10.0 | False | ... | 0.921689 | densenet121 | 0.115708 | 4.0 | 6.210737e-10 | 0.878865 | 0.945946 | NaN | NaN | COMPLETE | . 2 rows × 23 columns . We can see how often transforms were applied in the trials . df[&#39;params_apply_tfms&#39;].value_counts() . True 360 False 142 Name: params_apply_tfms, dtype: int64 . Viewing the number of trials for each model which had a value over 90 . df[&#39;params_model&#39;][df[&#39;value&#39;] &gt;= 0.90].value_counts() . densenet121 181 resnet50 9 squeezenet1_0 2 Name: params_model, dtype: int64 . Filtering a bit more aggressively (value above 94) . df94 = df[df[&#39;value&#39;] &gt;= 0.94] . len(df94) . 13 . How often were transforms applied for these trials . df94[&#39;params_apply_tfms&#39;].value_counts() . True 11 False 2 Name: params_apply_tfms, dtype: int64 . The number of unfrozen epochs . df94[&#39;params_unfrozen_epochs&#39;].value_counts() . 4.0 6 2.0 3 3.0 2 6.0 1 5.0 1 Name: params_unfrozen_epochs, dtype: int64 . Getting back to the validation time we can get the max, min and mean values . df[&#39;user_attrs_execute_time&#39;].max(), df[&#39;user_attrs_execute_time&#39;].min(), df[&#39;user_attrs_execute_time&#39;].mean() . (0.9760787487030028, 0.6313643455505371, 0.8461264789613903) . If we did care about reducing the execution time we could use these values to find the trial with the shortest execution time: . df94[&#39;user_attrs_execute_time&#39;].sort_values() . 96 0.837618 426 0.848863 394 0.849243 395 0.851704 438 0.852672 500 0.863168 344 0.875520 432 0.877422 302 0.878865 177 0.880459 473 0.884703 372 0.906770 294 0.907221 Name: user_attrs_execute_time, dtype: float64 . If we were happy with slightly lower performance we could pick the study with the shortest execution time which is still achieves a f1 above 94% . df94.loc[96] . number 96 value 0.945755 datetime_start 2020-06-07 15:57:20.382634 datetime_complete 2020-06-07 15:57:54.848296 duration 0 days 00:00:34.465662 params_apply_tfms False params_do_flip NaN params_epochs 9 params_flip_vert NaN params_learning_rate 8.47479e-05 params_max_lighting NaN params_max_rotate NaN params_max_zoom NaN params_model densenet121 params_mult NaN params_unfrozen_epochs 2 params_unfrozen_learning_rate 4.31178e-07 user_attrs_execute_time 0.837618 system_attrs_completed_rung_0 NaN system_attrs_completed_rung_1 NaN system_attrs_fixed_params NaN state COMPLETE Name: 96, dtype: object . This is a slightly artificial example but hopefully shows the possibility of logging user attributes which can then be accessed easily later without prematurely optimizing for something which may not be important. . Further reading . Hopefully this post has been a helpful overview of Optuna with a somewhat realistic use case. I would recommend reading the Optuna docs which covers things in much more detail. . References . 1. Auto ml [auto-ml, fastai blog(https://www.fast.ai/2018/07/16/auto-ml2/#auto-ml↩ . 2. introducting ulmfit nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html↩ . 3. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta,and Masanori Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In KDD.↩ .",
            "url": "https://danielvanstrien.xyz/hyperparameter%20optimisation/optimisation/optuna/fastai2/transfer%20learning/2020/07/01/optuna.html",
            "relUrl": "/hyperparameter%20optimisation/optimisation/optuna/fastai2/transfer%20learning/2020/07/01/optuna.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Multi-model metadata generation",
            "content": "Learning from multiple input types . Deep learning models usually take one type of input (image, text etc.) to predict output labels (category, entities etc). This usually makes sense if the data you are using to make predictions contains a lot of information. i.e. a chunk of text from a movie review or an image. . Recently I have been playing around with a Website Classification Dataset from the UK web archive. The dataset is derived from a manually curated web archive which contains a primary and secondary category for each web page. The UK web archive has made a dataset available based on this archive which contains the manually classified subject categories alongside the page URL and the page title. . As part of playing around with this dataset I was keen to see if a multi-input model would work well. In this case exploring a model that takes both text and tabular data as input. A preview of the data: . Primary Category Secondary Category Title URL . 0 Arts &amp; Humanities | Architecture | 68 Dean Street | http://www.sixty8.com/ | . 1 Arts &amp; Humanities | Architecture | Abandoned Communities | http://www.abandonedcommunities.co.uk/ | . 2 Arts &amp; Humanities | Architecture | Alexander Thomson Society | http://www.greekthomson.com/ | . 3 Arts &amp; Humanities | Architecture | Arab British Centre, The | http://www.arabbritishcentre.org.uk/ | . 4 Arts &amp; Humanities | Architecture | Architectural Association School of Architecture | http://www.aaschool.ac.uk/ | . Based on this data the UK web archive are interested: . &quot;in understanding whether high-level metadata like this can be used to train an appropriate automatic classification system so that we might use this manually generated dataset to partially automate the categorisation of our larger archives.&quot; . This is going to be fairly tricky but offers a nice excuse to try to use models with multiple inputs to predict our categories. . Looking at the data . Taking a closer look at the data: . Unique primary categories . len(df[&#39;Primary Category&#39;].unique()) . 24 . Unique secondary categories . len(df[&#39;Secondary Category&#39;].unique()) . 104 . Predicting a 104 different labels is going to be pretty difficult so I&#39;ve only used &#39;Primary Category&#39; as the the y target. What is the distribution of these categories like? . Arts &amp; Humanities 5299 Government, Law &amp; Politics 4832 Business, Economy &amp; Industry 2988 Society &amp; Culture 2984 Science &amp; Technology 2420 Medicine &amp; Health 2164 Education &amp; Research 2118 Company Web Sites 843 Digital Society 737 Sports and Recreation 710 Religion 417 Travel &amp; Tourism 374 Social Problems and Welfare 270 Politics, Political Theory and Political Systems 123 Crime, Criminology, Police and Prisons 101 Literature 87 Law and Legal System 81 Computer Science, Information Technology and Web Technology 54 Libraries, Archives and Museums 52 Environment 38 History 34 Publishing, Printing and Bookselling 26 Popular Science 23 Life Sciences 23 Name: Primary Category, dtype: int64 . 😬 We also have a fairly skewed datasets. I could drop some of rows which don&#39;t occur often but since the main objective here is to see if we can use a multi-input model we&#39;ll leave the data as it is for now. . Multi-input model . The rest of the notebook will describe some experiments with using fastai to create a model which takes tabular and text data as an input. The aim here wasn&#39;t for me to create the best model but get my head around how to combine models. I heavily relied on some existing notebooks, kaggle writeup and forum posts on the fastai forums. . Tabular model . In the dataset above we start of with two columns of data which can be used as inputs for the model. The title is fairly obviously something which we can treat like other text inputs. The URL is a little less obvious. It could be treated as a text input but an alternative is to treat a URL as parts which each contain some information which could be useful for our model. . http://www.specialschool.org/ http://www.bbc.co.uk/news/health-12668398 http://www.monarchit.co.uk/ . Each part of the URL could be split into smaller parts . [&#39;http://www&#39;, &#39;darwincountry&#39;, &#39;org/&#39;] . Whether a url has &#39;.org&#39; or &#39;.uk&#39; or &#39;.com&#39; could be meaningful for predicting our categories (it might also not be meaningful). It also offers us a way of taking the URLs and composing it into a format which looks more tabular. . scheme url1 url3 url4 url5 . 20011 http | www | org | NaN | NaN | . 15825 http | www | com | NaN | NaN | . 6068 http | www | co | uk | NaN | . 16507 http | www | co | uk | NaN | . 9723 http | www | co | uk | NaN | . So far I&#39;ve only done this very crudely. I suspect tidying up this part of the data will help improve things. At this point though we have something which is a little more tabular looking we can pass to fastai.tabular learner. Now we have some &#39;categories&#39; rather than unique urls. . print(len(df.url3.unique())) print(len(df.url4.unique())) . 279 56 . How does this tabular model do? . Once some preprocessing of the url has been done we train a model using the tabular learner. I didn&#39;t do much to try to optimize this model. Tracking best f2 score we end up with: . Better model found at epoch 36 with f_beta value: 0.17531482875347137 and an accuracy of 0.334121 . How well does a text model do? . Next I tried training using the title field in a NLP model. I tried a few things here. . SentencePiece tokenization . By default fastai uses SpaCy to do tokenization with a few additional special tokens added by fastai. I wanted to see if using sentencePiece would work better for processing title fields. SentencePiece allows for various sub-word tokeinzation. This can be useful for agglutinative languages but could also be useful when you have a lot of out of vocabulary words in your corpus. I wanted to see if this also was useful for processing titles since these may contain domain specific terms. I only tried using SentencePiece with &#39;unigram&#39; tokenization. The best score I got for this was: . Better model found at epoch 1 with f_beta value: 0.21195338666439056. . Default SpaCy tokenization . I compared the above to using the default fastai tokenizer which uses SpaCy. In this case the default approach worked better. This is probably because we didn&#39;t have a large pre-trained model using the SentencePiece tokenization to use as a starting point. The best score I got for this model was: . Better model found at epoch 27 with f_beta value: 0.33327043056488037. . Using the URL as text input . I wanted to do a quick comparison to the tabular model and use the URL as a text input instead. In this case I used SentencePiece with byte-pair-encoding (BPE). The best score in this case was: . Better model found at epoch 3 with f_beta value: 0.2568161189556122. . This might end up being a better approach compared to the tabular approach described above. . Combining inputs . Neither of these models is doing super well but my main question was whether combining the two would improve things at all. There are different approaches to combining these models. I followed existing examples and removed some layers from the text and tabular models which are then combined in a concat model. I won&#39;t cover all the steps here but all the notebooks can be found in this GitHub repo. . One of the things we need to do to create a model with multiple input is create a new Pytorch dataset which combines our text and tabular x inputs with our target. This is pretty straightforward: . class ConcatDataset(Dataset): def __init__(self, x1, x2, y): self.x1,self.x2,self.y = x1,x2,y def __len__(self): return len(self.y) def __getitem__(self, i): return (self.x1[i], self.x2[i]), self.y[i] . . One of the other pieces was creating a ConcatModel . class ConcatModel(nn.Module): def __init__(self, model_tab, model_nlp, layers, drops): super().__init__() self.model_tab = model_tab self.model_nlp = model_nlp lst_layers = [] activs = [nn.ReLU(inplace=True),] * (len(layers)-2) + [None] for n_in,n_out,p,actn in zip(layers[:-1], layers[1:], drops, activs): lst_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn) # https://docs.fast.ai/layers.html#bn_drop_lin self.layers = nn.Sequential(*lst_layers) def forward(self, *x): x_tab = self.model_tab(*x[0]) x_nlp = self.model_nlp(x[1])[0] x = torch.cat([x_tab, x_nlp], dim=1) return self.layers(x) . . lst_layer is dependent on the layers from the tabular and nlp models. This layer is manually defined at the moment, so if changes are made to the number of layers in the tab model this needs to be manually changed. . bn_drop_lin is a fastai helper function that returns a a sequence of batch normalization, dropout and a linear layer which is the final layer of the model. . How does this combined model do? &#129335;&#8205;&#9794;&#65039; . The best result I got wasf_beta value: 0.39341238141059875 with an accuracy of 0.595348. A summary of the scores for each models: . Model F2 score . SentencePiece text | 0.211 | . Spacy text | 0.333 | . Tabular | 0.175 | . Concat | 0.393 | . This provides some improvement on the tabular or nlp models on their own. I found the combined model was fairly tricky to train and suspect that there could be some improvements in how the model is set up that might improve it&#39;s performance. I am keen to try a similar approach with a dataset where there is more abundant information available to train with. . tl;dr . It wasn&#39;t possible to get a very good f2 score on this website classification dataset. As the UK web archive say: . We expect that a appropriate classifier might require more information about each site in order to produce reliable results, and are looking at augmenting this dataset with further information in the future. Options include:For each site, make the titles of every page on that site available. For each site, extract a set of keywords that summarise the site, via the full-text index. . I suspect that having a either of these additional components would help improve the performance of the classifier. .",
            "url": "https://danielvanstrien.xyz/metadata/multi-model/2020/05/03/multi-model.html",
            "relUrl": "/metadata/multi-model/2020/05/03/multi-model.html",
            "date": " • May 3, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Daniel. I’m a Machine Learning Librarian at Hugging Face. . I am interested in how ideas from library science can inform a healthy machine-learning ecosystem and how machine learning can support the work of galleries, libraries, archives, and museums (GLAM). . Some projects I am/have been involved in: . BigLAM: an initiative to make GLAM datasets with machine learning datasets more widely available. | ai4lam: an international, participatory community focused on advancing the use of artificial intelligence in, for and by libraries, archives and museums. | LivingwithMachines: a research collobaration between the British Library and the Alan Turing Institute focused on applying data science and machine learning methods to digitised historical collections. | . You can find more projects here. . You can find me on the Hugging Face Hub, Linkedin, Twitter and Mastodon .",
          "url": "https://danielvanstrien.xyz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Selected projects",
          "content": "This page collects selected projects that I have worked on. . Machine learning projects . flyswot: using computer vision to detect ‘fake flysheets’ . Related Posts . An increasing challenge for libraries is managing the scale of digitised material resulting from digitisation projects and ‘born digital’ materials. This project aims to detect mislabelled digitised manuscript pages. . A manuscript page with the correct label &quot;fse.ivr&quot; As a result of the limitations of a previous system for hosting digitised manuscript images, many images have incorrect page metadata associated with the image. An image has been correctly labelled as an ‘end flysheet’ in the example above. This label is represented by the fse label, which is included in the filename for the image. However other types of manuscript pages also have this label incorrectly assigned, i.e. a ‘cover’ has fse in the filename. There is around a petabyte of images to review before ingesting a new library system. This project uses computer vision to support library staff in processing this collection. At the moment, this project does the following: . pulls in an updated dataset of training examples | trains a model on these images | the model architecture has multiple heads to allow the model to make both a ‘crude’ prediction for whether the image is incorrectly labelled and a ‘full’ prediction for the true label. | once a new version of the model has been trained, it is pushed to the 🤗 model hub. | the end-user uses the model through a command-line tool which is pointed at a directory of images to be checked. | . The code for the command-line tool is available here: github.com/davanstrien/flyswot/ . Some of the tools used: fastai, DVC, Weights and Biases, 🤗 model hub, pytest, nox, poetry . Book Genre Detection . . This project created machine learning models which would predict whether a book was ‘fiction’ or ‘non- fiction’ based on the book title: . The project was developed to address a gap in metadata in a large scale digitised book collection. | The project used weak supervision to generate a more extensive training set beyond the initial human-generated annotations. | Currently, two models are publicly available, one via the 🤗 Model Hub and one via Zenodo. | The process of creating the models is documented in a Jupyter Book. This documentation aims to communicate the critical steps in the machine learning pipeline to aid other people in the sector develop similar models. https://huggingface.co/spaces https://huggingface.co/spaces/BritishLibraryLabs/British-Library-books-genre-classifier-v2 | . Some of the tools used: fastai, transformers, blurr, Hugging face model hub, Jupyter Book, Snorkel, Gradio . Datasets . British Library books . Extracted plain text and other metadata files from ALTO XML https://github.com/davanstrien/digitised-books-ocr-and-metadata | Added the dataset to the 🤗 datasets hub | . British Library Books Genre data . Created a datasets loading script and prepared a Dataset card for a dataset supporting book genre detection using machine learning: https://huggingface.co/datasets/blbooksgenre | . Datasets to support programming historian lessons . I think having more realistic datasets is important for teaching machine learning effectively. As a result, I created two datasets for two under review Programming Historian lessons. . 19th Century United States Newspaper Advert images with ‘illustrated’ or ‘non illustrated’ labels . | 19th Century United States Newspaper images predicted as Photographs with labels for “human”, “animal”, “human-structure” and “landscape” . | . Workshop datasets . Images from Newspaper Navigator predicted as maps, with human corrected labels | . Workshop materials . Computer Vision for the Humanities workshop: This workshop aims to provide an introduction to computer vision aimed for humanities applications. In particular this workshop focuses on providing a high level overivew of machine learning based approaches to computer vision focusing on supervised learning. The workshop includes discussion on working with historical data. The materials are based on in progress Programming Historian lessons. | Working with maps at scale using Computer Vision and Jupyter notebooks | Introduction to Jupyter Notebooks: the weird and the wonderful | Image Search: Materials for a workshop on image search with a focus on heritage data. The workshop is based on a blog post Image search with 🤗 datasets but goes into a little bit more detail. | . Tutorials . Jupyter book showing how to build an ML powered book genre classifier | A (brief) history of advertising in US Newspapers using computer vision | (Under review) Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification, Programming Historian lessons | (Under development) Intro to AI for GLAM, Carpentries Lesson | . Code . You can view much of my code related activity on GitHub . Publications . Semantic Scholar page | OCRCID page | .",
          "url": "https://danielvanstrien.xyz/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://danielvanstrien.xyz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}