{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Studio x Hugging Face datasets hub \n",
    "\n",
    "> Using label studio and the Hugging Face datasets hub to iteratively annotate a dataset \n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [huggingface, huggingface-datasets, annotation, full stack deep learning notes]\n",
    "- search_exclude: false\n",
    "- badges: true\n",
    "- image: https://raw.githubusercontent.com/davanstrien/blog/master/images/huggingface_hub_label_possum.webp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full stack deep learning: annotating data\n",
    "\n",
    "I'm currently going through the [Full Stack Deep Learning course](https://fullstackdeeplearning.com). As part of this we've been going through tools for different parts of the machine learning pipeline. This post talks about data annotation, and how we can combine Label Studio and the [Hugging Face Datasets hub](https://huggingface.co/datasets). I'll use the example of annotating image data for an image classification task. The details of why I'm annotating this data will wait for a future post! \n",
    "\n",
    "**note** this post assumes you already know roughly what the Hugging Face Hub is. If you don't [this](https://huggingface.co/course/chapter4/1) is a nice intro.\n",
    "\n",
    "## What is the goal?\n",
    "\n",
    "We want to have a way of easily moving from different stages of our machine learning project pipeline. For many projects, especially the weird stuff I'm likely to do, you will need to do some of your own annotating. It almost always makes sense to move quickly between annotating a first batch of data, trying to train a model and iterating. This can help:\n",
    "\n",
    "- flag issues with your data\n",
    "- identify if you have ambiguous labels\n",
    "- help you get some sense of how a model might perform on the task you are working on\n",
    "- allow you to deploy a model early so you can begin iterating on the whole pipeline \n",
    "- ...\n",
    "\n",
    "<a href=\"https://imgflip.com/i/6skdh9\"><img src=\"https://i.imgflip.com/6skdh9.jpg\" title=\"made at imgflip.com\"/></a><div><a href=\"https://imgflip.com/memegenerator\">from Imgflip Meme Generator</a></div>\n",
    "\n",
    "This approach can cause some challenges; how do you keep updating your annotations, how can you version the changes?\n",
    "\n",
    "## A more mundane challenge \n",
    "\n",
    "In the full stack deep learning course, one of the [labs](https://fullstackdeeplearning.com/course/2022/lab-6-data-annotation/) covered using [Label Studio](https://labelstud.io/guide/tasks) to annotate data. Label studio is a great open source tool for annotating data across a range of domains and for a variety of tasks. \n",
    "\n",
    "Label studio has great support for annotating image data. One challenge we can face, however, is how to load images into label studio. This can be particularly tricky if you only have the images locally since label studio prefers images to be available via a URL. There are [various ways around this](https://labelstud.io/guide/tasks.html#Import-data-from-a-local-directory) but we may also be able to tackle this challenge using the datasets hub. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by downloading a dataset we want annotate **warning** this dataset is pretty big ~44GB uncompressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://nlsfoundry.s3.amazonaws.com/data/nls-data-encyclopaediaBritannica.zip\n",
    "unzip *.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import some standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataset on the Hub \n",
    "Since we want to upload our data to the Hugging Face hub we'll create a new dataset on the Hugging Face Hub via the CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "huggingface-cli repo create encyclopaedia_britannica --type dataset \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, Hugging Face hub datasets (and models) are Git repositories. We'll clone this repo and move the downloaded dataset into this new Git repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica\n",
    "mv nls-data-encyclopaediaBritannica encyclopaedia_britannica/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of examples in this dataset is beyond what we're likely to annotate we do a bit of deleting of the dataset. You could also take a sample of the original but in this case I'm happy to reclaim some space on my hardrive! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we get rid of some alto folders that we don't need for the dataset we're aiming to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:34<00:00,  5.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for directory in tqdm(\n",
    "    list(\n",
    "        (\n",
    "            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n",
    "                \"*alto\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "):\n",
    "    shutil.rmtree(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are a few other `*.xml` files in this dataset we also remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:00<00:00, 1464.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for xml_file in tqdm(\n",
    "    list(\n",
    "        (\n",
    "            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n",
    "                \"*xml\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "):\n",
    "    xml_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how many images we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = list(\n",
    "    (Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\"*jpg\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155388"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not likely to annotate this many images, let's aim to have a max of 10,000 images. This is also likely to be more than we'll annotate but we may use a smaller sample for unsupervised pre-training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_remove = len(image_files) - 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now randomly remove the extra images we don't need beyond our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:33<00:00, 2659.02it/s]\n"
     ]
    }
   ],
   "source": [
    "to_remove = random.sample(image_files, num_to_remove)\n",
    "for file in tqdm(to_remove):\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\n",
    "    list(\n",
    "        (\n",
    "            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n",
    "                \"*jpg\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading our raw data to the hub \n",
    "\n",
    "We can now upload this data to the Hugging Face Hub. Under the hood the Hub uses Git so everything you love (and hate) about Git should be familiar. The main difference between using the hub and GitHub or another Git hosting platform is that the Hugging Face hub has support for large files. This means we can more easily work with large files (like our images). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd encyclopaedia_britannica\n",
    "git lfs track \"*.jpg\"\n",
    "git add .gitattributes\n",
    "git add nls-data-encyclopaediaBritannica\n",
    "git commit -m \"add image files\"\n",
    "git push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading local files and metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particular dataset we're working with also has a metadata file associated with it. We can  grab all of the images so we can put them in a DataFrame and merge this with metadata about these images. We may not use this extra metadata but it's nice to have this additional metadata about our items alongside our annotations. This can help us debug where our model is performing badly later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = list(\n",
    "    Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\"*.jpg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(image_files, columns=[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset also comes with some metadata. We'll load that in to another DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\n",
    "    \"encyclopaedia_britannica/nls-data-encyclopaediaBritannica/encyclopaediaBritannica-inventory.csv\",\n",
    "    header=None,\n",
    "    names=[\"id\", \"meta\"],\n",
    "    dtype={\"id\": \"int64\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"] = df.filename.apply(lambda x: x.parts[-3]).astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>193696083</td>\n",
       "      <td>Encyclopaedia Britannica - Seventh edition, Vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>193696083</td>\n",
       "      <td>Encyclopaedia Britannica - Seventh edition, Vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>193696083</td>\n",
       "      <td>Encyclopaedia Britannica - Seventh edition, Vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>193696083</td>\n",
       "      <td>Encyclopaedia Britannica - Seventh edition, Vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>193696083</td>\n",
       "      <td>Encyclopaedia Britannica - Seventh edition, Vo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filename         id  \\\n",
       "0     encyclopaedia_britannica/nls-data-encyclopaedi...  190273291   \n",
       "1     encyclopaedia_britannica/nls-data-encyclopaedi...  190273291   \n",
       "2     encyclopaedia_britannica/nls-data-encyclopaedi...  190273291   \n",
       "3     encyclopaedia_britannica/nls-data-encyclopaedi...  190273291   \n",
       "4     encyclopaedia_britannica/nls-data-encyclopaedi...  190273291   \n",
       "...                                                 ...        ...   \n",
       "9995  encyclopaedia_britannica/nls-data-encyclopaedi...  193696083   \n",
       "9996  encyclopaedia_britannica/nls-data-encyclopaedi...  193696083   \n",
       "9997  encyclopaedia_britannica/nls-data-encyclopaedi...  193696083   \n",
       "9998  encyclopaedia_britannica/nls-data-encyclopaedi...  193696083   \n",
       "9999  encyclopaedia_britannica/nls-data-encyclopaedi...  193696083   \n",
       "\n",
       "                                                   meta  \n",
       "0     Encyclopaedia Britannica - Third edition, Volu...  \n",
       "1     Encyclopaedia Britannica - Third edition, Volu...  \n",
       "2     Encyclopaedia Britannica - Third edition, Volu...  \n",
       "3     Encyclopaedia Britannica - Third edition, Volu...  \n",
       "4     Encyclopaedia Britannica - Third edition, Volu...  \n",
       "...                                                 ...  \n",
       "9995  Encyclopaedia Britannica - Seventh edition, Vo...  \n",
       "9996  Encyclopaedia Britannica - Seventh edition, Vo...  \n",
       "9997  Encyclopaedia Britannica - Seventh edition, Vo...  \n",
       "9998  Encyclopaedia Britannica - Seventh edition, Vo...  \n",
       "9999  Encyclopaedia Britannica - Seventh edition, Vo...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.merge(metadata_df, on=\"id\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating using label studio \n",
    "\n",
    "Now we have our images uploaded to the Hugging Face hub, how we go about annotating? As was mentioned already the Hugging Face hub is essentially a Git repo. Since we uploaded our image files individually i.e. not in a compressed folder, we can access each file from that repo. We mentioned before that label studio can load images from URLs. The hub has an API that we can use to interact with our repository. Let's see how we can use this to get our data ready for label studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_repo_files, hf_hub_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitattributes',\n",
       " 'nls-data-encyclopaediaBritannica/144133901/image/188082865.3.jpg']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = list_repo_files(\"davanstrien/encyclopaedia_britannica\", repo_type=\"dataset\")\n",
    "files[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll filter out some data we are not interested in annotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [file for file in files if not file.startswith(\".\")]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hf_hub_url` can be used to generate the URL for a particular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/sample/nls-data-encyclopaediaBritannica/192547788/image/192866824.3.jpg'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_hub_url(\n",
    "    \"davanstrien/encyclopaedia_britannica\",\n",
    "    \"192866824.3.jpg\",\n",
    "    subfolder=\"sample/nls-data-encyclopaediaBritannica/192547788/image\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to grab all of the URLs we're interested in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for file in files:\n",
    "    file = Path(file)\n",
    "    urls.append(\n",
    "        hf_hub_url(\n",
    "            \"davanstrien/encyclopedia_britannica\",\n",
    "            file.name,\n",
    "            subfolder=file.parents[0],\n",
    "            repo_type=\"dataset\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load these into a DataFrame, and save this to a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(urls, columns=[\"image\"]).to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10002 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image\n",
       "0      https://huggingface.co/datasets/davanstrien/en...\n",
       "1      https://huggingface.co/datasets/davanstrien/en...\n",
       "2      https://huggingface.co/datasets/davanstrien/en...\n",
       "3      https://huggingface.co/datasets/davanstrien/en...\n",
       "4      https://huggingface.co/datasets/davanstrien/en...\n",
       "...                                                  ...\n",
       "9997   https://huggingface.co/datasets/davanstrien/en...\n",
       "9998   https://huggingface.co/datasets/davanstrien/en...\n",
       "9999   https://huggingface.co/datasets/davanstrien/en...\n",
       "10000  https://huggingface.co/datasets/davanstrien/en...\n",
       "10001  https://huggingface.co/datasets/davanstrien/en...\n",
       "\n",
       "[10002 rows x 1 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading annotations into label studio \n",
    "\n",
    "We can use this file to load our data into label studio\n",
    "![Label Studio import](https://raw.githubusercontent.com/davanstrien/blog/master/images/label-studio-import.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we need to define our annotation task. We can then begin annotating data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export annotations \n",
    "\n",
    "You can either wait until you've finished doing all the labels, however, we may have a lot of data to annotate so it's likely instead that we will want to export once we've either hit a reasonable number of labels or get too bored of annotating. There are various different export formats available in this case we'll use JSON-Min\n",
    "\n",
    "![](https://raw.githubusercontent.com/davanstrien/blog/master/images/label-studio-export.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotations\n",
    "\n",
    "Now we have export our annotations lets load them into a new DatafFame. We'll only select the columns we're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataframe = pd.read_json(\"project-3-at-2022-09-08-15-16-4279e901.json\")[\n",
    "    [\"image\", \"choice\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1521 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image     choice\n",
       "0     https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "1     https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "2     https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "3     https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "4     https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "...                                                 ...        ...\n",
       "1516  https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "1517  https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "1518  https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "1519  https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "1520  https://huggingface.co/datasets/davanstrien/en...  text-only\n",
       "\n",
       "[1521 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the URL for one of the annotations, you'll see that we still have a nice path that mirrors the folder structure of the original data. This also means we can merge this annotations DataFrame with our previous metadata DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/nls-data-encyclopaediaBritannica/192693396/image/192979378.3.jpg'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_dataframe.loc[0, \"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nls-data-encyclopaediaBritannica', '192693396', 'image', '192979378.3.jpg']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_dataframe.loc[0, \"image\"].split(\"/\")[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataframe[\"filename\"] = annotation_dataframe[\"image\"].apply(\n",
    "    lambda x: \"/\".join(x.split(\"/\")[-4:])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataframe[\"filename\"] = annotation_dataframe[\"filename\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(annotation_dataframe, on=\"filename\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>meta</th>\n",
       "      <th>image</th>\n",
       "      <th>choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291.0</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291.0</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291.0</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291.0</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encyclopaedia_britannica/nls-data-encyclopaedi...</td>\n",
       "      <td>190273291.0</td>\n",
       "      <td>Encyclopaedia Britannica - Third edition, Volu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516</th>\n",
       "      <td>nls-data-encyclopaediaBritannica/144133901/ima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11517</th>\n",
       "      <td>nls-data-encyclopaediaBritannica/144133901/ima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11518</th>\n",
       "      <td>nls-data-encyclopaediaBritannica/144133901/ima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>nls-data-encyclopaediaBritannica/144133901/ima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11520</th>\n",
       "      <td>nls-data-encyclopaediaBritannica/144133901/ima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/datasets/davanstrien/en...</td>\n",
       "      <td>text-only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11521 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename           id  \\\n",
       "0      encyclopaedia_britannica/nls-data-encyclopaedi...  190273291.0   \n",
       "1      encyclopaedia_britannica/nls-data-encyclopaedi...  190273291.0   \n",
       "2      encyclopaedia_britannica/nls-data-encyclopaedi...  190273291.0   \n",
       "3      encyclopaedia_britannica/nls-data-encyclopaedi...  190273291.0   \n",
       "4      encyclopaedia_britannica/nls-data-encyclopaedi...  190273291.0   \n",
       "...                                                  ...          ...   \n",
       "11516  nls-data-encyclopaediaBritannica/144133901/ima...          NaN   \n",
       "11517  nls-data-encyclopaediaBritannica/144133901/ima...          NaN   \n",
       "11518  nls-data-encyclopaediaBritannica/144133901/ima...          NaN   \n",
       "11519  nls-data-encyclopaediaBritannica/144133901/ima...          NaN   \n",
       "11520  nls-data-encyclopaediaBritannica/144133901/ima...          NaN   \n",
       "\n",
       "                                                    meta  \\\n",
       "0      Encyclopaedia Britannica - Third edition, Volu...   \n",
       "1      Encyclopaedia Britannica - Third edition, Volu...   \n",
       "2      Encyclopaedia Britannica - Third edition, Volu...   \n",
       "3      Encyclopaedia Britannica - Third edition, Volu...   \n",
       "4      Encyclopaedia Britannica - Third edition, Volu...   \n",
       "...                                                  ...   \n",
       "11516                                                NaN   \n",
       "11517                                                NaN   \n",
       "11518                                                NaN   \n",
       "11519                                                NaN   \n",
       "11520                                                NaN   \n",
       "\n",
       "                                                   image     choice  \n",
       "0                                                    NaN        NaN  \n",
       "1                                                    NaN        NaN  \n",
       "2                                                    NaN        NaN  \n",
       "3                                                    NaN        NaN  \n",
       "4                                                    NaN        NaN  \n",
       "...                                                  ...        ...  \n",
       "11516  https://huggingface.co/datasets/davanstrien/en...  text-only  \n",
       "11517  https://huggingface.co/datasets/davanstrien/en...  text-only  \n",
       "11518  https://huggingface.co/datasets/davanstrien/en...  text-only  \n",
       "11519  https://huggingface.co/datasets/davanstrien/en...  text-only  \n",
       "11520  https://huggingface.co/datasets/davanstrien/en...  text-only  \n",
       "\n",
       "[11521 rows x 5 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we can keep our nice orignal metadata intact but also add our additional metadata where it exists. Let's check how many annotations we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text-only      1436\n",
       "illustrated      70\n",
       "Name: choice, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.choice.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how much of our dataset we have coverage for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13071781963371235"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.choice.notna()]) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use our annotations?  \n",
    "\n",
    "We now have some annoations inside a DataFrame. What should we do we these? We can also use the Hub for storing this. This comes with a few benefits:\n",
    "- we keep our data and annotations in the same place. \n",
    "- since the Hub uses Git under the hood we also get versioning for our dataset. We can use this version information to track for example how different models perform during training as we add more labels. \n",
    "\n",
    "Another nice thing about the Hub is that we can create dataset loading scripts to load our data. This script can use this CSV we've just created and only load the data we have examples for.\n",
    "\n",
    "First we'll save to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"annotations.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then copy these into the same repository used to host our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cp annotations.csv encyclopedia_britannica/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done this we can commit these and push our annotations to the hub:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd encyclopedia_britannica/\n",
    "git add annotations.csv\n",
    "git commit -m \"update annotations\"\n",
    "git push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "We now have a repository which contains a bunch of images, and a CSV file which contains annotations for some of these images. How do we use this for model training? From this point we can create a dataset [loading script](https://huggingface.co/docs/datasets/dataset_script) inside the same repository. \n",
    "\n",
    "This dataset loading script will allow us to load the data from the hub using the `datasets` library. Additionally we can write this script so that it only loads data we have annotations for. \n",
    "\n",
    "What does this mean:\n",
    "- we have a dataset we can use to train our model\n",
    "- the dataset is hosted on the Hugging Face hub which means it's easy to share with other people \n",
    "- we can keep adding new annotations to this dataset and pushing our changes to the hub \n",
    "- Since the `datasets` library has nice caching support it will only download the dataset if there are changes. This change will be triggered by changes to our annotations.csv file. \n",
    "\n",
    "## Where won't this work? \n",
    "\n",
    "This workflow is based on the assumption that the dataset you are annotating is public from the start. This is usually possible for the domain I work in (libraries) but could be a major blocker for other people. This workflow might also break if you have lots of people annotating. There are probably ways around this but things could start becoming a bit hacky..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('datasets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cf3e260cfb2cb88153adad9815027a977259444967c828c4468722abbd7df1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
