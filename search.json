[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, Iâ€™m Daniel a Machine Learning Librarian at Hugging Face."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html",
    "title": "flyswot",
    "section": "",
    "text": "My previous series of posts/notes following the full stack course ended slightly abruptly. However, I didnâ€™t give up on the dream of trying to get more experience of â€˜deployingâ€™ machine learning in a GLAM setting! One of the things I have been focusing on recently is a project which gave me the chance to use machine learning in somewhat of a production context.\nThis blog post will be a very short tl;dr on this project."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html#detecting-fake-flysheets-using-computer-vision",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html#detecting-fake-flysheets-using-computer-vision",
    "title": "flyswot",
    "section": "Detecting fake flysheets using computer vision",
    "text": "Detecting fake flysheets using computer vision\nThe Library has â€˜legacyâ€™ digitised content of manuscripts. Due to some limitations of the legacy system on which these manuscripts were shared many of the images have incorrect metadata. The metadata for these images is partially stored in the filename for the image. In particular many images which didnâ€™t fit into an available category were given â€˜end flysheetâ€™ labels (this basically means part of the filename contains the string fse). These images may actually be other things like a frontcover, a scroll image, a box etc.\n\n\n\nA screenshot of the digitised manuscript platform showing metadata about the page type of the manuscript\n\n\nThe library is moving to a new platform which wonâ€™t have these restrictions on possible page types. As a result there is a need/desire to find all of the images which have been given a â€˜fakeâ€™ flysheet label and correct this label with the correct label.\nThis is a task where computer vision seems like it might be helpful."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html#the-desired-outcome",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html#the-desired-outcome",
    "title": "flyswot",
    "section": "The desired outcome",
    "text": "The desired outcome\nThe desire of this project is to be able to use a computer vision model to check a bunch of image directories and see if there are any â€˜fakeâ€™ flysheets. There are some additional constraints on the project:\n\n$$$ this isnâ€™t a funded project so canâ€™t involve spending a bunch of money\nrelated to the above, the approach to annotation has to be pragmatic - no Mechanical Turk here\nthe machine learning should fit into existing workflows (this is something we have/are spending a lot of time on)\n\nSince this is intended to be a tl;dr I wonâ€™t go into more detail here about all of these requirements here."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html#flyswot",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html#flyswot",
    "title": "flyswot",
    "section": "flyswot",
    "text": "flyswot\nThe approach we ended up with is to deploy a model using a command line tool that weâ€™ve called flyswot. This tool can be pointed at a directory and it will recursively check for images which contain the fse pattern in the filename. These images are then checked using a computer vision model that looks check whether an image is a â€˜realâ€™ flysheet or a â€˜fakeâ€™ flysheet."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html#what-i-have-learned-so-far",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html#what-i-have-learned-so-far",
    "title": "flyswot",
    "section": "What I have learned (so far)",
    "text": "What I have learned (so far)\nThis project has been a great way of turning some of the theory of â€˜productionâ€™ ML into practice. In particular I have learned:\n\nIâ€™m super paranoid about domain drift.\n(some) of how to use ONNX\nMore robust testing approaches\nDVC (data version control)\nand a bunch more thingsâ€¦\n\nMost of these things are being documented elsewhere and will be available to share at some point in 2022. However, I will try and use this blog to document small things Iâ€™ve learned along the way too. These notes are mainly for myself. There are a lot of little things Iâ€™ve picked up from doing this project that I will forget if I donâ€™t spend a bit of time writting up."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html",
    "title": "Training an object detection model using Hugging Face",
    "section": "",
    "text": "The Hugging Face transformers library has increasingly expanded from its original focus on Natural Language Processing tasks to include more models covering a range of computer vision tasks. This blog post will look at how we can train an object detection model using the Hugging Face transformers and datasets libraries."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#training-a-detr-object-detection-model-using-hugging-face-transformers-and-datasets",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#training-a-detr-object-detection-model-using-hugging-face-transformers-and-datasets",
    "title": "Training an object detection model using Hugging Face",
    "section": "",
    "text": "The Hugging Face transformers library has increasingly expanded from its original focus on Natural Language Processing tasks to include more models covering a range of computer vision tasks. This blog post will look at how we can train an object detection model using the Hugging Face transformers and datasets libraries."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#what-is-object-detection",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#what-is-object-detection",
    "title": "Training an object detection model using Hugging Face",
    "section": "What is object detection?",
    "text": "What is object detection?\nObject detection is the task of predicting objects contained within an image.\n\nObject detection can be helpful in several applications where you want to know not only whether a thing is in an image but where (and how many) of that thing there are. Various approaches have been developed over the years for this task, often relying on various complex hand-crafted features.\nAs with other areas of computer vision, there has been an increasing adoption of transformer-based solutions to this task. One model using transformers is the Detr architecture."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#what-is-detr",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#what-is-detr",
    "title": "Training an object detection model using Hugging Face",
    "section": "What is Detr?",
    "text": "What is Detr?\n\n\n\nDiagram of the DETR architecture\n\n\nDetr (DEtection TRansformer) is a model architecture introduced in the paper End-to-End Object Detection with Transformers. We wonâ€™t dig into the architecture in massive detail in this blog since weâ€™re focused on the practical use of this model architecture in this post. One thing that is important to note here is that DETR still uses a CNN backbone. More recently, other models such as YOLOS use a transformer backbone too. Currently, however, these fully transformer-based approaches show some performance gap over more traditional techniques (because this is deep learning, â€˜traditionalâ€™ refers to stuff from last year, of course)."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#using-hugging-face-for-object-detection",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#using-hugging-face-for-object-detection",
    "title": "Training an object detection model using Hugging Face",
    "section": "Using Hugging Face for object detection",
    "text": "Using Hugging Face for object detection\nThere are existing examples for using the Hugging Face transformers library and datasets with the Trainer class to do image classification. There are also example notebooks showing how to fine-tune a Detr model on custom data. However, I didnâ€™t find examples that use the datasets library and the Trainer class to manage training. Training an object detection model using datasets and the transformers library is what this blog post covers.\n\nWhy the datasets library?\nYou may ask why it is helpful to provide an example of using the datasets library for training an object detection model, i.e.Â why not use PyTorch for the data loading, which already has many examples for training object detection models?\nThere are a few reasons why trying to use datasets for this can be helpful. A significant one for me is the close integration between the datasets library and the Hugging Face datasets hub. Loading a dataset from the hugging face hub often involves two lines of code (including the imports).\nQuickly loading a dataset and then using the same library to prepare the dataset for training an object detection model removes some friction. This becomes especially helpful when you are iterating on the process of creating training data, training a model, and creating more training data. In this iterative process, the hub can be used for storing models and datasets at each stage. Having a clear provenance of these changes (without relying on additional tools) is also a benefit of this workflow. This is the kind of pipeline hugit is intended to support (in this case, for image classification models).\n\n\nScope of this blog post\nAt the moment, this is mainly intended to give a quick overview of the steps involved. It isnâ€™t intended to be a proper tutorial. If I have time later, I may flesh this out (particularly if other projects Iâ€™m working on that use object detection progress further).\nEnough talk, letâ€™s get started. First we install required libraries.\n\n%%capture\n!pip install datasets transformers timm wandb rich[jupyter]\n\nIâ€™m a big fan of the rich library so almost always have this extension loaded.\n\n%load_ext rich\n\nThe next couple of lines gets us authenticated with the Hugging Face hub.\n\n!git config --global credential.helper store\n\n\nfrom huggingface_hub import notebook_login\n\n\nnotebook_login()\n\nLogin successful\nYour token has been saved to /root/.huggingface/token\n\n\nWeâ€™ll use Weights and Biases for tracking our model training.\n\nimport wandb\n\n\nwandb.login()\n\n\n%env WANDB_PROJECT=chapbooks\n%env WANDB_ENTITY=davanstrien"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#loading-the-dataset",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#loading-the-dataset",
    "title": "Training an object detection model using Hugging Face",
    "section": "Loading the dataset",
    "text": "Loading the dataset\nIn this blog post will use a dataset being added to the Hugging Face datasets hub as part of the BigLAM hackathon. This dataset has a configuration for object detection and image classification, so weâ€™ll need to specify which one we want. Since the dataset doesnâ€™t define train/test/valid splits for us, weâ€™ll grab the training split. I wonâ€™t provide a full description of the dataset in this blog post since the dataset is still in the process of being documented. The tl;dr summary is that the dataset includes images of digitized books with bounding boxes for illustrations.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\n    \"biglam/nls_chapbook_illustrations\", \"illustration-detection\", split=\"train\"\n)\n\nReusing dataset nls_chapbook_illustrations (/Users/dvanstrien/.cache/huggingface/datasets/biglam___nls_chapbook_illustrations/illustration-detection/1.0.0/75f355eb0ba564ef120939a78730eb187a4d3eb682e987ed1f682a5bea5466eb)\n\n\nLetâ€™s take a look at one example from this dataset to get a sense of how the data looks\n\ndataset[0]\n\n{\n    'image_id': 4,\n    'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x1080 at 0x7FDD6504FAD0&gt;,\n    'width': 600,\n    'height': 1080,\n    'url': None,\n    'date_captured': '',\n    'objects': [\n        {\n            'category_id': 0,\n            'image_id': '4',\n            'id': 1,\n            'area': 110901,\n            'bbox': [34.529998779296875, 556.8300170898438, 401.44000244140625, 276.260009765625],\n            'segmentation': [\n                [\n                    34.529998779296875,\n                    556.8300170898438,\n                    435.9700012207031,\n                    556.8300170898438,\n                    435.9700012207031,\n                    833.0900268554688,\n                    34.529998779296875,\n                    833.0900268554688\n                ]\n            ],\n            'iscrowd': False\n        }\n    ]\n}\n\n\n\nYou will see we hav some metadata for the image, the image itself and the field objects contains the annotations themselves. Looking just at an example of the annotations:\n\n# hide_output\n{\n    \"category_id\": 0,\n    \"image_id\": \"4\",\n    \"id\": 1,\n    \"area\": 110901,\n    \"bbox\": [\n        34.529998779296875,\n        556.8300170898438,\n        401.44000244140625,\n        276.260009765625,\n    ],\n    \"segmentation\": [\n        [\n            34.529998779296875,\n            556.8300170898438,\n            435.9700012207031,\n            556.8300170898438,\n            435.9700012207031,\n            833.0900268554688,\n            34.529998779296875,\n            833.0900268554688,\n        ]\n    ],\n    \"iscrowd\": False,\n}\n\n{'category_id': 0,\n 'image_id': '4',\n 'id': 1,\n 'area': 110901,\n 'bbox': [34.529998779296875,\n  556.8300170898438,\n  401.44000244140625,\n  276.260009765625],\n 'segmentation': [[34.529998779296875,\n   556.8300170898438,\n   435.9700012207031,\n   556.8300170898438,\n   435.9700012207031,\n   833.0900268554688,\n   34.529998779296875,\n   833.0900268554688]],\n 'iscrowd': False}\n\n\nWe see here, that we again have some metadata for each image. We also have a category_id and a bbox. Some of these fields should look familiar to you if you are familiar with the coco format. This will become relevant later, so donâ€™t worry if these arenâ€™t familiar to you.\nOne issue we can run into when training object detection models is stray bounding boxes (i.e.Â ones where the bounding boxes stretch beyond the edge of the image). We can check and remove these quite easily. This is some ugly code/there is probably a better way, but this is a quick check, so Iâ€™ll forgive myself.\n\nfrom tqdm.auto import tqdm\n\n\nremove_idx = []\nfor idx, row in tqdm(enumerate(dataset)):\n    objects_ = row[\"objects\"]\n    for ob in objects_:\n        bbox = ob[\"bbox\"]\n        negative = [box for box in bbox if box &lt; 0]\n        if negative:\n            remove_idx.append(idx)\n\n\n\n\n\nlen(remove_idx)\n\n1\n\n\n\n\nkeep = [i for i in range(len(dataset)) if i not in remove_idx]\nlen(keep)\n\n7257\n\n\n\nThe above code has given us a list of indexes to keep so we use the select method to grab those.\n\ndataset = dataset.select(keep)\n\nWe also create a test split. If we were properly doing this weâ€™d likely want to be a bit more thoughfull about how to do this split.\n\ndataset = dataset.train_test_split(0.1)"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#preparing-the-data",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#preparing-the-data",
    "title": "Training an object detection model using Hugging Face",
    "section": "Preparing the data",
    "text": "Preparing the data\nThis section of the blog post is the bit which focuses on getting data ready for an object detection model such as detr via the datasets library. This is, therefore, also the section which will differ most from the other examples showing how to train models using PyTorch data loaders.\n\nThe Feature Extractor\nIf you are familiar with Hugging Face for natural language tasks, you are probably familiar with using Tokenizer_for_blah_model when pre-processing text. Often if you are using a pre-trained model, you will use AutoTokenizer.from_pretrained, passing in the ID to the model you want to fine-tune. This tokenizer then ensures that the tokenization matches the approach used for the pre-trained model.\nThe Feature Extractor performs a similar task. Letâ€™s look at this more closely. Weâ€™ll use a pre-trained model for this example and fine-tune it. I also include commented-out code, which shows how you could use the same process with any CNN backbone. This may be useful if you have particular requirements about what backbone to use or if you have a CNN backbone that is already fine-tuned on your domain.\n\nmodel_checkpoint = \"facebook/detr-resnet-50\"\n\n\nfrom transformers import DetrFeatureExtractor\n\nfeature_extractor = DetrFeatureExtractor.from_pretrained(model_checkpoint)\n\n\n\n\nIf you wanted to use a different CNN backbone as your starting point you would instead define a config.\n# from transformers import DetrConfig\n# from transformers import DetrFeatureExtractor\n# feature_extractor = DetrFeatureExtractor()\n\nWhat does the feature extractor do?\nTo check what feature extractor does we can make use of the handy inspect function\n\nfrom rich import inspect\n\n\n# collapse_show\ninspect(feature_extractor, methods=True, dunder=True)\n\nâ•­â”€ DetrFeatureExtractor {   \"do_normalize\": true,   \"do_resize\": true,   \"feature_extractor_type\": \"DetrFeatureExâ”€â•®\nâ”‚ def (images: Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), List[PIL.Image.Image],           â”‚\nâ”‚ List[numpy.ndarray], List[ForwardRef('torch.Tensor')]], annotations: Union[List[Dict], List[List[Dict]]] =      â”‚\nâ”‚ None, return_segmentation_masks: Union[bool, NoneType] = False, masks_path: Union[pathlib.Path, NoneType] =     â”‚\nâ”‚ None, pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors: Union[str,                       â”‚\nâ”‚ transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt;                                           â”‚\nâ”‚ transformers.feature_extraction_utils.BatchFeature:                                                             â”‚\nâ”‚                                                                                                                 â”‚\nâ”‚ Constructs a DETR feature extractor.                                                                            â”‚\nâ”‚                                                                                                                 â”‚\nâ”‚                _auto_class = None                                                                               â”‚\nâ”‚                   __dict__ = {                                                                                  â”‚\nâ”‚                                  '_processor_class': None,                                                      â”‚\nâ”‚                                  'feature_extractor_type': 'DetrFeatureExtractor',                              â”‚\nâ”‚                                  'format': 'coco_detection',                                                    â”‚\nâ”‚                                  'do_resize': True,                                                             â”‚\nâ”‚                                  'size': 800,                                                                   â”‚\nâ”‚                                  'max_size': 1333,                                                              â”‚\nâ”‚                                  'do_normalize': True,                                                          â”‚\nâ”‚                                  'image_mean': [0.485, 0.456, 0.406],                                           â”‚\nâ”‚                                  'image_std': [0.229, 0.224, 0.225]                                             â”‚\nâ”‚                              }                                                                                  â”‚\nâ”‚               do_normalize = True                                                                               â”‚\nâ”‚                  do_resize = True                                                                               â”‚\nâ”‚                    __doc__ = '\\n    Constructs a DETR feature extractor.\\n\\n    This feature extractor inherits â”‚\nâ”‚                              from [`FeatureExtractionMixin`] which contains most of the main methods. Users\\n   â”‚\nâ”‚                              should refer to this superclass for more information regarding those               â”‚\nâ”‚                              methods.\\n\\n\\n    Args:\\n        format (`str`, *optional*, defaults to            â”‚\nâ”‚                              `\"coco_detection\"`):\\n            Data format of the annotations. One of           â”‚\nâ”‚                              \"coco_detection\" or \"coco_panoptic\".\\n        do_resize (`bool`, *optional*,       â”‚\nâ”‚                              defaults to `True`):\\n            Whether to resize the input to a certain         â”‚\nâ”‚                              `size`.\\n        size (`int`, *optional*, defaults to 800):\\n            Resize    â”‚\nâ”‚                              the input to the given size. Only has an effect if `do_resize` is set to `True`.   â”‚\nâ”‚                              If size is a\\n            sequence like `(width, height)`, output size will be     â”‚\nâ”‚                              matched to this. If size is an int, smaller edge of\\n            the image will be â”‚\nâ”‚                              matched to this number. i.e, if `height &gt; width`, then image will be rescaled to   â”‚\nâ”‚                              `(size *\\n            height / width, size)`.\\n        max_size (`int`,            â”‚\nâ”‚                              *optional*, defaults to `1333`):\\n            The largest size an image dimension  â”‚\nâ”‚                              can have (otherwise it\\'s capped). Only has an effect if `do_resize` is\\n          â”‚\nâ”‚                              set to `True`.\\n        do_normalize (`bool`, *optional*, defaults to `True`):\\n   â”‚\nâ”‚                              Whether or not to normalize the input with mean and standard deviation.\\n          â”‚\nâ”‚                              image_mean (`int`, *optional*, defaults to `[0.485, 0.456, 0.406]`):\\n             â”‚\nâ”‚                              The sequence of means for each channel, to be used when normalizing images.        â”‚\nâ”‚                              Defaults to the ImageNet mean.\\n        image_std (`int`, *optional*, defaults to  â”‚\nâ”‚                              `[0.229, 0.224, 0.225]`):\\n            The sequence of standard deviations for     â”‚\nâ”‚                              each channel, to be used when normalizing images. Defaults to the\\n                â”‚\nâ”‚                              ImageNet std.\\n    '                                                               â”‚\nâ”‚     feature_extractor_type = 'DetrFeatureExtractor'                                                             â”‚\nâ”‚                     format = 'coco_detection'                                                                   â”‚\nâ”‚                 image_mean = [0.485, 0.456, 0.406]                                                              â”‚\nâ”‚                  image_std = [0.229, 0.224, 0.225]                                                              â”‚\nâ”‚                   max_size = 1333                                                                               â”‚\nâ”‚          model_input_names = ['pixel_values', 'pixel_mask']                                                     â”‚\nâ”‚                 __module__ = 'transformers.models.detr.feature_extraction_detr'                                 â”‚\nâ”‚           _processor_class = None                                                                               â”‚\nâ”‚                       size = 800                                                                                â”‚\nâ”‚                __weakref__ = None                                                                               â”‚\nâ”‚                   __call__ = def __call__(images: Union[PIL.Image.Image, numpy.ndarray,                         â”‚\nâ”‚                              ForwardRef('torch.Tensor'), List[PIL.Image.Image], List[numpy.ndarray],            â”‚\nâ”‚                              List[ForwardRef('torch.Tensor')]], annotations: Union[List[Dict],                  â”‚\nâ”‚                              List[List[Dict]]] = None, return_segmentation_masks: Union[bool, NoneType] =       â”‚\nâ”‚                              False, masks_path: Union[pathlib.Path, NoneType] = None,                           â”‚\nâ”‚                              pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors:           â”‚\nâ”‚                              Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt;   â”‚\nâ”‚                              transformers.feature_extraction_utils.BatchFeature:                                â”‚\nâ”‚                              Main method to prepare for the model one or several image(s) and optional          â”‚\nâ”‚                              annotations. Images are by default                                                 â”‚\nâ”‚                              padded up to the largest image in a batch, and a pixel mask is created that        â”‚\nâ”‚                              indicates which pixels are                                                         â”‚\nâ”‚                              real/which are padding.                                                            â”‚\nâ”‚                center_crop = def center_crop(image, size):                                                      â”‚\nâ”‚                              Crops `image` to the given size using a center crop. Note that if the image is too â”‚\nâ”‚                              small to be cropped to the                                                         â”‚\nâ”‚                              size given, it will be padded (so the returned result has the size asked).         â”‚\nâ”‚                  __class__ = class __class__(format='coco_detection', do_resize=True, size=800, max_size=1333,  â”‚\nâ”‚                              do_normalize=True, image_mean=None, image_std=None, **kwargs): Constructs a DETR   â”‚\nâ”‚                              feature extractor.                                                                 â”‚\nâ”‚  convert_coco_poly_to_mask = def convert_coco_poly_to_mask(segmentations, height, width):                       â”‚\nâ”‚                convert_rgb = def convert_rgb(image): Converts `PIL.Image.Image` to RGB format.                  â”‚\nâ”‚        _create_or_get_repo = def _create_or_get_repo(repo_path_or_name: Union[str, NoneType] = None, repo_url:  â”‚\nâ”‚                              Union[str, NoneType] = None, organization: Union[str, NoneType] = None, private:   â”‚\nâ”‚                              bool = None, use_auth_token: Union[bool, str, NoneType] = None) -&gt;                 â”‚\nâ”‚                              huggingface_hub.repository.Repository:                                             â”‚\nâ”‚                __delattr__ = def __delattr__(name, /): Implement delattr(self, name).                           â”‚\nâ”‚                    __dir__ = def __dir__(): Default dir() implementation.                                       â”‚\nâ”‚   _ensure_format_supported = def _ensure_format_supported(image):                                               â”‚\nâ”‚                     __eq__ = def __eq__(value, /): Return self==value.                                          â”‚\nâ”‚                expand_dims = def expand_dims(image): Expands 2-dimensional `image` to 3 dimensions.             â”‚\nâ”‚                 __format__ = def __format__(format_spec, /): Default object formatter.                          â”‚\nâ”‚                  from_dict = def from_dict(feature_extractor_dict: Dict[str, Any], **kwargs) -&gt;                 â”‚\nâ”‚                              ForwardRef('SequenceFeatureExtractor'):                                            â”‚\nâ”‚                              Instantiates a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a â”‚\nâ”‚                              Python dictionary of                                                               â”‚\nâ”‚                              parameters.                                                                        â”‚\nâ”‚             from_json_file = def from_json_file(json_file: Union[str, os.PathLike]) -&gt;                          â”‚\nâ”‚                              ForwardRef('SequenceFeatureExtractor'):                                            â”‚\nâ”‚                              Instantiates a feature extractor of type                                           â”‚\nâ”‚                              [`~feature_extraction_utils.FeatureExtractionMixin`] from the path to              â”‚\nâ”‚                              a JSON file of parameters.                                                         â”‚\nâ”‚            from_pretrained = def from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike],        â”‚\nâ”‚                              **kwargs) -&gt; ForwardRef('SequenceFeatureExtractor'):                               â”‚\nâ”‚                              Instantiate a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a  â”‚\nâ”‚                              feature extractor, *e.g.* a                                                        â”‚\nâ”‚                              derived class of [`SequenceFeatureExtractor`].                                     â”‚\nâ”‚                     __ge__ = def __ge__(value, /): Return self&gt;=value.                                          â”‚\nâ”‚ get_feature_extractor_dict = def get_feature_extractor_dict(pretrained_model_name_or_path: Union[str,           â”‚\nâ”‚                              os.PathLike], **kwargs) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:                  â”‚\nâ”‚                              From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to  â”‚\nâ”‚                              be used for instantiating a                                                        â”‚\nâ”‚                              feature extractor of type [`~feature_extraction_utils.FeatureExtractionMixin`]     â”‚\nâ”‚                              using `from_dict`.                                                                 â”‚\nâ”‚    _get_repo_url_from_name = def _get_repo_url_from_name(repo_name: str, organization: Union[str, NoneType] =   â”‚\nâ”‚                              None, private: bool = None, use_auth_token: Union[bool, str, NoneType] = None) -&gt;  â”‚\nâ”‚                              str:                                                                               â”‚\nâ”‚           __getattribute__ = def __getattribute__(name, /): Return getattr(self, name).                         â”‚\nâ”‚                     __gt__ = def __gt__(value, /): Return self&gt;value.                                           â”‚\nâ”‚                   __hash__ = def __hash__(): Return hash(self).                                                 â”‚\nâ”‚                   __init__ = def __init__(format='coco_detection', do_resize=True, size=800, max_size=1333,     â”‚\nâ”‚                              do_normalize=True, image_mean=None, image_std=None, **kwargs): Set elements of     â”‚\nâ”‚                              `kwargs` as attributes.                                                            â”‚\nâ”‚          __init_subclass__ = def __init_subclass__(...) This method is called when a class is subclassed.       â”‚\nâ”‚           _is_valid_format = def _is_valid_format(format):                                                      â”‚\nâ”‚                     __le__ = def __le__(value, /): Return self&lt;=value.                                          â”‚\nâ”‚                     __lt__ = def __lt__(value, /): Return self&lt;value.                                           â”‚\nâ”‚               _max_by_axis = def _max_by_axis(the_list):                                                        â”‚\nâ”‚                     __ne__ = def __ne__(value, /): Return self!=value.                                          â”‚\nâ”‚                    __new__ = def __new__(*args, **kwargs): Create and return a new object.  See help(type) for  â”‚\nâ”‚                              accurate signature.                                                                â”‚\nâ”‚                 _normalize = def _normalize(image, mean, std, target=None): Normalize the image with a certain  â”‚\nâ”‚                              mean and std.                                                                      â”‚\nâ”‚                  normalize = def normalize(image, mean, std):                                                   â”‚\nâ”‚                              Normalizes `image` with `mean` and `std`. Note that this will trigger a conversion â”‚\nâ”‚                              of `image` to a NumPy array                                                        â”‚\nâ”‚                              if it's a PIL Image.                                                               â”‚\nâ”‚  pad_and_create_pixel_mask = def pad_and_create_pixel_mask(pixel_values_list: List[ForwardRef('torch.Tensor')], â”‚\nâ”‚                              return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] =      â”‚\nâ”‚                              None): Pad images up to the largest image in a batch and create a corresponding    â”‚\nâ”‚                              `pixel_mask`.                                                                      â”‚\nâ”‚               post_process = def post_process(outputs, target_sizes):                                           â”‚\nâ”‚                              Converts the output of [`DetrForObjectDetection`] into the format expected by the  â”‚\nâ”‚                              COCO api. Only supports                                                            â”‚\nâ”‚                              PyTorch.                                                                           â”‚\nâ”‚      post_process_instance = def post_process_instance(results, outputs, orig_target_sizes, max_target_sizes,   â”‚\nâ”‚                              threshold=0.5):                                                                    â”‚\nâ”‚                              Converts the output of [`DetrForSegmentation`] into actual instance segmentation   â”‚\nâ”‚                              predictions. Only supports                                                         â”‚\nâ”‚                              PyTorch.                                                                           â”‚\nâ”‚      post_process_panoptic = def post_process_panoptic(outputs, processed_sizes, target_sizes=None,             â”‚\nâ”‚                              is_thing_map=None, threshold=0.85): Converts the output of [`DetrForSegmentation`] â”‚\nâ”‚                              into actual panoptic predictions. Only supports PyTorch.                           â”‚\nâ”‚  post_process_segmentation = def post_process_segmentation(outputs, target_sizes, threshold=0.9,                â”‚\nâ”‚                              mask_threshold=0.5): Converts the output of [`DetrForSegmentation`] into image     â”‚\nâ”‚                              segmentation predictions. Only supports PyTorch.                                   â”‚\nâ”‚                    prepare = def prepare(image, target, return_segmentation_masks=False, masks_path=None):      â”‚\nâ”‚     prepare_coco_detection = def prepare_coco_detection(image, target, return_segmentation_masks=False):        â”‚\nâ”‚                              Convert the target in COCO format into the format expected by DETR.                â”‚\nâ”‚      prepare_coco_panoptic = def prepare_coco_panoptic(image, target, masks_path, return_masks=True):           â”‚\nâ”‚               _push_to_hub = def _push_to_hub(repo: huggingface_hub.repository.Repository, commit_message:      â”‚\nâ”‚                              Union[str, NoneType] = None) -&gt; str:                                               â”‚\nâ”‚                push_to_hub = def push_to_hub(repo_path_or_name: Union[str, NoneType] = None, repo_url:          â”‚\nâ”‚                              Union[str, NoneType] = None, use_temp_dir: bool = False, commit_message:           â”‚\nâ”‚                              Union[str, NoneType] = None, organization: Union[str, NoneType] = None, private:   â”‚\nâ”‚                              Union[bool, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None,   â”‚\nâ”‚                              **model_card_kwargs) -&gt; str:                                                       â”‚\nâ”‚                              Upload the feature extractor file to the ðŸ¤— Model Hub while synchronizing a local  â”‚\nâ”‚                              clone of the repo in                                                               â”‚\nâ”‚                              `repo_path_or_name`.                                                               â”‚\nâ”‚                 __reduce__ = def __reduce__(): Helper for pickle.                                               â”‚\nâ”‚              __reduce_ex__ = def __reduce_ex__(protocol, /): Helper for pickle.                                 â”‚\nâ”‚    register_for_auto_class = def register_for_auto_class(auto_class='AutoFeatureExtractor'):                    â”‚\nâ”‚                              Register this class with a given auto class. This should only be used for custom   â”‚\nâ”‚                              feature extractors as the ones                                                     â”‚\nâ”‚                              in the library are already mapped with `AutoFeatureExtractor`.                     â”‚\nâ”‚                   __repr__ = def __repr__(): Return repr(self).                                                 â”‚\nâ”‚                    _resize = def _resize(image, size, target=None, max_size=None):                              â”‚\nâ”‚                              Resize the image to the given size. Size can be min_size (scalar) or (w, h) tuple. â”‚\nâ”‚                              If size is an int, smaller                                                         â”‚\nâ”‚                              edge of the image will be matched to this number.                                  â”‚\nâ”‚                     resize = def resize(image, size, resample=2, default_to_square=True, max_size=None):        â”‚\nâ”‚                              Resizes `image`. Enforces conversion of input to PIL.Image.                        â”‚\nâ”‚            save_pretrained = def save_pretrained(save_directory: Union[str, os.PathLike], push_to_hub: bool =   â”‚\nâ”‚                              False, **kwargs):                                                                  â”‚\nâ”‚                              Save a feature_extractor object to the directory `save_directory`, so that it can  â”‚\nâ”‚                              be re-loaded using the                                                             â”‚\nâ”‚                              [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`] class method. â”‚\nâ”‚       _set_processor_class = def _set_processor_class(processor_class: str): Sets processor class as an         â”‚\nâ”‚                              attribute.                                                                         â”‚\nâ”‚                __setattr__ = def __setattr__(name, value, /): Implement setattr(self, name, value).             â”‚\nâ”‚                 __sizeof__ = def __sizeof__(): Size of object in memory, in bytes.                              â”‚\nâ”‚                    __str__ = def __str__(): Return str(self).                                                   â”‚\nâ”‚           __subclasshook__ = def __subclasshook__(...) Abstract classes can override this to customize          â”‚\nâ”‚                              issubclass().                                                                      â”‚\nâ”‚                    to_dict = def to_dict() -&gt; Dict[str, Any]: Serializes this instance to a Python dictionary.  â”‚\nâ”‚               to_json_file = def to_json_file(json_file_path: Union[str, os.PathLike]): Save this instance to a â”‚\nâ”‚                              JSON file.                                                                         â”‚\nâ”‚             to_json_string = def to_json_string() -&gt; str: Serializes this instance to a JSON string.            â”‚\nâ”‚             to_numpy_array = def to_numpy_array(image, rescale=None, channel_first=True):                       â”‚\nâ”‚                              Converts `image` to a numpy array. Optionally rescales it and puts the channel     â”‚\nâ”‚                              dimension as the first                                                             â”‚\nâ”‚                              dimension.                                                                         â”‚\nâ”‚               to_pil_image = def to_pil_image(image, rescale=None):                                             â”‚\nâ”‚                              Converts `image` to a PIL Image. Optionally rescales it and puts the channel       â”‚\nâ”‚                              dimension back as the last axis if                                                 â”‚\nâ”‚                              needed.                                                                            â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\n\n\nThe output of inspect can be pretty verbose, but I often find this a handy tool for quickly trying to work out a new library of API.\nWeâ€™ll look at the most critical parts in more detail, but Iâ€™ll point out a few things; youâ€™ll see some attributes that will probably sound familiar.\nimage_mean = [0.485, 0.456, 0.406]                                                              \nimage_std = [0.229, 0.224, 0.225]          \nThese are the mean and standard deviation used during the model training. Itâ€™s essential when weâ€™re doing inference or fine-tuning to replicate these, and having these all stored inside a feature_extractor means we donâ€™t have to go poking around in papers to try and work out what these values should be.\nAnother thing to point out is the push_to_hub method. We can store feature_extractors in the hub just as we can store models and tokenizers. Having to track the appropriate pre-processing steps for an image manually is super annoying to do manually. Storing this as we do other model components is much simpler and helps avoid errors resulting from tracing these things by hand.\nThe __call__ method for the DetrFeatureExtractor is what weâ€™ll use to prepare our images before we pass them into the model, letâ€™s dig more closely into this.\n\ninspect(\n    feature_extractor.__call__,\n)\n\nâ•­â”€ &lt;bound method DetrFeatureExtractor.__call__ of DetrFeatureExtractor {   \"do_normalize\": true,   \"do_resize\": tâ”€â•®\nâ”‚ def DetrFeatureExtractor.__call__(images: Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'),     â”‚\nâ”‚ List[PIL.Image.Image], List[numpy.ndarray], List[ForwardRef('torch.Tensor')]], annotations: Union[List[Dict],   â”‚\nâ”‚ List[List[Dict]]] = None, return_segmentation_masks: Union[bool, NoneType] = False, masks_path:                 â”‚\nâ”‚ Union[pathlib.Path, NoneType] = None, pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors:  â”‚\nâ”‚ Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt;                                â”‚\nâ”‚ transformers.feature_extraction_utils.BatchFeature:                                                             â”‚\nâ”‚                                                                                                                 â”‚\nâ”‚ Main method to prepare for the model one or several image(s) and optional annotations. Images are by default    â”‚\nâ”‚ padded up to the largest image in a batch, and a pixel mask is created that indicates which pixels are          â”‚\nâ”‚ real/which are padding.                                                                                         â”‚\nâ”‚                                                                                                                 â”‚\nâ”‚ 27 attribute(s) not shown. Run inspect(inspect) for options.                                                    â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\n\n\nUnderstanding what the __call__ method expected, and how to make sure that is whatâ€™s delivered by the datasets library is the key thing I needed to work out. What does it expect:\n\nimages: this can be a list or a single image (and stored in different formats)\nannotations this should be of type Union[List[Dict],â”‚â”‚ List[List[Dict]]].\n\nThe images part is not too tricky to understand. We can either pass in a single image, a NumPy array representing an image or a list of images or NumPy arrays.\nThe annotations part is where Python type annotations donâ€™t always do us many favours since we only know weâ€™re expecting a list of dictionaries, but we can safely assume those dictionaries probably need to have a particular format. We can try and see what happens if we pass in an image and a list of a random dictionary.\n\nimport io\n\nimport requests\nfrom PIL import Image\n\n\nim = Image.open(\n    io.BytesIO(\n        requests.get(\n            \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/cute-cat-photos-1593441022.jpg?crop=1.00xw:0.749xh;0,0.154xh&resize=980:*\"\n        ).content\n    )\n)\nim\n\n\n\n\n\n\n\n\n\nlabels = [\n    {\n        \"bbox\": [\n            0.0,\n            3,\n            3,\n            4,\n        ]\n    }\n]\n\n\nfeature_extractor(im, labels)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [23], in &lt;cell line: 1&gt;()\n----&gt; 1 feature_extractor(im, labels)\n\nFile /usr/local/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/models/detr/feature_extraction_detr.py:524, in DetrFeatureExtractor.__call__(self, images, annotations, return_segmentation_masks, masks_path, pad_and_return_pixel_mask, return_tensors, **kwargs)\n    521                         valid_annotations = True\n    523     if not valid_annotations:\n--&gt; 524         raise ValueError(\n    525             \"\"\"\n    526             Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object\n    527             detection, each dictionary should contain the keys 'image_id' and 'annotations', with the latter\n    528             being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\n    529             should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a list\n    530             of annotations in COCO format.\n    531             \"\"\"\n    532         )\n    534 # Check that masks_path has a valid type\n    535 if masks_path is not None:\n\nValueError: \n                    Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object\n                    detection, each dictionary should contain the keys 'image_id' and 'annotations', with the latter\n                    being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\n                    should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a list\n                    of annotations in COCO format.\n                    \n\n\n\nWe can see that this raises a ValueError. We also get some more information here that gives us a clue where we went wrong. Specifically we can see that the annotations for a single image should be a Dict or List[Dict] if weâ€™re using a batch of images. We also see that we should pass in this data in the COCO format. Since our data is already in this format we should be able to pass in an example.\n\nimage = dataset[\"train\"][0][\"image\"]\nimage\n\n\n\n\n\n\n\n\n\nannotations = dataset[\"train\"][0][\"objects\"]\nannotations\n\n[\n    {\n        'category_id': 0,\n        'image_id': '8081',\n        'id': 646,\n        'area': 114552,\n        'bbox': [81.0, 408.0, 387.0, 296.0],\n        'segmentation': [[81.0, 408.0, 468.0, 408.0, 468.0, 704.0, 81.0, 704.0]],\n        'iscrowd': False\n    }\n]\n\n\n\n\nfeature_extractor(images=image, annotations=annotations, return_tensors=\"pt\")\n\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ /var/folders/tj/54sfzlyj6_573fn82y996grc0000gr/T/ipykernel_47936/1569761185.py:1 in &lt;cell line:  â”‚\nâ”‚ 1&gt;                                                                                               â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ [Errno 2] No such file or directory:                                                             â”‚\nâ”‚ '/var/folders/tj/54sfzlyj6_573fn82y996grc0000gr/T/ipykernel_47936/1569761185.py'                 â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /usr/local/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/models/det â”‚\nâ”‚ r/feature_extraction_detr.py:524 in __call__                                                     â”‚\nâ”‚                                                                                                  â”‚\nâ”‚   521 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   valid_annotations = True                                   â”‚\nâ”‚   522 â”‚   â”‚   â”‚                                                                                  â”‚\nâ”‚   523 â”‚   â”‚   â”‚   if not valid_annotations:                                                      â”‚\nâ”‚ â± 524 â”‚   â”‚   â”‚   â”‚   raise ValueError(                                                          â”‚\nâ”‚   525 â”‚   â”‚   â”‚   â”‚   â”‚   \"\"\"                                                                    â”‚\nâ”‚   526 â”‚   â”‚   â”‚   â”‚   â”‚   Annotations must of type `Dict` (single image) or `List[Dict]` (batc   â”‚\nâ”‚   527 â”‚   â”‚   â”‚   â”‚   â”‚   detection, each dictionary should contain the keys 'image_id' and 'a   â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\nValueError: \n                    Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of \nobject\n                    detection, each dictionary should contain the keys 'image_id' and 'annotations', with the \nlatter\n                    being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\n                    should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a \nlist\n                    of annotations in COCO format.\n                    \n\n\n\nOh no! It still doesnâ€™t work. At this point, itâ€™s we probably either want to dig into the source code to work out what we should be passing to the feature_extractor. The relevant function is def prepare_coco_detection.\nWe also have another tutorial to consult. In this tutorial we see that the annotations are stored in a dictionary target with the keys image_id and annotations.\ntarget = {'image_id': image_id, 'annotations': target}\nencoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\nWith a bit more wrangling letâ€™s see if this works.\n\ntarget = {\"image_id\": 4, \"annotations\": annotations}\n\n\nfeature_extractor(images=image, annotations=target, return_tensors=\"pt\")\n\n{\n    'pixel_values': tensor([[[[-0.7650, -0.9705, -0.9705,  ..., -1.4158, -1.3815, -1.3815],\n          [-0.7822, -0.9020, -0.9020,  ..., -1.3815, -1.3644, -1.4500],\n          [-0.8164, -0.9020, -0.9192,  ..., -1.3815, -1.3815, -1.4500],\n          ...,\n          [ 1.5297,  1.5639,  1.5810,  ...,  1.4612,  1.4612,  1.4783],\n          [ 1.5125,  1.5297,  1.5468,  ...,  1.4783,  1.4783,  1.4954],\n          [ 1.4954,  1.5125,  1.5125,  ...,  1.5125,  1.5125,  1.5297]],\n\n         [[-0.6527, -0.8627, -0.8627,  ..., -1.3179, -1.2829, -1.2829],\n          [-0.7052, -0.8102, -0.8277,  ..., -1.3004, -1.2829, -1.3704],\n          [-0.7402, -0.8102, -0.8277,  ..., -1.3529, -1.3529, -1.4405],\n          ...,\n          [ 1.5357,  1.5707,  1.5882,  ...,  1.3957,  1.3957,  1.4132],\n          [ 1.5182,  1.5357,  1.5532,  ...,  1.4132,  1.4132,  1.4307],\n          [ 1.5007,  1.5182,  1.5182,  ...,  1.4482,  1.4482,  1.4657]],\n\n         [[-0.4275, -0.6367, -0.6367,  ..., -1.1073, -1.0898, -1.0898],\n          [-0.4624, -0.5670, -0.5844,  ..., -1.1247, -1.1247, -1.2119],\n          [-0.5147, -0.6018, -0.6193,  ..., -1.2293, -1.2467, -1.3339],\n          ...,\n          [ 1.4548,  1.4897,  1.5071,  ...,  1.3154,  1.3154,  1.3328],\n          [ 1.4374,  1.4548,  1.4722,  ...,  1.3328,  1.3328,  1.3502],\n          [ 1.4200,  1.4374,  1.4374,  ...,  1.3677,  1.3677,  1.3851]]]]),\n    'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]]),\n    'labels': [\n        {\n            'boxes': tensor([[0.4575, 0.5120, 0.6450, 0.2726]]),\n            'class_labels': tensor([0]),\n            'image_id': tensor([4]),\n            'area': tensor([172346.9688]),\n            'iscrowd': tensor([0]),\n            'orig_size': tensor([1086,  600]),\n            'size': tensor([1332,  736])\n        }\n    ]\n}\n\n\n\nThis is looking more like it! Now we have one example working we can translate this to a function that can prepare a batch into the same format.\nSince we get a batch at a time we might need to refactor things slightly. In this example Iâ€™ve just grabbed the relevant lists for the images, image_id and annotations. We then use a list compression to store these in the dictionary format expected by the feature_extractor.\n\ndef transform(example_batch):\n    images = example_batch[\"image\"]\n    ids_ = example_batch[\"image_id\"]\n    objects = example_batch[\"objects\"]\n    targets = [\n        {\"image_id\": id_, \"annotations\": object_} for id_, object_ in zip(ids_, objects)\n    ]\n    return feature_extractor(images=images, annotations=targets, return_tensors=\"pt\")\n\nWe could apply this to our data using map but it often makes more sense to applay these on the fly using the with_transform method.\n\ndataset[\"train\"] = dataset[\"train\"].with_transform(transform)\n\nLetâ€™s take a look at an example\n\ndataset[\"train\"][0]\n\n{\n    'pixel_values': tensor([[[-0.7650, -0.9705, -0.9705,  ..., -1.4158, -1.3815, -1.3815],\n         [-0.7822, -0.9020, -0.9020,  ..., -1.3815, -1.3644, -1.4500],\n         [-0.8164, -0.9020, -0.9192,  ..., -1.3815, -1.3815, -1.4500],\n         ...,\n         [ 1.5297,  1.5639,  1.5810,  ...,  1.4612,  1.4612,  1.4783],\n         [ 1.5125,  1.5297,  1.5468,  ...,  1.4783,  1.4783,  1.4954],\n         [ 1.4954,  1.5125,  1.5125,  ...,  1.5125,  1.5125,  1.5297]],\n\n        [[-0.6527, -0.8627, -0.8627,  ..., -1.3179, -1.2829, -1.2829],\n         [-0.7052, -0.8102, -0.8277,  ..., -1.3004, -1.2829, -1.3704],\n         [-0.7402, -0.8102, -0.8277,  ..., -1.3529, -1.3529, -1.4405],\n         ...,\n         [ 1.5357,  1.5707,  1.5882,  ...,  1.3957,  1.3957,  1.4132],\n         [ 1.5182,  1.5357,  1.5532,  ...,  1.4132,  1.4132,  1.4307],\n         [ 1.5007,  1.5182,  1.5182,  ...,  1.4482,  1.4482,  1.4657]],\n\n        [[-0.4275, -0.6367, -0.6367,  ..., -1.1073, -1.0898, -1.0898],\n         [-0.4624, -0.5670, -0.5844,  ..., -1.1247, -1.1247, -1.2119],\n         [-0.5147, -0.6018, -0.6193,  ..., -1.2293, -1.2467, -1.3339],\n         ...,\n         [ 1.4548,  1.4897,  1.5071,  ...,  1.3154,  1.3154,  1.3328],\n         [ 1.4374,  1.4548,  1.4722,  ...,  1.3328,  1.3328,  1.3502],\n         [ 1.4200,  1.4374,  1.4374,  ...,  1.3677,  1.3677,  1.3851]]]),\n    'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]]),\n    'labels': {\n        'boxes': tensor([[0.4575, 0.5120, 0.6450, 0.2726]]),\n        'class_labels': tensor([0]),\n        'image_id': tensor([8081]),\n        'area': tensor([172346.9688]),\n        'iscrowd': tensor([0]),\n        'orig_size': tensor([1086,  600]),\n        'size': tensor([1332,  736])\n    }\n}\n\n\n\nThe next thing we need to take care of is a collate function. â€˜Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.â€™ source.\n\ndef collate_fn(batch):\n    pixel_values = [item[\"pixel_values\"] for item in batch]\n    encoding = feature_extractor.pad_and_create_pixel_mask(\n        pixel_values, return_tensors=\"pt\"\n    )\n    labels = [item[\"labels\"] for item in batch]\n    batch = {} # collated batch  \n    batch['pixel_values'] = encoding['pixel_values']\n    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n    batch[\"labels\"] = labels\n    return batch"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#creating-a-detr-model",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#creating-a-detr-model",
    "title": "Training an object detection model using Hugging Face",
    "section": "Creating a detr model",
    "text": "Creating a detr model\n\nAvoiding ambiguous labels\nWeâ€™re almost at the point where we can start training the model. We just do a little bit of housekeeping to make sure our model knows what our encoded labels are. Itâ€™s super annoying when you are trying a model out on the Hugging Face Hub and you get back labels, 0 or 3 with no clue what these labels refer to. We can avoid this by telling our model what labels we have. This mapping will then be bundled with the model when we push it to the hub.\n\nid2label = dict(enumerate(dataset[\"train\"].features[\"objects\"][0][\"category_id\"].names))\nlabel2id = {v: k for k, v in id2label.items()}\nlabel2id\n\n{'early_printed_illustration': 0}\n\n\n\nNow we can create the DetrForObjectDetection model. This should all look familiar if youâ€™ve used transformers for other tasks.\n\nfrom transformers import DetrForObjectDetection\n\nmodel = DetrForObjectDetection.from_pretrained(\n    model_checkpoint,\n    num_labels=1,\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True,\n)\n\n\n\n\n\n\n\nDownloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_a1_0-14fe96d1.pth\nSome weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nIf you wanted to use another backbone you could do something like:\n# from transformers import DetrForObjectDetection\n\n# config = DetrConfig(backbone='regnetz_e8',id2label=id2label, label2id=label2id)\n# model = DetrForObjectDetection(config)"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#training-the-detr-model",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#training-the-detr-model",
    "title": "Training an object detection model using Hugging Face",
    "section": "Training the detr model",
    "text": "Training the detr model\nWe now specify our TrainingArguments\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"detr-resnet-50_fine_tuned_nls_chapbooks\",\n    per_device_train_batch_size=8,\n    num_train_epochs=10,\n    fp16=False,\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=1e-4,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=True,\n    hub_model_id=\"davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks\",\n)\n\nand create our Trainer\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=dataset[\"train\"],\n    tokenizer=feature_extractor,\n)\n\nCloning https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks into local empty directory.\n\n\n\n#collapse\ntrainer.train()\n\n/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 6531\n  Num Epochs = 10\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 8170\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\nChanges to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to the W&B docs.\n\n\nwandb: Currently logged in as: davanstrien (flyswot). Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.12.21\n\n\nRun data is saved locally in /content/wandb/run-20220724_144321-sgzdksxm\n\n\nSyncing run detr-resnet-50_fine_tuned_nls_chapbooks to Weights & Biases (docs)\n\n\n\n    \n      \n      \n      [8170/8170 5:15:38, Epoch 10/10]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n50\n1.975700\n\n\n100\n3.254000\n\n\n150\n1.563300\n\n\n200\n1.103100\n\n\n250\n1.468000\n\n\n300\n1.169700\n\n\n350\n1.326600\n\n\n400\n1.413800\n\n\n450\n1.101600\n\n\n500\n1.054500\n\n\n550\n0.946000\n\n\n600\n0.871600\n\n\n650\n0.723600\n\n\n700\n0.866800\n\n\n750\n0.740400\n\n\n800\n0.753300\n\n\n850\n0.748900\n\n\n900\n0.919600\n\n\n950\n0.805800\n\n\n1000\n0.902200\n\n\n1050\n0.788800\n\n\n1100\n0.734400\n\n\n1150\n0.635700\n\n\n1200\n0.769000\n\n\n1250\n0.673000\n\n\n1300\n0.766200\n\n\n1350\n0.664800\n\n\n1400\n0.653700\n\n\n1450\n0.589500\n\n\n1500\n0.580900\n\n\n1550\n0.583200\n\n\n1600\n0.736000\n\n\n1650\n0.594900\n\n\n1700\n0.701400\n\n\n1750\n0.600300\n\n\n1800\n0.470900\n\n\n1850\n0.522800\n\n\n1900\n0.590300\n\n\n1950\n0.566300\n\n\n2000\n0.586800\n\n\n2050\n0.623800\n\n\n2100\n0.523400\n\n\n2150\n0.562500\n\n\n2200\n0.604100\n\n\n2250\n0.518000\n\n\n2300\n0.525100\n\n\n2350\n0.499100\n\n\n2400\n0.564900\n\n\n2450\n0.455100\n\n\n2500\n0.465000\n\n\n2550\n0.533200\n\n\n2600\n0.512500\n\n\n2650\n0.465100\n\n\n2700\n0.521800\n\n\n2750\n0.519500\n\n\n2800\n0.456800\n\n\n2850\n0.444400\n\n\n2900\n0.429600\n\n\n2950\n0.445400\n\n\n3000\n0.425600\n\n\n3050\n0.439600\n\n\n3100\n0.468000\n\n\n3150\n0.426500\n\n\n3200\n0.433500\n\n\n3250\n0.479400\n\n\n3300\n0.421800\n\n\n3350\n0.449500\n\n\n3400\n0.399300\n\n\n3450\n0.424500\n\n\n3500\n0.447700\n\n\n3550\n0.428900\n\n\n3600\n0.403800\n\n\n3650\n0.448300\n\n\n3700\n0.424300\n\n\n3750\n0.396600\n\n\n3800\n0.405900\n\n\n3850\n0.436300\n\n\n3900\n0.371500\n\n\n3950\n0.412300\n\n\n4000\n0.389200\n\n\n4050\n0.391900\n\n\n4100\n0.403200\n\n\n4150\n0.386800\n\n\n4200\n0.382500\n\n\n4250\n0.402000\n\n\n4300\n0.374400\n\n\n4350\n0.355900\n\n\n4400\n0.390800\n\n\n4450\n0.402600\n\n\n4500\n0.397100\n\n\n4550\n0.399700\n\n\n4600\n0.363900\n\n\n4650\n0.373600\n\n\n4700\n0.391600\n\n\n4750\n0.339200\n\n\n4800\n0.351900\n\n\n4850\n0.381800\n\n\n4900\n0.381800\n\n\n4950\n0.326000\n\n\n5000\n0.388300\n\n\n5050\n0.359100\n\n\n5100\n0.380500\n\n\n5150\n0.357100\n\n\n5200\n0.389500\n\n\n5250\n0.386200\n\n\n5300\n0.373000\n\n\n5350\n0.340000\n\n\n5400\n0.337100\n\n\n5450\n0.357500\n\n\n5500\n0.338900\n\n\n5550\n0.334500\n\n\n5600\n0.362000\n\n\n5650\n0.426100\n\n\n5700\n0.329500\n\n\n5750\n0.321500\n\n\n5800\n0.328800\n\n\n5850\n0.322400\n\n\n5900\n0.385900\n\n\n5950\n0.373800\n\n\n6000\n0.326000\n\n\n6050\n0.335200\n\n\n6100\n0.341600\n\n\n6150\n0.309600\n\n\n6200\n0.295700\n\n\n6250\n0.338600\n\n\n6300\n0.326800\n\n\n6350\n0.305600\n\n\n6400\n0.287200\n\n\n6450\n0.307700\n\n\n6500\n0.297000\n\n\n6550\n0.296700\n\n\n6600\n0.292700\n\n\n6650\n0.305300\n\n\n6700\n0.289300\n\n\n6750\n0.290600\n\n\n6800\n0.277100\n\n\n6850\n0.296500\n\n\n6900\n0.291800\n\n\n6950\n0.285800\n\n\n7000\n0.291400\n\n\n7050\n0.282500\n\n\n7100\n0.271500\n\n\n7150\n0.278300\n\n\n7200\n0.272100\n\n\n7250\n0.273800\n\n\n7300\n0.313500\n\n\n7350\n0.288600\n\n\n7400\n0.258700\n\n\n7450\n0.275700\n\n\n7500\n0.248200\n\n\n7550\n0.280800\n\n\n7600\n0.268500\n\n\n7650\n0.258700\n\n\n7700\n0.302800\n\n\n7750\n0.288700\n\n\n7800\n0.278400\n\n\n7850\n0.260700\n\n\n7900\n0.271200\n\n\n7950\n0.247900\n\n\n8000\n0.234700\n\n\n8050\n0.263900\n\n\n8100\n0.251900\n\n\n8150\n0.246900\n\n\n\n\n\n\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/preprocessor_config.json\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/preprocessor_config.json\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/preprocessor_config.json\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(\n    global_step=8170,\n    training_loss=0.5031144771902768,\n    metrics={\n        'train_runtime': 18954.8014,\n        'train_samples_per_second': 3.446,\n        'train_steps_per_second': 0.431,\n        'total_flos': 5.190134584879058e+19,\n        'train_loss': 0.5031144771902768,\n        'epoch': 10.0\n    }\n)\n\n\n\nOnce weâ€™ve finished training we can use push_to_hub to share our model on the hub.\n\ntrainer.push_to_hub(\"finished training\")\n\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/preprocessor_config.json\nSeveral commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\n\n\n\n\n\n\n\n\nTo https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks\n   c94bb78..5c7b9d8  main -&gt; main\n\nDropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Object Detection', 'type': 'object-detection'}, 'dataset': {'name': 'nls_chapbook_illustrations', 'type': 'nls_chapbook_illustrations', 'args': 'illustration_detection'}}\nTo https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks\n   5c7b9d8..2ece586  main -&gt; main\n\n\n\n'https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks/commit/5c7b9d8907981c7ee0005334a3d96d5a8d623957'"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#conclusion",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#conclusion",
    "title": "Training an object detection model using Hugging Face",
    "section": "Conclusion",
    "text": "Conclusion\nWeâ€™ve seen how we can use the datasets library to perform object detection. The main thing we need to work out when weâ€™re trying to use datasets for a computer vision task in the transfortmer libraryy is how to ensure we can create a transform that gets the images and annotations into a format understood by the relevant feature_extractor. Once weâ€™ve done this for one example we need to replicate the same thing with a batch of examples so we can use it as a transform.\nOne this is in place many of the same approaches to defining the model and training arguments should look very familiar. I didnâ€™t spend much time on the training process in this post. Iâ€™ll dig into tha in a fguure post as well as covering the process of using/evaluating the model."
  },
  {
    "objectID": "posts/post-with-code/datasets-groupby/2023-09-18-datasets-groupby.html",
    "href": "posts/post-with-code/datasets-groupby/2023-09-18-datasets-groupby.html",
    "title": "How to do groupby for Hugging Face datasets",
    "section": "",
    "text": "There is no native support for GroupBy in the Hugging Face datasets library but since there is a polars integration, we can use that to do groupby operations. In this notebook, I will show you how to do a groupby operation on a Hugging Face dataset using polars.\n%pip install datasets polars matplotlib --quiet\n\nNote: you may need to restart the kernel to use updated packages.\nfrom datasets import load_dataset\nimport polars as pl\n\n/Users/davanstrien/Documents/daniel/new_blog/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nWe can use the datasets library to load the dataset and then convert it to a polars DataFrame. After that, we can use the group_by method to group the data by a specific column and then apply an aggregation function to the grouped data.\nds = load_dataset(\"argilla/databricks-dolly-15k-curated-en\")\nds['train'][0]\n\n{'id': '0',\n 'category': 'closed_qa',\n 'original-instruction': 'When did Virgin Australia start operating?',\n 'original-context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.[3] It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.[4]\",\n 'original-response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n 'new-instruction': {'user_id': [None],\n  'value': ['When did Virgin Australia start operating?'],\n  'status': ['submitted']},\n 'new-context': {'user_id': [None],\n  'value': [\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"],\n  'status': ['submitted']},\n 'new-response': {'user_id': [None],\n  'value': ['Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.'],\n  'status': ['submitted']},\n 'external_id': None}\ndf = ds['train'].to_polars()"
  },
  {
    "objectID": "posts/post-with-code/datasets-groupby/2023-09-18-datasets-groupby.html#big-datasets",
    "href": "posts/post-with-code/datasets-groupby/2023-09-18-datasets-groupby.html#big-datasets",
    "title": "How to do groupby for Hugging Face datasets",
    "section": "Big datasets",
    "text": "Big datasets\nSince polars supports lazy evaluation, you can work with very large datasets. This is very useful when you have a dataset that is too large to fit in memory. You can perform operations on the dataset without loading the entire dataset into memory. For a Hub dataset we can use the scan_parquet method to load the dataset lazily.\n\ndf = pl.scan_parquet('hf://datasets/BAAI/Infinity-Instruct/3M/*.parquet')\n\nYouâ€™ll see this returns a lazy DataFrame that you can use to perform operations on the dataset.\n\ntype(df)\n\npolars.lazyframe.frame.LazyFrame\n\n\nIf we do a similar groupby operation on this dataset\n\nresult = df.group_by('langdetect').agg(\n    pl.col('conversations').list.len().mean().alias('mean_conversation_length')\n)\n\nyouâ€™ll see that we donâ€™t get back an immediate result but a lazy DataFrame that we can use to perform operations on the dataset.\n\nresult\n\nNAIVE QUERY PLANrun LazyFrame.show_graph() to see the optimized version\n\n\n\n\n\npolars_query\n\n\n\np1\n\nAGG [col(\"conversations\").list.length().mean().alias(\"mean_conversation_length\")]\nBY\n[col(\"langdetect\")]\n\n\n\np2\n\nParquet SCAN [https://huggingface.co/datasets/BAAI%2FInfinity-Instruct/resolve/main/3M%2Ftrain-00000-of-00035.parquet, ... 34 other files]\nÏ€ */5;\n\n\n\np1--p2\n\n\n\n\n\n\nWhere possible polars will also add some optimizations to the query to make it faster.\n\nresult.show_graph()\n\n\n\n\n\n\n\n\nWe can then use the collect method to get the result of the query.\n\nresult.collect()\n\n\nshape: (2, 2)\n\n\n\nlangdetect\nmean_conversation_length\n\n\nstr\nf64\n\n\n\n\n\"en\"\n3.068929\n\n\n\"zh-cn\"\n3.214509"
  },
  {
    "objectID": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html",
    "href": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html",
    "title": "Dynamically updating a Hugging Face hub organization README",
    "section": "",
    "text": "tl;dr we can use the huggingface_hub library to auto generate a model card readme for the BigLAM organization."
  },
  {
    "objectID": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#what-are-we-aiming-to-do",
    "href": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#what-are-we-aiming-to-do",
    "title": "Dynamically updating a Hugging Face hub organization README",
    "section": "What are we aiming to do?",
    "text": "What are we aiming to do?\nThe Hugging Face hub allows organizations to create a README card to describe their organization.\n\nWhilst you can manually create this there might be some content that would be nice to auto populate. For example, for the BigLAM organization, weâ€™re mainly focused on collecting datasets. Since we have many tasks supported by these datasets we might want to create a list of datasets organized by task. Ideally we donâ€™t want to have to manually update this. Letâ€™s see how we can do this!\nFirst weâ€™ll install the huggingface_hub library which allows us to interact with the hub. Weâ€™ll install Jinja2 for templating and toolz because toolz makes Python infinitely more delightful!\n\n%pip install huggingface_hub toolz Jinja2\n\nRequirement already satisfied: huggingface_hub in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (0.11.1)\nRequirement already satisfied: toolz in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (0.12.0)\nRequirement already satisfied: Jinja2 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (3.1.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: packaging&gt;=20.9 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (23.0)\nRequirement already satisfied: requests in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (2.28.2)\nRequirement already satisfied: filelock in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (4.4.0)\nRequirement already satisfied: tqdm in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from Jinja2) (2.1.1)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (1.26.14)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (3.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (2022.12.7)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (3.0.1)\n\n[notice] A new release of pip available: 22.3.1 -&gt; 23.0.1\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport toolz\nfrom huggingface_hub import list_datasets\n\nWe list all the datasets under this organization\n\nbig_lam_datasets = list(iter(list_datasets(author=\"biglam\", limit=None, full=True)))\n\nWe want to check which tasks our organization currently has. If we look at an example of one dataset:\n\nbig_lam_datasets[0]\n\nDatasetInfo: {\n    id: biglam/illustrated_ads\n    sha: 688e7d96e99cd5730a17a5c55b0964d27a486904\n    lastModified: 2023-01-18T20:38:15.000Z\n    tags: ['task_categories:image-classification', 'task_ids:multi-class-image-classification', 'annotations_creators:expert-generated', 'size_categories:n&lt;1K', 'license:cc0-1.0', 'lam', 'historic newspapers']\n    private: False\n    author: biglam\n    description: The Dataset contains images derived from the Newspaper Navigator (news-navigator.labs.loc.gov/), a dataset of images drawn from the Library of Congress Chronicling America collection.\n    citation: @dataset{van_strien_daniel_2021_5838410,\n  author       = {van Strien, Daniel},\n  title        = {{19th Century United States Newspaper Advert images \n                   with 'illustrated' or 'non illustrated' labels}},\n  month        = oct,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {0.0.1},\n  doi          = {10.5281/zenodo.5838410},\n  url          = {https://doi.org/10.5281/zenodo.5838410}}\n    cardData: {'annotations_creators': ['expert-generated'], 'language': [], 'language_creators': [], 'license': ['cc0-1.0'], 'multilinguality': [], 'pretty_name': \"19th Century United States Newspaper Advert images with 'illustrated' or 'non illustrated' labels\", 'size_categories': ['n&lt;1K'], 'source_datasets': [], 'tags': ['lam', 'historic newspapers'], 'task_categories': ['image-classification'], 'task_ids': ['multi-class-image-classification']}\n    siblings: []\n    _id: 62b9bb453b3301c319d5b53e\n    disabled: False\n    gated: False\n    gitalyUid: 4a051da032bb27da0bc286b288384bb3362f56546a387b130121cd279db336e1\n    likes: 3\n    downloads: 11\n}\n\n\nWe can see the cardData attribute contains an item containing the tasks supported by a dataset\n\nbig_lam_datasets[0].cardData['task_categories']\n\n['image-classification']\n\n\n\ndef get_task_categories(dataset):\n    try:\n        yield from dataset.cardData['task_categories']\n    except KeyError:\n        return None\n\nWe can use the toolz.frequencies function to get counts of these tasks in our org.\n\ntask_frequencies = toolz.frequencies(\n    toolz.concat(map(get_task_categories, big_lam_datasets))\n)\ntask_frequencies\n\n{'image-classification': 8,\n 'text-classification': 6,\n 'image-to-text': 2,\n 'text-generation': 7,\n 'object-detection': 5,\n 'fill-mask': 2,\n 'text-to-image': 1,\n 'image-to-image': 1,\n 'token-classification': 1}\n\n\nSince we want to organize by task type, letâ€™s grab the names of all the tasks in the BigLAM organization.\n\ntasks = task_frequencies.keys()\ntasks\n\ndict_keys(['image-classification', 'text-classification', 'image-to-text', 'text-generation', 'object-detection', 'fill-mask', 'text-to-image', 'image-to-image', 'token-classification'])\n\n\nWe now want to group together datasets by the task(s) they support. We can use a default dict to create a dictionary where the keys are the task and the values are a list of datasets supporting that task. Note some datasets support multiple tasks so may appear under more than one task key.\n\nfrom collections import defaultdict\n\n\ndatasets_by_task = defaultdict(list)\n\n\nfor dataset in big_lam_datasets:\n    tasks = get_task_categories(dataset)\n    for task in tasks:\n        datasets_by_task[task].append(dataset)\n\nWe now have a dictionary which allows us to get all datasets supporting a task, for example fill-mask\n\ndatasets_by_task[\"fill-mask\"]\n\n[DatasetInfo: {\n    id: biglam/berlin_state_library_ocr\n    sha: a890935d5bd754ddc5b85f56b6f34f6d2bb4abba\n    lastModified: 2022-08-05T09:36:24.000Z\n    tags: ['task_categories:fill-mask', 'task_categories:text-generation', 'task_ids:masked-language-modeling', 'task_ids:language-modeling', 'annotations_creators:machine-generated', 'language_creators:expert-generated', 'multilinguality:multilingual', 'size_categories:1M&lt;n&lt;10M', 'language:de', 'language:nl', 'language:en', 'language:fr', 'language:es', 'license:cc-by-4.0', 'ocr', 'library']\n    private: False\n    author: biglam\n    description: None\n    citation: None\n    cardData: {'annotations_creators': ['machine-generated'], 'language': ['de', 'nl', 'en', 'fr', 'es'], 'language_creators': ['expert-generated'], 'license': ['cc-by-4.0'], 'multilinguality': ['multilingual'], 'pretty_name': 'Berlin State Library OCR', 'size_categories': ['1M&lt;n&lt;10M'], 'source_datasets': [], 'tags': ['ocr', 'library'], 'task_categories': ['fill-mask', 'text-generation'], 'task_ids': ['masked-language-modeling', 'language-modeling']}\n    siblings: []\n    _id: 62e0431281d9ca6484efac31\n    disabled: False\n    gated: False\n    gitalyUid: 3818ba9c8b624d79f1fcfb0c79bd197fb5b3a3f9de2452aed5028e8b6435f56a\n    likes: 3\n    downloads: 5\n },\n DatasetInfo: {\n    id: biglam/bnl_newspapers1841-1879\n    sha: 588db6c242ecae417b92830d5646121c15726fea\n    lastModified: 2022-11-15T09:25:43.000Z\n    tags: ['task_categories:text-generation', 'task_categories:fill-mask', 'task_ids:language-modeling', 'task_ids:masked-language-modeling', 'annotations_creators:no-annotation', 'language_creators:expert-generated', 'multilinguality:multilingual', 'size_categories:100K&lt;n&lt;1M', 'source_datasets:original', 'language:de', 'language:fr', 'language:lb', 'language:nl', 'language:la', 'language:en', 'license:cc0-1.0', 'newspapers', '1800-1900']\n    private: False\n    author: biglam\n    description: None\n    citation: None\n    cardData: {'annotations_creators': ['no-annotation'], 'language': ['de', 'fr', 'lb', 'nl', 'la', 'en'], 'language_creators': ['expert-generated'], 'license': ['cc0-1.0'], 'multilinguality': ['multilingual'], 'pretty_name': 'BnL Newspapers 1841-1879', 'size_categories': ['100K&lt;n&lt;1M'], 'source_datasets': ['original'], 'tags': ['newspapers', '1800-1900'], 'task_categories': ['text-generation', 'fill-mask'], 'task_ids': ['language-modeling', 'masked-language-modeling']}\n    siblings: []\n    _id: 6372286ce8891da06b2a5d2f\n    disabled: False\n    gated: False\n    gitalyUid: 039f217af964cfa1317f03d58c367ba6f0e415721b107a298cd4e75cbad50e8b\n    likes: 2\n    downloads: 3\n }]"
  },
  {
    "objectID": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#how-can-we-create-a-readme-that-dynamically-updates",
    "href": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#how-can-we-create-a-readme-that-dynamically-updates",
    "title": "Dynamically updating a Hugging Face hub organization README",
    "section": "How can we create a README that dynamically updates",
    "text": "How can we create a README that dynamically updates\nWe now have our datasets organized by task. However, at the moment, this is in the form of a Python dictionary. It would be much nicer to render it a more pleasing format. This is where a templating engine can help. In this case weâ€™ll use Jinja.\nA templating engine allows us to create a template which can dynamically be updated based on values we pass in. We wonâ€™t go in depth to templating engines/Jinja in this blog post because Iâ€™m not an expert in templating engines. This Real Python article is a nice introduction to Jinja.\n\nfrom jinja2 import Environment, FileSystemLoader\n\nWe can start by taking a look at our template. Since a lot of the template I created doesnâ€™t update, weâ€™ll use tail to look at the bottom of the template which is dynamically updating.\n\n!tail -n 12 templates/readme.jinja\n\nAn overview of datasets currently made available via BigLam organised by task type.\n\n{% for task_type, datasets in task_dictionary.items() %}\n\n&lt;details&gt;\n  &lt;summary&gt;{{ task_type }}&lt;/summary&gt;\n    {% for dataset in datasets %}\n  - [{{dataset.cardData['pretty_name']}}](https://huggingface.co/datasets/biglam/{{ dataset.id }})\n  {%- endfor %}\n\n&lt;/details&gt;\n{% endfor %}\n\n\nEven if you arenâ€™t familiar with templating engines, you can probably see roughly what this does. We look through all the keys and values in our dictionary, create a section for that task based on the dictionary key. We next loop through the dictionary values (which in this case is a list) and create a link for that dataset. Since weâ€™re looping through DatasetInfo objects in the list we can grab things like the pretty_name for the dataset and dynamically create a URL link.\nWe can load this template as follows\n\nenvironment = Environment(loader=FileSystemLoader(\"templates/\"))\ntemplate = environment.get_template(\"readme.jinja\")\n\nCreate a context dictionary which we use to pass through our dictionary\n\ncontext = {\n    \"task_dictionary\": datasets_by_task,\n}\n\nWe can now render this and see how it looks\n\nprint(template.render(context))\n\n---\ntitle: README\nemoji: ðŸ“š\ncolorFrom: pink\ncolorTo: gray\nsdk: static\npinned: false\n---\n\nBigScience ðŸŒ¸ is an open scientific collaboration of nearly 600 researchers from 50 countries and 250 institutions who collaborate on various projects within the natural language processing (NLP) space to broaden the accessibility of language datasets while working on challenging scientific questions around training language models.\n\n\nBigLAM started as a [datasets hackathon](https://github.com/bigscience-workshop/lam) focused on making data from Libraries, Archives, and Museums (LAMS) with potential machine-learning applications accessible via the Hugging Face Hub.\nWe are continuing to work on making more datasets available via the Hugging Face hub to help make these datasets more discoverable, open them up to new audiences, and help ensure that machine-learning datasets more closely reflect the richness of human culture.\n\n\n## Dataset Overview\n\nAn overview of datasets currently made available via BigLam organised by task type.\n\n\n\n&lt;details&gt;\n  &lt;summary&gt;image-classification&lt;/summary&gt;\n    \n  - [19th Century United States Newspaper Advert images with 'illustrated' or 'non illustrated' labels](https://huggingface.co/datasets/biglam/biglam/illustrated_ads)\n  - [Brill Iconclass AI Test Set ](https://huggingface.co/datasets/biglam/biglam/brill_iconclass)\n  - [National Library of Scotland Chapbook Illustrations](https://huggingface.co/datasets/biglam/biglam/nls_chapbook_illustrations)\n  - [Encyclopaedia Britannica Illustrated](https://huggingface.co/datasets/biglam/biglam/encyclopaedia_britannica_illustrated)\n  - [V4Design Europeana style dataset](https://huggingface.co/datasets/biglam/biglam/v4design_europeana_style_dataset)\n  - [Early Printed Books Font Detection Dataset](https://huggingface.co/datasets/biglam/biglam/early_printed_books_font_detection)\n  - [Dataset of Pages from Early Printed Books with Multiple Font Groups](https://huggingface.co/datasets/biglam/biglam/early_printed_books_with_multiple_font_groups)\n  - [DEArt: Dataset of European Art](https://huggingface.co/datasets/biglam/biglam/european_art)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;text-classification&lt;/summary&gt;\n    \n  - [Annotated dataset to assess the accuracy of the textual description of cultural heritage records](https://huggingface.co/datasets/biglam/biglam/cultural_heritage_metadata_accuracy)\n  - [Atypical Animacy](https://huggingface.co/datasets/biglam/biglam/atypical_animacy)\n  - [Old Bailey Proceedings](https://huggingface.co/datasets/biglam/biglam/old_bailey_proceedings)\n  - [Lampeter Corpus](https://huggingface.co/datasets/biglam/biglam/lampeter_corpus)\n  - [Hansard Speeches](https://huggingface.co/datasets/biglam/biglam/hansard_speech)\n  - [Contentious Contexts Corpus](https://huggingface.co/datasets/biglam/biglam/contentious_contexts)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;image-to-text&lt;/summary&gt;\n    \n  - [Brill Iconclass AI Test Set ](https://huggingface.co/datasets/biglam/biglam/brill_iconclass)\n  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;text-generation&lt;/summary&gt;\n    \n  - [Old Bailey Proceedings](https://huggingface.co/datasets/biglam/biglam/old_bailey_proceedings)\n  - [Hansard Speeches](https://huggingface.co/datasets/biglam/biglam/hansard_speech)\n  - [Berlin State Library OCR](https://huggingface.co/datasets/biglam/biglam/berlin_state_library_ocr)\n  - [Literary fictions of Gallica](https://huggingface.co/datasets/biglam/biglam/gallica_literary_fictions)\n  - [Europeana Newspapers ](https://huggingface.co/datasets/biglam/biglam/europeana_newspapers)\n  - [Gutenberg Poetry Corpus](https://huggingface.co/datasets/biglam/biglam/gutenberg-poetry-corpus)\n  - [BnL Newspapers 1841-1879](https://huggingface.co/datasets/biglam/biglam/bnl_newspapers1841-1879)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;object-detection&lt;/summary&gt;\n    \n  - [National Library of Scotland Chapbook Illustrations](https://huggingface.co/datasets/biglam/biglam/nls_chapbook_illustrations)\n  - [YALTAi Tabular Dataset](https://huggingface.co/datasets/biglam/biglam/yalta_ai_tabular_dataset)\n  - [YALTAi Tabular Dataset](https://huggingface.co/datasets/biglam/biglam/yalta_ai_segmonto_manuscript_dataset)\n  - [Beyond Words](https://huggingface.co/datasets/biglam/biglam/loc_beyond_words)\n  - [DEArt: Dataset of European Art](https://huggingface.co/datasets/biglam/biglam/european_art)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;fill-mask&lt;/summary&gt;\n    \n  - [Berlin State Library OCR](https://huggingface.co/datasets/biglam/biglam/berlin_state_library_ocr)\n  - [BnL Newspapers 1841-1879](https://huggingface.co/datasets/biglam/biglam/bnl_newspapers1841-1879)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;text-to-image&lt;/summary&gt;\n    \n  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;image-to-image&lt;/summary&gt;\n    \n  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;token-classification&lt;/summary&gt;\n    \n  - [Unsilencing Colonial Archives via Automated Entity Recognition](https://huggingface.co/datasets/biglam/biglam/unsilence_voc)\n\n&lt;/details&gt;\n\n\n\n\nwith open('/tmp/README.md','w') as f:\n    f.write(template.render(context))"
  },
  {
    "objectID": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#updating-the-readme-on-the-hugging-face-hub",
    "href": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#updating-the-readme-on-the-hugging-face-hub",
    "title": "Dynamically updating a Hugging Face hub organization README",
    "section": "Updating the README on the Hugging Face Hub",
    "text": "Updating the README on the Hugging Face Hub\nThis looks pretty good! It would be nice to also update the org README without having to manually edit the file. The huggingface_hub library helps us out here once again. Since the organization README is actually a special type of Hugging Face Space, we can interact with it in the same way we could for models or datasets.\n\nfrom huggingface_hub import HfApi\nfrom huggingface_hub import notebook_login\n\nWeâ€™ll create a HFApi instance.\n\napi = HfApi()\n\nSince weâ€™re planning to write to a repo weâ€™ll need to login to the hub.\n\nnotebook_login()\n\n\n\n\nWe can now upload the rendered README file we created above to our biglam/README space.\n\napi.upload_file(\n    path_or_fileobj=\"/tmp/readme.md\",\n    path_in_repo=\"README.md\",\n    repo_id=\"biglam/README\",\n    repo_type=\"space\",\n)\n\n'https://huggingface.co/spaces/biglam/README/blob/main/README.md'\n\n\nIf we look at our updated README, weâ€™ll see we now have some nice collapsible sections for each task type containing the datasets for that task\n\n\n\nAfter README\n\n\nNext steps, whilst this was already quite useful, at the moment we still have to run this code when we want to regenerate our README. Webhooks make it possible to make this fully automated by creating a webhook that monitors any changes to repos under the BigLAM org. Would love to hear from anyone who tries this out!"
  },
  {
    "objectID": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html",
    "href": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html",
    "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
    "section": "",
    "text": "The Hugging Face Hub has become the de facto place to share machine learning models and datasets. As the number of models and datasets grow the challenge of finding the right model or dataset for your needs may become more challenging. There are various ways in which we can try and make it easier for people to find relevant models and datasets. One of these is by associating metadata with datasets and models. This blog post will (very briefly) begin to explore metadata on the Hugging Face Hub. Often youâ€™ll want to explore models and datasets via the Hub website but this isnâ€™t the only way to explore the Hub. As part of the process of exploring metadata on the Hugging Face Hub weâ€™ll briefly look at how we can use the huggingface_hub library to programmatically interact with the Hub."
  },
  {
    "objectID": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#library-imports",
    "href": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#library-imports",
    "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
    "section": "Library imports",
    "text": "Library imports\nFor this post weâ€™ll need a few libraries, pandas, requests and matplotlib are likely old friends (or foesâ€¦). The huggingface_hub library might be new to you but will soon become a good friend too! The rich library is fantastically useful for quickly getting familiar with a library (i.e.Â avoiding reading all the docs!) so weâ€™ll import that too.\n\nimport requests\nfrom huggingface_hub import hf_api\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport rich\n\n\n%matplotlib inline\nplt.style.use(\"ggplot\")\n\nWeâ€™ll instantiate an instance of the HfApi class.\n\napi = hf_api.HfApi()\n\nWe can use rich inspect to get a better sense of what a function or class instance is all about. Letâ€™s see what methods the api has.\n\nrich.inspect(api, methods=True)\n\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ &lt;class 'huggingface_hub.hf_api.HfApi'&gt; â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚\nâ”‚ â”‚ &lt;huggingface_hub.hf_api.HfApi object at 0x136a2ce80&gt;                                                        â”‚ â”‚\nâ”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚\nâ”‚                                                                                                                 â”‚\nâ”‚                 endpoint = 'https://huggingface.co'                                                             â”‚\nâ”‚                    token = None                                                                                 â”‚\nâ”‚ change_discussion_status = def change_discussion_status(repo_id: str, discussion_num: int, new_status:          â”‚\nâ”‚                            Literal['open', 'closed'], *, token: Optional[str] = None, comment: Optional[str] =  â”‚\nâ”‚                            None, repo_type: Optional[str] = None) -&gt;                                            â”‚\nâ”‚                            huggingface_hub.community.DiscussionStatusChange: Closes or re-opens a Discussion or â”‚\nâ”‚                            Pull Request.                                                                        â”‚\nâ”‚       comment_discussion = def comment_discussion(repo_id: str, discussion_num: int, comment: str, *, token:    â”‚\nâ”‚                            Optional[str] = None, repo_type: Optional[str] = None) -&gt;                            â”‚\nâ”‚                            huggingface_hub.community.DiscussionComment: Creates a new comment on the given      â”‚\nâ”‚                            Discussion.                                                                          â”‚\nâ”‚            create_branch = def create_branch(repo_id: str, *, branch: str, token: Optional[str] = None,         â”‚\nâ”‚                            repo_type: Optional[str] = None) -&gt; None: Create a new branch from `main` on a repo  â”‚\nâ”‚                            on the Hub.                                                                          â”‚\nâ”‚            create_commit = def create_commit(repo_id: str, operations:                                          â”‚\nâ”‚                            Iterable[Union[huggingface_hub._commit_api.CommitOperationAdd,                       â”‚\nâ”‚                            huggingface_hub._commit_api.CommitOperationDelete]], *, commit_message: str,         â”‚\nâ”‚                            commit_description: Optional[str] = None, token: Optional[str] = None, repo_type:    â”‚\nâ”‚                            Optional[str] = None, revision: Optional[str] = None, create_pr: Optional[bool] =    â”‚\nâ”‚                            None, num_threads: int = 5, parent_commit: Optional[str] = None) -&gt;                  â”‚\nâ”‚                            huggingface_hub.hf_api.CommitInfo: Creates a commit in the given repo, deleting &    â”‚\nâ”‚                            uploading files as needed.                                                           â”‚\nâ”‚        create_discussion = def create_discussion(repo_id: str, title: str, *, token: Optional[str] = None,      â”‚\nâ”‚                            description: Optional[str] = None, repo_type: Optional[str] = None, pull_request:    â”‚\nâ”‚                            bool = False) -&gt; huggingface_hub.community.DiscussionWithDetails: Creates a          â”‚\nâ”‚                            Discussion or Pull Request.                                                          â”‚\nâ”‚      create_pull_request = def create_pull_request(repo_id: str, title: str, *, token: Optional[str] = None,    â”‚\nâ”‚                            description: Optional[str] = None, repo_type: Optional[str] = None) -&gt;               â”‚\nâ”‚                            huggingface_hub.community.DiscussionWithDetails: Creates a Pull Request . Pull       â”‚\nâ”‚                            Requests created programmatically will be in `\"draft\"` status.                       â”‚\nâ”‚              create_repo = def create_repo(repo_id: str, *, token: Optional[str] = None, private: bool = False, â”‚\nâ”‚                            repo_type: Optional[str] = None, exist_ok: bool = False, space_sdk: Optional[str] =  â”‚\nâ”‚                            None) -&gt; str: Create an empty repo on the HuggingFace Hub.                           â”‚\nâ”‚               create_tag = def create_tag(repo_id: str, *, tag: str, tag_message: Optional[str] = None,         â”‚\nâ”‚                            revision: Optional[str] = None, token: Optional[str] = None, repo_type:              â”‚\nâ”‚                            Optional[str] = None) -&gt; None: Tag a given commit of a repo on the Hub.              â”‚\nâ”‚             dataset_info = def dataset_info(repo_id: str, *, revision: Optional[str] = None, timeout:           â”‚\nâ”‚                            Optional[float] = None, files_metadata: bool = False, token: Union[bool, str,        â”‚\nâ”‚                            NoneType] = None) -&gt; huggingface_hub.hf_api.DatasetInfo: Get info on one specific    â”‚\nâ”‚                            dataset on huggingface.co.                                                           â”‚\nâ”‚            delete_branch = def delete_branch(repo_id: str, *, branch: str, token: Optional[str] = None,         â”‚\nâ”‚                            repo_type: Optional[str] = None) -&gt; None: Delete a branch from a repo on the Hub.    â”‚\nâ”‚              delete_file = def delete_file(path_in_repo: str, repo_id: str, *, token: Optional[str] = None,     â”‚\nâ”‚                            repo_type: Optional[str] = None, revision: Optional[str] = None, commit_message:     â”‚\nâ”‚                            Optional[str] = None, commit_description: Optional[str] = None, create_pr:           â”‚\nâ”‚                            Optional[bool] = None, parent_commit: Optional[str] = None) -&gt;                       â”‚\nâ”‚                            huggingface_hub.hf_api.CommitInfo: Deletes a file in the given repo.                 â”‚\nâ”‚            delete_folder = def delete_folder(path_in_repo: str, repo_id: str, *, token: Optional[str] = None,   â”‚\nâ”‚                            repo_type: Optional[str] = None, revision: Optional[str] = None, commit_message:     â”‚\nâ”‚                            Optional[str] = None, commit_description: Optional[str] = None, create_pr:           â”‚\nâ”‚                            Optional[bool] = None, parent_commit: Optional[str] = None) -&gt;                       â”‚\nâ”‚                            huggingface_hub.hf_api.CommitInfo: Deletes a folder in the given repo.               â”‚\nâ”‚              delete_repo = def delete_repo(repo_id: str, *, token: Optional[str] = None, repo_type:             â”‚\nâ”‚                            Optional[str] = None): Delete a repo from the HuggingFace Hub. CAUTION: this is      â”‚\nâ”‚                            irreversible.                                                                        â”‚\nâ”‚               delete_tag = def delete_tag(repo_id: str, *, tag: str, token: Optional[str] = None, repo_type:    â”‚\nâ”‚                            Optional[str] = None) -&gt; None: Delete a tag from a repo on the Hub.                  â”‚\nâ”‚  edit_discussion_comment = def edit_discussion_comment(repo_id: str, discussion_num: int, comment_id: str,      â”‚\nâ”‚                            new_content: str, *, token: Optional[str] = None, repo_type: Optional[str] = None)   â”‚\nâ”‚                            -&gt; huggingface_hub.community.DiscussionComment: Edits a comment on a Discussion /    â”‚\nâ”‚                            Pull Request.                                                                        â”‚\nâ”‚         get_dataset_tags = def get_dataset_tags() -&gt; huggingface_hub.utils.endpoint_helpers.DatasetTags: Gets   â”‚\nâ”‚                            all valid dataset tags as a nested namespace object.                                 â”‚\nâ”‚   get_discussion_details = def get_discussion_details(repo_id: str, discussion_num: int, *, repo_type:          â”‚\nâ”‚                            Optional[str] = None, token: Optional[str] = None) -&gt;                                â”‚\nâ”‚                            huggingface_hub.community.DiscussionWithDetails: Fetches a Discussion's / Pull       â”‚\nâ”‚                            Request 's details from the Hub.                                                     â”‚\nâ”‚       get_full_repo_name = def get_full_repo_name(model_id: str, *, organization: Optional[str] = None, token:  â”‚\nâ”‚                            Union[bool, str, NoneType] = None):                                                  â”‚\nâ”‚                            Returns the repository name for a given model ID and optional                        â”‚\nâ”‚                            organization.                                                                        â”‚\nâ”‚           get_model_tags = def get_model_tags() -&gt; huggingface_hub.utils.endpoint_helpers.ModelTags: Gets all   â”‚\nâ”‚                            valid model tags as a nested namespace object                                        â”‚\nâ”‚     get_repo_discussions = def get_repo_discussions(repo_id: str, *, repo_type: Optional[str] = None, token:    â”‚\nâ”‚                            Optional[str] = None) -&gt; Iterator[huggingface_hub.community.Discussion]: Fetches     â”‚\nâ”‚                            Discussions and Pull Requests for the given repo.                                    â”‚\nâ”‚  hide_discussion_comment = def hide_discussion_comment(repo_id: str, discussion_num: int, comment_id: str, *,   â”‚\nâ”‚                            token: Optional[str] = None, repo_type: Optional[str] = None) -&gt;                     â”‚\nâ”‚                            huggingface_hub.community.DiscussionComment: Hides a comment on a Discussion / Pull  â”‚\nâ”‚                            Request.                                                                             â”‚\nâ”‚            list_datasets = def list_datasets(*, filter:                                                         â”‚\nâ”‚                            Union[huggingface_hub.utils.endpoint_helpers.DatasetFilter, str, Iterable[str],      â”‚\nâ”‚                            NoneType] = None, author: Optional[str] = None, search: Optional[str] = None, sort:  â”‚\nâ”‚                            Union[Literal['lastModified'], str, NoneType] = None, direction:                     â”‚\nâ”‚                            Optional[Literal[-1]] = None, limit: Optional[int] = None, cardData: Optional[bool]  â”‚\nâ”‚                            = None, full: Optional[bool] = None, token: Optional[str] = None) -&gt;                 â”‚\nâ”‚                            List[huggingface_hub.hf_api.DatasetInfo]: Get the list of all the datasets on        â”‚\nâ”‚                            huggingface.co                                                                       â”‚\nâ”‚             list_metrics = def list_metrics() -&gt; List[huggingface_hub.hf_api.MetricInfo]: Get the public list   â”‚\nâ”‚                            of all the metrics on huggingface.co                                                 â”‚\nâ”‚              list_models = def list_models(*, filter: Union[huggingface_hub.utils.endpoint_helpers.ModelFilter, â”‚\nâ”‚                            str, Iterable[str], NoneType] = None, author: Optional[str] = None, search:          â”‚\nâ”‚                            Optional[str] = None, emissions_thresholds: Optional[Tuple[float, float]] = None,    â”‚\nâ”‚                            sort: Union[Literal['lastModified'], str, NoneType] = None, direction:               â”‚\nâ”‚                            Optional[Literal[-1]] = None, limit: Optional[int] = None, full: Optional[bool] =    â”‚\nâ”‚                            None, cardData: bool = False, fetch_config: bool = False, token: Union[bool, str,    â”‚\nâ”‚                            NoneType] = None) -&gt; List[huggingface_hub.hf_api.ModelInfo]: Get the list of all the â”‚\nâ”‚                            models on huggingface.co                                                             â”‚\nâ”‚          list_repo_files = def list_repo_files(repo_id: str, *, revision: Optional[str] = None, repo_type:      â”‚\nâ”‚                            Optional[str] = None, timeout: Optional[float] = None, token: Union[bool, str,       â”‚\nâ”‚                            NoneType] = None) -&gt; List[str]: Get the list of files in a given repo.               â”‚\nâ”‚              list_spaces = def list_spaces(*, filter: Union[str, Iterable[str], NoneType] = None, author:       â”‚\nâ”‚                            Optional[str] = None, search: Optional[str] = None, sort:                            â”‚\nâ”‚                            Union[Literal['lastModified'], str, NoneType] = None, direction:                     â”‚\nâ”‚                            Optional[Literal[-1]] = None, limit: Optional[int] = None, datasets: Union[str,      â”‚\nâ”‚                            Iterable[str], NoneType] = None, models: Union[str, Iterable[str], NoneType] = None, â”‚\nâ”‚                            linked: bool = False, full: Optional[bool] = None, token: Optional[str] = None) -&gt;   â”‚\nâ”‚                            List[huggingface_hub.hf_api.SpaceInfo]: Get the public list of all Spaces on         â”‚\nâ”‚                            huggingface.co                                                                       â”‚\nâ”‚       merge_pull_request = def merge_pull_request(repo_id: str, discussion_num: int, *, token: Optional[str] =  â”‚\nâ”‚                            None, comment: Optional[str] = None, repo_type: Optional[str] = None): Merges a Pull â”‚\nâ”‚                            Request.                                                                             â”‚\nâ”‚               model_info = def model_info(repo_id: str, *, revision: Optional[str] = None, timeout:             â”‚\nâ”‚                            Optional[float] = None, securityStatus: Optional[bool] = None, files_metadata: bool  â”‚\nâ”‚                            = False, token: Union[bool, str, NoneType] = None) -&gt;                                â”‚\nâ”‚                            huggingface_hub.hf_api.ModelInfo: Get info on one specific model on huggingface.co   â”‚\nâ”‚                move_repo = def move_repo(from_id: str, to_id: str, *, repo_type: Optional[str] = None, token:   â”‚\nâ”‚                            Optional[str] = None): Moving a repository from namespace1/repo_name1 to             â”‚\nâ”‚                            namespace2/repo_name2                                                                â”‚\nâ”‚        rename_discussion = def rename_discussion(repo_id: str, discussion_num: int, new_title: str, *, token:   â”‚\nâ”‚                            Optional[str] = None, repo_type: Optional[str] = None) -&gt;                            â”‚\nâ”‚                            huggingface_hub.community.DiscussionTitleChange: Renames a Discussion.               â”‚\nâ”‚                repo_info = def repo_info(repo_id: str, *, revision: Optional[str] = None, repo_type:            â”‚\nâ”‚                            Optional[str] = None, timeout: Optional[float] = None, files_metadata: bool = False, â”‚\nâ”‚                            token: Union[bool, str, NoneType] = None) -&gt; Union[huggingface_hub.hf_api.ModelInfo, â”‚\nâ”‚                            huggingface_hub.hf_api.DatasetInfo, huggingface_hub.hf_api.SpaceInfo]: Get the info  â”‚\nâ”‚                            object for a given repo of a given type.                                             â”‚\nâ”‚         set_access_token = def set_access_token(access_token: str):                                             â”‚\nâ”‚                            Saves the passed access token so git can correctly authenticate the                  â”‚\nâ”‚                            user.                                                                                â”‚\nâ”‚               space_info = def space_info(repo_id: str, *, revision: Optional[str] = None, timeout:             â”‚\nâ”‚                            Optional[float] = None, files_metadata: bool = False, token: Union[bool, str,        â”‚\nâ”‚                            NoneType] = None) -&gt; huggingface_hub.hf_api.SpaceInfo: Get info on one specific      â”‚\nâ”‚                            Space on huggingface.co.                                                             â”‚\nâ”‚       unset_access_token = def unset_access_token(): Resets the user's access token.                            â”‚\nâ”‚   update_repo_visibility = def update_repo_visibility(repo_id: str, private: bool = False, *, token:            â”‚\nâ”‚                            Optional[str] = None, organization: Optional[str] = None, repo_type: Optional[str] = â”‚\nâ”‚                            None, name: Optional[str] = None) -&gt; Dict[str, bool]: Update the visibility setting  â”‚\nâ”‚                            of a repository.                                                                     â”‚\nâ”‚              upload_file = def upload_file(*, path_or_fileobj: Union[str, bytes, BinaryIO], path_in_repo: str,  â”‚\nâ”‚                            repo_id: str, token: Optional[str] = None, repo_type: Optional[str] = None,          â”‚\nâ”‚                            revision: Optional[str] = None, commit_message: Optional[str] = None,                â”‚\nâ”‚                            commit_description: Optional[str] = None, create_pr: Optional[bool] = None,          â”‚\nâ”‚                            parent_commit: Optional[str] = None) -&gt; str:                                         â”‚\nâ”‚                            Upload a local file (up to 50 GB) to the given repo. The upload is done              â”‚\nâ”‚                            through a HTTP post request, and doesn't require git or git-lfs to be                â”‚\nâ”‚                            installed.                                                                           â”‚\nâ”‚            upload_folder = def upload_folder(*, repo_id: str, folder_path: Union[str, pathlib.Path],            â”‚\nâ”‚                            path_in_repo: Optional[str] = None, commit_message: Optional[str] = None,            â”‚\nâ”‚                            commit_description: Optional[str] = None, token: Optional[str] = None, repo_type:    â”‚\nâ”‚                            Optional[str] = None, revision: Optional[str] = None, create_pr: Optional[bool] =    â”‚\nâ”‚                            None, parent_commit: Optional[str] = None, allow_patterns: Union[List[str], str,     â”‚\nâ”‚                            NoneType] = None, ignore_patterns: Union[List[str], str, NoneType] = None):          â”‚\nâ”‚                            Upload a local folder to the given repo. The upload is done                          â”‚\nâ”‚                            through a HTTP requests, and doesn't require git or git-lfs to be                    â”‚\nâ”‚                            installed.                                                                           â”‚\nâ”‚                   whoami = def whoami(token: Optional[str] = None) -&gt; Dict: Call HF API to know \"whoami\".       â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\n\n\nYouâ€™ll see from looking through this there is a bunch of different things we can now do programmatically via the hub. For this post weâ€™re interested in the list_datasets and list_models methods. If we look at one of these we can see it has a bunch of different options we can use when listing datasets or models.\n\nrich.inspect(api.list_models)\n\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ &lt;bound method HfApi.list_models of &lt;huggingface_hub.hf_api.HfApi object at 0x136a2ce80&gt;&gt; â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ def HfApi.list_models(*, filter: Union[huggingface_hub.utils.endpoint_helpers.ModelFilter, str, Iterable[str],  â”‚\nâ”‚ NoneType] = None, author: Optional[str] = None, search: Optional[str] = None, emissions_thresholds:             â”‚\nâ”‚ Optional[Tuple[float, float]] = None, sort: Union[Literal['lastModified'], str, NoneType] = None, direction:    â”‚\nâ”‚ Optional[Literal[-1]] = None, limit: Optional[int] = None, full: Optional[bool] = None, cardData: bool = False, â”‚\nâ”‚ fetch_config: bool = False, token: Union[bool, str, NoneType] = None) -&gt;                                        â”‚\nâ”‚ List[huggingface_hub.hf_api.ModelInfo]:                                                                         â”‚\nâ”‚                                                                                                                 â”‚\nâ”‚ Get the list of all the models on huggingface.co                                                                â”‚\nâ”‚                                                                                                                 â”‚\nâ”‚ 28 attribute(s) not shown. Run inspect(inspect) for options.                                                    â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\n\n\nFor our use case we want everything, so we set limit=None, we donâ€™t want any filters so we set this to None (this is the default behaviour, but we set them explicitly here to make it clearer for our future selves). We also set full=True so we get back more verbose information about our dataset and models. We also wrap the result in iter and list since the behaviour of these methods will change in future versions to support paging.\n\nhub_datasets = list(iter(api.list_datasets(limit=None, filter=None, full=True)))\n\n\nhub_models = list(iter(api.list_models(limit=None, filter=None, full=True)))\n\nLetâ€™s peek at an example of what we get back\n\nhub_models[0]\n\nModelInfo: {\n    modelId: albert-base-v1\n    sha: aeffd769076a5c4f83b2546aea99ca45a15a5da4\n    lastModified: 2021-01-13T15:08:24.000Z\n    tags: ['pytorch', 'tf', 'albert', 'fill-mask', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1909.11942', 'transformers', 'exbert', 'license:apache-2.0', 'autotrain_compatible', 'has_space']\n    pipeline_tag: fill-mask\n    siblings: [RepoFile(rfilename='.gitattributes', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='README.md', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='config.json', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='pytorch_model.bin', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='spiece.model', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='tf_model.h5', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='tokenizer.json', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='with-prefix-tf_model.h5', size='None', blob_id='None', lfs='None')]\n    private: False\n    author: None\n    config: None\n    securityStatus: None\n    _id: 621ffdc036468d709f174328\n    id: albert-base-v1\n    gitalyUid: 4f35551ea371da7a8762caab54319a54ade836044f0ca7690d21e86b159867eb\n    likes: 1\n    downloads: 75182\n    library_name: transformers\n}\n\n\n\nhub_datasets[0]\n\nDatasetInfo: {\n    id: acronym_identification\n    sha: 173af1516c409eb4596bc63a69626bdb5584c40c\n    lastModified: 2022-11-18T17:25:49.000Z\n    tags: ['task_categories:token-classification', 'annotations_creators:expert-generated', 'language_creators:found', 'multilinguality:monolingual', 'size_categories:10K&lt;n&lt;100K', 'source_datasets:original', 'language:en', 'license:mit', 'acronym-identification', 'arxiv:2010.14678']\n    private: False\n    author: None\n    description: Acronym identification training and development sets for the acronym identification task at SDU@AAAI-21.\n    citation: @inproceedings{veyseh-et-al-2020-what,\n   title={{What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation}},\n   author={Amir Pouran Ben Veyseh and Franck Dernoncourt and Quan Hung Tran and Thien Huu Nguyen},\n   year={2020},\n   booktitle={Proceedings of COLING},\n   link={https://arxiv.org/pdf/2010.14678v1.pdf}\n}\n    cardData: {'annotations_creators': ['expert-generated'], 'language_creators': ['found'], 'language': ['en'], 'license': ['mit'], 'multilinguality': ['monolingual'], 'size_categories': ['10K&lt;n&lt;100K'], 'source_datasets': ['original'], 'task_categories': ['token-classification'], 'task_ids': [], 'paperswithcode_id': 'acronym-identification', 'pretty_name': 'Acronym Identification Dataset', 'train-eval-index': [{'config': 'default', 'task': 'token-classification', 'task_id': 'entity_extraction', 'splits': {'eval_split': 'test'}, 'col_mapping': {'tokens': 'tokens', 'labels': 'tags'}}], 'tags': ['acronym-identification'], 'dataset_info': {'features': [{'name': 'id', 'dtype': 'string'}, {'name': 'tokens', 'sequence': 'string'}, {'name': 'labels', 'sequence': {'class_label': {'names': {'0': 'B-long', '1': 'B-short', '2': 'I-long', '3': 'I-short', '4': 'O'}}}}], 'splits': [{'name': 'train', 'num_bytes': 7792803, 'num_examples': 14006}, {'name': 'validation', 'num_bytes': 952705, 'num_examples': 1717}, {'name': 'test', 'num_bytes': 987728, 'num_examples': 1750}], 'download_size': 8556464, 'dataset_size': 9733236}}\n    siblings: []\n    _id: 621ffdd236468d709f181d58\n    disabled: False\n    gated: False\n    gitalyUid: 6570517623fa521aa189178e7c7e73d9d88c01b295204edef97f389a15eae144\n    likes: 9\n    downloads: 6074\n    paperswithcode_id: acronym-identification\n}\n\n\nSince we want both models and datasets we create a dictionary which stores the types of item i.e.Â is it a dataset or a model.\n\nhub_data = {\"model\": hub_models, \"dataset\": hub_datasets}\n\nWeâ€™ll be putting our data inside a pandas DataFrame, so weâ€™ll grab the .__dict__ attribute for each hub item, so itâ€™s more pandas friendly.\n\nhub_item_dict = []\nfor hub_type, hub_item in hub_data.items():\n    for item in hub_item:\n        data = item.__dict__\n        data[\"type\"] = hub_type\n        hub_item_dict.append(data)\n\n\ndf = pd.DataFrame.from_dict(hub_item_dict)\n\nHow many hub items do we have?\n\nlen(df)\n\n151343\n\n\nWhat info do we have?\n\ndf.columns\n\nIndex(['modelId', 'sha', 'lastModified', 'tags', 'pipeline_tag', 'siblings',\n       'private', 'author', 'config', 'securityStatus', '_id', 'id',\n       'gitalyUid', 'likes', 'downloads', 'library_name', 'type',\n       'description', 'citation', 'cardData', 'disabled', 'gated',\n       'paperswithcode_id'],\n      dtype='object')"
  },
  {
    "objectID": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#tags",
    "href": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#tags",
    "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
    "section": "Tags",
    "text": "Tags\nModels and datasets have a bunch of metadata i.e.Â last modified and number of downloads. Weâ€™ll focus on tags here. Letâ€™s start by looking at a single example.\n\ndf.loc[30, \"tags\"]\n\n['pytorch',\n 'tf',\n 'rust',\n 'safetensors',\n 'distilbert',\n 'text-classification',\n 'en',\n 'dataset:sst2',\n 'dataset:glue',\n 'doi:10.57967/hf/0181',\n 'transformers',\n 'license:apache-2.0',\n 'model-index',\n 'has_space']\n\n\nWe can see that tags capture can relate to tasks i.e.Â text-classification, libraries supported i.e.Â tf, or the licence associated with a model or dataset. As a starting point for exploring tags we can take a look at how many tags models and datasets have. Weâ€™ll add a new column to capture this number.\n\ndef calculate_number_of_tags(tags: [str]) -&gt; int:\n    return len(tags)\n\n\ndf[\"number_of_tags\"] = df[\"tags\"].apply(lambda x: calculate_number_of_tags(x))\n\nWe can now use describe to see the breakdown of this number.\n\ndf.number_of_tags.describe()\n\ncount    151343.000000\nmean          3.855566\nstd           6.878613\nmin           0.000000\n25%           0.000000\n50%           4.000000\n75%           6.000000\nmax         650.000000\nName: number_of_tags, dtype: float64\n\n\nWe can see that we have quite a range of tag numbers ranging from 0 to 650! If your brain works anything like mine you probably want to know what this high value is about!\n\ndf[df.number_of_tags &gt; 640][[\"id\", \"tags\"]]\n\n\n\n\n\n\n\n\nid\ntags\n\n\n\n\n136372\nbible-nlp/biblenlp-corpus\n[task_categories:translation, annotations_crea...\n\n\n\n\n\n\n\n\ndf[df.number_of_tags &gt; 640][\"tags\"].tolist()\n\n[['task_categories:translation',\n  'annotations_creators:no-annotation',\n  'language_creators:expert-generated',\n  'multilinguality:translation',\n  'multilinguality:multilingual',\n  'size_categories:1M&lt;n&lt;10M',\n  'source_datasets:original',\n  'language:aau',\n  'language:aaz',\n  'language:abx',\n  'language:aby',\n  'language:acf',\n  'language:acu',\n  'language:adz',\n  'language:aey',\n  'language:agd',\n  'language:agg',\n  'language:agm',\n  'language:agn',\n  'language:agr',\n  'language:agu',\n  'language:aia',\n  'language:ake',\n  'language:alp',\n  'language:alq',\n  'language:als',\n  'language:aly',\n  'language:ame',\n  'language:amk',\n  'language:amp',\n  'language:amr',\n  'language:amu',\n  'language:anh',\n  'language:anv',\n  'language:aoi',\n  'language:aoj',\n  'language:apb',\n  'language:apn',\n  'language:apu',\n  'language:apy',\n  'language:arb',\n  'language:arl',\n  'language:arn',\n  'language:arp',\n  'language:aso',\n  'language:ata',\n  'language:atb',\n  'language:atd',\n  'language:atg',\n  'language:auc',\n  'language:aui',\n  'language:auy',\n  'language:avt',\n  'language:awb',\n  'language:awk',\n  'language:awx',\n  'language:azg',\n  'language:azz',\n  'language:bao',\n  'language:bbb',\n  'language:bbr',\n  'language:bch',\n  'language:bco',\n  'language:bdd',\n  'language:bea',\n  'language:bel',\n  'language:bgs',\n  'language:bgt',\n  'language:bhg',\n  'language:bhl',\n  'language:big',\n  'language:bjr',\n  'language:bjv',\n  'language:bkd',\n  'language:bki',\n  'language:bkq',\n  'language:bkx',\n  'language:bla',\n  'language:blw',\n  'language:blz',\n  'language:bmh',\n  'language:bmk',\n  'language:bmr',\n  'language:bnp',\n  'language:boa',\n  'language:boj',\n  'language:bon',\n  'language:box',\n  'language:bqc',\n  'language:bre',\n  'language:bsn',\n  'language:bsp',\n  'language:bss',\n  'language:buk',\n  'language:bus',\n  'language:bvr',\n  'language:bxh',\n  'language:byx',\n  'language:bzd',\n  'language:bzj',\n  'language:cab',\n  'language:caf',\n  'language:cao',\n  'language:cap',\n  'language:car',\n  'language:cav',\n  'language:cax',\n  'language:cbc',\n  'language:cbi',\n  'language:cbk',\n  'language:cbr',\n  'language:cbs',\n  'language:cbt',\n  'language:cbu',\n  'language:cbv',\n  'language:cco',\n  'language:ces',\n  'language:cgc',\n  'language:cha',\n  'language:chd',\n  'language:chf',\n  'language:chk',\n  'language:chq',\n  'language:chz',\n  'language:cjo',\n  'language:cjv',\n  'language:cle',\n  'language:clu',\n  'language:cme',\n  'language:cmn',\n  'language:cni',\n  'language:cnl',\n  'language:cnt',\n  'language:cof',\n  'language:con',\n  'language:cop',\n  'language:cot',\n  'language:cpa',\n  'language:cpb',\n  'language:cpc',\n  'language:cpu',\n  'language:crn',\n  'language:crx',\n  'language:cso',\n  'language:cta',\n  'language:ctp',\n  'language:ctu',\n  'language:cub',\n  'language:cuc',\n  'language:cui',\n  'language:cut',\n  'language:cux',\n  'language:cwe',\n  'language:daa',\n  'language:dad',\n  'language:dah',\n  'language:ded',\n  'language:deu',\n  'language:dgr',\n  'language:dgz',\n  'language:dif',\n  'language:dik',\n  'language:dji',\n  'language:djk',\n  'language:dob',\n  'language:dwr',\n  'language:dww',\n  'language:dwy',\n  'language:eko',\n  'language:emi',\n  'language:emp',\n  'language:eng',\n  'language:epo',\n  'language:eri',\n  'language:ese',\n  'language:etr',\n  'language:faa',\n  'language:fai',\n  'language:far',\n  'language:for',\n  'language:fra',\n  'language:fuf',\n  'language:gai',\n  'language:gam',\n  'language:gaw',\n  'language:gdn',\n  'language:gdr',\n  'language:geb',\n  'language:gfk',\n  'language:ghs',\n  'language:gia',\n  'language:glk',\n  'language:gmv',\n  'language:gng',\n  'language:gnn',\n  'language:gnw',\n  'language:gof',\n  'language:grc',\n  'language:gub',\n  'language:guh',\n  'language:gui',\n  'language:gul',\n  'language:gum',\n  'language:guo',\n  'language:gvc',\n  'language:gvf',\n  'language:gwi',\n  'language:gym',\n  'language:gyr',\n  'language:hat',\n  'language:haw',\n  'language:hbo',\n  'language:hch',\n  'language:heb',\n  'language:heg',\n  'language:hix',\n  'language:hla',\n  'language:hlt',\n  'language:hns',\n  'language:hop',\n  'language:hrv',\n  'language:hub',\n  'language:hui',\n  'language:hus',\n  'language:huu',\n  'language:huv',\n  'language:hvn',\n  'language:ign',\n  'language:ikk',\n  'language:ikw',\n  'language:imo',\n  'language:inb',\n  'language:ind',\n  'language:ino',\n  'language:iou',\n  'language:ipi',\n  'language:ita',\n  'language:jac',\n  'language:jao',\n  'language:jic',\n  'language:jiv',\n  'language:jpn',\n  'language:jvn',\n  'language:kaq',\n  'language:kbc',\n  'language:kbh',\n  'language:kbm',\n  'language:kdc',\n  'language:kde',\n  'language:kdl',\n  'language:kek',\n  'language:ken',\n  'language:kew',\n  'language:kgk',\n  'language:kgp',\n  'language:khs',\n  'language:kje',\n  'language:kjs',\n  'language:kkc',\n  'language:kky',\n  'language:klt',\n  'language:klv',\n  'language:kms',\n  'language:kmu',\n  'language:kne',\n  'language:knf',\n  'language:knj',\n  'language:kos',\n  'language:kpf',\n  'language:kpg',\n  'language:kpj',\n  'language:kpw',\n  'language:kqa',\n  'language:kqc',\n  'language:kqf',\n  'language:kql',\n  'language:kqw',\n  'language:ksj',\n  'language:ksr',\n  'language:ktm',\n  'language:kto',\n  'language:kud',\n  'language:kue',\n  'language:kup',\n  'language:kvn',\n  'language:kwd',\n  'language:kwf',\n  'language:kwi',\n  'language:kwj',\n  'language:kyf',\n  'language:kyg',\n  'language:kyq',\n  'language:kyz',\n  'language:kze',\n  'language:lac',\n  'language:lat',\n  'language:lbb',\n  'language:leu',\n  'language:lex',\n  'language:lgl',\n  'language:lid',\n  'language:lif',\n  'language:lww',\n  'language:maa',\n  'language:maj',\n  'language:maq',\n  'language:mau',\n  'language:mav',\n  'language:maz',\n  'language:mbb',\n  'language:mbc',\n  'language:mbh',\n  'language:mbl',\n  'language:mbt',\n  'language:mca',\n  'language:mcb',\n  'language:mcd',\n  'language:mcf',\n  'language:mcp',\n  'language:mdy',\n  'language:med',\n  'language:mee',\n  'language:mek',\n  'language:meq',\n  'language:met',\n  'language:meu',\n  'language:mgh',\n  'language:mgw',\n  'language:mhl',\n  'language:mib',\n  'language:mic',\n  'language:mie',\n  'language:mig',\n  'language:mih',\n  'language:mil',\n  'language:mio',\n  'language:mir',\n  'language:mit',\n  'language:miz',\n  'language:mjc',\n  'language:mkn',\n  'language:mks',\n  'language:mlh',\n  'language:mlp',\n  'language:mmx',\n  'language:mna',\n  'language:mop',\n  'language:mox',\n  'language:mph',\n  'language:mpj',\n  'language:mpm',\n  'language:mpp',\n  'language:mps',\n  'language:mpx',\n  'language:mqb',\n  'language:mqj',\n  'language:msb',\n  'language:msc',\n  'language:msk',\n  'language:msm',\n  'language:msy',\n  'language:mti',\n  'language:muy',\n  'language:mva',\n  'language:mvn',\n  'language:mwc',\n  'language:mxb',\n  'language:mxp',\n  'language:mxq',\n  'language:mxt',\n  'language:myu',\n  'language:myw',\n  'language:myy',\n  'language:mzz',\n  'language:nab',\n  'language:naf',\n  'language:nak',\n  'language:nay',\n  'language:nbq',\n  'language:nca',\n  'language:nch',\n  'language:ncj',\n  'language:ncl',\n  'language:ncu',\n  'language:ndj',\n  'language:nfa',\n  'language:ngp',\n  'language:ngu',\n  'language:nhg',\n  'language:nhi',\n  'language:nho',\n  'language:nhr',\n  'language:nhu',\n  'language:nhw',\n  'language:nhy',\n  'language:nif',\n  'language:nin',\n  'language:nko',\n  'language:nld',\n  'language:nlg',\n  'language:nna',\n  'language:nnq',\n  'language:not',\n  'language:nou',\n  'language:npl',\n  'language:nsn',\n  'language:nss',\n  'language:ntj',\n  'language:ntp',\n  'language:nwi',\n  'language:nyu',\n  'language:obo',\n  'language:ong',\n  'language:ons',\n  'language:ood',\n  'language:opm',\n  'language:ote',\n  'language:otm',\n  'language:otn',\n  'language:otq',\n  'language:ots',\n  'language:pab',\n  'language:pad',\n  'language:pah',\n  'language:pao',\n  'language:pes',\n  'language:pib',\n  'language:pio',\n  'language:pir',\n  'language:pjt',\n  'language:plu',\n  'language:pma',\n  'language:poe',\n  'language:poi',\n  'language:pon',\n  'language:poy',\n  'language:ppo',\n  'language:prf',\n  'language:pri',\n  'language:ptp',\n  'language:ptu',\n  'language:pwg',\n  'language:quc',\n  'language:quf',\n  'language:quh',\n  'language:qul',\n  'language:qup',\n  'language:qvc',\n  'language:qve',\n  'language:qvh',\n  'language:qvm',\n  'language:qvn',\n  'language:qvs',\n  'language:qvw',\n  'language:qvz',\n  'language:qwh',\n  'language:qxh',\n  'language:qxn',\n  'language:qxo',\n  'language:rai',\n  'language:rkb',\n  'language:rmc',\n  'language:roo',\n  'language:rop',\n  'language:rro',\n  'language:ruf',\n  'language:rug',\n  'language:rus',\n  'language:sab',\n  'language:san',\n  'language:sbe',\n  'language:seh',\n  'language:sey',\n  'language:sgz',\n  'language:shj',\n  'language:shp',\n  'language:sim',\n  'language:sja',\n  'language:sll',\n  'language:smk',\n  'language:snc',\n  'language:snn',\n  'language:sny',\n  'language:som',\n  'language:soq',\n  'language:spa',\n  'language:spl',\n  'language:spm',\n  'language:sps',\n  'language:spy',\n  'language:sri',\n  'language:srm',\n  'language:srn',\n  'language:srp',\n  'language:srq',\n  'language:ssd',\n  'language:ssg',\n  'language:ssx',\n  'language:stp',\n  'language:sua',\n  'language:sue',\n  'language:sus',\n  'language:suz',\n  'language:swe',\n  'language:swh',\n  'language:swp',\n  'language:sxb',\n  'language:tac',\n  'language:tav',\n  'language:tbc',\n  'language:tbl',\n  'language:tbo',\n  'language:tbz',\n  'language:tca',\n  'language:tee',\n  'language:ter',\n  'language:tew',\n  'language:tfr',\n  'language:tgp',\n  'language:tif',\n  'language:tim',\n  'language:tiy',\n  'language:tke',\n  'language:tku',\n  'language:tna',\n  'language:tnc',\n  'language:tnn',\n  'language:tnp',\n  'language:toc',\n  'language:tod',\n  'language:toj',\n  'language:ton',\n  'language:too',\n  'language:top',\n  'language:tos',\n  'language:tpt',\n  'language:trc',\n  'language:tsw',\n  'language:ttc',\n  'language:tue',\n  'language:tuo',\n  'language:txu',\n  'language:ubr',\n  'language:udu',\n  'language:ukr',\n  'language:uli',\n  'language:ura',\n  'language:urb',\n  'language:usa',\n  'language:usp',\n  'language:uvl',\n  'language:vid',\n  'language:vie',\n  'language:viv',\n  'language:vmy',\n  'language:waj',\n  'language:wal',\n  'language:wap',\n  'language:wat',\n  'language:wbp',\n  'language:wed',\n  'language:wer',\n  'language:wim',\n  'language:wmt',\n  'language:wmw',\n  'language:wnc',\n  'language:wnu',\n  'language:wos',\n  'language:wrk',\n  'language:wro',\n  'language:wsk',\n  'language:wuv',\n  'language:xav',\n  'language:xed',\n  'language:xla',\n  'language:xnn',\n  'language:xon',\n  'language:xsi',\n  'language:xtd',\n  'language:xtm',\n  'language:yaa',\n  'language:yad',\n  'language:yal',\n  'language:yap',\n  'language:yaq',\n  'language:yby',\n  'language:ycn',\n  'language:yka',\n  'language:yml',\n  'language:yre',\n  'language:yuj',\n  'language:yut',\n  'language:yuw',\n  'language:yva',\n  'language:zaa',\n  'language:zab',\n  'language:zac',\n  'language:zad',\n  'language:zai',\n  'language:zaj',\n  'language:zam',\n  'language:zao',\n  'language:zar',\n  'language:zas',\n  'language:zat',\n  'language:zav',\n  'language:zaw',\n  'language:zca',\n  'language:zia',\n  'language:ziw',\n  'language:zos',\n  'language:zpc',\n  'language:zpl',\n  'language:zpo',\n  'language:zpq',\n  'language:zpu',\n  'language:zpv',\n  'language:zpz',\n  'language:zsr',\n  'language:ztq',\n  'language:zty',\n  'language:zyp',\n  'language:be',\n  'language:br',\n  'language:cs',\n  'language:ch',\n  'language:zh',\n  'language:de',\n  'language:en',\n  'language:eo',\n  'language:fr',\n  'language:ht',\n  'language:he',\n  'language:hr',\n  'language:id',\n  'language:it',\n  'language:ja',\n  'language:la',\n  'language:nl',\n  'language:ru',\n  'language:sa',\n  'language:so',\n  'language:es',\n  'language:sr',\n  'language:sv',\n  'language:to',\n  'language:uk',\n  'language:vi',\n  'license:cc-by-4.0',\n  'license:other']]\n\n\nWe can see that in this case many of the tags relate to language. Since the dataset is bible related and the bible has been heavily translated this might not be as surprising.\nAlthough these high-level stats are somewhat interesting we probably want to break these numbers down. At a high level we can groupby datasets vs models.\n\ndf.groupby(\"type\")[\"number_of_tags\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ntype\n\n\n\n\n\n\n\n\n\n\n\n\ndataset\n19576.0\n2.46935\n13.137220\n0.0\n0.0\n0.0\n2.0\n650.0\n\n\nmodel\n131767.0\n4.06151\n5.327066\n0.0\n0.0\n4.0\n6.0\n413.0\n\n\n\n\n\n\n\nWe can see that the mean number of tags for models is higher than datasets. We can also see at the 75% percentile models also have more tags compared to datasets. The possible reasons for this (and whether this is a problem or not) is something we may wish to explore furtherâ€¦\nSince the hub hosts models from different libraries we may want to also breakdown by library. First letâ€™s grab only the model part of our DataFrame.\n\nmodels_df = df[df[\"type\"] == \"model\"]\n\nThe library_name column contains info about the library. Letâ€™s see how many unique libraries we have.\n\nmodels_df.library_name.unique().shape\n\n(63,)\n\n\nThis is quite a few! We can do a groupby on this column\n\nmodels_df.groupby(\"library_name\")[\"number_of_tags\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nlibrary_name\n\n\n\n\n\n\n\n\n\n\n\n\nBERT\n1.0\n7.0\nNaN\n7.0\n7.0\n7.0\n7.0\n7.0\n\n\nDoc-UFCN\n2.0\n4.0\n0.000000\n4.0\n4.0\n4.0\n4.0\n4.0\n\n\nEveryDream\n2.0\n7.0\n0.000000\n7.0\n7.0\n7.0\n7.0\n7.0\n\n\nFastAI\n1.0\n1.0\nNaN\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nJoeyNMT\n1.0\n4.0\nNaN\n4.0\n4.0\n4.0\n4.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nultralytics\n4.0\n10.0\n1.414214\n8.0\n9.5\n10.5\n11.0\n11.0\n\n\nultralyticsplus\n1.0\n9.0\nNaN\n9.0\n9.0\n9.0\n9.0\n9.0\n\n\nyolor\n2.0\n9.0\n0.000000\n9.0\n9.0\n9.0\n9.0\n9.0\n\n\nyolov5\n36.0\n9.0\n0.000000\n9.0\n9.0\n9.0\n9.0\n9.0\n\n\nyolov6detect\n1.0\n10.0\nNaN\n10.0\n10.0\n10.0\n10.0\n10.0\n\n\n\n\n62 rows Ã— 8 columns\n\n\n\nWe might find this a bit tricky to look at. We may want to only include the top n libraries since some of these libraries may be less well used.\n\nmodels_df.library_name.value_counts()[:15]\n\ntransformers             63754\nstable-baselines3         3183\ndiffusers                 2802\nsentence-transformers     1273\nml-agents                  763\nkeras                      470\ntimm                       383\nespnet                     381\nspacy                      296\nsample-factory             273\nadapter-transformers       201\nsklearn                    113\nnemo                       103\nfastai                      99\nspeechbrain                 94\nName: library_name, dtype: int64\n\n\n\ntop_libraries = models_df.library_name.value_counts()[:9].index.to_list()\n\n\ntop_libraries_df = models_df[models_df.library_name.isin(top_libraries)]\n\n\ntop_libraries_df.groupby(\"library_name\")[\"number_of_tags\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nlibrary_name\n\n\n\n\n\n\n\n\n\n\n\n\ndiffusers\n2802.0\n4.374732\n2.171226\n1.0\n3.0\n4.0\n5.0\n18.0\n\n\nespnet\n381.0\n6.965879\n0.595060\n3.0\n7.0\n7.0\n7.0\n9.0\n\n\nkeras\n470.0\n3.842553\n14.422674\n1.0\n1.0\n2.0\n5.0\n311.0\n\n\nml-agents\n763.0\n6.965924\n0.273775\n2.0\n7.0\n7.0\n7.0\n7.0\n\n\nsentence-transformers\n1273.0\n6.984289\n3.221840\n2.0\n6.0\n6.0\n7.0\n36.0\n\n\nspacy\n296.0\n4.611486\n0.985180\n2.0\n4.0\n5.0\n5.0\n10.0\n\n\nstable-baselines3\n3183.0\n4.997801\n0.163426\n3.0\n5.0\n5.0\n5.0\n8.0\n\n\ntimm\n383.0\n3.548303\n1.315291\n2.0\n3.0\n3.0\n3.0\n13.0\n\n\ntransformers\n63754.0\n6.912037\n5.262633\n1.0\n5.0\n6.0\n8.0\n240.0\n\n\n\n\n\n\n\nLetâ€™s take a quick look at some examples from the library with the highest and lowest number or tags.\n\ntop_libraries_df[top_libraries_df.library_name == \"sentence-transformers\"].sample(15)[\n    \"tags\"\n]\n\n6123      [pytorch, gpt_neo, arxiv:2202.08904, sentence-...\n2488      [pytorch, distilbert, sentence-transformers, f...\n37669     [pytorch, distilbert, sentence-transformers, f...\n71483     [pytorch, bert, sentence-transformers, feature...\n20710     [pytorch, tf, roberta, ko, sentence-transforme...\n27073     [pytorch, tf, jax, roberta, arxiv:1908.10084, ...\n92037     [pytorch, mpnet, sentence-transformers, featur...\n90320     [pytorch, mpnet, sentence-transformers, featur...\n63555     [pytorch, bert, sentence-transformers, feature...\n87707     [pytorch, mpnet, sentence-transformers, featur...\n80570     [pytorch, bert, sentence-transformers, feature...\n111407    [pytorch, bert, sentence-transformers, feature...\n82690     [pytorch, mpnet, sentence-transformers, featur...\n36217     [pytorch, bert, pl, dataset:Wikipedia, arxiv:1...\n100086    [pytorch, roberta, sentence-transformers, feat...\nName: tags, dtype: object\n\n\n\ntop_libraries_df[top_libraries_df.library_name == \"timm\"].sample(15)[\"tags\"]\n\n104432                [pytorch, timm, image-classification]\n110296    [pytorch, arxiv:2301.00808, timm, image-classi...\n24158                 [pytorch, timm, image-classification]\n26471                 [pytorch, timm, image-classification]\n104437                [pytorch, timm, image-classification]\n61630     [pytorch, dataset:beans, timm, image-classific...\n110298    [pytorch, arxiv:2301.00808, timm, image-classi...\n104015                [pytorch, timm, image-classification]\n101124                [pytorch, timm, image-classification]\n57882     [coreml, onnx, en, dataset:imagenet-1k, arxiv:...\n83459     [pytorch, timm, image-classification, vision, ...\n99461                 [pytorch, timm, image-classification]\n104029                [pytorch, timm, image-classification]\n84402     [pytorch, timm, image-classification, vision, ...\n104428                [pytorch, timm, image-classification]\nName: tags, dtype: object\n\n\nWe can see here that some tags for sentence-transformers are very closely tied to that libraries purpose e.g.Â the sentence-similarity tag. This tag migth be useful when a user is looking for models to do sentence-similarity but might be less useful if you are trying to choose between models for this task i.e.Â trying to find the setence-transformer model that will be useful for you. We should be careful, therefore, in treating number of tags as a proxy for quality.\n\nGrouping by pipeline tags\nWe have a column in our dataframe pipeline tag, which refers to the type of task a model is for. We should be careful relying too much on this but we can have a quick look at how often these are used.\n\nmodels_df[\"pipeline_tag\"].value_counts()\n\ntext-classification               14479\ntext2text-generation               8102\ntext-generation                    7602\nreinforcement-learning             6885\ntoken-classification               6386\nautomatic-speech-recognition       6238\nfill-mask                          5447\nquestion-answering                 3147\nfeature-extraction                 2661\ntranslation                        1837\nconversational                     1770\nimage-classification               1760\ntext-to-image                      1604\nsentence-similarity                1248\nsummarization                       735\nunconditional-image-generation      428\ntext-to-speech                      244\naudio-classification                234\nmultiple-choice                     169\nobject-detection                    158\nimage-segmentation                  134\naudio-to-audio                      130\ntabular-classification               97\nzero-shot-classification             97\nimage-to-text                        76\nzero-shot-image-classification       56\nvideo-classification                 50\ntable-question-answering             47\ntabular-regression                   44\nimage-to-image                       43\ndepth-estimation                     37\ndocument-question-answering          18\nvisual-question-answering            13\nvoice-activity-detection              6\nother                                 4\ntime-series-forecasting               1\nName: pipeline_tag, dtype: int64\n\n\nWe may also want to see if there are some type of task that have more tags.\n\nmodels_df.groupby(\"pipeline_tag\")[\"number_of_tags\"].mean().sort_values().plot.barh()\n\n\n\n\n\n\n\n\nWe can also look at the breakdown for a particular task\n\ntext_classification_df = models_df[models_df[\"pipeline_tag\"] == \"text-classification\"]\n\n\ntext_classification_df[\"number_of_tags\"].describe()\n\ncount    14479.000000\nmean         5.948822\nstd          3.718800\nmin          1.000000\n25%          4.000000\n50%          5.000000\n75%          7.000000\nmax        240.000000\nName: number_of_tags, dtype: float64\n\n\nAgain, we have some extreme outliers\n\ntext_classification_df[text_classification_df.number_of_tags &gt; 230][[\"tags\", \"modelId\"]]\n\n\n\n\n\n\n\n\ntags\nmodelId\n\n\n\n\n22457\n[pytorch, tf, roberta, text-classification, mu...\nm3hrdadfi/zabanshenas-roberta-base-mix\n\n\n101628\n[pytorch, canine, text-classification, ace, af...\nSebOchs/canine-c-lang-id\n\n\n\n\n\n\n\nWe see that these mostly seem to relate to language. Letâ€™s remove these outliers and look at the distribution in the number of tags without these.\n\ntext_classification_df_no_outliers = text_classification_df[\n    text_classification_df[\"number_of_tags\"]\n    &lt;= text_classification_df[\"number_of_tags\"].quantile(0.95)\n]\ntext_classification_df_no_outliers[\"number_of_tags\"].plot.hist(bins=9)"
  },
  {
    "objectID": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#why-counting-tags-might-not-make-sense",
    "href": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#why-counting-tags-might-not-make-sense",
    "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
    "section": "Why counting tags might not make sense",
    "text": "Why counting tags might not make sense\nIâ€™e already hinted at why looking at raw number of tags might not be a good idea. Letâ€™s close this blog by briefly digging into at least one reason why. Weâ€™ll use the toolz library for some of this analysis.\n\nfrom toolz import concat\n\nFirst we grab all the tags and put them in a single list.\n\nall_tags = list(concat(df.tags.tolist()))\n\nIf we look at some examples, weâ€™ll see some tags are in the form of something:somethingelse.\n\nall_tags[:10]\n\n['pytorch',\n 'tf',\n 'albert',\n 'fill-mask',\n 'en',\n 'dataset:bookcorpus',\n 'dataset:wikipedia',\n 'arxiv:1909.11942',\n 'transformers',\n 'exbert']\n\n\nfor example dataset:wikipedia, we should therefore avoid treating all tags as the same since tags can have a particular purpose. i.e.Â indicating a dataset is associated with a model.\n\ndef is_special_tag(tag: str):\n    return \":\" in tag\n\n\nfrom toolz import countby, valmap\n\n\nspecial_tag_vs_normal = countby(is_special_tag, all_tags)\n\n\nspecial_tag_vs_normal\n\n{False: 467758, True: 115755}\n\n\n\ntotal = sum(special_tag_vs_normal.values())\nvalmap(lambda x: x / total, special_tag_vs_normal)\n\n{False: 0.8016239569641121, True: 0.1983760430358878}\n\n\nWe can see that a good chunk of tags are â€˜specialâ€™ tags. i.e.Â they have a â€˜typeâ€™ associated with them. If we want to explore tags on the hub more carefully weâ€™ll need to take this into accountâ€¦"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "",
    "text": "#hide\n!pip -q install fastai2 optuna swifter toolz \n\n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194kB 3.5MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 8.2MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 10.9MB/s \n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n    Preparing wheel metadata ... done\n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 7.6MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.6MB 16.8MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450kB 45.4MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 7.4MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 53.0MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 6.5MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 8.0MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 54.2MB/s \n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 11.6MB/s \n  Building wheel for alembic (PEP 517) ... done\n  Building wheel for optuna (setup.py) ... done\n  Building wheel for psutil (setup.py) ... done\n  Building wheel for pyperclip (setup.py) ... done\n  Building wheel for locket (setup.py) ... done\n  Building wheel for contextvars (setup.py) ... done\nERROR: distributed 2.18.0 has requirement tornado&gt;=5; python_version &lt; \"3.8\", but you'll have tornado 4.5.3 which is incompatible.\n#hide\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n\nEnter your authorization code:\nÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·\nMounted at /content/drive\n#hide\nfrom pathlib import Path\nfrom fastai2.text.all import *\nfrom fastai2.vision.all import *\nfrom fastai2.vision.all import *\nimport pandas as pd\nimport optuna\nimport pprint\n#hide\nPath('data').mkdir(exist_ok=True)\nout_path = Path('/content/drive/My Drive/Models/')\npath = Path('data/')"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#tldr",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#tldr",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "tl;dr",
    "text": "tl;dr\nThis post covers:\n\nthe motivations for â€˜pragmatic hyperparameters optimizationâ€™\nhow to do this using Optuna (with an example applied to the fastai2 library)"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#optimizing-hyperparameters",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#optimizing-hyperparameters",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Optimizing hyperparameters?",
    "text": "Optimizing hyperparameters?\nDeep learning models have a range of Hyperparameters. These include the basic building blocks of a model like the number of layers used or the size of embedding layers, and the parameters for the training of models such as learning rate. Changing some of these parameters will improve the performance of a model. There is therefore a potential win from finding the right values for these parameters.\n\nAuto ML vs pragmatic hyperparameters optimization\nAs a way of framing â€˜pragmatic searchâ€™, it is useful to contrast it to Auto ML. If you havenâ€™t come across it before:\n\nThe term AutoML has traditionally been used to describe automated methods for model selection and/or hyperparameter optimization. - {% fn 1 %}.\n\nIn particular what is termed Auto ML often includes a search across model and Hyperparameters but can also refer to â€˜Neural Architecture Searchâ€™ in which the objective is to piece together a new model type for a specific problem or dataset. An underlying assumption of some of this Auto ML approach is that each problem or dataset requires a unique model architecture.\nIn contrast a more â€˜pragmaticâ€™ approach uses an existing model architectures which have been shown to work across a range of datasets and tasks, and utilise transfer learning and other â€˜tricksâ€™ like cyclical learning rates and data augmentation. In a heritage context, it is likely that there are going to be bigger issues with imbalanced classes, noisy labels etc, and focusing on designing a custom architecture is probably going to lead to modest improvements in the performance of the model.\n\n\nSo what remains to be optimized?\nIn contrast to Auto ML which can involve looking at huge range of potential architectures and parameters we could instead limit our focus to smaller set of things which may have a large impact on the performance of your model.\nAs an example use case for hyperparameters optimization Iâ€™ll use two datasets which contain transcripts of trials from the Old Bailey online and which are classified into various categories (theft, deception, etc). One of the datasets is drawn the decade 1830 the other one 1730.\nThe approach taken to classifying these trials will be to follow the â€œUniversal Language Model Fine-tuning for Text Classificationâ€ approach. {% fn 2 %}.\nI wonâ€™t give an in depth summary of the approach here but idea is that:\n\nA language model - in this case a LSTM based model - is trained on a Wikipedia text. This provides a â€œgeneralâ€ language model that learns to â€œunderstandâ€ general features of a language, in this case English\nthis language model is then fine-tuned on a target dataset, in the orginal paper this is IMDB movie reviews.\none this language model has been fine-tuned on the target dataset this fine-tuned language model is used as input for a classifier\n\nThe intuition here is that by utilising a pre-trained language model the Wikipedia part, and the fine-tuning part we get the benefits of a massive training set (Wikipedia) whilst also being able to â€˜focusâ€™ the language model on a target corpus which will use language differently. This makes a lot of intuitive sense, but a question in this use case is how much to fine-tune the language model on our target datasets. A reasonable assumption might be that since language will be more different in 1730 compared to 1830 we may want to fine tune the language model trained on Wikipedia more on the 1730 dataset.\nWe could of course test through some trial and error experiments, but this is a question which may benefit from some more systematic searching for appropriate hyperparameters. Before we get into this example in more depth Iâ€™ll discuss the library Iâ€™m working with for doing this hyperparameter searching."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#optuna-a-hyperparameter-optimization-framework",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#optuna-a-hyperparameter-optimization-framework",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Optuna: A hyperparameter optimization framework",
    "text": "Optuna: A hyperparameter optimization framework\nIn this post I will be using Optuna â€œan automatic hyperparameter optimization software framework, particularly designed for machine learningâ€. {% fn 3 %}.\nThere are some really nice features in Optuna which Iâ€™ll cover in this post as I explore the question of language model fine-tuning, so hopefully even if you donâ€™t care about the specific use case it might still provide a useful overview of Optuna.\nIn this blog post my examples will use version two of the fastai library but there really isnâ€™t anything that wonâ€™t translate to other frameworks. Optuna has integrations for a number of libraries (including version 1 of fastai) but for this blog I wonâ€™t use this integration."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#a-simple-optimization-example",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#a-simple-optimization-example",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "A simple optimization example",
    "text": "A simple optimization example\nTo show the approach used in Optuna Iâ€™ll use a simple image classification example. In this case using a toy example of classifying people vs cats in images taken from 19th Century books.\nOptuna has two main concepts to understand: study and trial. A study is the overarching process of optimization based on some objective function. A trial is a single test/execution of the objective function. Weâ€™ll return to this in more detail. For now lets look at a simple example.\nFor our first example weâ€™ll just use Optuna to test whether to use a pre-trained model or not. If the option is True then the ResNet18 model we use will use weights from pre-training on ImageNet, if False the model will start with random weights.\nLooking at the high level steps of using Optuna (Iâ€™ll go into more detail later). We create an objective function:\n\n#collapse-hide\n!wget -q https://zenodo.org/record/3689444/files/humancats.zip?download=1\n!unzip -q *humancats.zip* -d data/\n\n\ndef objective(trial):\n    is_pretrained = trial.suggest_categorical('pre_trained', [True, False])\n    dls = ImageDataLoaders.from_folder('data/human_vs_cats/', valid_pct=0.4, item_tfms=Resize(64))\n    learn = cnn_learner(dls, resnet18, pretrained=is_pretrained, metrics=[accuracy])\n    learn.fit(1)\n    acc = learn.recorder.values[-1][-1]\n    return acc\n\nMost of this will look familiar if you are have used fastai before. Once we have this we create a study:\n\nstudy = optuna.create_study(direction='maximize')\n\nand then optimize this study:\n\nstudy.optimize(objective, n_trials=2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.503035\n0.710954\n0.555556\n00:06\n\n\n\n\n\n[I 2020-06-04 16:58:49,862] Finished trial#0 with value: 0.5555555820465088 with parameters: {'pre_trained': False}. Best is trial#0 with value: 0.5555555820465088.\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.691165\n1.218440\n0.555556\n00:05\n\n\n\n\n\n[I 2020-06-04 16:58:56,272] Finished trial#1 with value: 0.5555555820465088 with parameters: {'pre_trained': False}. Best is trial#0 with value: 0.5555555820465088.\n\n\nOnce weâ€™ve run some trials we can inspect the study object for the best value weâ€™re optimizing for. In this case this is the accuracy but it will be whatever is returned by our function. We can also see the parameters which led to this value.\n\nstudy.best_value, study.best_params\n\n(0.5555555820465088, {'pre_trained': False})\n\n\nThis toy example wasnâ€™t particularly useful (it just confirmed we probably want to use a pre-trained model) but going through the steps provides an overview of the main things required by Optuna. Starting with defining a function objective\ndef objective(trial):\nthis is the function we want to optimize. We could call it something else but following the convention in the Optuna docs the function weâ€™ll call it objective. This function takes â€˜trialâ€™ as an argument.\nis_pretrained = trial.suggest_categorical('pre_trained', [True, False])\nhere we use trial to â€œsuggestâ€ a categorical in this case one of two options (whether pre trained is set to true or false). We do this using trial.suggest_categorical and pass it the potential options (in this case True or False).\ntrial.suggest_blah defines the paramater â€œsearch spaceâ€ for Optuna. Weâ€™ll look at all of the options for this later on. The final step in defining our objective function i.e.Â the thing we want to optimize:\nreturn acc\nThis return value is objective value that Optuna will optimize. Because this is just the return value of a function there is a lot of flexibility in what this can be. In this example it is accuracy but it could be training or validation loss, or another training metrics. Later on weâ€™ll look at this in more detail.\nNow letâ€™s look at the study part:\n\nstudy = optuna.create_study(direction='maximize')\n\nThis is the most simple way of creating a study. This creates a study object, again, weâ€™ll look at more options as we go along. The one option we pass here is the direction. This refers to to whether Optuna should try to increase the return value of our optimization function or decrease it. This depends on what you a tracking i.e.Â youâ€™d want to minimize error or validation loss but increase accuracy or F1 score.\nLooking at the overview provided in the Optuna docs we have three main building blocks:\n\nTrial: A single call of the objective function\nStudy: An optimization session, which is a set of trials\nParameter: A variable whose value is to be optimized"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#parameter-search-space",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#parameter-search-space",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Parameter search space",
    "text": "Parameter search space\nBorrowing once more from the docs:\n\nThe difficulty of optimization increases roughly exponentially with regard to the number of parameters. That is, the number of necessary trials increases exponentially when you increase the number of parameters, so it is recommended to not add unimportant parameters\n\nThis is a crucial point. Particularly if we want to use optimization in a pragmatic way. When we have existing knowledge or evidence about what works well for a particular problem, we should use that rather than asking Optuna to find this out for us. There are some extra tricks to make our search for the best parameters more efficient which will be explored below but for now letâ€™s get back to the example use case."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#fine-tuning-a-language-model",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#fine-tuning-a-language-model",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Fine-tuning a language model",
    "text": "Fine-tuning a language model\n\n#collapse-hide\ndf_1830 = pd.read_csv('https://gist.githubusercontent.com/davanstrien/4bc85d8a4127a2791732280ffaa43293/raw/cd1a3cc53674b64c8f130edbcb34e835afa665fb/1830trial.csv')\ndf_1730 = pd.read_csv('https://gist.githubusercontent.com/davanstrien/4bc85d8a4127a2791732280ffaa43293/raw/cd1a3cc53674b64c8f130edbcb34e835afa665fb/1730trial.csv')\n\nFor the sake of brevity I wonâ€™t cover the steps to generate this dataset the instructions for doing so for the 1830s trials can be found here (and can be easily adapted for the 1730s trial).\n\n#hide_input \ndf_1830.head(2)\n\n\n\n\n\n\n\n\nUnnamed: 0\nUnnamed: 0.1\n0\nfile\nbroad\nnarrow\ntext\n\n\n\n\n0\n14463.0\nt18361128-57a\ntheft-housebreaking\nt18361128-57a.txt\ntheft\nhousebreaking\n\\n\\n\\n\\n\\n57. \\n\\n\\n\\n\\nJOHN BYE\\n the younger and \\n\\n\\n\\n\\nFREDERICK BYE\\n were indicted for\\n\\n feloniously breaking and entering the dwelling-house of \\n\\n\\n\\nJohn Bye, on the \\n21st of November, at \\nSt. Giles-in-the-Fields, and stealing therein 12 apples, value 9d.; 1 box, value 1d.; 24 pence, and 1 twopenny-piece; the goods and monies of \\n\\n\\n\\nMary Byrne.\\n\\n\\n\\n\\n\\n\\nMARY BYRNE\\n. I sell fruit; I live in Titchbourne-court, Holborn. On the 21st of November I went out at one o'clock, and locked my door?I left 2s. worth of penny-pieces in my drawer, and two dozen large apples?I came...\n\n\n1\n19021.0\nt18380917-2214\ntheft-pocketpicking\nt18380917-2214.txt\ntheft\npocketpicking\n\\n\\n\\n\\n2214. \\n\\n\\n\\n\\nMARY SMITH\\n was indicted\\n\\n for stealing, on the \\n16th of September, 1 purse, value 2d.; 3 half-crowns, and twopence; the goods and monies of \\n\\n\\n\\nGeorge Sainsbury, from his person.\\n\\n\\n\\n\\n\\n\\nGEORGE SAINSBURY\\n. Between twelve and one o'clock, on the 16th of September, I went to sleep in the fields, at Barnsbury-park, Islington, I had three half-crowns, and twopence, in my pocket?I was awoke, and missed my money?I went to the prisoner, and charged her with it?she said she had not got it?I followed her, and saw her drop ray purse down, it had two penny piece...\n\n\n\n\n\n\n\nWe load the data using fastai2 TextDataLoaders\n\n# collapse-show\ndef load_lm_data(df):\n    data_lm = TextDataLoaders.from_df(\n        df.sample(frac=0.5), text_col=\"text\", is_lm=True, bs=128\n    )\n    return data_lm\n\n\n# Classification data\ndef load_class_data(df, data_lm):\n    data_class = TextDataLoaders.from_df(\n        df.sample(frac=0.5),\n        text_col=\"text\",\n        label_col=\"broad\",\n        valid_pct=0.3,\n        bs=128,\n        text_vocab=data_lm.vocab,\n    )\n    return data_class\n\n\ndata_lm = load_lm_data(df_1830)\ndata_class = load_class_data(df_1830, data_lm)\n\n\n\n\n\n\n\nCreate the language model learner and classifier learner:\n\n# collapse-show\ndef create_lm():\n    return language_model_learner(data_lm, AWD_LSTM, pretrained=True).to_fp16()\n\n\ndef create_class_learn():\n    return text_classifier_learner(\n        data_class, AWD_LSTM, metrics=[accuracy, F1Score(average=\"weighted\")]\n    ).to_fp16()"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#optuna-trial-suggest",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#optuna-trial-suggest",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Optuna trial suggest",
    "text": "Optuna trial suggest\nIn the example above trial.suggest_categorical was used to define the potential parameter. Optuna has five kinds of parameters which can be optimized. These all work through the trial.suggest method.\n\nCategorical\nThis can be used for models, optimizers, and for True/False flags.\noptimizer = trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam'])\n\n\nInteger\nn_epochs = trial.suggest_int('num_epochs', 1, 3)\n\n\nUniform\nmax_zoom = trial.suggest_uniform('max_zoom', 0.0, 1.0)\n\n\nLoguniform\nlearning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n\n\nDiscrete-uniform\ndrop_path_rate = trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0)\nThe string value provides a key for the parameters which is used to access these parameters later, itâ€™s therefore important to give them a sensible name.\n\n\nLimiting parameters?\nAdding additional trial.suggest to your optimization function increases the search space for Optuna to optimize over so you should avoid adding additional parameters if they are not necessary.\nThe other way in which the search space can be constrained is to limit the range of the search i.e.Â for learning rate\nlearning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\nis preferable over\nlearning_rate = trial.suggest_loguniform('learning_rate', 1e-10, 1e-1)\nif itâ€™s not likely the optimal learning rate will sit outside of this range.\nHow many parameters you include will also depend on the type of model you are trying to train. In the use case of fine-tuning a language model we will want to limit the options more since language models are generally quite slow to train. If, on the other hand, we were trying to improve an image classification model which only takes minutes to train then searching through a larger parameter space would become more feasible."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#objective-function-for-fine-tuning-a-language-model",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#objective-function-for-fine-tuning-a-language-model",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Objective function for fine-tuning a language model",
    "text": "Objective function for fine-tuning a language model\nThe objective function below has two stages; train a language model, use the encoder from this language model for a classifier.\nThe parameters weâ€™re trying to optimize in this case are:\n\nlearning rate for the frozen language model\nnumber of epochs to train only the final layers of the language model\nlearning rate for the unfrozen language model\nnumber of epochs for training the whole language model\n\nWe use lm_learn.no_bar() as a context manager to reduce the amount of logging.\n\ndef objective(trial):\n    lm_learn = create_lm()\n    lr_frozen = trial.suggest_loguniform(\"learning_rate_frozen\", 1e-4, 1e-1)\n    head_epochs = trial.suggest_int(\"head_epochs\", 1, 5)\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen)\n    # Unfrozen Language model\n    lr_unfreeze = trial.suggest_loguniform(\"learning_rate_unfrozen\", 1e-7, 1e-1)\n    body_epochs = trial.suggest_int(\"lm_body_epochs\", 1, 5)\n    lm_learn.unfreeze()\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(body_epochs, lr_unfreeze)\n    lm_learn.save_encoder(\"finetuned\")\n    # Classification\n    cl_learn = create_class_learn()\n    cl_learn.load_encoder(\"finetuned\")\n    cl_learn.fit_one_cycle(3)\n    f1 = cl_learn.recorder.values[-1][-1]\n    return f1\n\nWe can give our study a name and also store it in a database. This allows for resuming previous trials later and accessing the history of previous trials. There are various options for database backends outlined in the documentation.\n\nCreating the study\n\nstudy_name = \"tunelm1830\"\nstudy = optuna.create_study(\n    study_name=study_name,\n    direction=\"maximize\",\n    storage=f\"sqlite:///{out_path}/optuma/example.db\",\n)\n\n[I 2020-06-05 15:09:05,470] A new study created with name: tunelm1830\n\n\n\n\nOptimize\nNow weâ€™ll run 3 trials and use show_progress_bar=True to give an ETA on when the trials will finish.\n\nstudy.optimize(objective, n_trials=3, show_progress_bar=True)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nProgress bar is experimental (supported from v1.2.0). The interface can change in the future.\n\n\n\n\n\n\n\n\n\n(#4) [0,5.382655620574951,4.875850200653076,'00:24']\n(#4) [1,5.292355537414551,4.737764835357666,'00:24']\n(#4) [2,5.183778285980225,4.647550106048584,'00:24']\n(#4) [3,5.11093282699585,4.608272552490234,'00:24']\n(#4) [4,5.072442054748535,4.601930618286133,'00:24']\n(#4) [0,4.7495622634887695,4.241390228271484,'00:27']\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n2.326032\n2.070412\n0.020000\n0.017034\n00:10\n\n\n1\n2.302230\n2.136864\n0.023333\n0.003590\n00:10\n\n\n2\n2.269061\n2.180663\n0.016667\n0.004408\n00:10\n\n\n\n\n\n[I 2020-06-05 15:12:20,128] Finished trial#0 with value: 0.00440805109922757 with parameters: {'learning_rate_frozen': 0.00014124685078723662, 'head_epochs': 5, 'learning_rate_unfrozen': 0.00010276862511970148, 'lm_body_epochs': 1}. Best is trial#0 with value: 0.00440805109922757.\n(#4) [0,4.713407516479492,3.7350399494171143,'00:24']\n(#4) [1,3.998744249343872,3.3055806159973145,'00:24']\n(#4) [2,3.6486754417419434,3.192685842514038,'00:24']\n(#4) [3,3.4996860027313232,3.1756556034088135,'00:24']\n(#4) [0,3.4227023124694824,3.163315534591675,'00:27']\n(#4) [1,3.3954737186431885,3.140226364135742,'00:27']\n(#4) [2,3.3778774738311768,3.125929117202759,'00:27']\n(#4) [3,3.357388973236084,3.119621753692627,'00:27']\n(#4) [4,3.3542206287384033,3.1186859607696533,'00:27']\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n2.368984\n2.121307\n0.013333\n0.000759\n00:11\n\n\n1\n2.335033\n2.022853\n0.250000\n0.368652\n00:10\n\n\n2\n2.296630\n1.948786\n0.313333\n0.452365\n00:10\n\n\n\n\n\n[I 2020-06-05 15:16:49,562] Finished trial#1 with value: 0.45236502121696065 with parameters: {'learning_rate_frozen': 0.0060643425219262335, 'head_epochs': 4, 'learning_rate_unfrozen': 2.734844423029637e-05, 'lm_body_epochs': 5}. Best is trial#1 with value: 0.45236502121696065.\n(#4) [0,5.3748459815979,4.851675987243652,'00:24']\n(#4) [1,5.247058868408203,4.672318935394287,'00:24']\n(#4) [2,5.111597061157227,4.559732437133789,'00:24']\n(#4) [3,5.026832103729248,4.512131690979004,'00:24']\n(#4) [4,4.982809066772461,4.5044732093811035,'00:24']\n(#4) [0,4.915407657623291,4.423311233520508,'00:27']\n(#4) [1,4.857243061065674,4.394893646240234,'00:27']\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n2.368439\n2.036706\n0.240000\n0.360355\n00:10\n\n\n1\n2.359790\n2.093103\n0.033333\n0.045878\n00:09\n\n\n2\n2.331945\n2.140194\n0.016667\n0.013589\n00:10\n\n\n\n\n\n[I 2020-06-05 15:20:20,119] Finished trial#2 with value: 0.013588651008106425 with parameters: {'learning_rate_frozen': 0.0001971120155925954, 'head_epochs': 5, 'learning_rate_unfrozen': 1.0649951798153689e-05, 'lm_body_epochs': 2}. Best is trial#1 with value: 0.45236502121696065."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#results",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#results",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Results",
    "text": "Results\nYou can see how trials are peforming in the logs with the last part of the log reporting the best trial so far. We can now access the best value and best_params.\n\nstudy.best_value, study.best_params\n\n(0.45236502121696065,\n {'head_epochs': 4,\n  'learning_rate_frozen': 0.0060643425219262335,\n  'learning_rate_unfrozen': 2.734844423029637e-05,\n  'lm_body_epochs': 5})"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#suggesting-a-learning-rate",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#suggesting-a-learning-rate",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Suggesting a learning rate",
    "text": "Suggesting a learning rate\nOne of the yummiest features in fastai which has also made it into other deep-learning libraries is the learning rate finer lr_find(). As a reminder:\n\nthe LR Finder trains the model with exp onentially growing learning rates from start_lr to end_lr for num_it and stops in case of divergence (unless stop_div=False) then plots the losses vs the learning rates with a log scale.\n\nSince the Learning rate finder often gives a good learning rate we should see if we can use this as a starting point for our trials.\n\nEnqueue trial\nUsing enqueue_trial you can queue up trials with specied paramters. This can be for all of the parameters or just a subset. We can use lr_find to suggest a learning rate for the language model and then que a trial with this learning rate.\n\nlm_learn = create_lm()\nlm_learn.unfreeze()\n\n\nlr_min,lr_steep = lm_learn.lr_find(suggestions=True)\n\n\n\n\n\n\n\n\n\n\n\n\nlr_min, lr_steep\n\n(0.014454397559165954, 0.033113110810518265)\n\n\n\nstudy.enqueue_trial({'learning_rate_unfrozen': lr_steep})\nstudy.optimize(objective, n_trials=1)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nenqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n\n\n\n(#4) [0,5.322241306304932,4.736147403717041,'00:24']\n(#4) [1,5.095097541809082,4.474568843841553,'00:24']\n(#4) [2,4.91882848739624,4.365659713745117,'00:24']\n(#4) [3,4.820737838745117,4.348053455352783,'00:24']\n(#4) [0,3.5270116329193115,3.0885186195373535,'00:27']\n(#4) [1,3.1028788089752197,2.8053553104400635,'00:27']\n(#4) [2,2.7882776260375977,2.611638069152832,'00:27']\n(#4) [3,2.49800705909729,2.539992094039917,'00:27']\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n2.325723\n2.102552\n0.010000\n0.006709\n00:10\n\n\n1\n2.293266\n2.006258\n0.216667\n0.332841\n00:10\n\n\n2\n2.258634\n1.928858\n0.566667\n0.686662\n00:10\n\n\n\n\n\n[I 2020-06-05 15:28:58,707] Finished trial#3 with value: 0.6866621960133127 with parameters: {'head_epochs': 4, 'learning_rate_frozen': 0.0003841551576945897, 'learning_rate_unfrozen': 0.033113110810518265, 'lm_body_epochs': 4}. Best is trial#3 with value: 0.6866621960133127.\n\n\nUsing the learning rate from the LR_finder gives us our best trial so far. This is likely to be because learning rate is a particularly important hyper-parameter. The suggested learning rate from lr_find may not always be the best but using either the suggested one or picking one based on the plot as a starting point for the trial may help Optuna to start from sensible starting point while still giving the freedom for optuna to diverge away from this in later trials if helps the objective function."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#pruning-trials",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#pruning-trials",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Pruning trials",
    "text": "Pruning trials\nThe next feature of Optuna which helps make parameter searching more efficient is pruning. Pruning is a process for stopping bad trials early.\nFor example if we have the following three trials: - Trial 1 - epoch 1: 87% accuracy - Trial 2 - epoch 1: 85% accuracy - Trial 3 - epoch 1: 60% accuracy\nprobably itâ€™s not worth continuing with trial 3. Pruning trials helps focus computational resources on trials which are likely to improve on previous trials. The likely here is important. It is possible that some trials may be pruned early which actually would have done better in the end. Optuna offers a number of different pruning algorithms, I wonâ€™t cover these here but the documentation gives a good overview and includes links to the papers which propose the implemented pruning algorithms.\n\nHow to do pruning in Optuna?\nOptuna has intergrations with various machine learning libraries. These intergrations can help with the pruning but setting up pruning manually is also pretty straight forward to do.\nThe two things we need to do is report the value and the stage in the training porcess:\ntrial.report(metric, step)\nthen we call:\nif trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\nDepending on your objective function this will be put in different places. In the example of fine-tuning the language model, because weâ€™re trying to optimize the classification part it, it means the pruning step can only be called quite late in the traing loop. Ideally it would be called earlier but we still save a little bit of time on unpromising trials.\nThe new objective function with pruning:\n\ndef objective(trial):\n    lm_learn = create_lm()\n    lr_frozen = trial.suggest_loguniform(\"learning_rate_frozen\", 1e-4, 1e-1)\n    head_epochs = trial.suggest_int(\"head_epochs\", 1, 5)\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen)\n    # Unfrozen Language model\n    lr_unfreeze = trial.suggest_loguniform(\"learning_rate_unfrozen\", 1e-7, 1e-1)\n    body_epochs = trial.suggest_int(\"lm_body_epochs\", 1, 5)\n    lm_learn.unfreeze()\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(body_epochs, lr_unfreeze)\n    lm_learn.save_encoder(\"finetuned\")\n    # Classification\n    cl_learn = create_class_learn()\n    cl_learn.load_encoder(\"finetuned\")\n    for step in range(3):\n        cl_learn.fit(1)\n        # Pruning\n        intermediate_f1 = cl_learn.recorder.values[-1][\n            -1\n        ]  # get f1 score for current step\n        trial.report(intermediate_f1, step)  # report f1\n        if trial.should_prune():  # let optuna decide whether to prune\n            raise optuna.exceptions.TrialPruned()\n    f1 = cl_learn.recorder.values[-1][-1]\n    return f1\n\nWe can load the same study as before using the python load_if_exists flag.\n\nstudy_name = \"tunelm1830\"\nstudy = optuna.create_study(\n    study_name=study_name,\n    direction=\"maximize\",\n    storage=f\"sqlite:///{out_path}/optuma/example.db\",\n    load_if_exists=True,\n    pruner=optuna.pruners.SuccessiveHalvingPruner(),\n)\n\n[I 2020-06-06 14:30:47,724] Using an existing study with name 'tunelm1830' instead of creating a new one.\n\n\nWe can now run some more trials. Instead of specifying the number of trials we can also specify how long optuma should search for.\n\nstudy.enqueue_trial({'learning_rate_unfrozen': lr_steep})\nstudy.optimize(objective, timeout=60*60*0.5)\n\nand get the best trial:\n\nstudy.best_trial\n\nFrozenTrial(number=13, value=0.8657462002717475, datetime_start=datetime.datetime(2020, 6, 5, 15, 59, 26, 230967), datetime_complete=datetime.datetime(2020, 6, 5, 16, 3, 26, 392390), params={'head_epochs': 4, 'learning_rate_frozen': 0.0012866609022148768, 'learning_rate_unfrozen': 1.3302852136460371e-06, 'lm_body_epochs': 4}, distributions={'head_epochs': IntUniformDistribution(high=5, low=1, step=1), 'learning_rate_frozen': LogUniformDistribution(high=0.1, low=0.0001), 'learning_rate_unfrozen': LogUniformDistribution(high=0.1, low=1e-07), 'lm_body_epochs': IntUniformDistribution(high=5, low=1, step=1)}, user_attrs={}, system_attrs={'completed_rung_0': 0.8156506309537317}, intermediate_values={0: 0.251088767516275, 1: 0.8156506309537317, 2: 0.8657462002717475}, trial_id=14, state=TrialState.COMPLETE)\n\n\nand best value and pararms:\n\nstudy.best_value, study.best_params\n\n(0.8657462002717475,\n {'head_epochs': 4,\n  'learning_rate_frozen': 0.0012866609022148768,\n  'learning_rate_unfrozen': 1.3302852136460371e-06,\n  'lm_body_epochs': 4})"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#paramters-for-the-1730s-trials-data",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#paramters-for-the-1730s-trials-data",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Paramters for the 1730s trials data",
    "text": "Paramters for the 1730s trials data\nWe can do the same process with the 1730s trials, starting with a suggested learning rate.\n\n#collapse-show\ndata_lm = load_lm_data(df_1730)\ndata_class = load_class_data(df_1730, data_lm)\nlm_learn = create_lm()\nlm_learn.unfreeze()\nlr_min,lr_steep = lm_learn.lr_find(suggestions=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# collapse-hide\ndef objective(trial):\n    lm_learn = create_lm()\n    lr_frozen = trial.suggest_loguniform(\"learning_rate_frozen\", 1e-4, 1e-1)\n    head_epochs = trial.suggest_int(\"head_epochs\", 1, 5)\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen)\n    # Unfrozen Language model\n    lr_unfreeze = trial.suggest_loguniform(\"learning_rate_unfrozen\", 1e-7, 1e-1)\n    body_epochs = trial.suggest_int(\"lm_body_epochs\", 1, 5)\n    lm_learn.unfreeze()\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(body_epochs, lr_unfreeze)\n    lm_learn.save_encoder(\"finetuned\")\n    # Classification\n    cl_learn = create_class_learn()\n    cl_learn.load_encoder(\"finetuned\")\n    for step in range(3):\n        cl_learn.fit(1)\n        intermediate_f1 = cl_learn.recorder.values[-1][-1]\n        trial.report(intermediate_f1, step)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n    f1 = cl_learn.recorder.values[-1][-1]\n    return f1\n\n\n# collapse-hide\nstudy_name = \"tunelm1730\"\nstudy = optuna.create_study(\n    study_name=study_name,\n    direction=\"maximize\",\n    storage=f\"sqlite:///{out_path}/optuma/example.db\",\n    load_if_exists=True,\n    pruner=optuna.pruners.SuccessiveHalvingPruner(),\n)\n\n[I 2020-06-08 15:06:54,474] Using an existing study with name 'tunelm1730' instead of creating a new one.\n\n\n\nstudy.enqueue_trial({'learning_rate_unfrozen': lr_steep})\nstudy.optimize(objective, timeout=60*60*0.5)\n\nTrials can be accssed as part of the study object. Running trials for 30 mins with early pruning results in 20 trials\n\nlen(study.trials)\n\n20\n\n\nWe can also see which was the best trial.\n\nstudy.best_trial.number\n\n2\n\n\nThe number of trials run depends mainly on how long your model takes to train, the size of the paramter search space and your patience. If trials are failing to improve better scores for a long time itâ€™s probably better to actively think about how to improve your approach to the problem (better data, more data, chaning model design etc.) rather than hoping hyperaparmet tuning will fix the problem."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#comparing-language-model-parameters",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#comparing-language-model-parameters",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Comparing language model parameters",
    "text": "Comparing language model parameters\nPrevious trials can be loaded using load_study\n\nstudy1830 = optuna.load_study('tunelm1830', storage=f'sqlite:///{out_path}/optuma/example.db')\nstudy1730 = optuna.load_study('tunelm1730', storage=f'sqlite:///{out_path}/optuma/example.db')\n\nFirst comparing the best f1 values for both datasets:\n\nprint(f'Best 1830 value was: {study1830.best_value:.3}')\nprint(f'Best 1730 value was: {study1730.best_value:.3}')\n\nBest 1830 value was: 0.866\nBest 1730 value was: 0.781\n\n\nThe paramters used to get the best value:\n\n1830 parameters\n\n#hide_input\nstudy1830.best_params\n\n{'head_epochs': 4,\n 'learning_rate_frozen': 0.0012866609022148768,\n 'learning_rate_unfrozen': 1.3302852136460371e-06,\n 'lm_body_epochs': 4}\n\n\n\n\n1730 parameters\n\n#hide_input\nstudy1730.best_params\n\n{'head_epochs': 3,\n 'learning_rate_frozen': 0.002145480897071231,\n 'learning_rate_unfrozen': 9.889236991663078e-06,\n 'lm_body_epochs': 1}\n\n\nSpecific parameters can also be accessed\n\nstudy1830.best_params['learning_rate_unfrozen'], study1730.best_params['learning_rate_unfrozen']\n\n(1.3302852136460371e-06, 9.889236991663078e-06)"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#visualizations",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#visualizations",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Visualizations",
    "text": "Visualizations\nOptuna has a variety of visulizations, I will only briefly show a few of these here.\nplot_intermediate_values shows the intermediate values. This can be useful for getting a sense of how trials progress and also help give a sense of whether some trials are being pruned prematurely\n\noptuna.visualization.plot_intermediate_values(study1830)\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nplot_parallel_coordinate plots parameters choices in relation to values. It can be hard to read these plots but they can also be helpful for giving a sense of which choices for parameters work best.\n\noptuna.visualization.plot_parallel_coordinate(study1830)\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\nParameter importance\nOptuna has experimental support for getting parameter importance.\n\noptuna.importance.get_param_importances(study1730)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nget_param_importances is experimental (supported from v1.3.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning:\n\nMeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future.\n\n\n\nOrderedDict([('learning_rate_frozen', 0.43423246892923717),\n             ('learning_rate_unfrozen', 0.2904735896601219),\n             ('head_epochs', 0.2433021650269149),\n             ('lm_body_epochs', 0.031991776383726155)])\n\n\n\noptuna.importance.get_param_importances(study1830)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nget_param_importances is experimental (supported from v1.3.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning:\n\nMeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future.\n\n\n\nOrderedDict([('learning_rate_unfrozen', 0.35548906967729954),\n             ('learning_rate_frozen', 0.33998779901100146),\n             ('head_epochs', 0.21196438930810765),\n             ('lm_body_epochs', 0.09255874200359132)])\n\n\nThese are broadly similar although learning rate frozen/unfrozen are in different places for the 1730 and 1830 trials."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#the-data",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#the-data",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "The data",
    "text": "The data\nThe data is images of maps and other things from historic newspapers. The aim is to classify whether the image is a map or something else.\n\ndls = ImageDataLoaders.from_folder(\n    \"data/1905_maps/\", valid_pct=0.3, item_tfms=Resize(256)\n)\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n#hide\nlearn = cnn_learner(dls, resnet50, metrics=[F1Score(average='weighted')])\n\n\nlearn.unfreeze()\nlr_min,unfrozen_lr_steep = learn.lr_find(suggestions=True)\n\n\n\n\n\n\n\n\n\n\n\n\nExcessive model parameter search\nSince the time to train the model is more reasonable we can add a more parameters to the search space. In practice this is pretty overkill but is useful as an example of working with the outputs of trials with many parameters.\n\ndef objective(trial):\n    apply_tfms = trial.suggest_categorical(\"apply_tfms\", [True, False])\n    if apply_tfms:\n        aug_tfms = aug_transforms(\n            mult=trial.suggest_uniform(\"mult\", 0.0, 1.0),\n            do_flip=trial.suggest_categorical(\"do_flip\", [True, False]),\n            flip_vert=trial.suggest_categorical(\"flip_vert\", [True, False]),\n            max_rotate=trial.suggest_uniform(\"max_rotate\", 0, 180),\n            max_zoom=trial.suggest_uniform(\"max_zoom\", 0, 3.0),\n            max_lighting=trial.suggest_uniform(\"max_lighting\", 0.0, 1.0),\n        )\n    else:\n        aug_tfms = None\n    dls = ImageDataLoaders.from_folder(\n        \"data/1905_maps/\", valid_pct=0.3, item_tfms=Resize(256), aug_transforms=aug_tfms\n    )\n    model = trial.suggest_categorical(\n        \"model\", [\"resnet18\", \"resnet50\", \"xresnet50\", \"squeezenet1_0\", \"densenet121\"]\n    )\n    learn = cnn_learner(\n        dls, arch=eval(model), pretrained=True, metrics=[F1Score(average=\"weighted\")]\n    ).to_fp16()\n    epochs = trial.suggest_int(\"epochs\", 1, 10)\n    for step in range(epochs):\n        with learn.no_bar():\n            learn.fit_one_cycle(\n                1, base_lr=trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1)\n            )\n    unfrozen_epochs = trial.suggest_int(\"unfrozen_epochs\", 1, 10)\n    unfrozen_lr = trial.suggest_loguniform(\"unfrozen_learning_rate\", 1e-10, 1e-1)\n    learn.unfreeze()\n    for step in range(unfrozen_epochs):\n        with learn.no_bar():\n            learn.fit_one_cycle(1, lr_max=unfrozen_lr)\n            int_f1 = learn.recorder.values[-1][-1]\n            trial.report(int_f1, step)\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n    t0 = time.time()\n    learn.validate()\n    t1 = time.time()\n    execute_time = t1 - t0\n    trial.set_user_attr(\"execute_time\", execute_time)\n    f1 = learn.recorder.values[-1][-1]\n    return f1\n\nCreate the study\n\nstudy_name = \"mapsmegastudyXL\"  # Unique identifier of the study.\nstudy = optuna.create_study(\n    direction=\"maximize\",\n    load_if_exists=True,\n    study_name=study_name,\n    storage=f\"sqlite:///{out_path}/optuma/blog.db\",\n    pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=2),\n)\n\n[I 2020-06-07 15:03:24,138] Using an existing study with name 'mapsmegastudyXL' instead of creating a new one.\n\n\nQueue up with some parameters\n\nstudy.enqueue_trial(\n    {\n        \"pre_trained\": True,\n        \"apply_tfms\": True,\n        \"epochs\": 5,\n        \"learning_rate\": lr_steep,\n        \"model\": \"resnet50\",\n        \"unfrozen_learning_rate\": unfrozen_lr_steep,\n    }\n)\nstudy.optimize(objective, n_trials=1, show_progress_bar=True)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nenqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nProgress bar is experimental (supported from v1.2.0). The interface can change in the future.\n\n\n\n\n\n\n(#5) [0,1.162647008895874,1.0643662214279175,0.38594077225581136,'00:04']\n(#5) [0,1.1730060577392578,0.8583190441131592,0.45458674870439575,'00:02']\n(#5) [0,0.7940309047698975,0.40110471844673157,0.8101934029975151,'00:02']\n(#5) [0,0.3774714767932892,0.3251221776008606,0.8738329238329239,'00:02']\n(#5) [0,0.20592834055423737,0.304998517036438,0.8914149443561209,'00:02']\n(#5) [0,0.14400754868984222,0.3399428725242615,0.9003332765709003,'00:03']\n(#5) [0,0.11649172753095627,0.3571062982082367,0.8729641116526362,'00:03']\n\n\n\n\n\n[I 2020-06-07 15:03:57,405] Finished trial#0 with value: 0.8729641116526362 with parameters: {'apply_tfms': True, 'do_flip': False, 'epochs': 5, 'flip_vert': True, 'learning_rate': 0.00015848931798245758, 'max_lighting': 0.5155363265412508, 'max_rotate': 93.50185801538605, 'max_zoom': 2.5014402368129147, 'model': 'resnet50', 'mult': 0.7973732804273224, 'unfrozen_epochs': 2, 'unfrozen_learning_rate': 1.4454397387453355e-05}. Best is trial#0 with value: 0.8729641116526362.\n\n\n\nqueue up with some less sensible defaults\n\nstudy.enqueue_trial(\n    {\"pre_trained\": False, \"apply_tfms\": False, \"epochs\": 1, \"unfrozen_epochs\": 1}\n)\nstudy.optimize(objective, n_trials=1, show_progress_bar=True)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nenqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nProgress bar is experimental (supported from v1.2.0). The interface can change in the future.\n\n\n\n\n\n\n(#5) [0,1.1455873250961304,1.7333940267562866,0.3123010228273386,'00:01']\n(#5) [0,1.0485259294509888,1.4432364702224731,0.4545249081834448,'00:01']\n\n\n\n\n\n[I 2020-06-07 15:04:18,823] Finished trial#1 with value: 0.4545249081834448 with parameters: {'apply_tfms': False, 'epochs': 1, 'learning_rate': 1.4039901997074766e-05, 'model': 'resnet18', 'unfrozen_epochs': 1, 'unfrozen_learning_rate': 4.041607859100835e-07}. Best is trial#0 with value: 0.8729641116526362.\n\n\n\nNow optimize for 500 trials\n\nstudy.optimize(objective, n_trials=500,show_progress_bar=True)\n\n\nstudy = optuna.load_study('mapsmegastudyXL', storage=f'sqlite:///{out_path}/optuma/blog.db')\n\nThe best finishing values and parameters:\n\nstudy.best_value, study.best_params\n\n(0.963975663975664,\n {'apply_tfms': True,\n  'do_flip': True,\n  'epochs': 10,\n  'flip_vert': False,\n  'learning_rate': 0.0785689562916925,\n  'max_lighting': 0.5064203068969654,\n  'max_rotate': 168.972217754609,\n  'max_zoom': 1.6141746329756919,\n  'model': 'densenet121',\n  'mult': 0.6087267126078458,\n  'unfrozen_epochs': 4,\n  'unfrozen_learning_rate': 7.6080876225791396e-06})\n\n\n\nVisualization\nTaking a look at parallel_coordinate in this case gives some sense of which options work best.\n\noptuna.visualization.plot_parallel_coordinate(study)\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\nImportance\n\noptuna.importance.get_param_importances(study)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nget_param_importances is experimental (supported from v1.3.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning:\n\nMeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future.\n\n\n\nOrderedDict([('unfrozen_learning_rate', 0.6945560978629778),\n             ('epochs', 0.13207296757949719),\n             ('model', 0.07996254760084977),\n             ('unfrozen_epochs', 0.04455237119259635),\n             ('learning_rate', 0.04014544684326522),\n             ('apply_tfms', 0.008710568920813712)])\n\n\nLearning rate is by far the most important learning rate, again this suggests that using learning rate finder makes a lot of sense as a starting point."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#working-with-optuna-trial-data",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#working-with-optuna-trial-data",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Working with Optuna trial data",
    "text": "Working with Optuna trial data\nThere are now ~500 trials which are stored in the study. Each of these trials contains the parameters used, metadata about the trial, the value of the thing being optimized, and importantly for this example the user attribute which stores the validation time. Optuna makes it very easy to export this information to a dataframe.\n\ndf = study.trials_dataframe()\ndf.head(3)\n\n\n\n\n\n\n\n\nnumber\nvalue\ndatetime_start\ndatetime_complete\nduration\nparams_apply_tfms\nparams_do_flip\nparams_epochs\nparams_flip_vert\nparams_learning_rate\nparams_max_lighting\nparams_max_rotate\nparams_max_zoom\nparams_model\nparams_mult\nparams_unfrozen_epochs\nparams_unfrozen_learning_rate\nuser_attrs_execute_time\nsystem_attrs_completed_rung_0\nsystem_attrs_completed_rung_1\nsystem_attrs_fixed_params\nstate\n\n\n\n\n0\n0\n0.872964\n2020-06-07 15:03:29.911841\n2020-06-07 15:03:57.151460\n00:00:27.239619\nTrue\nFalse\n5.0\nTrue\n0.000158\n0.515536\n93.501858\n2.50144\nresnet50\n0.797373\n2.0\n1.445440e-05\n0.82319\nNaN\nNaN\n{'pre_trained': True, 'apply_tfms': True, 'epochs': 5, 'learning_rate': 0.00015848931798245758, 'model': 'resnet50', 'unfrozen_learning_rate': 1.4454397387453355e-05}\nCOMPLETE\n\n\n1\n1\n0.454525\n2020-06-07 15:04:11.520248\n2020-06-07 15:04:18.419082\n00:00:06.898834\nFalse\nNaN\n1.0\nNaN\n0.000014\nNaN\nNaN\nNaN\nresnet18\nNaN\n1.0\n4.041608e-07\n0.67698\nNaN\nNaN\n{'pre_trained': False, 'apply_tfms': False, 'epochs': 1, 'unfrozen_epochs': 1}\nCOMPLETE\n\n\n2\n2\nNaN\n2020-06-07 15:04:32.047588\nNaT\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nRUNNING\n\n\n\n\n\n\n\n\n#hide\ndf.to_csv('optuna_study.csv')\n\n\n#hide\ndf = pd.read_csv('https://gist.githubusercontent.com/davanstrien/0c9670d02cdf8a9a866b8a467664b690/raw/cb3222f1baf8ae894923e2b8898beaa22ebeadd8/optuna_trials.csv')\n\nWe can now easily work with the trial data using pandas. Lets start by getting the best two values\n\ndf.sort_values(['value'], ascending=False).head(2)\n\n\n\n\n\n\n\n\nUnnamed: 0\nnumber\nvalue\ndatetime_start\ndatetime_complete\nduration\nparams_apply_tfms\nparams_do_flip\nparams_epochs\nparams_flip_vert\n...\nparams_max_zoom\nparams_model\nparams_mult\nparams_unfrozen_epochs\nparams_unfrozen_learning_rate\nuser_attrs_execute_time\nsystem_attrs_completed_rung_0\nsystem_attrs_completed_rung_1\nsystem_attrs_fixed_params\nstate\n\n\n\n\n177\n177\n177\n0.963976\n2020-06-07 16:48:36.232551\n2020-06-07 16:49:21.393454\n0 days 00:00:45.160903000\nTrue\nTrue\n10.0\nFalse\n...\n1.614175\ndensenet121\n0.608727\n4.0\n7.608088e-06\n0.880459\n0.954955\nNaN\nNaN\nCOMPLETE\n\n\n302\n302\n302\n0.955064\n2020-06-07 18:11:00.667449\n2020-06-07 18:11:45.658241\n0 days 00:00:44.990792000\nTrue\nTrue\n10.0\nFalse\n...\n0.921689\ndensenet121\n0.115708\n4.0\n6.210737e-10\n0.878865\n0.945946\nNaN\nNaN\nCOMPLETE\n\n\n\n\n2 rows Ã— 23 columns\n\n\n\nWe can see how often transforms were applied in the trials\n\ndf['params_apply_tfms'].value_counts()\n\nTrue     360\nFalse    142\nName: params_apply_tfms, dtype: int64\n\n\nViewing the number of trials for each model which had a value over 90\n\ndf['params_model'][df['value'] &gt;= 0.90].value_counts()\n\ndensenet121      181\nresnet50           9\nsqueezenet1_0      2\nName: params_model, dtype: int64\n\n\nFiltering a bit more aggressively (value above 94)\n\ndf94 = df[df['value'] &gt;= 0.94]\n\n\nlen(df94)\n\n13\n\n\nHow often were transforms applied for these trials\n\ndf94['params_apply_tfms'].value_counts()\n\nTrue     11\nFalse     2\nName: params_apply_tfms, dtype: int64\n\n\nThe number of unfrozen epochs\n\ndf94['params_unfrozen_epochs'].value_counts()\n\n4.0    6\n2.0    3\n3.0    2\n6.0    1\n5.0    1\nName: params_unfrozen_epochs, dtype: int64\n\n\nGetting back to the validation time we can get the max, min and mean values\n\ndf['user_attrs_execute_time'].max(), df['user_attrs_execute_time'].min(), df['user_attrs_execute_time'].mean()\n\n(0.9760787487030028, 0.6313643455505371, 0.8461264789613903)\n\n\nIf we did care about reducing the execution time we could use these values to find the trial with the shortest execution time:\n\ndf94['user_attrs_execute_time'].sort_values()\n\n96     0.837618\n426    0.848863\n394    0.849243\n395    0.851704\n438    0.852672\n500    0.863168\n344    0.875520\n432    0.877422\n302    0.878865\n177    0.880459\n473    0.884703\n372    0.906770\n294    0.907221\nName: user_attrs_execute_time, dtype: float64\n\n\nIf we were happy with slightly lower performance we could pick the study with the shortest execution time which is still achieves a f1 above 94%\n\ndf94.loc[96]\n\nnumber                                                   96\nvalue                                              0.945755\ndatetime_start                   2020-06-07 15:57:20.382634\ndatetime_complete                2020-06-07 15:57:54.848296\nduration                             0 days 00:00:34.465662\nparams_apply_tfms                                     False\nparams_do_flip                                          NaN\nparams_epochs                                             9\nparams_flip_vert                                        NaN\nparams_learning_rate                            8.47479e-05\nparams_max_lighting                                     NaN\nparams_max_rotate                                       NaN\nparams_max_zoom                                         NaN\nparams_model                                    densenet121\nparams_mult                                             NaN\nparams_unfrozen_epochs                                    2\nparams_unfrozen_learning_rate                   4.31178e-07\nuser_attrs_execute_time                            0.837618\nsystem_attrs_completed_rung_0                           NaN\nsystem_attrs_completed_rung_1                           NaN\nsystem_attrs_fixed_params                               NaN\nstate                                              COMPLETE\nName: 96, dtype: object\n\n\nThis is a slightly artificial example but hopefully shows the possibility of logging user attributes which can then be accessed easily later without prematurely optimizing for something which may not be important."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#further-reading",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#further-reading",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Further reading",
    "text": "Further reading\nHopefully this post has been a helpful overview of Optuna with a somewhat realistic use case. I would recommend reading the Optuna docs which covers things in much more detail."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#references",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#references",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "References",
    "text": "References\n{{â€˜Auto ml [auto-ml, fastai blog(https://www.fast.ai/2018/07/16/auto-ml2/#auto-mlâ€™ | fndetail: 1 }}\n{{â€˜introducting ulmfit nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.htmlâ€™ | fndetail: 2}}\n{{â€˜Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta,and Masanori Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In KDD.â€™ | fndetail: 3 }}"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html",
    "href": "posts/post-with-code/2022-01-13-image_search.html",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "",
    "text": "tl;dr itâ€™s really easy to use the huggingface datasets library to create an image search application but it might not be suitable for sharing. update an updated version of this post is on the ðŸ¤— blog!\nWhen datasets was first launched it was more usually associated with text data and nlp. However, datasets has got support for images. In particular there is now a datasets feature type for images{% fn %}. In this blog post I try and play around with this new datatype, in combination with some other nice features of the library to make an image search app.\nTo start lets take a look at the image feature. We can use the wonderful rich libary to poke around python objects (functions, classes etc.)\nfrom rich import inspect\nfrom datasets.features import features\ninspect(features.Image, help=True)\n\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ &lt;class 'datasets.features.image.Image'&gt; â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ def Image(id: Union[str, NoneType] = None) -&gt; None:                              â”‚\nâ”‚                                                                                  â”‚\nâ”‚ Image feature to read image data from an image file.                             â”‚\nâ”‚                                                                                  â”‚\nâ”‚ Input: The Image feature accepts as input:                                       â”‚\nâ”‚ - A :obj:`str`: Absolute path to the image file (i.e. random access is allowed). â”‚\nâ”‚ - A :obj:`dict` with the keys:                                                   â”‚\nâ”‚                                                                                  â”‚\nâ”‚     - path: String with relative path of the image file to the archive file.     â”‚\nâ”‚     - bytes: Bytes of the image file.                                            â”‚\nâ”‚                                                                                  â”‚\nâ”‚   This is useful for archived files with sequential access.                      â”‚\nâ”‚                                                                                  â”‚\nâ”‚ - An :obj:`np.ndarray`: NumPy array representing an image.                       â”‚\nâ”‚ - A :obj:`PIL.Image.Image`: PIL image object.                                    â”‚\nâ”‚                                                                                  â”‚\nâ”‚   dtype = 'dict'                                                                 â”‚\nâ”‚      id = None                                                                   â”‚\nâ”‚ pa_type = None                                                                   â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\nWe can see there a few different ways in which we can pass in our images. Weâ€™ll come back to this in a little while.\nA really nice feature of the datasets library (beyond the functionality for processing data, memory mapping etc.) is that you get some nice things for free. One of these is the ability to add a faiss index. faiss is a â€œlibrary for efficient similarity search and clustering of dense vectorsâ€.\nThe datasets docs show and example of using faiss for text retrieval. What Iâ€™m curious about doing is using the faiss index to search for images. This can be super useful for a number of reasons but also comes with some potential issues."
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#the-dataset-digitised-books---images-identified-as-embellishments.-c.-1510---c.-1900.-jpg",
    "href": "posts/post-with-code/2022-01-13-image_search.html#the-dataset-digitised-books---images-identified-as-embellishments.-c.-1510---c.-1900.-jpg",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "The dataset: â€œDigitised Books - Images identified as Embellishments. c.Â 1510 - c.Â 1900. JPGâ€",
    "text": "The dataset: â€œDigitised Books - Images identified as Embellishments. c.Â 1510 - c.Â 1900. JPGâ€\nThis is a dataset of images which have been pulled from a collection of digitised books from the British Library. These images come from books across a wide time period and from a broad range of domains. These images were extracted using information in the OCR output for each book. As a result itâ€™s known which book the images came from but not necessarily anything else about that image i.e.Â what it is of.\nSome attempts to help overcome this have included uploading the images to flickr. This allows people to tag the images or put them into various different categories.\nThere have also been projects to tag the dataset using machine learning. This work already makes it possible to search by tags but we might want a â€˜richerâ€™ ability to search. For this particular experiment I will work with a subset of the collections which contain â€œembellishmentsâ€. This dataset is a bit smaller so will be better for experimenting with. We can get the data from the BL repository: https://doi.org/10.21250/db17\n\n# hide_output\n!aria2c -x8 -o dig19cbooks-embellishments.zip \"https://bl.iro.bl.uk/downloads/ba1d1d12-b1bd-4a43-9696-7b29b56cdd20?locale=en\"\n\n\n01/12 20:18:42 [NOTICE] Downloading 1 item(s)\n\n01/12 20:18:42 [NOTICE] Removed the defunct control file /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip.aria2 because the download file /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip doesn't exist.\n\n01/12 20:18:42 [NOTICE] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n *** Download Progress Summary as of Wed Jan 12 20:19:43 2022 ***              58s](98%)]m\n===============================================================================\n[#f38a5d 553MiB/43GiB(1%) CN:5 DL:36MiB ETA:19m55s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:20:44 2022 ***              46s]\n===============================================================================\n[#f38a5d 2.6GiB/43GiB(6%) CN:5 DL:37MiB ETA:18m38s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:21:45 2022 ***              m1s]m\n===============================================================================\n[#f38a5d 4.7GiB/43GiB(10%) CN:5 DL:36MiB ETA:18m8s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:22:45 2022 ***              m46s]\n===============================================================================\n[#f38a5d 6.7GiB/43GiB(15%) CN:5 DL:31MiB ETA:19m38s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:23:46 2022 ***              m23s]\n===============================================================================\n[#f38a5d 8.8GiB/43GiB(20%) CN:5 DL:35MiB ETA:16m42s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:24:46 2022 ***              2s]mm\n===============================================================================\n[#f38a5d 10GiB/43GiB(24%) CN:5 DL:28MiB ETA:19m59s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:25:47 2022 ***              28s]\n===============================================================================\n[#f38a5d 12GiB/43GiB(29%) CN:5 DL:36MiB ETA:14m19s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:26:47 2022 ***              43s]\n===============================================================================\n[#f38a5d 14GiB/43GiB(33%) CN:5 DL:35MiB ETA:13m57s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:27:48 2022 ***              36s]\n===============================================================================\n[#f38a5d 16GiB/43GiB(37%) CN:5 DL:33MiB ETA:13m51s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:28:49 2022 ***              32s]\n===============================================================================\n[#f38a5d 18GiB/43GiB(42%) CN:5 DL:37MiB ETA:11m31s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:29:49 2022 ***              14s]\n===============================================================================\n[#f38a5d 20GiB/43GiB(46%) CN:5 DL:32MiB ETA:12m2s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:30:50 2022 ***              6s]m\n===============================================================================\n[#f38a5d 22GiB/43GiB(51%) CN:5 DL:36MiB ETA:9m55s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:31:51 2022 ***              9s]m\n===============================================================================\n[#f38a5d 24GiB/43GiB(56%) CN:5 DL:37MiB ETA:8m38s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:32:51 2022 ***              4s]\n===============================================================================\n[#f38a5d 26GiB/43GiB(61%) CN:5 DL:36MiB ETA:7m43s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:33:52 2022 ***              0s]\n===============================================================================\n[#f38a5d 28GiB/43GiB(66%) CN:5 DL:37MiB ETA:6m38s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:34:52 2022 ***              8s]\n===============================================================================\n[#f38a5d 31GiB/43GiB(71%) CN:5 DL:36MiB ETA:5m47s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:35:53 2022 ***              s]m\n===============================================================================\n[#f38a5d 32GiB/43GiB(75%) CN:5 DL:17MiB ETA:10m7s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:36:53 2022 ***              1s]m\n===============================================================================\n[#f38a5d 34GiB/43GiB(79%) CN:5 DL:27MiB ETA:5m31s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:37:54 2022 ***              4s]\n===============================================================================\n[#f38a5d 35GiB/43GiB(82%) CN:5 DL:29MiB ETA:4m22s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:38:55 2022 ***              5s]\n===============================================================================\n[#f38a5d 37GiB/43GiB(86%) CN:5 DL:26MiB ETA:3m36s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:39:55 2022 ***              5s]\n===============================================================================\n[#f38a5d 39GiB/43GiB(91%) CN:5 DL:36MiB ETA:1m44s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n *** Download Progress Summary as of Wed Jan 12 20:40:56 2022 ***              ]mm\n===============================================================================\n[#f38a5d 41GiB/43GiB(95%) CN:5 DL:34MiB ETA:52s]\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n-------------------------------------------------------------------------------\n\n[#f38a5d 43GiB/43GiB(99%) CN:2 DL:35MiB]0m]m\n01/12 20:41:49 [NOTICE] Download complete: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\nDownload Results:\ngid   |stat|avg speed  |path/URI\n======+====+===========+=======================================================\nf38a5d|OK  |    33MiB/s|/Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\nStatus Legend:\n(OK):download completed.\n\n\n\n!unzip -q dig19cbooks-embellishments.zip"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#install-required-packages",
    "href": "posts/post-with-code/2022-01-13-image_search.html#install-required-packages",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "Install required packages",
    "text": "Install required packages\nThere are a few packages weâ€™ll need for this work. To start with weâ€™ll need the datasets library.\n\n# hide output\nimport sys\n!{sys.executable} -m pip install datasets \n\nNow we have the data downloaded weâ€™ll try and load it into datasets. There are various ways of doing this. To start with we can grab all of the files we need.\n\nfrom pathlib import Path\n\n\nfiles = list(Path('embellishments/').rglob(\"*.jpg\"))\n\nSince the file path encodes the year of publication for the book the image came from letâ€™s create a function to grab that.\n\ndef get_parts(f:Path):\n    _,year,fname =  f.parts\n    return year, fname"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#loading-the-images",
    "href": "posts/post-with-code/2022-01-13-image_search.html#loading-the-images",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "ðŸ“¸ Loading the images",
    "text": "ðŸ“¸ Loading the images\nThe images are fairly large, since this is an experiment weâ€™ll resize them a little using the thumbnail method (this makes sure we keep the same aspect ratio for our images)\n\nfrom PIL import Image\nimport io\n\n\ndef load_image(path):\n    with open(path, 'rb') as f:\n        im = Image.open(io.BytesIO(f.read()))\n        im.thumbnail((224,224))\n    return im \n\n\nim = load_image(files[0])\nim\n\n\n\n\n\n\n\n\n\nWhere is the image ðŸ¤”\nYou may have noticed that the load_image function doesnâ€™t load the filepath into pillow directly. Often we would do Image.open(filepath.jpg). This is done deliberately. If we load it this way when we inspect the resulting image youâ€™ll see that the filepath attribute is empty.\n\n#collapse_output\ninspect(im)\n\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt; â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚\nâ”‚ â”‚ &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=200x224 at 0x7FBBB392D040&gt;     â”‚ â”‚\nâ”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚\nâ”‚                                                                                           â”‚\nâ”‚                app = {'APP0': b'JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00'}            â”‚\nâ”‚            applist = [('APP0', b'JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00')]          â”‚\nâ”‚               bits = 8                                                                    â”‚\nâ”‚    custom_mimetype = None                                                                 â”‚\nâ”‚      decoderconfig = (2, 0)                                                               â”‚\nâ”‚    decodermaxblock = 65536                                                                â”‚\nâ”‚      encoderconfig = (False, -1, -1, b'')                                                 â”‚\nâ”‚        encoderinfo = {}                                                                   â”‚\nâ”‚           filename = ''                                                                   â”‚\nâ”‚             format = 'JPEG'                                                               â”‚\nâ”‚ format_description = 'JPEG (ISO 10918)'                                                   â”‚\nâ”‚                 fp = None                                                                 â”‚\nâ”‚             height = 224                                                                  â”‚\nâ”‚         huffman_ac = {}                                                                   â”‚\nâ”‚         huffman_dc = {}                                                                   â”‚\nâ”‚            icclist = []                                                                   â”‚\nâ”‚                 im = &lt;ImagingCore object at 0x7fbba120dc10&gt;                               â”‚\nâ”‚               info = {                                                                    â”‚\nâ”‚                          'jfif': 257,                                                     â”‚\nâ”‚                          'jfif_version': (1, 1),                                          â”‚\nâ”‚                          'jfif_unit': 0,                                                  â”‚\nâ”‚                          'jfif_density': (1, 1)                                           â”‚\nâ”‚                      }                                                                    â”‚\nâ”‚              layer = [(1, 2, 2, 0), (2, 1, 1, 1), (3, 1, 1, 1)]                           â”‚\nâ”‚             layers = 3                                                                    â”‚\nâ”‚                map = None                                                                 â”‚\nâ”‚               mode = 'RGB'                                                                â”‚\nâ”‚            palette = None                                                                 â”‚\nâ”‚           pyaccess = None                                                                 â”‚\nâ”‚       quantization = {                                                                    â”‚\nâ”‚                          0: [                                                             â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              1,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              6,                                                           â”‚\nâ”‚                              7,                                                           â”‚\nâ”‚                              9,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              4,                                                           â”‚\nâ”‚                              8,                                                           â”‚\nâ”‚                              8,                                                           â”‚\nâ”‚                              8,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              6,                                                           â”‚\nâ”‚                              8,                                                           â”‚\nâ”‚                              10,                                                          â”‚\nâ”‚                              8,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              4,                                                           â”‚\nâ”‚                              7,                                                           â”‚\nâ”‚                              12,                                                          â”‚\nâ”‚                              11,                                                          â”‚\nâ”‚                              9,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              5,                                                           â”‚\nâ”‚                              8,                                                           â”‚\nâ”‚                              10,                                                          â”‚\nâ”‚                              15,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              11,                                                          â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              5,                                                           â”‚\nâ”‚                              8,                                                           â”‚\nâ”‚                              9,                                                           â”‚\nâ”‚                              11,                                                          â”‚\nâ”‚                              15,                                                          â”‚\nâ”‚                              16,                                                          â”‚\nâ”‚                              13,                                                          â”‚\nâ”‚                              7,                                                           â”‚\nâ”‚                              9,                                                           â”‚\nâ”‚                              11,                                                          â”‚\nâ”‚                              12,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              17,                                                          â”‚\nâ”‚                              17,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              10,                                                          â”‚\nâ”‚                              13,                                                          â”‚\nâ”‚                              13,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              16,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14                                                           â”‚\nâ”‚                          ],                                                               â”‚\nâ”‚                          1: [                                                             â”‚\nâ”‚                              2,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              7,                                                           â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              4,                                                           â”‚\nâ”‚                              9,                                                           â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              3,                                                           â”‚\nâ”‚                              4,                                                           â”‚\nâ”‚                              8,                                                           â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              7,                                                           â”‚\nâ”‚                              9,                                                           â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14,                                                          â”‚\nâ”‚                              14                                                           â”‚\nâ”‚                          ]                                                                â”‚\nâ”‚                      }                                                                    â”‚\nâ”‚           readonly = 0                                                                    â”‚\nâ”‚               size = (200, 224)                                                           â”‚\nâ”‚               tile = []                                                                   â”‚\nâ”‚              width = 200                                                                  â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\n\n\nYou can also directly see this\n\nim.filename\n\n''\n\n\nPillow usually loads images in a lazy way i.e.Â it only opens them when they are needed. The filepath is used to access the image. We can see the filename attribute is present if we open it from the filepath\n\nim_file = Image.open(files[0])\nim_file.filename\n\n'/Users/dvanstrien/Documents/daniel/blog/_notebooks/embellishments/1855/000811462_05_000205_1_The Pictorial History of England  being a history of the people  as well as a hi_1855.jpg'\n\n\nThe reason I donâ€™t want the filename attribute present here is because not only do I want to use datasets to process our images but also store the images. If we pass a Pillow object with the filename attribute datasets will also use this for loading the images. This is often what weâ€™d want but we donâ€™t want this here for reasons weâ€™ll see shortly.\n\n\nPreparing images for datasets\nWe can now load our images. What weâ€™ll do is is loop through all our images and then load the information for each image into a dictionary.\n\nfrom collections import defaultdict\n\n\ndata = defaultdict(list)\n\n\nfrom tqdm import tqdm\n\n\nfor file in tqdm(files):\n    try:\n        #load_image(file)\n        year, fname = get_parts(file)\n        data['fname'].append(fname)\n        data['year'].append(year)\n        data['path'].append(str(file))\n    except:\n        Image.UnidentifiedImageError\n        pass\n        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416944/416944 [00:05&lt;00:00, 77169.45it/s]\n\n\nWe can now load the from_dict method to create a new dataset.\n\nfrom datasets import Dataset\n\n\ndataset = Dataset.from_dict(data)\n\nWe can look at one example to see what this looks like.\n\ndataset[0]\n\n{'fname': '000811462_05_000205_1_The Pictorial History of England  being a history of the people  as well as a hi_1855.jpg',\n 'year': '1855',\n 'path': 'embellishments/1855/000811462_05_000205_1_The Pictorial History of England  being a history of the people  as well as a hi_1855.jpg'}\n\n\n\n\nLoading our images\nAt the moment our dataset has the filename and full path for each image. However, we want to have an actual image loaded into our dataset. We already have a load_image function. This gets us most of the way there but we might also want to add some ability to deal with image errors. The datasets library has gained increased uspport for handling None types- this includes support for None types for images see pull request 3195.\nWeâ€™ll wrap our load_image function in a try block, catch a Image.UnidentifiedImageError error and return None if we canâ€™t load the image.\n\ndef try_load_image(filename):\n    try:\n        image = load_image(filename)\n        if isinstance(image, Image.Image):\n            return image\n    except Image.UnidentifiedImageError:\n        return None\n\n\n%%time\ndataset = dataset.map(lambda example: {\"img\": try_load_image(example['path'])},writer_batch_size=50)\n\n\n\n\nCPU times: user 51min 42s, sys: 4min 31s, total: 56min 13s\nWall time: 1h 10min 31s\n\n\nLetâ€™s see what this looks like\n\ndataset\n\nDataset({\n    features: ['fname', 'year', 'path', 'img'],\n    num_rows: 416944\n})\n\n\nWe have an image column, letâ€™s check the type of all our features\n\ndataset.features\n\n{'fname': Value(dtype='string', id=None),\n 'year': Value(dtype='string', id=None),\n 'path': Value(dtype='string', id=None),\n 'img': Image(id=None)}\n\n\nThis is looking great already. Since we might have some None types for images letâ€™s get rid of these.\n\ndataset = dataset.filter(lambda example: example['img'] is not None)\n\n\n\n\n\ndataset\n\nDataset({\n    features: ['fname', 'year', 'path', 'img'],\n    num_rows: 416935\n})\n\n\nYouâ€™ll see we lost a few rows by doing this filtering. We should now just have images which are successfully loaded.\nIf we access an example and index into the img column weâ€™ll see our image ðŸ˜ƒ\n\ndataset[10]['img']"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#push-all-the-things-to-the-hub",
    "href": "posts/post-with-code/2022-01-13-image_search.html#push-all-the-things-to-the-hub",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "Push all the things to the hub!",
    "text": "Push all the things to the hub!\n\n\n\nPush all the things to the hub!\n\n\nOne of the super awesome things about the huggingface ecosystem is the huggingface hub. We can use the hub to access models and datasets. Often this is used for sharing work with others but it can also be a useful tool for work in progress. The datasets library recently added a push_to_hub method that allows you to push a dataset to the hub with minimal fuss. This can be really helpful by allowing you to pass around a dataset with all the transformers etc. already done.\nWhen I started playing around with this feature I was also keen to see if it could be used as a way of â€˜bundlingâ€™ everything together. This is where I noticed that if you push a dataset containing images which have been loaded in from filepaths by pillow the version on the hub wonâ€™t have the images attached. If you always have the image files in the same place when you work with the dataset then this doesnâ€™t matter. If you want to have the images stored in the parquet file(s) associated with the dataset we need to load it without the filename attribute present (there might be another way of ensuring that datasets doesnâ€™t rely on the image file being on the file system â€“ if you of this Iâ€™d love to hear about it).\nSince we loaded our images this way when we download the dataset from the hub onto a different machine we have the images already there ðŸ¤—\nFor now weâ€™ll push the dataset to the hub and keep them private initially.\n\ndataset.push_to_hub('davanstrien/embellishments', private=True)\n\nThe repository already exists: the `private` keyword argument will be ignored.\n\n\n\n\n\n\n\n\n\nSwitching machines\nAt this point Iâ€™ve created a dataset and moved it to the huggingface hub. This means it is possible to pickup the work/dataset elsewhere.\nIn this particular example, having access to a GPU is important. So the next parts of this notebook are run on Colab instead of locally on my laptop.\nWeâ€™ll need to login since the dataset is currently private.\n\n!huggingface-cli login\n\n\n        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n        \nToken: \nLogin successful\nYour token has been saved to /root/.huggingface/token\nAuthenticated through git-credential store but this isn't the helper defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n\ngit config --global credential.helper store\n\n\nOnce weâ€™ve done this we can load our dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"davanstrien/embellishments\", use_auth_token=True)\n\n\n\n\nUsing custom data configuration davanstrien--embellishments-543da8e15e8f0242\n\n\nDownloading and preparing dataset None/None (download: 2.38 GiB, generated: 2.50 GiB, post-processed: Unknown size, total: 4.88 GiB) to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-543da8e15e8f0242/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-543da8e15e8f0242/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121. Subsequent calls will reuse this data."
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#creating-embeddings",
    "href": "posts/post-with-code/2022-01-13-image_search.html#creating-embeddings",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "Creating embeddings ðŸ•¸",
    "text": "Creating embeddings ðŸ•¸\nWe now have a dataset with a bunch of images in it. To begin creating our image search app we need to create some embeddings for these images. There are various ways in which we can try and do this but one possible way is to use the clip models via the sentence_transformers library. The clip model from OpenAI learns a joint representation for both images and text which is very useful for what we want to do since we want to be able to input text and get back an image. We can download the model using the SentenceTransformer class.\n\nfrom sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('clip-ViT-B-32')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n\n\nThis model will encode either an image or some text returning an embedding. We can use the map method to encode all our images.\n\nds_with_embeddings = dataset.map(\n    lambda example: {'embeddings':model.encode(example['img'],device='cuda')},\n                                 batch_size=32)\n\n\n\n\nWe can â€œsaveâ€ our work by pushing back to the hub\n\nds_with_embeddings.push_to_hub('davanstrien/embellishments', private=True)\n\nPushing split train to the Hub.\nThe repository already exists: the `private` keyword argument will be ignored.\n\n\n\n\n\n\n\n\nIf we were to move to a different machine we could grab our work again by loading it from the hub ðŸ˜ƒ\n\nfrom datasets import load_dataset\n\nds_with_embeddings = load_dataset(\"davanstrien/embellishments\", use_auth_token=True)\n\n\n\n\nUsing custom data configuration davanstrien--embellishments-c2c1f142f272db02\n\n\nDownloading and preparing dataset None/None (download: 3.19 GiB, generated: 3.30 GiB, post-processed: Unknown size, total: 6.49 GiB) to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-c2c1f142f272db02/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-c2c1f142f272db02/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121. Subsequent calls will reuse this data.\n\n\n\n\n\nWe now have a new column which contains the embeddings for our images. We could manually search through these and compare it to some input embedding but datasets has an add_faiss_index method. This uses the faiss library to create an efficient index for searching embeddings. For more background on this library you can watch this youtube video\n\n\n\nds_with_embeddings['train'].add_faiss_index(column='embeddings')\n\n\n\n\nDataset({\n    features: ['fname', 'year', 'path', 'img', 'embeddings'],\n    num_rows: 416935\n})"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#image-search",
    "href": "posts/post-with-code/2022-01-13-image_search.html#image-search",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "Image search",
    "text": "Image search\nWe now have everything we need to create a simple image search. We can use the same model we used to encode our images to encode some input text. This will act as the prompt we try and find close examples for. Letâ€™s start with â€˜a steam engineâ€™.\n\nprompt = model.encode(\"A steam engine\")\n\nWe can see what this looks like\n\n#collapse_output\nprompt\n\narray([-4.68399227e-02, -1.25237599e-01,  1.25164926e-01,  1.23583399e-01,\n        5.36684394e-02, -2.80560672e-01,  2.89631691e-02, -9.63450074e-01,\n       -1.52872965e-01, -3.83016393e-02,  9.01967064e-02, -5.84575422e-02,\n        1.04646191e-01,  2.44443744e-01,  1.38233244e-01, -3.97525132e-02,\n        4.35137331e-01, -4.26820181e-02, -8.48560631e-02, -6.94137365e-02,\n        6.25562131e-01,  3.68572891e-01,  3.34365219e-01, -3.37864846e-01,\n       -2.53632635e-01, -3.01467925e-01, -1.56484097e-01,  2.94869483e-01,\n       -1.89204350e-01, -1.13111593e-01, -1.46938376e-02,  2.97405511e-01,\n       -2.92487741e-01,  3.56931627e-01,  1.44009173e-01,  7.53008351e-02,\n       -1.02462962e-01,  2.26309776e-01, -3.77506733e-01,  4.75439876e-02,\n       -8.52131844e-03,  7.40285963e-03, -1.36876494e-01,  1.12041593e-01,\n        3.65501434e-01, -9.37360153e-02,  1.00782610e-01, -3.86462212e-01,\n       -1.39045209e-01, -2.31989667e-01, -2.62126565e-01,  8.75059143e-02,\n       -9.87479314e-02,  7.31039718e-02, -5.99793851e-01, -5.31058311e-01,\n        1.46116272e-01,  1.58094600e-01, -5.31955510e-02,  1.91384852e-01,\n        1.16943717e-01, -4.84316409e-01, -1.38332695e-01,  1.76510841e-01,\n       -2.17938051e-01, -1.00890748e-01, -4.45051998e-01,  2.71521568e-01,\n       -1.12926617e-01, -3.37198257e-01, -1.34169117e-01, -7.38745630e-02,\n       -1.23398125e-01,  3.62316787e-01,  9.09636840e-02, -3.20305794e-01,\n        5.82561374e-01, -3.51719618e-01, -1.05368085e-02, -3.90766770e-01,\n       -3.18382740e-01,  5.37567735e-02, -6.24650240e-01,  2.18755245e-01,\n        3.51645321e-01, -3.01214606e-02, -8.49011913e-02, -3.29971045e-01,\n        2.13861912e-01, -1.10820271e-02, -3.52595486e-02, -3.70746814e-02,\n       -1.35805202e+00,  3.35692495e-01, -2.83742435e-02, -1.39813796e-01,\n        3.66676860e-02,  2.62957454e-01,  2.52151459e-01, -6.14355244e-02,\n        2.01516539e-01, -4.14117992e-01, -2.58466527e-02,  1.06067717e-01,\n        3.14981639e-02, -1.45749748e-02, -5.94865866e-02,  2.55122900e-01,\n       -3.30369681e-01,  6.39781356e-04,  1.65513411e-01,  7.37893358e-02,\n       -4.69729975e-02,  3.36943477e-01,  4.38236594e-02, -4.21047479e-01,\n       -1.14590853e-01,  1.49240956e-01,  1.34405270e-01,  3.97198983e-02,\n       -1.20852023e-01, -7.22009778e-01,  1.17442548e-01, -7.35135227e-02,\n        5.45979321e-01,  1.76602621e-02,  6.59747049e-02,  8.00846070e-02,\n        3.87920737e-01, -3.57501693e-02,  1.19425125e-01, -2.89906412e-01,\n       -2.84183323e-02,  5.73142338e+00,  1.24172762e-01, -1.59575850e-01,\n       -5.33452034e-02, -1.77120879e-01,  2.14188576e-01, -3.49292234e-02,\n       -4.76958305e-02, -1.05941862e-01, -1.58911452e-01,  1.87136307e-02,\n       -2.16531213e-02,  1.37230158e-01,  4.62583750e-02,  2.19857365e-01,\n        3.41235586e-02, -3.29913348e-02,  9.88523886e-02, -1.30611554e-01,\n       -1.53349772e-01,  2.20886514e-01,  1.53534949e-01, -4.27889526e-01,\n       -4.12531018e-01,  2.70397663e-01,  1.88448757e-01,  4.66853082e-02,\n        2.63707846e-01, -9.56512764e-02, -3.26435685e-01, -1.24463499e-01,\n        4.49354291e-01, -4.17843968e-01, -5.27932420e-02, -1.28314078e-01,\n       -1.19249836e-01, -1.19294032e-01,  3.73742878e-01,  2.07954675e-01,\n       -1.41953439e-01,  3.89361024e-01, -1.99988037e-01,  3.62350583e-01,\n       -8.77851099e-02, -1.08132876e-01, -9.82177258e-03,  1.80039972e-01,\n        1.35815665e-02,  3.20201695e-01, -1.74580999e-02, -1.08204901e-01,\n       -2.29793668e-01, -2.09628209e-01,  4.13929313e-01, -1.73814282e-01,\n       -4.10574347e-01, -1.59104809e-01, -6.01581074e-02,  6.22577034e-02,\n       -3.67693931e-01,  1.85215116e-01, -2.03229636e-01, -8.92911255e-02,\n       -4.25831258e-01, -1.45366028e-01,  2.45514482e-01, -1.65927559e-01,\n       -2.54413635e-02, -2.91361034e-01, -8.33243579e-02, -4.79405448e-02,\n        6.35769814e-02,  8.04642588e-02,  5.31384498e-02,  2.50850171e-02,\n       -8.98692310e-02,  4.97757077e-01,  6.37893498e-01, -2.58815974e-01,\n        4.14507166e-02,  9.45882648e-02, -9.01474580e-02, -9.18833911e-02,\n       -2.48883665e-01,  9.16991904e-02, -2.93194801e-01, -1.49350330e-01,\n        7.20755905e-02, -9.76985693e-03, -4.70465049e-02, -2.78597653e-01,\n       -7.63949528e-02, -3.14843357e-01,  3.18657011e-01, -3.06758255e-01,\n       -2.06573829e-01, -2.20574200e-01,  1.81351285e-02,  2.57636189e-01,\n        2.39799708e-01, -2.31798366e-01, -8.34087562e-03,  6.13241374e-01,\n       -2.10393399e-01,  2.52263397e-01,  1.66839644e-01, -2.71174073e-01,\n        2.31348664e-01,  1.15150154e-01,  2.23357946e-01,  1.37287825e-01,\n       -8.56669843e-02,  3.43877286e-01, -1.09687179e-01,  3.24211985e-01,\n       -4.53893900e-01, -2.30711773e-01, -2.48840563e-02,  1.80964172e-01,\n        4.73472506e-01,  5.22104502e-01,  9.96741354e-02,  1.87694326e-01,\n        2.41730541e-01, -2.78556377e-01,  7.48419687e-02,  2.80560136e-01,\n       -1.25464931e-01,  1.51028201e-01,  1.39490321e-01,  5.16689643e-02,\n        5.30310348e-02,  1.61938250e-01,  3.72225225e-01, -4.49403644e-01,\n        1.19608052e-01,  2.43661910e-01,  9.89501849e-02,  2.74168640e-01,\n        4.84039634e-02, -1.19901955e-01, -1.57916725e-01, -2.20868304e-01,\n        1.03498720e-01,  3.99750322e-01,  1.03758566e-01,  8.08660090e-02,\n        1.68566346e-01, -3.42532575e-01,  2.51480471e-02,  1.23976640e-01,\n       -2.10433707e-01,  2.81242996e-01,  2.39082754e-01,  2.01786831e-02,\n        4.61297363e-01,  5.62884361e-02,  2.15039015e-01, -1.65275872e-01,\n        1.01690084e-01, -4.50959802e-03, -4.46137577e-01,  4.31368239e-02,\n       -4.51804757e-01, -2.26415813e-01,  1.31732523e-01, -2.00945437e-02,\n        1.77461311e-01, -1.64631978e-02,  4.40553159e-01,  1.41214132e-01,\n        3.42677176e-01, -2.23303795e-01, -2.10693538e-01,  1.94943929e-03,\n       -2.33348235e-01,  4.64889407e-03,  5.71020804e-02,  1.99669391e-01,\n        5.72273111e+00, -2.95036316e-01, -5.13455391e-01,  1.87334672e-01,\n        4.09545094e-01, -7.09135592e-01,  1.89325869e-01, -6.14660345e-02,\n        3.29098284e-01,  2.82059342e-01,  3.48631829e-01, -9.74263549e-02,\n       -4.83064592e-01, -1.35906041e-04,  3.44773471e-01, -3.56532484e-01,\n        5.36619090e-02, -1.85481656e+00,  3.87955368e-01, -1.83132842e-01,\n       -1.34021699e-01, -1.84214741e-01,  6.85371086e-02,  1.10808179e-01,\n       -6.64586425e-02,  6.85550272e-02,  1.81145087e-01, -2.15605676e-01,\n       -1.09192222e-01, -7.09795505e-02,  1.77813157e-01, -2.76037157e-01,\n        2.19184965e-01, -3.35977226e-01,  1.01434961e-01,  4.24576849e-02,\n        6.37579709e-04, -1.23296835e-01, -6.84914351e-01,  5.02923191e-01,\n        2.19384342e-01,  4.92008686e-01, -1.94621727e-01, -2.48740703e-01,\n       -1.32586688e-01, -1.77171156e-02, -4.71081585e-03,  1.58246011e-01,\n       -3.27363521e-01, -3.30681592e-01, -2.68038437e-02, -1.85811728e-01,\n       -1.84623767e-02, -3.22798610e-01,  3.07092518e-01,  1.06014945e-01,\n        3.20541680e-01, -2.55453944e-01, -2.30755419e-01, -1.19963072e-01,\n       -2.04865620e-01,  4.02828932e-01, -3.01321566e-01,  4.01021272e-01,\n       -3.02002877e-01,  1.42853945e-01,  2.94484437e-01, -2.06042349e-01,\n       -3.03069353e-01, -2.83185482e-01, -1.03388466e-02, -1.03018671e-01,\n        4.25990820e-02, -2.94244856e-01,  3.19168091e-01,  3.89839858e-02,\n       -1.95185751e-01, -9.88216847e-02, -4.01682496e-01,  4.60841119e-01,\n        1.40236557e-01,  1.49914265e-01, -4.25037295e-01,  2.63067722e-01,\n        1.31706342e-01,  3.21884871e-01, -2.39963964e-01,  4.01636630e-01,\n       -2.55293436e-02, -7.36447945e-02, -8.34826380e-03,  1.11923724e-01,\n       -2.71807779e-02, -3.35412771e-02,  2.33933121e-01,  3.33954431e-02,\n        3.56481314e-01, -8.09433609e-02, -1.82573602e-01,  1.75429478e-01,\n       -3.23554099e-01,  9.15928558e-03,  1.54344559e-01,  2.50909716e-01,\n        1.45193070e-01,  2.48686507e-01, -9.65276286e-02, -2.73654372e-01,\n        5.46456315e-02,  1.83476061e-02, -1.61773548e-01, -2.97708124e-01,\n       -1.74462572e-01, -1.14246726e-01,  2.32043359e-02,  1.98346555e-01,\n        2.31929243e-01, -9.74937603e-02, -2.26448864e-01, -6.31427839e-02,\n        2.23113708e-02, -3.72859359e-01,  2.47197479e-01, -3.65516663e-01,\n       -3.24409932e-01,  1.83964625e-01, -3.17104161e-03, -2.66632497e-01,\n       -1.86478943e-01,  1.11006252e-01, -3.93829793e-02, -3.11926544e-01,\n        2.88751245e-01,  2.66543150e-01, -1.74334750e-01, -4.89967108e-01,\n        3.38638097e-01,  2.47487854e-02, -3.66539627e-01,  5.78703731e-03,\n        1.11349493e-01, -2.60909855e-01, -4.34429348e-02, -4.47440267e-01,\n        2.80311018e-01, -6.46181554e-02, -2.93976814e-02, -3.02857161e-01,\n        2.10391358e-03, -3.70345414e-02,  7.15476647e-02,  4.39802915e-01,\n        2.11817563e-01,  6.87709302e-02,  5.68117499e-01, -2.40518659e-01,\n        2.59056687e-01, -1.32284269e-01,  1.28509507e-01, -1.94875181e-01,\n       -2.68568173e-02, -7.85035193e-02, -2.49556839e-01,  1.44016743e-01,\n       -2.98127495e-02, -1.41643599e-01,  1.77106410e-02,  1.83453292e-01,\n       -1.39113069e-02, -1.97993904e-01,  3.07995021e-01,  3.31339300e-01,\n        2.07652867e-01,  1.27762616e-01,  2.26422980e-01,  1.94940835e-01,\n       -4.90801185e-02, -5.35061479e-01, -2.99495637e-01,  3.68627608e-02,\n       -4.15636569e-01,  6.44698918e-01, -4.50457260e-02,  7.05935210e-02,\n       -1.11036956e-01, -1.42384216e-01, -7.05560222e-02,  2.86495592e-03,\n        3.45641613e-01, -5.66974521e-01, -1.34682715e-01, -2.59017110e-01,\n        3.27597320e-01,  1.04890786e-01, -3.11988890e-01, -2.32627541e-01,\n        3.14653963e-02,  2.76591361e-01,  1.66302443e-01, -2.39517853e-01],\n      dtype=float32)\n\n\nWe can use another method from the datasets library get_nearest_examples to get images which have an embedding close to our input prompt embedding. We can pass in a number of results we want to get back.\n\nscores, retrieved_examples = ds_with_embeddings['train'].get_nearest_examples('embeddings', prompt,k=9)\n\nWe can index into the first example this retrieves:\n\nretrieved_examples['img'][0]\n\n\n\n\n\n\n\n\nThis isnâ€™t quite a steam engine but itâ€™s also not a completely weird result. We can plot the other results to see what was returned.\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(20, 20))\ncolumns = 3\nfor i in range(9):\n    image = retrieved_examples['img'][i]\n    plt.subplot(9 / columns + 1, columns, i + 1)\n    plt.imshow(image)\n\n\n\n\n\n\n\n\nSome of these results look fairly close to our input prompt. We can wrap this in a function so can more easily play around with different prompts\n\ndef get_image_from_text(text_prompt, number_to_retrieve=9):\n    prompt = model.encode(text_prompt)\n    scores, retrieved_examples = ds_with_embeddings['train'].get_nearest_examples('embeddings', prompt,k=number_to_retrieve)\n    plt.figure(figsize=(20, 20))\n    columns = 3\n    for i in range(9):\n        image = retrieved_examples['img'][i]\n        plt.title(text_prompt)\n        plt.subplot(9 / columns + 1, columns, i + 1)\n        plt.imshow(image)\n\n\nget_image_from_text(\"An illustration of the sun behind a mountain\")\n\n\n\n\n\n\n\n\n\nTrying a bunch of prompts âœ¨\nNow we have a function for getting a few results we can try a bunch of different prompts:\n\nFor some of these Iâ€™ll choose prompts which are a broad â€˜categoryâ€™ i.e.Â â€˜a musical instrumentâ€™ or â€˜an animalâ€™, others are specific i.e.Â â€˜a guitarâ€™.\nOut of interest I also tried a boolean operator: â€œAn illustration of a cat or a dogâ€.\nFinally I tried something a little more abstract: â€œan empty abyssâ€\n\n\nprompts = [\"A musical instrument\", \"A guitar\", \"An animal\", \"An illustration of a cat or a dog\", \"an empty abyss\"]\n\n\nfor prompt in prompts:\n    get_image_from_text(prompt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see these results arenâ€™t always right but they are usually some reasonable results in there. It already seems like this could be useful for searching for a the semantic content of an image in this dataset. However we might hold off on sharing this as isâ€¦"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#creating-a-huggingface-space",
    "href": "posts/post-with-code/2022-01-13-image_search.html#creating-a-huggingface-space",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "Creating a huggingface space? ðŸ¤·ðŸ¼",
    "text": "Creating a huggingface space? ðŸ¤·ðŸ¼\nOne obvious next step for this kind of project is to create a hugginface spaces demo. This is what Iâ€™ve done for other models\nIt was a fairly simple process to get a Gradio app setup from the point we got to here. Here is a screenshot of this app.\n\nHowever, Iâ€™m a little bit vary about making this public straightaway. Looking at the model card for the CLIP model we can look at the primary intended uses:\n\nPrimary intended uses\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models. source\n\nThis is fairly close to what we are interested in here. Particularly we might be interested in how well the model deals with the kinds of images in our dataset (illustrations from mostly 19th century books). The images in our dataset are (probably) fairly different from the training data. The fact that some of the images also contain text might help CLIP since it displays some OCR ability.\nHowever, looking at the out-of-scope use cases in the model card:\n\nOut-of-Scope Use Cases\nAny deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIPâ€™s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. source\n\nsuggests that â€˜deploymentâ€™ is not a good idea. Whilst the results I got are interesting I havenâ€™t played around with the model enough yet (and havenâ€™t done anything more systematic to evaluate its performance and biases). Another additional consideration is the target dataset itself. The images are drawn from books covering a variety of subjects and time periods. There are plenty of books which represent colonial attitudes and as a result some of the images included may represent certain groups of people in a negative way. This could potentially be a bad combo with a tool which allows any arbitrary text input to be encoded as a prompt.\nThere may be ways around this issue but this will require a bit more thought."
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#conclusion",
    "href": "posts/post-with-code/2022-01-13-image_search.html#conclusion",
    "title": "Using ðŸ¤— datasets for image search",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough I donâ€™t have a nice demo to show for it I did get to work out a few more details of how datasets handles images. Iâ€™ve already used it to train some classification models and everything seems to be working smoothly. The ability to push images around on the hub will be super useful for many use cases too.\nI plan to spend a bit more time thinking about whether there is a better way of sharing a clip powered image search for the BL book images or notâ€¦\n{{ â€œIf you arenâ€™t familiar with datasets. A feature represents the datatype for different data you can have inside a dataset. For example you my have int32, timestamps and strings. You can read more about how features work in the docsâ€ | fndetail: 1}}"
  },
  {
    "objectID": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html",
    "href": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html",
    "title": "Exploring language metadata for datasets on the Hugging Face Hub",
    "section": "",
    "text": "%pip install huggingface_hub backoff wordcloud tabulate toolz matplotlib\n\nRequirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.15.1)\nRequirement already satisfied: backoff in ./.venv/lib/python3.11/site-packages (2.2.1)\nRequirement already satisfied: wordcloud in ./.venv/lib/python3.11/site-packages (1.9.2)\nRequirement already satisfied: tabulate in ./.venv/lib/python3.11/site-packages (0.9.0)\nRequirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (3.12.2)\nRequirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2023.6.0)\nRequirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.65.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.7.1)\nRequirement already satisfied: packaging&gt;=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (23.1)\nRequirement already satisfied: numpy&gt;=1.6.1 in ./.venv/lib/python3.11/site-packages (from wordcloud) (1.25.0)\nRequirement already satisfied: pillow in ./.venv/lib/python3.11/site-packages (from wordcloud) (10.0.0)\nRequirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (from wordcloud) (3.7.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (4.40.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (1.4.4)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (3.1.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (2.8.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in ./.venv/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (3.1.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in ./.venv/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (2.0.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (2023.5.7)\nRequirement already satisfied: six&gt;=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;wordcloud) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nfrom huggingface_hub import list_datasets\nfrom toolz import valmap, countby, groupby, topk, valmap\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#load-datasets",
    "href": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#load-datasets",
    "title": "Exploring language metadata for datasets on the Hugging Face Hub",
    "section": "Load datasets",
    "text": "Load datasets\n\ndatasets = list(iter(list_datasets(full=True, sort=\"downloads\", direction=-1)))\n\n\ndef get_lang(dataset):\n    card_data = dataset.cardData\n    if card_data:\n        lang = card_data.get(\"language\")\n        if lang is None:\n            return False\n        if len(lang) &gt;= 1:\n            return True\n    if not card_data:\n        return \"No card data\"\n\n\nhas_lang = groupby(get_lang, datasets)\n\n\nhas_lang.keys()\n\ndict_keys([True, 'No card data', False, None])\n\n\n\nhas_language_freqs = countby(get_lang, datasets)\nhas_language_percents = valmap(\n    lambda x: round(x / sum(has_language_freqs.values()) * 100, ndigits=2),\n    has_language_freqs,\n)\n\n\n\nplt.style.use(\"ggplot\")\ndata = {True: 13.31, \"No card data\": 42.03, False: 44.5}\n# Convert the keys to strings\nkeys = [str(key) for key in data]\n\n# Separate the values from the dictionary\nvalues = list(data.values())\n\n# Create a bar chart\nplt.bar(keys, values)\n\n# Set the labels for x and y axes\nplt.ylabel(\"Percent\")\n\n# Set the title of the chart\nplt.title(\"Has language information?\")\n\n# Display the chart\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#filter-to-datasets-with-language-information",
    "href": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#filter-to-datasets-with-language-information",
    "title": "Exploring language metadata for datasets on the Hugging Face Hub",
    "section": "Filter to datasets with language information",
    "text": "Filter to datasets with language information\n\nwith_lang = has_lang[True]\n\n\ndef count_langs(dataset):\n    langs = dataset.cardData.get(\"language\")\n    return len(langs)"
  },
  {
    "objectID": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#get-languages",
    "href": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#get-languages",
    "title": "Exploring language metadata for datasets on the Hugging Face Hub",
    "section": "Get languages",
    "text": "Get languages\n\ndef get_langs(dataset):\n    return dataset.cardData.get(\"language\")\n\n\nfrom toolz import concat, frequencies\n\n\nlang_freqs = frequencies(concat(get_langs(d) for d in with_lang))\n\nNumber of unique languages specified on the hub\n\nlen(lang_freqs.keys())\n\n1719\n\n\n\nimport pandas as pd\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 import pandas as pd\n\nModuleNotFoundError: No module named 'pandas'\n\n\n\n\ndf = pd.DataFrame({\"Language\": lang_freqs.keys(), \"Frequency\": lang_freqs.values()})\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nLanguage\nFrequency\n\n\n\n\n0\nen\n3949\n\n\n1\nja\n221\n\n\n2\nace\n19\n\n\n3\nacm\n6\n\n\n4\nacq\n5\n\n\n...\n...\n...\n\n\n1714\nmr-\n1\n\n\n1715\nxx\n1\n\n\n1716\nnbl\n2\n\n\n1717\nsep\n1\n\n\n1718\nssw\n2\n\n\n\n\n1719 rows Ã— 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.sort_values(\"Frequency\", ascending=False).iloc[:20].set_index(\"Language\").plot.bar()\n\n\n\n\n\n\n\n\n\ndf[\"Percent\"] = (df.Frequency / df.Frequency.sum() * 100).round(3)\n\n\nprint(\n    df.sort_values(\"Frequency\", ascending=False)\n    .iloc[:20]\n    .set_index(\"Language\")\n    .to_markdown()\n)\n\n| Language   |   Frequency |   Percent |\n|:-----------|------------:|----------:|\n| en         |        3949 |    19.04  |\n| fr         |         394 |     1.9   |\n| zh         |         390 |     1.88  |\n| es         |         358 |     1.726 |\n| de         |         350 |     1.687 |\n| ru         |         333 |     1.606 |\n| pt         |         238 |     1.147 |\n| it         |         229 |     1.104 |\n| ja         |         221 |     1.066 |\n| pl         |         207 |     0.998 |\n| ar         |         195 |     0.94  |\n| ko         |         184 |     0.887 |\n| nl         |         182 |     0.877 |\n| tr         |         156 |     0.752 |\n| vi         |         155 |     0.747 |\n| sv         |         153 |     0.738 |\n| id         |         149 |     0.718 |\n| hi         |         148 |     0.714 |\n| th         |         145 |     0.699 |\n| fi         |         144 |     0.694 |\n\n\n\ndf = df.drop(columns=\"Percent\")\n\n\ndf.sort_values(\"Frequency\", ascending=False).iloc[:20].set_index(\"Language\").plot.barh()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ndf[df.Language != \"en\"].sort_values(\"Frequency\", ascending=False).iloc[:50].set_index(\n    \"Language\"\n).plot.bar(ax=ax)\n\nax.set_xlabel(\"Language\")\nax.set_ylabel(\"Frequency\")\nax.set_title(\"Top 50 Languages (excluding English)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Define Hugging Face brand colors\nhugging_face_colors = [\"#FFD21E\", \"#FF9D00\", \"#6B7280\"]\n# Create custom colormap\ncolor_map = LinearSegmentedColormap.from_list(\"hugging_face\", hugging_face_colors)\n\n\nwordcloud = WordCloud(width=800, height=400, colormap=color_map)\n\nwordcloud.generate_from_frequencies(lang_freqs)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#introduction-to-colpali",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#introduction-to-colpali",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Introduction to ColPali",
    "text": "Introduction to ColPali\ntl;dr this blog post covers how to generate a dataset for training/fine-tuning ColPali models an open VLM to generate queries. You can find the dataset produced by this approach here.\nColPali is a new multimodal approach to retrieval which aims to replace existing document retrievers which often rely on an OCR step with an end-to-end multimodal approach. This approach also aims to take into account the visual content and layout of the documents, in addition to the textual content. Looking at an example of a document:\n\nwe could rely only on the text, but the page also has a table which could be relevant for retrieving the most useful document either for direct use or in a RAG pipeline. In many documents we will find that pages donâ€™t just contain images but also other rich sources of visual information that could be relevant for retrieval."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#how-copali-works",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#how-copali-works",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "How CoPali works?",
    "text": "How CoPali works?\nColPali is a document retrieval model that leverages a Vision Language Model (VLM) to understand and retrieve documents based on their visual content. The key steps are:\n\nDocument Encoding: ColPali takes document pages as images and processes them through a VLM (specifically, PaliGemma-3B) to generate embeddings for each image patch.\nQuery Encoding: User queries are encoded using the same VLMâ€™s text processing capabilities.\nLate Interaction: Instead of comparing a single vector per document, ColPali uses a â€œlate interactionâ€ mechanism that compares each query token embedding to all document patch embeddings.\n\nScoring: The relevance score between a query and a document is computed by summing the maximum similarities between each query token and all document patches.\nThis approach allows ColPali to understand both textual and visual elements in documents, enabling more comprehensive and accurate retrieval compared to traditional text-only methods. It eliminates the need for complex document parsing pipelines, as it works directly with document images.\nWhilst, late interaction methods have some additional computational costs, for many documents with rich visual content there is also a high potential processing cost in the document parsing pipeline â€” and this pipeline can also be rather brittle.\n\nThe training data for ColPali\nLetâ€™s take a look at what the training data looks like for training ColPali.\n\n\nYouâ€™ll see that each row contains a bunch of metadata about the source of the document and other information but the key parts for the actual training of the model are the image and the queries pairs. When training ColPali we want a dataset of images with queries that relate to the image. This will allow the model to learn how queries are related to the images. To help the model learn it can also be helpful to have negative examples.\nLetâ€™s take a closer look at a few examples from the data before we jump into generating our own.\nFor this notebook Iâ€™m using uv to manage Python installs because I find it to be a lot quicker but you can use pip if you prefer.\n\n%pip install uv\n\nCollecting uv\n  Downloading uv-0.4.15-py3-none-macosx_11_0_arm64.whl.metadata (11 kB)\nDownloading uv-0.4.15-py3-none-macosx_11_0_arm64.whl (10.9 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10.9/10.9 MB 22.7 MB/s eta 0:00:001m22.2 MB/s eta 0:00:01\nInstalling collected packages: uv\nSuccessfully installed uv-0.4.15\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n!uv pip install accelerate qwen-vl-utils torchvision torch datasets huggingface_hub[hf_transfer] polars --system\n!uv pip install git+https://github.com/huggingface/transformers.git  --system\n\nAudited 6 packages in 11ms\nResolved 54 packages in 230ms                                        \nAudited 54 packages in 0.08ms\n\n\n\n!uv pip install flash-attn --no-build-isolation --system\n\nAudited 1 package in 6ms\n\n\nWe can take a look at a few examples from the data using Polars.\n\nimport polars as pl\n\nsplits = {\"train\": \"data/train-*.parquet\", \"test\": \"data/test-00000-of-00001.parquet\"}\ndf = pl.scan_parquet(\n    \"hf://datasets/vidore/colpali_train_set/\" + splits[\"train\"],\n)\n\nLetâ€™s see what columns we have in the dataset.\n\ndf.columns\n\n['image',\n 'image_filename',\n 'query',\n 'answer',\n 'source',\n 'options',\n 'page',\n 'model',\n 'prompt',\n 'answer_type']\n\n\nSince weâ€™re shortly going to turn to how we can generate our own queries, letâ€™s take a look at a few examples from the data. Weâ€™ll filter to focus on the pdf source, since these are the ones created by the authors of ColPali (the other sources are from existing datasets).\n\nfiltered_df = (\n    df.filter(pl.col(\"source\").str.contains(\"pdf\")).select([\"query\"]).head(10).collect()\n)\nquery_list = filtered_df[\"query\"].to_list()\nquery_list\n\n['What is the duration of the course mentioned in the image?',\n 'What is the primary purpose of the PTC in lithium batteries?',\n 'How is the baseline CO2 emissions calculated for affected EGUs in the low load natural gas-fired or oil-fired subcategories?',\n 'What are some suggestions Liberty Medical Group should consider to improve their accounts receivable turnover and days sales in receivables ratios?',\n 'What measures will the Secretary determine to assess the quality of care furnished by the ACO?',\n 'How is the CT kerma index measured?',\n 'What is the difference between hindsight and foresight according to the passage?',\n 'How are the vapor jets arranged during the transitional and film boiling regimes?',\n 'What types of batteries are covered by the European Union Batteries Directive?',\n 'What are some factors to consider for CAES facility development in different parts of New York?']\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne thing you might notice about these queries is that many of them are more focused on â€œquestionsâ€ about documents rather than traditional search queries. Weâ€™ll shortly see the prompting approach used to generate these queries but we might already want to consider, depending on our use case, whether we want to generate more â€œsearchâ€-like queries or more â€œquestionâ€-like queries."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#creating-queries-from-documents",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#creating-queries-from-documents",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Creating queries from documents",
    "text": "Creating queries from documents\nFor the data using in ColPali, part of the dataset was sourced from existing document question answering datasets. Another component was generated using Claude 3.0 Sonnet with the following prompt:\n\nprompt = \"\"\"\nYou are an assistant specialized in Multimodal RAG tasks.\n\nThe task is the following: given an image from a pdf page, you will have to generate questions that can be asked by a user to retrieve information from a large documentary corpus.\n\nThe question should be relevant to the page, and should not be too specific or too general. The question should be about the subject of the page, and the answer needs to be found in the page.\n\nRemember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus.\n\nGenerate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question.\n\nGenerate at most THREE pairs of questions and answers per page in a dictionary with the following format, answer ONLY this dictionary NOTHING ELSE:\n\n{\n    \"questions\": [\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        },\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        },\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        }\n    ]\n}\nwhere XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n\nNote: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n\nHere is the page:\"\"\"\n\nAs you can see this prompt is focused on generating questions that are relevant to the page and that could be asked by a user and answered by the document. One thing I noticed from the queries generated with this prompt is that I think some of the generated queries strayed a bit from the prompts request to not assume the user knew the content of the document. Some of the questions were quite specific and it seemed like they were tailored to the particular page they were generated from.\nWhilst out of the box performance of ColPali is likely to be good for many domains and use cases it it likely that fine tuning ColPali on domain specific data will lead to improved performance. Weâ€™ll now turn to how we can generate our own queries for ColPali."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#ufo-colpali-creating-a-domain-specific-dataset",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#ufo-colpali-creating-a-domain-specific-dataset",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "UFO ColPali: creating a domain specific dataset",
    "text": "UFO ColPali: creating a domain specific dataset\nLetâ€™s now turn to how we could approach generating our own query image pairs for training â€” or more likely fine tuning â€” a ColPali model on domain specific data.\nTo make the example slightly more interesting weâ€™ll stray away from using an existing document dataset and use a UFO dateset which has been sourced from an Internet Archive Collection of UFO newsletters. This dataset was created from a sample of PDFs from this collection which were then split into single page images using the pdf-to-page-images-dataset Hugging Face Space. If you have a collection of PDFs related to your domain you could also use this Space to quickly create a dataset for your use case.\n\nimport os\nfrom datasets import load_dataset\n\nIf you are running on a machine with quite a fast connection, the following environment variable may increase the speed of the model and dataset downloads.\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nLetâ€™s start by loading the UFO dataset and taking a look at a few examples.\n\nds = load_dataset(\"davanstrien/ufo\", split=\"train\")\n\n\n\n\nLetâ€™s see what a row looks like\n\nds[0]\n\n{'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=596x842&gt;}\n\n\nand look at an example document\n\nds[3][\"image\"]\n\n\n\n\n\n\n\n\nWe can see that the dataset currently just contains images which map to a single page of a document. We can also see from the example document that the document does contain some visual elements which could be relevant for retrieval.\nWhat we need to train or fine tune ColPali is at least one query for each of our the document images in our dataset."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#using-qwen2-vl-to-generate-queries",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#using-qwen2-vl-to-generate-queries",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Using Qwen2-VL to generate queries",
    "text": "Using Qwen2-VL to generate queries\nWhilst the original ColPali paper used the Claude 3.0 Sonnet model to generate queries for the UFO dataset, weâ€™ll use the Qwen2-VL model to generate queries for our UFO dataset. Specifcally weâ€™ll use the 7B variant of the model (Qwen/Qwen2-VL-7B-Instruct). This is an open VLLM (Apache 2.0 licensed) Vision Language Model which has shown strong performance on a variety of vision and language tasks. Whilst the model wonâ€™t run on a standard T4 Google Colab instance we can run the model either on a L4 or an A100 GPU on Google Colab or you can use the Hugging Face Jupyter Spaces template to run the model on an L40s as I did.\nTo start letâ€™s load the model and get a sense of how we can use it. Weâ€™ll do this through the Transformers library. Itâ€™s also possible to run the model using the vLLM library but since weâ€™re only focusing on generating a relatively small number of queries this extra set up probably isnâ€™t worth it in this case (Iâ€™d be interested to hear in the comments if you try it out and how it goes).\nTo start weâ€™ll load the model. Weâ€™ll use flash_attention_2 which should help a bit with the performance and memory usage of the model.\n\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n    device_map=\"auto\",\n)\n\nUnrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\nYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n\n\n\n\n\nWe next define the processor. This is the component that will help us prepare the inputs for the model.\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\nAs with many more recent models, we can use a chat template to help us prepare the inputs for the model. Here is the example from the Qwen2-VL-7B-Instruct model card.\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\nWe can now pass this to the processor to get the inputs ready for the model. Youâ€™ll see here we first pass in the messages to the apply_chat_template method of the processor and then use the process_vision_info helper function which comes from the qwen_vl_utils library to prepare images and videos (which are not relevant here).\n\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\n\nIf we take a look at the text we can see that the processor has applied a chat template to the messages.\n\ntext\n\nWe now pass the text, image and video inputs (in this case None) to the processor and prepare the inputs for the model.\n\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\nwe can see what this input looks like\n\ninputs\n\n{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.8501,  0.8501,  0.8647,  ...,  1.3922,  1.3922,  1.3922],\n        [ 0.9376,  0.9376,  0.9376,  ...,  1.4491,  1.4491,  1.4491],\n        [ 0.9084,  0.9376,  0.9376,  ...,  1.4065,  1.4207,  1.4207],\n        ...,\n        [-0.1280, -0.1280, -0.1426,  ..., -0.2431, -0.2715, -0.3000],\n        [-0.3324, -0.3324, -0.3032,  ..., -0.3000, -0.2715, -0.2857],\n        [-0.3762, -0.4054, -0.4054,  ..., -0.4279, -0.4422, -0.4564]],\n       device='cuda:0'), 'image_grid_thw': tensor([[  1,  98, 146]], device='cuda:0')}\n\n\nNow the inputs are ready to pass to the model. BTW, all of this inference code is copied straight from the Qwen2-VL-7B-Instruct model card on Hugging Face. There are a few things we might want to tweak but the basic examples are pretty much all we need for our use case.\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=200)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\noutput_text\n\n[\"The image depicts a serene beach scene with a woman and her dog enjoying a moment together. The woman is sitting on the sandy beach, facing the ocean, and appears to be engaging in a playful activity with her dog. She is wearing a plaid shirt and dark pants, and her hair is long and dark. The dog, which is a large breed, possibly a Labrador Retriever, is sitting in front of her, wearing a harness. The dog is extending its front paw towards the woman's hand, as if they are giving each other a high-five. The woman is smiling, indicating a joyful interaction.\\n\\nThe beach is relatively empty, with gentle waves lapping at the shore. The sky is clear, and the sun is low on the horizon, casting a warm, golden light over the scene. The lighting suggests that the photo was taken either in the early morning or late afternoon, creating a peaceful and tranquil atmosphere. The sand is smooth and lightly textured, typical of a well-m\"]"
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#building-colpali-queries",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#building-colpali-queries",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Building Colpali queries",
    "text": "Building Colpali queries\nWe now have a sense of how to generate responses using both a text and image input using the Qwen2-VL-7B-Instruct model. Weâ€™ll now use this model to generate queries for our UFO dataset. To start, letâ€™s see how the prompt from the paper looks. The only thing we modified from the original prompt was to ask for JSON output rather than dictionaries since this model seemed to work better with this approach in my (somewhat limited) testing.\nprompt = \"\"\"\nYou are an assistant specialized in Multimodal RAG tasks.\n\nThe task is the following: given an image from a pdf page, you will have to generate questions that can be asked by a user to retrieve information from a large documentary corpus.\n\nThe question should be relevant to the page, and should not be too specific or too general. The question should be about the subject of the page, and the answer needs to be found in the page.\n\nRemember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus.\n\nGenerate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question.\n\nGenerate at most THREE pairs of questions and answers per page as JSON with the following format, answer ONLY using JSON, NOTHING ELSE:\n\n{\n    \"questions\": [\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        },\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        },\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        }\n    ]\n}\n\nwhere XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n\nNote: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n\nHere is the page:\n\"\"\"\nWeâ€™ll copy and paste all of the previous code to generate a response from the model for an example from our UFO dataset. Weâ€™ll wrap this in a function once weâ€™ve got a better sense of what weâ€™re looking for.\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": ds[0][\"image\"],\n            },\n            {\"type\": \"text\", \"text\": prompt},\n        ],\n    }\n]\n\n\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=200)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\noutput_text\n\n['{\\n    \"questions\": [\\n        {\\n            \"question\": \"What is the main subject of the page?\",\\n            \"answer\": [\"astronomy\", \"space exploration\", \"New York Times\"]\\n        },\\n        {\\n            \"question\": \"What is the significance of the Sun newspaper in the context of the page?\",\\n            \"answer\": [\"published a report attributed to an astronomer\", \"founded by Sir John Herschel\", \"published a report on the discovery of a race of singing aliens\"]\\n        },\\n        {\\n            \"question\": \"What is the main event mentioned in the page?\",\\n            \"answer\": [\"discovery of singing aliens on the Moon\"]\\n        }\\n    ]\\n}']\n\n\nYouâ€™ll see we get some responses like this \"What is the main event mentioned in the page?\" which is a bit too specific and tailored to the particular page. There are a few reasons this might be happening but the first thing we should play around with is changing the prompt.\n\nValidating the responses\nOne of the big challenges you can have in generating synthetic data at scale is ensuring that you get valid responses that you can use in downstream tasks for training without having to do a lot of manual verification. Letâ€™s see if we can load the response as valid JSON\n\nimport json\n\n\njson.loads(output_text[0])\n\n{'questions': [{'question': 'What is the main subject of the page?',\n   'answer': ['astronomy', 'space exploration', 'New York Times']},\n  {'question': 'What is the significance of the Sun newspaper in the context of the page?',\n   'answer': ['published a report attributed to an astronomer',\n    'founded by Sir John Herschel',\n    'published a report on the discovery of a race of singing aliens']},\n  {'question': 'What is the main event mentioned in the page?',\n   'answer': ['discovery of singing aliens on the Moon']}]}\n\n\n\nprint(output_text[0])\n\n{\n    \"questions\": [\n        {\n            \"question\": \"What is the main subject of the page?\",\n            \"answer\": [\"astronomy\", \"space exploration\", \"New York Times\"]\n        },\n        {\n            \"question\": \"What is the significance of the Sun newspaper in the context of the page?\",\n            \"answer\": [\"published a report attributed to an astronomer\", \"founded by Sir John Herschel\", \"published a report on the discovery of a race of singing aliens\"]\n        },\n        {\n            \"question\": \"What is the main event mentioned in the page?\",\n            \"answer\": [\"discovery of singing aliens on the Moon\"]\n        }\n    ]\n}\n\n\nHaving a valid JSON is a good start but in many synthetic data generation tasks, people are increasingly using Pydantic to ensure outputs are valid in other ways. Letâ€™s take a look at a rewritten prompt I created for generating queries. In this prompt we ask the VLLM model to generate 3 different types of retrieval queries:\n\nprompt = \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n\nPlease generate 3 different types of retrieval queries:\n\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Frame the queries as if someone is searching for this document, not asking questions about its content.\n- Make the queries diverse and representative of different search strategies.\n\nFor each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"broad_topical_query\": \"Your query here\",\n  \"broad_topical_explanation\": \"Brief explanation\",\n  \"specific_detail_query\": \"Your query here\",\n  \"specific_detail_explanation\": \"Brief explanation\",\n  \"visual_element_query\": \"Your query here\",\n  \"visual_element_explanation\": \"Brief explanation\"\n}\n\nIf there are no relevant visual elements, replace the third query with another specific detail query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\"\n\nWe ask the model for JSON output, we can represent this using a simple Pydantic model.\n\nfrom pydantic import BaseModel\nfrom typing import Tuple\n\nclass GeneralRetrievalQuery(BaseModel):\n    broad_topical_query: str\n    broad_topical_explanation: str\n    specific_detail_query: str\n    specific_detail_explanation: str\n    visual_element_query: str\n    visual_element_explanation: str\n\nWe could add additional constraints to our Pydantic model for example we could set a minimum and maximum length for the queries and answers. Weâ€™ll get back to this at the end of the post but for now we can make a start with this simpler approach.\nWeâ€™ll now wrap this in a function to generate a response from the model using our Pydantic model."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#an-update-retrieval-focused-prompt",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#an-update-retrieval-focused-prompt",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "An update retrieval focused prompt",
    "text": "An update retrieval focused prompt\n\n\ndef get_retrieval_prompt(prompt_name: str) -&gt; Tuple[str, GeneralRetrievalQuery]:\n    if prompt_name != \"general\":\n        raise ValueError(\"Only 'general' prompt is available in this version\")\n\n    prompt = \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n\nPlease generate 3 different types of retrieval queries:\n\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Frame the queries as if someone is searching for this document, not asking questions about its content.\n- Make the queries diverse and representative of different search strategies.\n\nFor each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"broad_topical_query\": \"Your query here\",\n  \"broad_topical_explanation\": \"Brief explanation\",\n  \"specific_detail_query\": \"Your query here\",\n  \"specific_detail_explanation\": \"Brief explanation\",\n  \"visual_element_query\": \"Your query here\",\n  \"visual_element_explanation\": \"Brief explanation\"\n}\n\nIf there are no relevant visual elements, replace the third query with another specific detail query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\"\n\n    return prompt, GeneralRetrievalQuery\n\n\n# Example usage:\nprompt_name = \"general\"\nprompt, pydantic_model = get_retrieval_prompt(prompt_name)\nprint(f\"Prompt for '{prompt_name}':\")\nprint(prompt)\nprint(f\"\\nPydantic model for this prompt: {pydantic_model}\")\n\nPrompt for 'general':\nYou are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n\nPlease generate 3 different types of retrieval queries:\n\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Frame the queries as if someone is searching for this document, not asking questions about its content.\n- Make the queries diverse and representative of different search strategies.\n\nFor each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"broad_topical_query\": \"Your query here\",\n  \"broad_topical_explanation\": \"Brief explanation\",\n  \"specific_detail_query\": \"Your query here\",\n  \"specific_detail_explanation\": \"Brief explanation\",\n  \"visual_element_query\": \"Your query here\",\n  \"visual_element_explanation\": \"Brief explanation\"\n}\n\nIf there are no relevant visual elements, replace the third query with another specific detail query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\n\nPydantic model for this prompt: &lt;class '__main__.GeneralRetrievalQuery'&gt;\n\n\nWeâ€™ll now also wrap this in a function to generate a response from the model using our Pydantic model. We could probably do a bit more refactoring here but this will do for now.\n\ndef generate_response(prompt, image):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": image,\n                },\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    image_inputs, video_inputs = process_vision_info(messages)\n\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(\"cuda\")\n\n    generated_ids = model.generate(**inputs, max_new_tokens=200)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :]\n        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n\n    output_text = processor.batch_decode(\n        generated_ids_trimmed,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False,\n    )\n\n    return output_text\n\nLetâ€™s now generate a response from the model for an example image from our UFO dataset.\n\ngenerate_response(prompt, ds[2][\"image\"])\n\n['{\\n  \"broad_topical_query\": \"Document discussing the possibility of a cameraman going on the record in the future\",\\n  \"broad_topical_explanation\": \"This query focuses on the main topic of the document, which is the discussion about the cameraman\\'s potential to go on the record.\",\\n  \"specific_detail_query\": \"Document mentioning Ray Santilli and his attempts to persuade the cameraman\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail in the document, which is Ray Santilli\\'s efforts to contact the cameraman.\",\\n  \"visual_element_query\": \"Document containing images of a damaged leg and an alien\\'s foot\",\\n  \"visual_element_explanation\": \"This query refers to the visual elements present in the document, which are images of a damaged leg and an alien\\'s foot.\"\\n}']\n\n\nWe can see the model is doing a reasonable job of generating queries.\n['{\\n  \"broad_topical_query\": \"Document discussing the possibility of a cameraman going on the record in the future\",\\n  \"broad_topical_explanation\": \"This query focuses on the main topic of the document, which is the discussion about the cameraman\\'s potential to go on the record.\",\\n  \"specific_detail_query\": \"Document mentioning Ray Santilli and his attempts to persuade the cameraman\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail in the document, which is Ray Santilli\\'s efforts to contact the cameraman.\",\\n  \"visual_element_query\": \"Document containing images of a damaged leg and an alien\\'s foot\",\\n  \"visual_element_explanation\": \"This query refers to the visual elements present in the document, which are images of a damaged leg and an alien\\'s foot.\"\\n}']\nOne thing I have found in my experiments with generating synthetic data is that adding a request for an \"explanation\" from the model sometimes seems to help improve the quality of the generated data. I assume this is already noted somewhere in the literature (if not I'll call this `explain then generate`!). This seems to particularly helpful when generating more complex queries. Having the explanation can also give you a sense of how the model \"understands\" the task. This obviously comes with the donwside that it takes longer to generate the data and more tokens are required but it often seems worth trying."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#generating-the-full-dataset",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#generating-the-full-dataset",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Generating the full dataset",
    "text": "Generating the full dataset\nAs we play with the prompts and refine the queries we will often iterate quite quickly on a few examples. Once weâ€™re reasonably confident in the queries we can generate a larger dataset to see how well our prompts work across a larger set of examples.\n\nds\n\nDataset({\n    features: ['image'],\n    num_rows: 212\n})\n\n\n\nsample = ds.take(100)\n\nTo generate our full dataset we just wrap our previous code in a loop and run it for all the examples in our dataset. We add a very broad exception handler to catch any errors and continue with the next example. This is obviously not production code but itâ€™s good enough to get started with. If we scale to a much bigger dataset we might want to add some more robust error handling.\n\nfrom tqdm.auto import tqdm\nresponses = []\nfor row in tqdm(sample):\n    try:\n        resp = generate_response(prompt, row[\"image\"])\n        responses.append(resp)\n    except Exception as e:\n        responses.append(None)\n\n\n\n\n\nresponses[0]\n\n['{\\n  \"broad_topical_query\": \"Document about the Sun and its influence on life\",\\n  \"broad_topical_explanation\": \"This query focuses on the main subject of the document, which is the Sun and its impact on life.\",\\n  \"specific_detail_query\": \"New York Times article on the Sun\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail mentioned in the document, which is the New York Times article on the Sun.\",\\n  \"visual_element_query\": \"Document with a black and white image\",\\n  \"visual_element_explanation\": \"This query refers to the visual element present in the document, which is a black and white image.\"\\n}']\n\n\nWe can see how many errors we have in our dataset.\n\nlen([r for r in responses if r is None])\n\n0\n\n\nNo bad generations!\nWe can also look at the first response to see what it looks like.\n\nresponses[0][0]\n\n'{\\n  \"broad_topical_query\": \"Document about the Sun and its influence on life\",\\n  \"broad_topical_explanation\": \"This query focuses on the main subject of the document, which is the Sun and its impact on life.\",\\n  \"specific_detail_query\": \"New York Times article on the Sun\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail mentioned in the document, which is the New York Times article on the Sun.\",\\n  \"visual_element_query\": \"Document with a black and white image\",\\n  \"visual_element_explanation\": \"This query refers to the visual element present in the document, which is a black and white image.\"\\n}'\n\n\nLetâ€™s see if we can parse this into a JSON object.\n\njson.loads(responses[0][0])\n\n{'broad_topical_query': 'Document about the Sun and its influence on life',\n 'broad_topical_explanation': 'This query focuses on the main subject of the document, which is the Sun and its impact on life.',\n 'specific_detail_query': 'New York Times article on the Sun',\n 'specific_detail_explanation': 'This query targets a specific detail mentioned in the document, which is the New York Times article on the Sun.',\n 'visual_element_query': 'Document with a black and white image',\n 'visual_element_explanation': 'This query refers to the visual element present in the document, which is a black and white image.'}\n\n\nfirst example seems to work, now letâ€™s add this to our dataset and see how many we can parse.\n\nsample = sample.add_column(\"raw_queries\", responses)\n\n\nsample\n\nDataset({\n    features: ['image', 'raw_queries'],\n    num_rows: 100\n})\n\n\nTo deal with bad generations weâ€™ll create just fill out these column with None values. We can grab all the required keys from the valid first response.\n\nkeys = list(json.loads(responses[0][0]).keys())\nkeys\n\n['broad_topical_query',\n 'broad_topical_explanation',\n 'specific_detail_query',\n 'specific_detail_explanation',\n 'visual_element_query',\n 'visual_element_explanation']\n\n\nand do something like this to fill out this row\n\n{k: None for k in keys}\n\n{'broad_topical_query': None,\n 'broad_topical_explanation': None,\n 'specific_detail_query': None,\n 'specific_detail_explanation': None,\n 'visual_element_query': None,\n 'visual_element_explanation': None}\n\n\nWe create a function to extract the data from the raw queries and parse them into a JSON object.\n\ndef extract_data(row):\n    try:\n        data = json.loads(row[\"raw_queries\"][0])\n        data[\"parsed_into_json\"] = True\n        return data\n    except Exception:\n        data = {k: None for k in keys}\n        data[\"parsed_into_json\"] = False\n        return data\n\nWe can now use the map method to apply this function to our dataset.\n\nparsed_ds = sample.map(extract_data)\n\nWe can then see how many weâ€™ve successfully parsed.\n\nfrom collections import Counter\n\nCounter(parsed_ds[\"parsed_into_json\"])\n\nCounter({True: 95, False: 5})\n\n\nSo in this case 5% of the responses were not parseable. This isnâ€™t too bad and we might be able to live with this."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#pushing-to-the-hub",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#pushing-to-the-hub",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Pushing to the Hub",
    "text": "Pushing to the Hub\nWe can now push this dataset to the Hugging Face Hub. This will also allow us to view the dataset in the Dataset Viewer. This can often be a very nice way of quickly checking through examples in a dataset to see how the quality looks.\nIf you are not authenticated you can use the login function to authenticate with the Hub.\n\nfrom huggingface_hub import login\n\n\nlogin()\n\n\n\n\n\nparsed_ds.push_to_hub(\"davanstrien/ufo-ColPali\")\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/datasets/davanstrien/ufo-ColPali/commit/c50c8e42cd01d2dde17ef64e66ea1b826c0ca4ec', commit_message='Upload dataset', commit_description='', oid='c50c8e42cd01d2dde17ef64e66ea1b826c0ca4ec', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/davanstrien/ufo-ColPali', endpoint='https://huggingface.co', repo_type='dataset', repo_id='davanstrien/ufo-ColPali'), pr_revision=None, pr_num=None)\n\n\nHere is what the dataset looks like in the Hugging Face Hub. Youâ€™ll see there are actually more than a 100 examples since I did a larger generation of data since doing the first batch of a 100. You can be your own judge but my sense it that the queries are looking pretty good already."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#conclusion-improvements-and-next-steps",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#conclusion-improvements-and-next-steps",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Conclusion, improvements and next steps",
    "text": "Conclusion, improvements and next steps\nThere are a few improvements that we could make to this process.\n\nStructured Generation\nOne of the first things I think would be worth exploring further is using structured generation to improve the quality of the generated queries. This would allow us to properly use the Pydantic models to constrain the outputs. The Outlines library has functionality for doing this with for VLMs. Once I am more satisified with the quality of the queries Iâ€™ll come back to this.\n\n\nMore diverse queries\nI focused on generating a single type of query for each example in the UFO dataset. I think for a dataset of this size it would be worth taking a more diverse set of generations. Below you can see an apendix with a few options for these kinds of queries.\n\n\nNext Steps\nI am keen to test this approach with a few more domains and also work on the actual fine-tuning of ColPali models."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#appendix-more-diverse-queries",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#appendix-more-diverse-queries",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Appendix more diverse queries",
    "text": "Appendix more diverse queries\nHere are a few more examples of prompts I am experimenting with.\n\nfrom pydantic import BaseModel\nfrom typing import List, Optional, Tuple, Union\n\n# Pydantic models for each prompt type\n\n\nclass GeneralRetrievalQuery(BaseModel):\n    broad_topical_query: str\n    broad_topical_explanation: str\n    specific_detail_query: str\n    specific_detail_explanation: str\n    visual_element_query: str\n    visual_element_explanation: str\n\n\nclass MultiDocumentComparisonQuery(BaseModel):\n    comparison_query: str\n    comparison_explanation: str\n    corroboration_contradiction_query: str\n    corroboration_contradiction_explanation: str\n\n\nclass DomainSpecificQuery(BaseModel):\n    identified_domain: str\n    domain_specific_query: str\n    domain_specific_explanation: str\n    data_findings_query: str\n    data_findings_explanation: str\n    applications_implications_query: str\n    applications_implications_explanation: str\n\n\nclass VisualElementFocusQuery(BaseModel):\n    similar_visual_element_query: str\n    similar_visual_element_explanation: str\n    text_visual_combination_query: str\n    text_visual_combination_explanation: str\n    visual_content_understanding_query: str\n    visual_content_understanding_explanation: str\n\n\nclass TemporalMetadataQuery(BaseModel):\n    temporal_query: str\n    temporal_explanation: str\n    topic_metadata_combination_query: str\n    topic_metadata_combination_explanation: str\n    update_related_document_query: str\n    update_related_document_explanation: str\n\n\nclass DifficultyAmbiguityQuery(BaseModel):\n    simple_query: str\n    simple_explanation: str\n    complex_query: str\n    complex_explanation: str\n    ambiguous_query: str\n    ambiguous_explanation: str\n\n\nclass MultilingualMultimodalQuery(BaseModel):\n    multilingual_query: str\n    multilingual_explanation: str\n    multimodal_combination_query: str\n    multimodal_combination_explanation: str\n    text_visual_understanding_query: str\n    text_visual_understanding_explanation: str\n\n\ndef get_retrieval_prompt(\n    prompt_name: str,\n) -&gt; Tuple[\n    str,\n    Union[\n        GeneralRetrievalQuery,\n        MultiDocumentComparisonQuery,\n        DomainSpecificQuery,\n        VisualElementFocusQuery,\n        TemporalMetadataQuery,\n        DifficultyAmbiguityQuery,\n        MultilingualMultimodalQuery,\n    ],\n]:\n    prompts = {\n        \"general\": (\n            \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n\nPlease generate 3 different types of retrieval queries:\n\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Frame the queries as if someone is searching for this document, not asking questions about its content.\n- Make the queries diverse and representative of different search strategies.\n\nFor each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"broad_topical_query\": \"Your query here\",\n  \"broad_topical_explanation\": \"Brief explanation\",\n  \"specific_detail_query\": \"Your query here\",\n  \"specific_detail_explanation\": \"Brief explanation\",\n  \"visual_element_query\": \"Your query here\",\n  \"visual_element_explanation\": \"Brief explanation\"\n}\n\nIf there are no relevant visual elements, replace the third query with another specific detail query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            GeneralRetrievalQuery,\n        ),\n        \"comparison\": (\n            \"\"\"Imagine this document page is part of a larger corpus. Your task is to generate retrieval queries that would require comparing this document with others in the corpus.\n\nPlease generate 2 retrieval queries:\n\n1. A query comparing this document's topic with a related subject\n2. A query seeking documents that contradict or support the main point of this page\n\nFor each query, provide a brief explanation of how it encourages document comparison and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"comparison_query\": \"Your query here\",\n  \"comparison_explanation\": \"Brief explanation\",\n  \"corroboration_contradiction_query\": \"Your query here\",\n  \"corroboration_contradiction_explanation\": \"Brief explanation\"\n}\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            MultiDocumentComparisonQuery,\n        ),\n        \"domain\": (\n            \"\"\"Your task is to create retrieval queries that a professional in the document's domain might use to find this document in a large corpus.\n\nFirst, identify the domain of the document (e.g., scientific, financial, legal, medical, technical).\n\nThen, generate 3 retrieval queries:\n\n1. A query using domain-specific terminology\n2. A query seeking specific data or findings presented in the document\n3. A query related to the document's potential applications or implications\n\nFor each query, provide a brief explanation of its relevance to the domain and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"identified_domain\": \"Domain name\",\n  \"domain_specific_query\": \"Your query here\",\n  \"domain_specific_explanation\": \"Brief explanation\",\n  \"data_findings_query\": \"Your query here\",\n  \"data_findings_explanation\": \"Brief explanation\",\n  \"applications_implications_query\": \"Your query here\",\n  \"applications_implications_explanation\": \"Brief explanation\"\n}\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            DomainSpecificQuery,\n        ),\n        \"visual\": (\n            \"\"\"Your task is to generate retrieval queries focusing on the visual elements in this document page (charts, tables, images, diagrams).\n\nPlease generate 3 retrieval queries:\n\n1. A query specifically asking for documents with similar visual elements\n2. A query combining textual and visual information\n3. A query that would require understanding the content of the visual element to retrieve this document\n\nFor each query, provide a brief explanation of how it incorporates visual elements and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"similar_visual_element_query\": \"Your query here\",\n  \"similar_visual_element_explanation\": \"Brief explanation\",\n  \"text_visual_combination_query\": \"Your query here\",\n  \"text_visual_combination_explanation\": \"Brief explanation\",\n  \"visual_content_understanding_query\": \"Your query here\",\n  \"visual_content_understanding_explanation\": \"Brief explanation\"\n}\n\nIf the document lacks significant visual elements, explain this and generate alternative queries focusing on the document's structure or layout.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            VisualElementFocusQuery,\n        ),\n        \"temporal\": (\n            \"\"\"Assuming this document is part of a large, diverse corpus, your task is to generate retrieval queries that incorporate metadata or temporal aspects.\n\nPlease generate 3 retrieval queries:\n\n1. A query specifying a likely time frame for this document\n2. A query combining topical information with a metadata element (e.g., author, publication type)\n3. A query seeking updated or related documents on the same topic\n\nFor each query, provide a brief explanation of how it uses temporal or metadata information and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"temporal_query\": \"Your query here\",\n  \"temporal_explanation\": \"Brief explanation\",\n  \"topic_metadata_combination_query\": \"Your query here\",\n  \"topic_metadata_combination_explanation\": \"Brief explanation\",\n  \"update_related_document_query\": \"Your query here\",\n  \"update_related_document_explanation\": \"Brief explanation\"\n}\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            TemporalMetadataQuery,\n        ),\n        \"difficulty\": (\n            \"\"\"Your task is to create retrieval queries for this document at different levels of complexity and ambiguity.\n\nPlease generate 3 retrieval queries:\n\n1. A simple, straightforward query\n2. A complex query requiring understanding of multiple aspects of the document\n3. An ambiguous query that could retrieve this document among others\n\nFor each query, provide a brief explanation of its complexity level or ambiguity and why it would be effective or challenging for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"simple_query\": \"Your query here\",\n  \"simple_explanation\": \"Brief explanation\",\n  \"complex_query\": \"Your query here\",\n  \"complex_explanation\": \"Brief explanation\",\n  \"ambiguous_query\": \"Your query here\",\n  \"ambiguous_explanation\": \"Brief explanation\"\n}\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            DifficultyAmbiguityQuery,\n        ),\n        \"multilingual\": (\n            \"\"\"Your task is to generate retrieval queries considering potential multilingual and multi-modal aspects of the document.\n\nPlease generate 3 retrieval queries:\n\n1. A query in a different language (if applicable) that would retrieve this document\n2. A query combining textual and non-textual elements\n3. A query that requires understanding both the text and visual elements to retrieve this document accurately\n\nFor each query, provide a brief explanation of its multilingual or multi-modal nature and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"multilingual_query\": \"Your query here\",\n  \"multilingual_explanation\": \"Brief explanation\",\n  \"multimodal_combination_query\": \"Your query here\",\n  \"multimodal_combination_explanation\": \"Brief explanation\",\n  \"text_visual_understanding_query\": \"Your query here\",\n  \"text_visual_understanding_explanation\": \"Brief explanation\"\n}\n\nIf the document is not suitable for multilingual queries, explain why and provide an alternative query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            MultilingualMultimodalQuery,\n        ),\n    }\n\n    if prompt_name not in prompts:\n        raise ValueError(\n            f\"Invalid prompt name. Choose from: {', '.join(prompts.keys())}\"\n        )\n\n    return prompts[prompt_name]\n\n\n# Example usage:\nprompt_name = \"general\"  # You can change this to any of the available prompt names\nprompt, pydantic_model = get_retrieval_prompt(prompt_name)\nprint(f\"Prompt for '{prompt_name}':\")\nprint(prompt)\nprint(f\"\\nPydantic model for this prompt: {pydantic_model}\")"
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html",
    "title": "Searching for machine learning models using semantic search",
    "section": "",
    "text": "The Hugging Face model hub has (at the time of the last checking) 60,509 models publicly available. Some of these models are useful as base models for further fine-tuning; these include your classics like bert-base-uncased.\nThe hub also has more obscure indie hits that might already do a good job on your desired downstream task or be a closer start. For example, if one wanted to classify the genre of 18th Century books, it might make sense to start with a model for classifying 19th Century books."
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#finding-candidate-models",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#finding-candidate-models",
    "title": "Searching for machine learning models using semantic search",
    "section": "Finding candidate models",
    "text": "Finding candidate models\nIdeally, weâ€™d like a quick way to identify if a model might already do close to what we want. From there, we would likely want to review a bunch of other info about the model before deciding if it might be helpful for us or not.\nUnfortunately, finding suitable models on the hub isnâ€™t always that easy. Even knowing that models for genre classification exist on the hub, we donâ€™t find any results.\n\n\n\nSearch results\n\n\nItâ€™s not documented exactly how the search on the hub works, but it seems to be based mainly on the modelâ€™s name rather than the README or other information. In this blog post, I will continue some previous experiments with embeddings to see if there might be different ways in which we could identify potential models.\nThis will be a very rough experiment and is more about establishing whether this is an avenue worth exploring rather than a fully fleshed-out approach.\nFirst install some libraries weâ€™ll use:\n\nimport torch\n\n\ndeps = [\"datasets\" ,\"sentence-transformers\", \"rich['jupyter']\", \"requests\"]\nif torch.cuda.is_available():\n    deps.append(\"faiss-gpu\")\nelse:\n    deps.append(\"faise-cpu\")\n\n\n%%capture\n!pip install {\" \".join(deps)} --upgrade\n\n\n!git config --global credential.helper store\n\nThese days I almost always have the rich extension loaded!\n\n%load_ext rich"
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#using-the-huggingface_hub-api-to-download-some-model-metadata",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#using-the-huggingface_hub-api-to-download-some-model-metadata",
    "title": "Searching for machine learning models using semantic search",
    "section": "Using the huggingface_hub API to download some model metadata",
    "text": "Using the huggingface_hub API to download some model metadata\nOur goal is to see if we might be able to find suitable models more efficiently using some form of semantic search (i.e.Â using embeddings). To do this, we should grab some model data from the hub. The easiest way to do this is using the hub API.\n\nfrom huggingface_hub import hf_api\nimport re\nfrom rich import print\n\n\napi = hf_api.HfApi()\n\n\napi\n\n&lt;huggingface_hub.hf_api.HfApi object at 0x7f63832ff810&gt;\n\n\n\nWe can take a look at some example models\n\nall_models = api.list_models()\nall_models[:3]\n\n[\n    ModelInfo: {\n        modelId: hfl/chinese-macbert-base\n        sha: None\n        lastModified: None\n        tags: []\n        pipeline_tag: fill-mask\n        siblings: None\n        private: False\n        author: None\n        config: None\n        id: hfl/chinese-macbert-base\n},\n    ModelInfo: {\n        modelId: bert-base-uncased\n        sha: None\n        lastModified: None\n        tags: []\n        pipeline_tag: fill-mask\n        siblings: None\n        private: False\n        author: None\n        config: None\n        id: bert-base-uncased\n},\n    ModelInfo: {\n        modelId: microsoft/deberta-base\n        sha: None\n        lastModified: None\n        tags: []\n        pipeline_tag: None\n        siblings: None\n        private: False\n        author: None\n        config: None\n        id: microsoft/deberta-base\n}\n]\n\n\n\nFor a particular model we can also see what files there are.\n\nfiles = api.list_repo_files(all_models[0].modelId)\n\n\nfiles\n\n[\n    '.gitattributes',\n    'README.md',\n    'added_tokens.json',\n    'config.json',\n    'flax_model.msgpack',\n    'pytorch_model.bin',\n    'special_tokens_map.json',\n    'tf_model.h5',\n    'tokenizer.json',\n    'tokenizer_config.json',\n    'vocab.txt'\n]\n\n\n\n\nFiltering\nTo limit the scope of this blog post, weâ€™ll focus only on Pytorch models and â€˜text classificationâ€™ models. The metadata about the model type is likely usually pretty reliable. The model task metadata, on the other hand, is not always reliable in my experience. This means we probably have some models that arenâ€™t text-classification models and donâ€™t include some actual text classification models in our dataset. For now, we wonâ€™t worry too much about this.\n\nfrom huggingface_hub import ModelSearchArguments\n\n\nmodel_args = ModelSearchArguments()\n\n\nfrom huggingface_hub import ModelFilter\n\nmodel_filter = ModelFilter(\n    task=model_args.pipeline_tag.TextClassification, \n    library=model_args.library.PyTorch\n)\napi.list_models(filter=model_filter)[0]\n\nModelInfo: {\n        modelId: distilbert-base-uncased-finetuned-sst-2-english\n        sha: 00c3f1ef306e837efb641eaca05d24d161d9513c\n        lastModified: 2022-07-22T08:00:55.000Z\n        tags: ['pytorch', 'tf', 'rust', 'distilbert', 'text-classification', 'en', 'dataset:sst2', 'dataset:glue', 'transformers', 'license:apache-2.0', 'model-index']\n        pipeline_tag: text-classification\n        siblings: [RepoFile(rfilename='.gitattributes'), RepoFile(rfilename='README.md'), RepoFile(rfilename='config.json'), RepoFile(rfilename='map.jpeg'), RepoFile(rfilename='pytorch_model.bin'), RepoFile(rfilename='rust_model.ot'), RepoFile(rfilename='tf_model.h5'), RepoFile(rfilename='tokenizer_config.json'), RepoFile(rfilename='vocab.txt')]\n        private: False\n        author: None\n        config: None\n        id: distilbert-base-uncased-finetuned-sst-2-english\n        downloads: 5185721\n        likes: 76\n        library_name: transformers\n}\n\n\n\nNow we have a filter weâ€™ll use that to grab all the models that match this filter.\n\nall_models = api.list_models(filter=model_filter)\n\n\nall_models[0]\n\nModelInfo: {\n        modelId: distilbert-base-uncased-finetuned-sst-2-english\n        sha: 00c3f1ef306e837efb641eaca05d24d161d9513c\n        lastModified: 2022-07-22T08:00:55.000Z\n        tags: ['pytorch', 'tf', 'rust', 'distilbert', 'text-classification', 'en', 'dataset:sst2', 'dataset:glue', 'transformers', 'license:apache-2.0', 'model-index']\n        pipeline_tag: text-classification\n        siblings: [RepoFile(rfilename='.gitattributes'), RepoFile(rfilename='README.md'), RepoFile(rfilename='config.json'), RepoFile(rfilename='map.jpeg'), RepoFile(rfilename='pytorch_model.bin'), RepoFile(rfilename='rust_model.ot'), RepoFile(rfilename='tf_model.h5'), RepoFile(rfilename='tokenizer_config.json'), RepoFile(rfilename='vocab.txt')]\n        private: False\n        author: None\n        config: None\n        id: distilbert-base-uncased-finetuned-sst-2-english\n        downloads: 5185721\n        likes: 76\n        library_name: transformers\n}\n\n\n\nLetâ€™s see how many models that gives us.\n\nlen(all_models)\n\n6860\n\n\n\nLater on, in this blog, weâ€™ll want to work with the config.json files (weâ€™ll get back to why later!), so weâ€™ll quickly check that all our models have this.\n\ndef has_config(model):\n    has_config = False\n    files = model.siblings\n    for file in files:\n        if \"config.json\" in file.rfilename:\n            has_config = True\n            return has_config\n        else:\n            continue\n\n\nhas_config(all_models[0])\n\nTrue\n\n\n\n\nhas_config = [model for model in all_models if has_config(model)]\n\nLetâ€™s check how many we have now\n\nlen(has_config)\n\n6858\n\n\n\nWe can also download a particular file from the hub\n\nfrom huggingface_hub import hf_hub_download\nfile = hf_hub_download(repo_id=all_models[0].modelId, filename=\"config.json\")\n\n\n\n\n\nfile\n\n'/root/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/00c3f1ef306e837efb641eaca05d24d161d9513c/config.json'\n\n\n\nUnable to display output for mime type(s): application/vnd.google.colaboratory.intrinsic+json\n\n\n\nimport json\nwith open(file) as f:\n    data = json.load(f)\n\n\ndata\n\n{\n    'activation': 'gelu',\n    'architectures': ['DistilBertForSequenceClassification'],\n    'attention_dropout': 0.1,\n    'dim': 768,\n    'dropout': 0.1,\n    'finetuning_task': 'sst-2',\n    'hidden_dim': 3072,\n    'id2label': {'0': 'NEGATIVE', '1': 'POSITIVE'},\n    'initializer_range': 0.02,\n    'label2id': {'NEGATIVE': 0, 'POSITIVE': 1},\n    'max_position_embeddings': 512,\n    'model_type': 'distilbert',\n    'n_heads': 12,\n    'n_layers': 6,\n    'output_past': True,\n    'pad_token_id': 0,\n    'qa_dropout': 0.1,\n    'seq_classif_dropout': 0.2,\n    'sinusoidal_pos_embds': False,\n    'tie_weights_': True,\n    'vocab_size': 30522\n}\n\n\n\nWe can also check if the model has a README.md\n\ndef has_file_in_repo(model,file_name):\n    has_file = False\n    files = model.siblings\n    for file in files:\n        if file_name in file.rfilename:\n            has_file = True\n            return has_file\n        else:\n            continue    \n\n\nhas_file_in_repo(has_config[0],'README.md')\n\nTrue\n\n\n\n\nhas_readme = [model for model in has_config if has_file_in_repo(model,\"README.md\")]\n\nWe can see that there are more configs than READMEs\n\nlen(has_readme)\n\n3482\n\n\n\n\nlen(has_config)\n\n6858\n\n\n\nWe now write some functions to grab both the README.md and config.json files from the hub.\n\nfrom requests.exceptions import JSONDecodeError\nimport concurrent.futures\n\n\n@lru_cache(maxsize=None)\ndef get_model_labels(model):\n    try:\n        url = hf_hub_url(repo_id=model.modelId, filename=\"config.json\")\n        return model.modelId, list(requests.get(url).json()['label2id'].keys())\n    except (KeyError, JSONDecodeError, AttributeError):\n        return model.modelId, None\n    \n\n\nget_model_labels(has_config[0])\n\n('distilbert-base-uncased-finetuned-sst-2-english', ['NEGATIVE', 'POSITIVE'])\n\n\n\n\ndef get_model_readme(model):\n    url = hf_hub_url(repo_id=model.modelId, filename=\"README.md\")\n    return requests.get(url).text\n\n\ndef get_data(model):\n    readme = get_model_readme(model)\n    _, labels = get_model_labels(model)\n    return model.modelId, labels, readme\n\nSince this takes a little while we make a progress bar and do this using multiple threads\n\nfrom tqdm.auto import tqdm\n\n\nwith tqdm(total=len(has_config)) as progress:\n    with concurrent.futures.ThreadPoolExecutor() as e:\n        tasks = []\n        for model in has_config:\n            future = e.submit(get_data, model)\n            future.add_done_callback(lambda p: progress.update())\n            tasks.append(future)\nresults = [task.result() for task in tasks]\n\n\n\n\nLoad our data using Pandas.\n\nimport pandas as pd\n\n\ndf = pd.DataFrame(results,columns=['modelId','label','readme'])\n\n\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nmodelId\nlabel\nreadme\n\n\n\n\n0\ndistilbert-base-uncased-finetuned-sst-2-english\n[NEGATIVE, POSITIVE]\n---\\nlanguage: en\\nlicense: apache-2.0\\ndatase...\n\n\n1\ncross-encoder/ms-marco-MiniLM-L-12-v2\n[LABEL_0]\n---\\nlicense: apache-2.0\\n---\\n# Cross-Encoder...\n\n\n2\ncardiffnlp/twitter-xlm-roberta-base-sentiment\n[Negative, Neutral, Positive]\n---\\nlanguage: multilingual\\nwidget:\\n- text: ...\n\n\n3\nfacebook/bart-large-mnli\n[contradiction, entailment, neutral]\n---\\nlicense: mit\\nthumbnail: https://huggingf...\n\n\n4\nProsusAI/finbert\n[positive, negative, neutral]\n---\\nlanguage: \"en\"\\ntags:\\n- financial-sentim...\n\n\n...\n...\n...\n...\n\n\n6845\njinwooChoi/SKKU_AP_SA_KBT6\n[LABEL_0, LABEL_1, LABEL_2]\nEntry not found\n\n\n6846\njinwooChoi/SKKU_AP_SA_KBT7\n[LABEL_0, LABEL_1, LABEL_2]\nEntry not found\n\n\n6847\nnaem1023/electra-phrase-clause-classification-...\nNone\nEntry not found\n\n\n6848\nnaem1023/electra-phrase-clause-classification-...\nNone\n---\\nlicense: apache-2.0\\n---\\n\n\n\n6849\nYYAH/Bert_Roman_Urdu\n[LABEL_0, LABEL_1, LABEL_2, LABEL_3]\n---\\nlicense: unknown\\n---\\n\n\n\n\n\n6850 rows Ã— 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can see we now have a DataFrame containing the modelID, the model labels and the README.md for each model (where it exists).\nSince the README.md (the model card) is the obvious source of information about a model weâ€™ll start here. One question we may have is how long our the README.md is. Some models have very detailed model cards whilst others have very little information in the model card. We can get a bit of a sense of this by looking at the range of README.md lenghts:\n\ndf['readme'].apply(len).describe()\n\ncount     6850.000000\nmean      1009.164818\nstd       1750.509155\nmin          0.000000\n25%         15.000000\n50%         20.500000\n75%       1736.000000\nmax      56172.000000\nName: readme, dtype: float64\n\n\n\nWe might want to filter on the length of the README so weâ€™ll store that info in a new column.\n\ndf['readme_len'] = df['readme'].apply(len)\n\nSince we might want to work with this data again, letâ€™s load it into a datasets Dataset and use push_to_hub to store a copy.\n\nfrom datasets import Dataset\n\n\nds = Dataset.from_pandas(df)\nds\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len'],\n    num_rows: 6850\n})\n\n\n\n\nfrom huggingface_hub import notebook_login\n\n\nnotebook_login()\n\nLogin successful\nYour token has been saved to /root/.huggingface/token\n\n\n\nds.push_to_hub('davanstrien/hf_model_metadata')\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:1951: FutureWarning: `identical_ok` has no effect and is deprecated. It will be removed in 0.11.0.\n  FutureWarning,\n\n\nWe can now load it again using load_dataset.\n\nfrom datasets import load_dataset\n\n\nds = load_dataset('davanstrien/hf_model_metadata', split='train')\n\n\n\n\nUsing custom data configuration davanstrien--hf_model_metadata-019f1ad4bdf705b5\n\n\nDownloading and preparing dataset None/None (download: 3.71 MiB, generated: 10.64 MiB, post-processed: Unknown size, total: 14.35 MiB) to /root/.cache/huggingface/datasets/davanstrien___parquet/davanstrien--hf_model_metadata-019f1ad4bdf705b5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/davanstrien___parquet/davanstrien--hf_model_metadata-019f1ad4bdf705b5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n\n\nClean up some memoryâ€¦\n\ndel df"
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#semantic-search-of-model-cards",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#semantic-search-of-model-cards",
    "title": "Searching for machine learning models using semantic search",
    "section": "Semantic search of model cards",
    "text": "Semantic search of model cards\nWe now get to the main point of all of this. Can we use semantic search to try and find models of interest? For this, weâ€™ll use the sentence-transformers library. This blog wonâ€™t cover all the background of this library. The docs give a helpful overview and some tutorials.\nTo start, weâ€™ll see if we can search using the information in the README.md. This should, in theory, contain data that might be similar to the kinds of things we want to search for when finding candidate models. We might prefer to use semantic search over an exact match because the terms we use might be different, or there is a related concept/model that might be close enough to make it worthwhile for fine-tuning.\nFirst, we import the SentenceTransformer class and some util functions.\n\nfrom sentence_transformers import SentenceTransformer, util\n\nWeâ€™ll now download an embedding model. There are many we could choose from but since weâ€™re just trying things out at the moment we wonâ€™t stress about the particular model we use here.\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLetâ€™s start on longer READMEâ€™s, here i mean a long readme that is just not super shortâ€¦\n\nds_longer_readmes = ds.filter(lambda x: x['readme_len']&gt;100)\n\n\n\n\nWe now create embeddings for the readme column and store this in a new embedding column\n\ndef encode_readme(readme):\n    return model.encode(readme,device='cuda')\n\n\nds_with_embeddings = ds_longer_readmes.map(lambda example: \n                                           {\"embedding\":encode_readme(example['readme'])},batched=True, batch_size=16)\n                                                                      \n\n\n\n\n\nds_with_embeddings\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'embedding'],\n    num_rows: 3284\n})\n\n\n\nWe can now use the add_fais_index to create an index which allows us to efficiently query these embeddings\n\nds_with_embeddings.add_faiss_index(column='embedding')\n\n\n\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'embedding'],\n    num_rows: 3284\n})\n\n\n\n\nSimilar models\nTo start, weâ€™ll take a readme for a model and see how well the model performs on finding similar models.\n\nquery_readme = ds_with_embeddings[35]['readme']\n\n\nprint(query_readme)\n\n# Twitter-roBERTa-base for Irony Detection\n\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark.\n\n- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = [\n    ]\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) &gt; 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='irony'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) &gt; 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Great, it broke the first day...\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Great, it broke the first day...\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking]\n    s = scores[ranking]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) irony 0.914\n2) non_irony 0.086\n```\n\n\n\n\nWe pass this README into the model we used to create our embedding. This creates a query embedding for this README.\n\nq = model.encode(query_readme)\n\nWe can use get_nearest_examples to look for the most similar results to this query.\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embedding', q, k=10)\n\nLetâ€™s take a look at the first result\n\n print(retrieved_examples['modelId'][0])\n\ncardiffnlp/twitter-roberta-base-irony\n\n\n\n\nprint(retrieved_examples[\"readme\"][0])\n\n# Twitter-roBERTa-base for Irony Detection\n\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark.\n\n- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = [\n    ]\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) &gt; 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='irony'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) &gt; 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Great, it broke the first day...\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Great, it broke the first day...\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking]\n    s = scores[ranking]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) irony 0.914\n2) non_irony 0.086\n```\n\n\n\n\nand a lower similarity result\n\nprint(retrieved_examples[\"readme\"][9])\n\n---\nlanguage: \"en\"\ntags:\n- roberta\n- sentiment\n- twitter\n\nwidget:\n- text: \"Oh no. This is bad..\"\n- text: \"To be or not to be.\"\n- text: \"Oh Happy Day\"\n\n---\n\nThis RoBERTa-based model can classify the sentiment of English language text in 3 classes:\n\n- positive ðŸ˜€\n- neutral ðŸ˜\n- negative ðŸ™\n\nThe model was fine-tuned on 5,304 manually annotated social media posts. \nThe hold-out accuracy is 86.1%. \nFor details on the training approach see Web Appendix F in Hartmann et al. (2021). \n\n# Application\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\", \nreturn_all_scores=True)\nclassifier(\"This is so nice!\")\n```\n\n```python\nOutput:\n[[{'label': 'negative', 'score': 0.00016451838018838316},\n  {'label': 'neutral', 'score': 0.000174045650055632},\n  {'label': 'positive', 'score': 0.9996614456176758}]]\n```\n\n# Reference\nPlease cite (https://journals.sagepub.com/doi/full/10.1177/00222437211037258) when you use our model. Feel free to \nreach out to (mailto:j.p.hartmann@rug.nl) with any questions or feedback you may have.\n```\n@article{hartmann2021,\n  title={The Power of Brand Selfies},\n  author={Hartmann, Jochen and Heitmann, Mark and Schamp, Christina and Netzer, Oded},\n  journal={Journal of Marketing Research}\n  year={2021}\n}\n```\n\n\n\nThe results seem pretty reasonable; the first result appears to be a duplicate. The lower result is for a slightly different task using social media data.\n\n\nSearching\nBeing able to find similar model cards is a start but we actually wanted to be able to search directly. Letâ€™s see how our results do if we instead search for some terms we might use to try and find suitable models.\n\nq = model.encode(\"fake news\")\n\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embedding', q, k=10)\n\n\nprint(retrieved_examples[\"readme\"][0])\n\nThis model is fined tuned for the Fake news classifier: Train a text classification model to detect fake news \narticles. Base on the Kaggle dataset(https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset).\n\n\n\n\n\nprint(retrieved_examples[\"readme\"][1])\n\nFake news classifier\nThis model trains a text classification model to detect fake news articles, \n\nit uses distilbert-base-uncased-finetuned-sst-2-english pretrained model to work on \n\nfake and real news dataset from kaggle (https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)\n\n\n\n\nprint(retrieved_examples[\"readme\"][2])\n\n---\nlicense: mit\n---\n# Fake and real news classification task \n\nModel  : [DistilRoBERTa base model](https://huggingface.co/distilroberta-base)\n\nDataset : [Fake and real news dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)\n\n\n\n\n\n\n\n\n\nNot a bad start. Letâ€™s try another one\n\nq = model.encode(\"financial sentiment\")\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embedding', q, k=10)\nprint(retrieved_examples[\"readme\"][0])\n\n---\nlanguage: en\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\ndatasets:\n- financial_phrasebank\nwidget:\n- text: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 %\nof net sales.\n- text: Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR \n4,000.\n- text: Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in the \ncorresponding period of 2008.\n---\n### FinancialBERT for Sentiment Analysis\n\n[*FinancialBERT*](https://huggingface.co/ahmedrachid/FinancialBERT) is a BERT model pre-trained on a large corpora \nof financial texts. The purpose is to enhance financial NLP research and practice in financial domain, hoping that \nfinancial practitioners and researchers can benefit from this model without the necessity of the significant \ncomputational resources required to train the model. \n\nThe model was fine-tuned for Sentiment Analysis task on _Financial PhraseBank_ dataset. Experiments show that this \nmodel outperforms the general BERT and other financial domain-specific models.\n \nMore details on `FinancialBERT`'s pre-training process can be found at: \nhttps://www.researchgate.net/publication/358284785_FinancialBERT_-_A_Pretrained_Language_Model_for_Financial_Text_M\nining\n\n### Training data\nFinancialBERT model was fine-tuned on [Financial \nPhraseBank](https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10), a dataset consisting of \n4840 Financial News categorised by sentiment (negative, neutral, positive).\n\n### Fine-tuning hyper-parameters\n- learning_rate = 2e-5\n- batch_size = 32\n- max_seq_length = 512\n- num_train_epochs = 5\n\n### Evaluation metrics\nThe evaluation metrics used are: Precision, Recall and F1-score. The following is the classification report on the \ntest set.\n\n| sentiment  | precision        | recall           | f1-score  | support  |\n| ------------- |:-------------:|:-------------:|:-------------:| -----:|\n| negative | 0.96      | 0.97 | 0.97 | 58 |\n| neutral | 0.98      | 0.99 | 0.98 | 279 |\n| positive | 0.98     | 0.97 | 0.97 | 148 |\n| macro avg | 0.97     | 0.98 | 0.98 | 485 |\n| weighted avg | 0.98     | 0.98 | 0.98 | 485 |\n\n ### How to use \nThe model can be used thanks to Transformers pipeline for sentiment analysis.\n```python\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nmodel = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\ntokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n\nnlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\nsentences = [\"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing\n7.7 % of net sales.\",  \n             \"Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least \nEUR 4,000.\", \n             \"Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in \nthe corresponding period of 2008.\", \n             ]\nresults = nlp(sentences)\nprint(results)\n\n[{'label': 'positive', 'score': 0.9998133778572083},\n {'label': 'neutral', 'score': 0.9997822642326355},\n {'label': 'negative', 'score': 0.9877365231513977}]\n```\n\n&gt; Created by [Ahmed Rachid Hazourli](https://www.linkedin.com/in/ahmed-rachid/)\n\n\n\n\n\nprint(retrieved_examples[\"readme\"][1])\n\n---\nlanguage: \"en\"\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: \"Stocks rallied and the British pound gained.\"\n---\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT\nlanguage model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial \nsentiment classification. [Financial \nPhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientation\ns_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper \n[FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our \nrelated (https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on \nMedium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and \ninvesting globally in markets with long-term growth potential, Prosus builds leading consumer internet companies \nthat empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araciprosuscom and Zulkuf Genc zulkuf.gencprosuscom about any FinBERT related issues\nand questions.\n\n\n\n\nprint(retrieved_examples[\"readme\"][9])\n\n---\nlicense: apache-2.0\ntags:\n- Finance-sentiment-analysis\n- generated_from_trainer\nmetrics:\n- f1\n- accuracy\n- precision\n- recall\nmodel-index:\n- name: bert-base-finance-sentiment-noisy-search\n  results: []\nwidget:\n - text: \"Third quarter reported revenues were $10.9 billion, up 5 percent compared to prior year and up 8 percent \non a currency-neutral basis\"\n   example_title: \"Positive\"\n - text: \"The London-listed website for businesses reported a pretax loss of $26.6 million compared with a loss of \n$12.9 million the previous year\"\n   example_title: \"Negative\"\n - text:  \"Microsoft updates Outlook, Teams, and PowerPoint to be hybrid work ready\"\n   example_title: \"Neutral\"\n---\n\n&lt;!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. --&gt;\n\n# bert-base-finance-sentiment-noisy-search\n\nThis model is a fine-tuned version of (https://huggingface.co/bert-base-uncased) on Kaggle finance news sentiment \nanalysis with data enhancement using noisy search. The process is explained below:\n\n1. First \"bert-base-uncased\" was fine-tuned on Kaggle's finance news sentiment analysis \nhttps://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news dataset achieving accuracy of about 88%\n2. We then used a logistic-regression classifier on the same data. Here we looked at coefficients that contributed \nthe most to the \"Positive\" and \"Negative\" classes by inspecting only bi-grams. \n3. Using the top 25 bi-grams per class (i.e. \"Positive\" / \"Negative\") we invoked Bing news search with those \nbi-grams and retrieved up to 50 news items per bi-gram phrase.\n4. We called it \"noisy-search\" because it is assumed the positive bi-grams (e.g. \"profit rose\" , \"growth net\") give\nrise to positive examples whereas negative bi-grams (e.g. \"loss increase\", \"share loss\") result in negative \nexamples but note that we didn't test for the validity of this assumption (hence: noisy-search)\n5. For each article we kept the title + excerpt and labeled it according to pre-assumptions on class associations.\n6. We then trained the same model on the noisy data and apply it to an held-out test set from the original data set\nsplit.\n7. Training with couple of thousands noisy \"positives\" and \"negatives\" examples yielded a test set accuracy of \nabout 95%. \n8. It shows that by automatically collecting noisy examples using search we can boost accuracy performance from \nabout 88% to more than 95%.\n\nAccuracy results for Logistic Regression (LR) and BERT (base-cased) are shown in the attached pdf:\n\nhttps://drive.google.com/file/d/1MI9gRdppactVZ_XvhCwvoaOV1aRfprrd/view?usp=sharing \n\n\n## Model description\n\nBERT model trained on noisy data from search results. See PDF for more details.\n\n## Intended uses & limitations\n\nIntended for use on finance news sentiment analysis with 3 options: \"Positive\", \"Neutral\" and \"Negative\"\nTo get the best results feed the classifier with the title and either the 1st paragraph or a short news \nsummarization e.g. of up to 64 tokens. \n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n\n\n\n\nThese seem like a good starting point. However, we have a few issues relying on model cards alone. Firstly a lot of models donâ€™t include them and the quality of them can be mixed. Itâ€™s maybe a question if we want to use a model that has no model card at all but it is possible that despite a good model card we donâ€™t capture everything weâ€™d need for searching in the README."
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#can-we-search-using-model-labels",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#can-we-search-using-model-labels",
    "title": "Searching for machine learning models using semantic search",
    "section": "Can we search using model labels?",
    "text": "Can we search using model labels?\nWeâ€™re only working with classification models in this case. For most Pytorch models on the hub, we have a config file. This config usually contains the modelâ€™s labels. For example, â€˜positiveâ€™, â€˜negativeâ€™.\nMaybe instead of relying only on the metadata, we can search â€˜insideâ€™ the model. The labels will often be a helpful reflection of what weâ€™re looking for. For example, we want to find a sentiment classification model that roughly puts text into positive or negative sentiment. Again, relying on exact label matches may not work well, but maybe embeddings get around this problem. Letâ€™s try it out!\nLetâ€™s look at an example label.\n\nds[0]['label']\n\n['NEGATIVE', 'POSITIVE']\n\n\n\nSince weâ€™re expecting labels to match this format lets filter out any that donâ€™t fit this structure.\n\nds = ds.filter(lambda example: isinstance(example['label'],list))\n\n\n\n\n\nHow to create embeddings for our labels?\nHow should we encode our labels? At the moment, we have a list of labels. One option would be to create an embedding for every single label, which will require us to query multiple embeddings to check for a match. We may also prefer intuatively to have an embedding for the combination of labels. This is because we probably know more about the model type from all its labels rather than looking at one label at a time. Weâ€™ll deal with the labels very crudely by joining them on , and creating a single string out of all the labels. Iâ€™m sure this isnâ€™t the best possible approach, but it might be a good place to start testing this idea.\n\nds = ds.map(lambda example: {\"string_label\": \",\".join(example['label'])})\n\n\n\n\n\nds\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'string_label'],\n    num_rows: 4175\n})\n\n\n\n\nds_with_embeddings = ds.map(lambda example: \n                                           {\"label_embedding\":encode_readme(example['string_label'])},batched=True, batch_size=16)\n                                                                      \n\n\n\n\n\nds_with_embeddings\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'string_label', 'label_embedding'],\n    num_rows: 4175\n})\n\n\n\n\n\nSearching with labels\nNow we have some embeddings for the labels, letâ€™s try searching. Letâ€™s start with an existing set of labels to see how well we can match those.\n\nds_with_embeddings[0]['string_label']\n\n'NEGATIVE,POSITIVE'\n\n\n\nUnable to display output for mime type(s): application/vnd.google.colaboratory.intrinsic+json\n\n\n\nq = model.encode(\"negative\")\n\n\nds_with_embeddings.add_faiss_index(column='label_embedding')\n\n\n\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'string_label', 'label_embedding'],\n    num_rows: 4175\n})\n\n\n\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('label_embedding', q, k=10)\n\n\nretrieved_examples['label'][:10]\n\n[\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive']\n]\n\n\n\nSo far, these results look pretty good, although we havenâ€™t done anything we couldnâ€™t do with simple string matching. Letâ€™s see what happens if we use a slightly more abstract search.\n\nq = model.encode(\"music\")\n\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('label_embedding', q, k=10)\n\n\nretrieved_examples['label'][:10]\n\n[\n    ['Dance', 'Heavy Metal', 'Hip Hop', 'Indie', 'Pop', 'Rock'],\n    ['Dance', 'Heavy Metal', 'Hip Hop', 'Indie', 'Pop', 'Rock'],\n    ['Dance', 'Heavy Metal', 'Hip Hop', 'Indie', 'Pop', 'Rock'],\n    [\n        'Alternative',\n        'Country',\n        'Eletronic Music',\n        'Gospel and Worship Songs',\n        'Hip-Hop',\n        'Jazz/Blues',\n        'Pop',\n        'R&B/Soul',\n        'Reggae',\n        'Rock'\n    ],\n    ['business', 'entertainment', 'sports'],\n    ['_silence_', '_unknown_', 'down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes'],\n    ['angry', 'happy', 'others', 'sad'],\n    ['Feeling', 'Thinking'],\n    [\n        'am_thuc',\n        'bong_da',\n        'cho_thue',\n        'doi_song',\n        'dong_vat',\n        'mua_ban',\n        'nhac',\n        'phim',\n        'phu_kien',\n        'sach',\n        'showbiz',\n        'the_thao',\n        'thoi_trang_nam',\n        'thoi_trang_nu',\n        'thuc_vat',\n        'tin_bds',\n        'tin_tuc',\n        'tri_thuc'\n    ],\n    ['intimacy']\n]\n\n\n\nWe can see that we get back labels related to music genre: ['Dance', 'Heavy Metal', 'Hip Hop', 'Indie', 'Pop', 'Rock'], for our first four results. After that, we get back ['business', 'entertainment', 'sports'], which might not be too far off what we want if we searched for music.\nHow about another search term\n\nq = model.encode(\"hateful\")\n\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('label_embedding', q, k=10)\n\n\nretrieved_examples['label'][:10]\n\n[\n    ['Hateful', 'Not hateful'],\n    ['Hateful', 'Not hateful'],\n    ['hateful', 'non-hateful'],\n    ['hateful', 'non-hateful'],\n    ['hateful', 'non-hateful'],\n    ['HATE', 'NOT_HATE'],\n    ['NON_HATE', 'HATE'],\n    ['NON_HATE', 'HATE'],\n    ['NON_HATE', 'HATE'],\n    ['NON_HATE', 'HATE']\n]\n\n\n\nAgain here we have something quite close to what weâ€™d get with string matching, but we have a bit more flexibility in how we spell/define our labels which might help surface more possible results.\nWeâ€™ll try a bunch more thingsâ€¦\n\ndef query_labels(query:str):\n    q = model.encode(query)\n    scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('label_embedding', q, k=10)\n    print(f\"results for: {query}\")\n    print(list(zip(retrieved_examples['label'][:10],retrieved_examples['modelId'][:10])))\n\n\nquery_labels(\"politics\")\n\nresults for: politics\n\n\n\n[\n    (['Democrat', 'Republican'], 'm-newhauser/distilbert-political-tweets'),\n    (['Geopolitical', 'Personal', 'Political', 'Religious'], 'dee4hf/autotrain-deephate2-1093539673'),\n    (['None', 'Environmental', 'Social', 'Governance'], 'yiyanghkust/finbert-esg'),\n    (['business', 'entertainment', 'sports'], 'bipin/malayalam-news-classifier'),\n    (\n        ['CRIME', 'ENTERTAINMENT', 'Finance', 'POLITICS', 'SPORTS', 'Terrorism'],\n        'Yarn007/autotrain-Napkin-872827783'\n    ),\n    (['business', 'entertainment', 'politics', 'sport', 'tech'], 'abhishek/autonlp-bbc-roberta-37249301'),\n    (\n        ['business', 'entertainment', 'politics', 'sport', 'tech'],\n        'abhishek/autonlp-bbc-news-classification-37229289'\n    ),\n    (['business', 'entertainment', 'politics', 'sport', 'tech'], 'Yarn/autotrain-Traimn-853827191'),\n    (['Neutral', 'Propaganda'], 'Real29/my-model-proppy'),\n    (['Neutral', 'Propaganda'], 'Real29/my-model-ptc')\n]\n\n\n\n\nquery_labels(\"fiction, non_fiction\")\n\nresults for: fiction, non_fiction\n\n\n\n[\n    (\n        ['action', 'drama', 'horror', 'sci_fi', 'superhero', 'thriller'],\n        'Tejas3/distillbert_110_uncased_movie_genre'\n    ),\n    (['action', 'drama', 'horror', 'sci_fi', 'superhero', 'thriller'], 'Tejas3/distillbert_110_uncased_v1'),\n    (\n        ['action', 'animation', 'comedy', 'drama', 'romance', 'thriller'],\n        'langfab/distilbert-base-uncased-finetuned-movie-genre'\n    ),\n    (['HATE', 'NON_HATE'], 'anthonny/dehatebert-mono-spanish-finetuned-sentiments_reviews_politicos'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-english'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-german'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-italian'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-spanish'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-portugese'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-polish')\n]\n\n\n\nLetâ€™s try the set of emotions one should feel everyday.\n\nquery_labels(\"worry, disgust, anxiety, fear\")\n\nresults for: worry, disgust, anxiety, fear\n\n\n\n[\n    (['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'], 'crcb/isear_bert'),\n    (\n        ['anger', 'disgust', 'fear', 'joy', 'others', 'sadness', 'surprise'],\n        'pysentimiento/robertuito-emotion-analysis'\n    ),\n    (\n        ['anger', 'disgust', 'fear', 'joy', 'others', 'sadness', 'surprise'],\n        'daveni/twitter-xlm-roberta-emotion-es'\n    ),\n    (\n        ['anger', 'disgust', 'fear', 'joy', 'others', 'sadness', 'surprise'],\n        'finiteautomata/beto-emotion-analysis'\n    ),\n    (\n        ['anger', 'disgust', 'fear', 'joy', 'others', 'sadness', 'surprise'],\n        'finiteautomata/bertweet-base-emotion-analysis'\n    ),\n    (['ANGER', 'DISGUST', 'FEAR', 'HAPPINESS', 'NEUTRALITY', 'SADNESS', 'SURPRISED'], 'Gunulhona/tbecmodel'),\n    (\n        ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust'],\n        'Yuetian/bert-base-uncased-finetuned-plutchik-emotion'\n    ),\n    (['anger', 'fear', 'happy', 'love', 'sadness'], 'jasonpratamas7/Thesis-Model-1'),\n    (['anger', 'fear', 'happy', 'love', 'sadness'], 'jasonpratamas7/Thesis-Model1'),\n    (['anger', 'fear', 'happy', 'love', 'sadness'], 'StevenLimcorn/indonesian-roberta-base-emotion-classifier')\n]\n\n\n\nThis example of searching for a set of labels might be a better approach in general since the query will better match the format of the intitial search."
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#conclusion",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#conclusion",
    "title": "Searching for machine learning models using semantic search",
    "section": "Conclusion",
    "text": "Conclusion\nIt seems like there is some merit in exploring some of these ideas further. There are a lot of improvements that could be made: - how the embeddings are created - removing some â€˜noiseâ€™ from the README, for example, by first parsing the Markdown - improving how the embeddings are created for the labels - combining the embeddings in some way either upfront or when queryig - a bunch of other thingsâ€¦\nIf I find some spare time, I plan to dig into these topics a bit further. This is also a nice excuse to play with one of the new open source embedding databases that have popped up in the last couple of years."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Indexing and searching a UFO dataset with Qdrant and ColPali\n\n\n\n\n\nUsing Qdrant multivectors to index and search a UFO document dataset\n\n\n\n\n\nOct 2, 2024\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset\n\n\n\n\n\nUsing an open VLM to generate queries for a multimodal retrieval model\n\n\n\n\n\nSep 23, 2024\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nTracing Text Generation Inference calls\n\n\n\n\n\nHow to trace text generation inference calls with Langfuse.\n\n\n\n\n\nApr 5, 2024\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nHow to load a Hugging Face dataset into Qdrant?\n\n\n\n\n\nLoading a Hugging Face dataset into Qdrant is easy. This post shows how to do it.\n\n\n\n\n\nNov 8, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do groupby for Hugging Face datasets\n\n\n\n\n\nHow can you groupby in Hugging Face datasets?\n\n\n\n\n\nSep 18, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nExploring language metadata for datasets on the Hugging Face Hub\n\n\n\n\n\nUsing the huggingface_hub library to asses metadata on the hub\n\n\n\n\n\nJun 7, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nDynamically updating a Hugging Face hub organization README\n\n\n\n\n\nUsing the huggingface_hub library and Jinja to update a README dynamically\n\n\n\n\n\nMar 7, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Hugging Face AutoTrain to train an image classifier without writing any code.\n\n\n\n\n\n\nautotrain\n\n\n\nHow can we train useful machine learning models without writing code?\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA (very brief) intro to exploring metadata on the Hugging Face Hub\n\n\n\n\n\nHow we can use the huggingface_hub library to explore metadata on the Hugging Face Hub.\n\n\n\n\n\nJan 16, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nLabel Studio x Hugging Face datasets hub\n\n\n\n\n\nUsing label studio and the Hugging Face datasets hub to iteratively annotate a dataset\n\n\n\n\n\nSep 7, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nTraining an object detection model using Hugging Face\n\n\n\n\n\nTraining a Detr object detection model using Hugging Face transformers and datasets\n\n\n\n\n\nAug 16, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nSearching for machine learning models using semantic search\n\n\n\n\n\nFinding models on the Hugging Face hub using semantic search\n\n\n\n\n\nJul 26, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nCombining Hugging Face datasets with dask\n\n\n\n\n\nUsing ðŸ¤— datasets in combination with dask\n\n\n\n\n\nJun 20, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ðŸ¤— datasets for image search\n\n\n\n\n\nUsing the ðŸ¤— datasets to make an image search engine for British Library Book Illustrations\n\n\n\n\n\nJan 13, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the ðŸ¤— Hub for model storage\n\n\n\n\n\nHow Iâ€™m planning to use the huggingface hub for storing flyswot models\n\n\n\n\n\nDec 30, 2021\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nflyswot\n\n\n\n\n\n\nglam\n\n\nflyswot\n\n\n\nAttempting to deploy machine learning in an existing workflow\n\n\n\n\n\nDec 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nImage labeling vs classification models\n\n\n\n\n\nComparing the loss functions of label and classification models\n\n\n\n\n\nOct 12, 2020\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Optimization for Transfer Learning\n\n\n\n\n\nOptimising Hyperparameters using optuna and fastai2\n\n\n\n\n\nJul 1, 2020\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-model metadata generation\n\n\n\n\n\nExperiment in combining text and tabular models to generate web archive metadata\n\n\n\n\n\nMay 3, 2020\n\n\nDaniel van Strien\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html",
    "href": "posts/post-with-code/2020-05-03-multi-model.html",
    "title": "Multi-model metadata generation",
    "section": "",
    "text": "Deep learning models usually take one type of input (image, text etc.) to predict output labels (category, entities etc). This usually makes sense if the data you are using to make predictions contains a lot of information. i.e.Â a chunk of text from a movie review or an image.\nRecently I have been playing around with a Website Classification Dataset from the UK web archive. The dataset is derived from a manually curated web archive which contains a primary and secondary category for each web page. The UK web archive has made a dataset available based on this archive which contains the manually classified subject categories alongside the page URL and the page title.\nAs part of playing around with this dataset I was keen to see if a multi-input model would work well. In this case exploring a model that takes both text and tabular data as input. A preview of the data:\n\n#hide_input\nimport pandas as pd\ntsv ='https://gist.githubusercontent.com/davanstrien/5e22b725046eddc2f1ee06b108f27e48/raw/71426e6b92c7fa98140a95728a5ea55171b948cd/classification.tsv'\ndf = pd.read_csv(tsv, error_bad_lines=False, index_col=0)\ndf.head()\n\n\n\n\n\n\n\n\nPrimary Category\nSecondary Category\nTitle\nURL\n\n\n\n\n0\nArts & Humanities\nArchitecture\n68 Dean Street\nhttp://www.sixty8.com/\n\n\n1\nArts & Humanities\nArchitecture\nAbandoned Communities\nhttp://www.abandonedcommunities.co.uk/\n\n\n2\nArts & Humanities\nArchitecture\nAlexander Thomson Society\nhttp://www.greekthomson.com/\n\n\n3\nArts & Humanities\nArchitecture\nArab British Centre, The\nhttp://www.arabbritishcentre.org.uk/\n\n\n4\nArts & Humanities\nArchitecture\nArchitectural Association School of Architecture\nhttp://www.aaschool.ac.uk/\n\n\n\n\n\n\n\nBased on this data the UK web archive are interested: &gt;â€œin understanding whether high-level metadata like this can be used to train an appropriate automatic classification system so that we might use this manually generated dataset to partially automate the categorisation of our larger archives.â€\nThis is going to be fairly tricky but offers a nice excuse to try to use models with multiple inputs to predict our categories.\n\n\nTaking a closer look at the data:\n\n#hide_input \ntsv = 'https://gist.githubusercontent.com/davanstrien/5e22b725046eddc2f1ee06b108f27e48/raw/71426e6b92c7fa98140a95728a5ea55171b948cd/classification.tsv'\ndf = pd.read_csv(tsv, error_bad_lines=False,)\n\n\n\n\nlen(df['Primary Category'].unique())\n\n24\n\n\n\n\n\n\nlen(df['Secondary Category'].unique())\n\n104\n\n\nPredicting a 104 different labels is going to be pretty difficult so Iâ€™ve only used â€˜Primary Categoryâ€™ as the the y target. What is the distribution of these categories like?\n\n#hide_input\ndf['Primary Category'].value_counts()\n\nArts & Humanities                                              5299\nGovernment, Law & Politics                                     4832\nBusiness, Economy & Industry                                   2988\nSociety & Culture                                              2984\nScience & Technology                                           2420\nMedicine & Health                                              2164\nEducation & Research                                           2118\nCompany Web Sites                                               843\nDigital Society                                                 737\nSports and Recreation                                           710\nReligion                                                        417\nTravel & Tourism                                                374\nSocial Problems and Welfare                                     270\nPolitics, Political Theory and Political Systems                123\nCrime, Criminology, Police and Prisons                          101\nLiterature                                                       87\nLaw and Legal System                                             81\nComputer Science, Information Technology and Web Technology      54\nLibraries, Archives and Museums                                  52\nEnvironment                                                      38\nHistory                                                          34\nPublishing, Printing and Bookselling                             26\nPopular Science                                                  23\nLife Sciences                                                    23\nName: Primary Category, dtype: int64\n\n\nðŸ˜¬ We also have a fairly skewed datasets. I could drop some of rows which donâ€™t occur often but since the main objective here is to see if we can use a multi-input model weâ€™ll leave the data as it is for now."
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#looking-at-the-data",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#looking-at-the-data",
    "title": "Multi-model metadata generation",
    "section": "",
    "text": "Taking a closer look at the data:\n\n#hide_input \ntsv = 'https://gist.githubusercontent.com/davanstrien/5e22b725046eddc2f1ee06b108f27e48/raw/71426e6b92c7fa98140a95728a5ea55171b948cd/classification.tsv'\ndf = pd.read_csv(tsv, error_bad_lines=False,)\n\n\n\n\nlen(df['Primary Category'].unique())\n\n24\n\n\n\n\n\n\nlen(df['Secondary Category'].unique())\n\n104\n\n\nPredicting a 104 different labels is going to be pretty difficult so Iâ€™ve only used â€˜Primary Categoryâ€™ as the the y target. What is the distribution of these categories like?\n\n#hide_input\ndf['Primary Category'].value_counts()\n\nArts & Humanities                                              5299\nGovernment, Law & Politics                                     4832\nBusiness, Economy & Industry                                   2988\nSociety & Culture                                              2984\nScience & Technology                                           2420\nMedicine & Health                                              2164\nEducation & Research                                           2118\nCompany Web Sites                                               843\nDigital Society                                                 737\nSports and Recreation                                           710\nReligion                                                        417\nTravel & Tourism                                                374\nSocial Problems and Welfare                                     270\nPolitics, Political Theory and Political Systems                123\nCrime, Criminology, Police and Prisons                          101\nLiterature                                                       87\nLaw and Legal System                                             81\nComputer Science, Information Technology and Web Technology      54\nLibraries, Archives and Museums                                  52\nEnvironment                                                      38\nHistory                                                          34\nPublishing, Printing and Bookselling                             26\nPopular Science                                                  23\nLife Sciences                                                    23\nName: Primary Category, dtype: int64\n\n\nðŸ˜¬ We also have a fairly skewed datasets. I could drop some of rows which donâ€™t occur often but since the main objective here is to see if we can use a multi-input model weâ€™ll leave the data as it is for now."
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#tabular-model",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#tabular-model",
    "title": "Multi-model metadata generation",
    "section": "Tabular model",
    "text": "Tabular model\nIn the dataset above we start of with two columns of data which can be used as inputs for the model. The title is fairly obviously something which we can treat like other text inputs. The URL is a little less obvious. It could be treated as a text input but an alternative is to treat a URL as parts which each contain some information which could be useful for our model.\n\n#hide_input\nprint(df.URL.sample(10).to_list()[3])\nprint(df.URL.sample(10).to_list()[4])\nprint(df.URL.sample(10).to_list()[3])\n\nhttp://www.specialschool.org/\nhttp://www.bbc.co.uk/news/health-12668398\nhttp://www.monarchit.co.uk/\n\n\nEach part of the URL could be split into smaller parts\n\n#hide_input\nprint(df.URL.sample(10).to_list()[3].split('.'))\n\n['http://www', 'darwincountry', 'org/']\n\n\nWhether a url has â€˜.orgâ€™ or â€˜.ukâ€™ or â€˜.comâ€™ could be meaningful for predicting our categories (it might also not be meaningful). It also offers us a way of taking the URLs and composing it into a format which looks more tabular.\n\n#hide_input\ncsv ='https://gist.githubusercontent.com/davanstrien/5e22b725046eddc2f1ee06b108f27e48/raw/4c2a27772bf4d959bf3e58cfa8de9e0b9be69ca7/03_classification_valid_train.csv'\ndf = pd.read_csv(csv, index_col=0)\ndf[['scheme','url1','url3','url4','url5']].sample(5)\n\n\n\n\n\n\n\n\nscheme\nurl1\nurl3\nurl4\nurl5\n\n\n\n\n20011\nhttp\nwww\norg\nNaN\nNaN\n\n\n15825\nhttp\nwww\ncom\nNaN\nNaN\n\n\n6068\nhttp\nwww\nco\nuk\nNaN\n\n\n16507\nhttp\nwww\nco\nuk\nNaN\n\n\n9723\nhttp\nwww\nco\nuk\nNaN\n\n\n\n\n\n\n\nSo far Iâ€™ve only done this very crudely. I suspect tidying up this part of the data will help improve things. At this point though we have something which is a little more tabular looking we can pass to fastai.tabular learner. Now we have some â€˜categoriesâ€™ rather than unique urls.\n\nprint(len(df.url3.unique()))\nprint(len(df.url4.unique()))\n\n279\n56"
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#how-does-this-tabular-model-do",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#how-does-this-tabular-model-do",
    "title": "Multi-model metadata generation",
    "section": "How does this tabular model do?",
    "text": "How does this tabular model do?\nOnce some preprocessing of the url has been done we train a model using the tabular learner. I didnâ€™t do much to try to optimize this model. Tracking best f2 score we end up with:\nBetter model found at epoch 36 with f_beta value: 0.17531482875347137 and an accuracy of 0.334121"
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#how-well-does-a-text-model-do",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#how-well-does-a-text-model-do",
    "title": "Multi-model metadata generation",
    "section": "How well does a text model do?",
    "text": "How well does a text model do?\nNext I tried training using the title field in a NLP model. I tried a few things here.\n\nSentencePiece tokenization\nBy default fastai uses SpaCy to do tokenization with a few additional special tokens added by fastai. I wanted to see if using sentencePiece would work better for processing title fields. SentencePiece allows for various sub-word tokeinzation. This can be useful for agglutinative languages but could also be useful when you have a lot of out of vocabulary words in your corpus. I wanted to see if this also was useful for processing titles since these may contain domain specific terms. I only tried using SentencePiece with â€˜unigramâ€™ tokenization. The best score I got for this was:\nBetter model found at epoch 1 with f_beta value: 0.21195338666439056. ### Default SpaCy tokenization\nI compared the above to using the default fastai tokenizer which uses SpaCy. In this case the default approach worked better. This is probably because we didnâ€™t have a large pre-trained model using the SentencePiece tokenization to use as a starting point. The best score I got for this model was:\nBetter model found at epoch 27 with f_beta value: 0.33327043056488037.\n\n\nUsing the URL as text input\nI wanted to do a quick comparison to the tabular model and use the URL as a text input instead. In this case I used SentencePiece with byte-pair-encoding (BPE). The best score in this case was:\nBetter model found at epoch 3 with f_beta value: 0.2568161189556122.\nThis might end up being a better approach compared to the tabular approach described above."
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#how-does-this-combined-model-do",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#how-does-this-combined-model-do",
    "title": "Multi-model metadata generation",
    "section": "How does this combined model do? ðŸ¤·â€â™‚ï¸",
    "text": "How does this combined model do? ðŸ¤·â€â™‚ï¸\nThe best result I got wasf_beta value: 0.39341238141059875 with an accuracy of 0.595348. A summary of the scores for each models:\n\n\n\nModel\nF2 score\n\n\n\n\nSentencePiece text\n0.211\n\n\nSpacy text\n0.333\n\n\nTabular\n0.175\n\n\nConcat\n0.393\n\n\n\nThis provides some improvement on the tabular or nlp models on their own. I found the combined model was fairly tricky to train and suspect that there could be some improvements in how the model is set up that might improve itâ€™s performance. I am keen to try a similar approach with a dataset where there is more abundant information available to train with."
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html",
    "href": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html",
    "title": "Indexing and searching a UFO dataset with Qdrant and ColPali",
    "section": "",
    "text": "tl;dr: How can use Qdrant + ColPali to index and search a UFO dataset.\nColPali is a new multimodal retrieval approach that aims to replace existing document retrievers, which often rely on an OCR step, with an end-to-end multimodal approach. This approach also aims to take into account the visual content and layout of the documents in addition to the textual content.\nIn a previous blog post, I showed how a VLM can create synthetic data for fine-tuning a ColPali model. In this post, we will examine how to index and search a UFO dataset using Qdrant and ColPali.\n\n\nThe last few years have seen an explosion of vector databases. These databases are optimized for storing and searching for high-dimensional data. Traditionally most of the vectors these databases have been storing are single-vector embeddings i.e a vector that looks like:\n[0.1, 0.2, 0.3, 0.4, 0.5]\nThese vectors can store rich representations of the data they are encoding. However, more recently, several new techniques have been proposed which donâ€™t rely on a single vector but instead use a set of vectors i.e.Â something like this:\n[[0.1, 0.2, 0.3, 0.4, 0.5], \n[0.1, 0.2, 0.3, 0.4, 0.5], \n[0.1, 0.2, 0.3, 0.4, 0.5]]\nIn the case of ColBERT, these multiple vectors represent the query and document tokens. These are then compared using a late interaction mechanism. This â€œlate interactionâ€ essentially means that rather than creating a pooled representation of the document and query, we compare each query token to each document token individually.\nThis similarity has mostly been calculated using MaxSim, which takes the element-wise maximum similarity between the query and document tokens. ColPali uses a similar late interaction mechanism to compare the query and document tokens. The tl;dr is that instead of comparing a single vector to another single vector, we compare a set of vectors to another set of vectors. This approach is borrowed from ColBERT. The diagram below shows how this works in the case of ColBERT.\n\n\n\nVisual representation of ColBERT (source: https://www.answer.ai/posts/colbert-pooling.html)\n\n\nI wonâ€™t go into the details too much here, since other posts cover this already. Instead, I will show how we can use ColPali using the Qdrant vector database.\n\n\nQdrant is a vector database. Many open-source and closed-source vector databases exist, and many â€œtraditionalâ€ databases have also added support for vectors.\nCurrently, only a small number of databases support multivectors. I am aware of the following:\n\nVespa\nQdrant\n\nFrom what I have understood, Weaviate has an example recipe for ColPali. However, they donâ€™t currently natively support MaxSim so using this in practice might be a bit more challenging.\n\n\n\n\n\n\nTip\n\n\n\nVespa, and in particular, Jo Kristian Bergum, have done a lot of work on ColPali and are well worth following if you are interested in this area. Itâ€™s on my list to finally try Vespa!\n\n\nWeâ€™ll start by installing the requirements. For the example, weâ€™ll use a local in-memory Qdrant DB, so you donâ€™t need to worry about any setup. You can easily swap this out for a remote DB or an on-disk DB if you want to use this approach in production.\n\n!pip install uv\n!uv pip install --system colpali_engine&gt;=0.3.1 datasets huggingface_hub[hf_transfer] qdrant-client transformers&gt;=4.45.0 stamina rich\n\nRequirement already satisfied: uv in /usr/local/lib/python3.10/dist-packages (0.4.18)\nAudited 7 packages in 69ms\n\n\n\nimport os\nimport torch\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom tqdm import tqdm\nfrom datasets import load_dataset\n\nWe can set the HF_HUB_ENABLE_HF_TRANSFER environment variable to 1 to enable faster downloads. This is optional but should help if you have a fast connection.\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nFor this example blog, weâ€™ll use the ufo-ColPali dataset that I created as part of my previous post. If you have your own PDFs you want to work with you can use the PDF to Page Images Dataset Space to convert your PDFs into a Hugging Face dataset of page images.\n\ndataset = load_dataset(\"davanstrien/ufo-ColPali\", split=\"train\")\n\nAs a reminder of what the dataset looks like\n\ndataset\n\nDataset({\n    features: ['image', 'raw_queries', 'broad_topical_query', 'broad_topical_explanation', 'specific_detail_query', 'specific_detail_explanation', 'visual_element_query', 'visual_element_explanation', 'parsed_into_json'],\n    num_rows: 2243\n})\n\n\nIn this particular example, weâ€™ll just work with the images, but for many â€˜real worldâ€™ document collections, you are likely to have at least some metadata associated with the documents that could also be indexed in the DB. These could include things like the title, author, date, etc. These additional metadata fields can also be used as part of the search query and for retrieval.\nLetâ€™s look at an example image to see what sort of data we are working with.\n\ndataset[0][\"image\"]\n\n\n\n\n\n\n\n\n\n\n\n\nLetâ€™s now walk through how to index the dataset using Qdrant.\n\n\nWe can use the Qdrant Python client to interact with a Qdrant database. One of the nice features of this client is its in-memory implementation, which is very useful for testing. Itâ€™s not recommended for production use, but itâ€™s great for getting started and means we can jump right into the code without having to worry about setting up a database.\n\n# Only for colab, otherwise can load from a `.env` file or similar\n# from google.colab import userdata\n\nTo prove that switching between the in-memory and remote clients is easy, the code below is all I need to use a DB running on a Qdrant cloud free tier. Qdrant is open source, so you can run it on your infrastructure.\n\n# qdrant_client = QdrantClient(\n#     url=\"https://e25145aa-1e00-489a-948f-4633d3dd8a37.europe-west3-0.gcp.cloud.qdrant.io\",\n#     api_key=userdata.get('qdrantcloud'),\n# )\n\ncollections=[CollectionDescription(name='ufo2'), CollectionDescription(name='ufo')]\n\n\n\nqdrant_client = QdrantClient(\n    \":memory:\"\n)  # Use \":memory:\" for in-memory database or \"path/to/db\" for persistent storage\n\n\n\n\nWeâ€™ll use a ColPali model davanstrien/finetune_colpali_v1_2-ufo-4bit fine-tuned on the ufo-ColPali dataset. This model was trained using the excellent notebook in the ColPali cookbooks repo from Tony Wu.\nIf you want to use a standard ColPali model that is not optimized for UFO document retrieval, you may want to try the newly released vidore/colqwen2-v0.1 model instead.\n\n# from colpali_engine.models import ColQwen2, ColQwen2Processor\n\n# model = ColQwen2.from_pretrained(\n#     \"vidore/colqwen2-v0.1\",\n#     torch_dtype=torch.bfloat16,\n#     device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n# )\n# processor = ColQwen2Processor.from_pretrained(\"vidore/colqwen2-v0.1\")\n\n\nfrom colpali_engine.models import ColPali, ColPaliProcessor\n\n# Initialize ColPali model and processor\nmodel_name = (\n    \"davanstrien/finetune_colpali_v1_2-ufo-4bit\"  # Use the latest version available\n)\ncolpali_model = ColPali.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"cuda:0\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n)\ncolpali_processor = ColPaliProcessor.from_pretrained(\n    \"vidore/colpaligemma-3b-pt-448-base\"\n)\n\n\n\n\n\n\n\nTo use multivectors in Qdrant, we need to specify the dimensions of the vectors. From the Qdrant docs:\n\nThe length of the matrix is fixed, but the number of vectors in the matrix can be different for each point.\n\nSo, in order to use multivectors, we need to specify their length. We can check this by passing in an example image to the model and checking the shape of the output.\n\nsample_image = dataset[0][\"image\"]\nwith torch.no_grad():\n    sample_batch = colpali_processor.process_images([sample_image]).to(\n        colpali_model.device\n    )\n    sample_embedding = colpali_model(**sample_batch)\n\nSince itâ€™s always fun to look at tensors, letâ€™s see what they output embeddings from the ColPali model looks like\n\nsample_embedding\n\ntensor([[[ 0.0093, -0.0045,  0.1445,  ...,  0.0248, -0.0820, -0.1641],\n         [-0.0021,  0.1758,  0.1699,  ...,  0.0179, -0.0952, -0.1138],\n         [-0.0237,  0.0366,  0.0732,  ...,  0.0449, -0.0918, -0.1738],\n         ...,\n         [-0.0588, -0.0243,  0.1650,  ..., -0.0703,  0.0767,  0.0486],\n         [-0.1108,  0.0986,  0.1826,  ...,  0.0278, -0.0576, -0.0520],\n         [-0.0693,  0.1123,  0.2207,  ...,  0.0172, -0.0679, -0.0830]]],\n       device='cuda:0', dtype=torch.bfloat16)\n\n\nand check the shape\n\nsample_embedding.shape\n\ntorch.Size([1, 1030, 128])\n\n\nAs we can see, compared to a usual dense embedding, the output of the ColPali model is a multivector. We can see that the shape of the output is (1, 1030, 128). We can check the length of the vectors by looking at the last dimension of the tensor. In this case, the length of the vectors is 128.\n\nvector_size = sample_embedding.shape[2]\nvector_size\n\n128"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html#multivector-databases-support",
    "href": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html#multivector-databases-support",
    "title": "Indexing and searching a UFO dataset with Qdrant and ColPali",
    "section": "",
    "text": "The last few years have seen an explosion of vector databases. These databases are optimized for storing and searching for high-dimensional data. Traditionally most of the vectors these databases have been storing are single-vector embeddings i.e a vector that looks like:\n[0.1, 0.2, 0.3, 0.4, 0.5]\nThese vectors can store rich representations of the data they are encoding. However, more recently, several new techniques have been proposed which donâ€™t rely on a single vector but instead use a set of vectors i.e.Â something like this:\n[[0.1, 0.2, 0.3, 0.4, 0.5], \n[0.1, 0.2, 0.3, 0.4, 0.5], \n[0.1, 0.2, 0.3, 0.4, 0.5]]\nIn the case of ColBERT, these multiple vectors represent the query and document tokens. These are then compared using a late interaction mechanism. This â€œlate interactionâ€ essentially means that rather than creating a pooled representation of the document and query, we compare each query token to each document token individually.\nThis similarity has mostly been calculated using MaxSim, which takes the element-wise maximum similarity between the query and document tokens. ColPali uses a similar late interaction mechanism to compare the query and document tokens. The tl;dr is that instead of comparing a single vector to another single vector, we compare a set of vectors to another set of vectors. This approach is borrowed from ColBERT. The diagram below shows how this works in the case of ColBERT.\n\n\n\nVisual representation of ColBERT (source: https://www.answer.ai/posts/colbert-pooling.html)\n\n\nI wonâ€™t go into the details too much here, since other posts cover this already. Instead, I will show how we can use ColPali using the Qdrant vector database.\n\n\nQdrant is a vector database. Many open-source and closed-source vector databases exist, and many â€œtraditionalâ€ databases have also added support for vectors.\nCurrently, only a small number of databases support multivectors. I am aware of the following:\n\nVespa\nQdrant\n\nFrom what I have understood, Weaviate has an example recipe for ColPali. However, they donâ€™t currently natively support MaxSim so using this in practice might be a bit more challenging.\n\n\n\n\n\n\nTip\n\n\n\nVespa, and in particular, Jo Kristian Bergum, have done a lot of work on ColPali and are well worth following if you are interested in this area. Itâ€™s on my list to finally try Vespa!\n\n\nWeâ€™ll start by installing the requirements. For the example, weâ€™ll use a local in-memory Qdrant DB, so you donâ€™t need to worry about any setup. You can easily swap this out for a remote DB or an on-disk DB if you want to use this approach in production.\n\n!pip install uv\n!uv pip install --system colpali_engine&gt;=0.3.1 datasets huggingface_hub[hf_transfer] qdrant-client transformers&gt;=4.45.0 stamina rich\n\nRequirement already satisfied: uv in /usr/local/lib/python3.10/dist-packages (0.4.18)\nAudited 7 packages in 69ms\n\n\n\nimport os\nimport torch\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom tqdm import tqdm\nfrom datasets import load_dataset\n\nWe can set the HF_HUB_ENABLE_HF_TRANSFER environment variable to 1 to enable faster downloads. This is optional but should help if you have a fast connection.\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nFor this example blog, weâ€™ll use the ufo-ColPali dataset that I created as part of my previous post. If you have your own PDFs you want to work with you can use the PDF to Page Images Dataset Space to convert your PDFs into a Hugging Face dataset of page images.\n\ndataset = load_dataset(\"davanstrien/ufo-ColPali\", split=\"train\")\n\nAs a reminder of what the dataset looks like\n\ndataset\n\nDataset({\n    features: ['image', 'raw_queries', 'broad_topical_query', 'broad_topical_explanation', 'specific_detail_query', 'specific_detail_explanation', 'visual_element_query', 'visual_element_explanation', 'parsed_into_json'],\n    num_rows: 2243\n})\n\n\nIn this particular example, weâ€™ll just work with the images, but for many â€˜real worldâ€™ document collections, you are likely to have at least some metadata associated with the documents that could also be indexed in the DB. These could include things like the title, author, date, etc. These additional metadata fields can also be used as part of the search query and for retrieval.\nLetâ€™s look at an example image to see what sort of data we are working with.\n\ndataset[0][\"image\"]"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html#indexing-the-dataset",
    "href": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html#indexing-the-dataset",
    "title": "Indexing and searching a UFO dataset with Qdrant and ColPali",
    "section": "",
    "text": "Letâ€™s now walk through how to index the dataset using Qdrant.\n\n\nWe can use the Qdrant Python client to interact with a Qdrant database. One of the nice features of this client is its in-memory implementation, which is very useful for testing. Itâ€™s not recommended for production use, but itâ€™s great for getting started and means we can jump right into the code without having to worry about setting up a database.\n\n# Only for colab, otherwise can load from a `.env` file or similar\n# from google.colab import userdata\n\nTo prove that switching between the in-memory and remote clients is easy, the code below is all I need to use a DB running on a Qdrant cloud free tier. Qdrant is open source, so you can run it on your infrastructure.\n\n# qdrant_client = QdrantClient(\n#     url=\"https://e25145aa-1e00-489a-948f-4633d3dd8a37.europe-west3-0.gcp.cloud.qdrant.io\",\n#     api_key=userdata.get('qdrantcloud'),\n# )\n\ncollections=[CollectionDescription(name='ufo2'), CollectionDescription(name='ufo')]\n\n\n\nqdrant_client = QdrantClient(\n    \":memory:\"\n)  # Use \":memory:\" for in-memory database or \"path/to/db\" for persistent storage\n\n\n\n\nWeâ€™ll use a ColPali model davanstrien/finetune_colpali_v1_2-ufo-4bit fine-tuned on the ufo-ColPali dataset. This model was trained using the excellent notebook in the ColPali cookbooks repo from Tony Wu.\nIf you want to use a standard ColPali model that is not optimized for UFO document retrieval, you may want to try the newly released vidore/colqwen2-v0.1 model instead.\n\n# from colpali_engine.models import ColQwen2, ColQwen2Processor\n\n# model = ColQwen2.from_pretrained(\n#     \"vidore/colqwen2-v0.1\",\n#     torch_dtype=torch.bfloat16,\n#     device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n# )\n# processor = ColQwen2Processor.from_pretrained(\"vidore/colqwen2-v0.1\")\n\n\nfrom colpali_engine.models import ColPali, ColPaliProcessor\n\n# Initialize ColPali model and processor\nmodel_name = (\n    \"davanstrien/finetune_colpali_v1_2-ufo-4bit\"  # Use the latest version available\n)\ncolpali_model = ColPali.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"cuda:0\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n)\ncolpali_processor = ColPaliProcessor.from_pretrained(\n    \"vidore/colpaligemma-3b-pt-448-base\"\n)\n\n\n\n\n\n\n\nTo use multivectors in Qdrant, we need to specify the dimensions of the vectors. From the Qdrant docs:\n\nThe length of the matrix is fixed, but the number of vectors in the matrix can be different for each point.\n\nSo, in order to use multivectors, we need to specify their length. We can check this by passing in an example image to the model and checking the shape of the output.\n\nsample_image = dataset[0][\"image\"]\nwith torch.no_grad():\n    sample_batch = colpali_processor.process_images([sample_image]).to(\n        colpali_model.device\n    )\n    sample_embedding = colpali_model(**sample_batch)\n\nSince itâ€™s always fun to look at tensors, letâ€™s see what they output embeddings from the ColPali model looks like\n\nsample_embedding\n\ntensor([[[ 0.0093, -0.0045,  0.1445,  ...,  0.0248, -0.0820, -0.1641],\n         [-0.0021,  0.1758,  0.1699,  ...,  0.0179, -0.0952, -0.1138],\n         [-0.0237,  0.0366,  0.0732,  ...,  0.0449, -0.0918, -0.1738],\n         ...,\n         [-0.0588, -0.0243,  0.1650,  ..., -0.0703,  0.0767,  0.0486],\n         [-0.1108,  0.0986,  0.1826,  ...,  0.0278, -0.0576, -0.0520],\n         [-0.0693,  0.1123,  0.2207,  ...,  0.0172, -0.0679, -0.0830]]],\n       device='cuda:0', dtype=torch.bfloat16)\n\n\nand check the shape\n\nsample_embedding.shape\n\ntorch.Size([1, 1030, 128])\n\n\nAs we can see, compared to a usual dense embedding, the output of the ColPali model is a multivector. We can see that the shape of the output is (1, 1030, 128). We can check the length of the vectors by looking at the last dimension of the tensor. In this case, the length of the vectors is 128.\n\nvector_size = sample_embedding.shape[2]\nvector_size\n\n128"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html#conclusion",
    "href": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html#conclusion",
    "title": "Indexing and searching a UFO dataset with Qdrant and ColPali",
    "section": "Conclusion",
    "text": "Conclusion\nColPali remains a relatively novel approach to retrieval, and itâ€™s likely to take some time for all vector DBs to implement proper support for multi-vectors. The community is likely to also discover new optimization approaches to both the ColPali model itself and its integration with vector DBs.\nI left some further resources below and am happy to get feedback on any improvements I can make to this blog post.\n\nFurther resources\n\nHugging Face Hub organization for the ColPali authors: https://huggingface.co/vidore\nBlog post on the rise of vision-driven document retrieval for RAG: https://blog.vespa.ai/the-rise-of-vision-driven-document-retrieval-for-rag/\nBlog post on scaling ColPali using the Vespa database: https://blog.vespa.ai/scaling-colpali-to-billions/\n\n\n\nBonus tips\nA few other things I think are worth exploring if you plan to use this in a production setting.\n\nPre-filtering\nItâ€™s likely in many real use cases that you either have, or can create, some metadata for your documents. These can also be indexed in Qdrant and may help improve the search results and performance of the a multivector approach. If for example, you can filter to remove all documents before/after a certain date, you can reduce the amount of vectors that Qdrant needs to search through.\n\n\n\nCombining vectors\nItâ€™s also possible to use other vectors for the same points in Qdrant. For example, you may want to use a dense vector to store a representation of the text extracted through an existing OCR pipeline (if you already have this to hand itâ€™s worth trying to use it), you can then use these vectors do a first large search and then use the multivector embeddings to do a second stage search limited only to results from the first step. See the filtering docs for more info on how to do this. This could look something like:\n\nsearch for â€œtop secretâ€ using the dense vectors and get the top 1000 results\nsearch for â€œtop secretâ€ using the multivectors but only search in the top 1000 results from the first search\n\n\nBinary Quantization\nItâ€™s very likely to make sense to explore using Binary Quantization for storing your vectors since it can reduce the memory footprint and speed up the search process. You can set this up by doing something like this in the collection creation step\n\nquantization_config = models.BinaryQuantization(\n    binary=models.BinaryQuantizationConfig(\n        always_ram=True,\n    )\n)\n\n\n\n\nSome more search examples\n\nresults_ds = search_by_text_and_return_images(\"sightings data table\")\n\nfor row in results_ds:\n    # display image\n    display(row[\"image\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresults_ds = search_by_text_and_return_images(\"ufo experiences\")\nfor row in results_ds:\n    display(row[\"image\"])"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html#glossary",
    "href": "posts/post-with-code/colpali-qdrant/using_colpali_with_qdrant.html#glossary",
    "title": "Indexing and searching a UFO dataset with Qdrant and ColPali",
    "section": "Glossary",
    "text": "Glossary\n\nColPali: A multimodal retrieval approach for document search.\nQdrant: An open-source vector database.\nVector Database: A database optimized for storing and searching high-dimensional vector data.\nMultivector: A set of vectors used to represent a single item, as opposed to a single vector.\nLate Interaction: A mechanism used in ColPali to compare query and document tokens individually at query time.\nMaxSim: A similarity calculation method used in ColPali and ColBERT.\nQuantization: A compression technique used to reduce the memory footprint of vectors.\nScalar Quantization: A specific type of quantization that reduces the number of bits used to represent vector components.\nOCR (Optical Character Recognition): Technology used to convert images of text into machine-readable text.\nVLM (Vision Language Model): A type of AI model that can process both image and text data.\nMultimodal: Relating to multiple modes of data, such as text and images.\nEmbedding: A numerical representation of data in a vector space.\nCosine Similarity: A measure of similarity between two vectors.\nIndexing: The process of organizing data in a database to optimize search and retrieval.\nBinary Quantization: A form of quantization that converts vectors to binary (0 or 1) values.\nRAG (Retrieval-Augmented Generation): A technique that combines retrieval of relevant information with text generation."
  },
  {
    "objectID": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html",
    "href": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html",
    "title": "Combining Hugging Face datasets with dask",
    "section": "",
    "text": "Hugging Face datasets is a super useful library for loading, processing and sharing datasets with other people.\nFor many pre-processing steps it works beautifully. The one area where it can be a bit trickier to use is for EDA style analysis. This column-wise EDA is often important as an early step in working with some data or for preparing a data card.\nFortunately combining datasets and another data library, dask works pretty smoothly. This isnâ€™t intended to be a full intro to either datasets or dask but hopefully gives you a sense of how both libaries work and how they can complement each other.\nFirst, make sure we have the required libraries. Rich is there for a little added visual flair âœ¨\n%%capture\n!pip install datasets toolz rich[jupyter] dask\n%load_ext rich"
  },
  {
    "objectID": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#load-some-data",
    "href": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#load-some-data",
    "title": "Combining Hugging Face datasets with dask",
    "section": "Load some data",
    "text": "Load some data\nFor this example we will use a the blbooksgenre dataset that contains metadata about some digitised books from the British Library. This collection also includes some annotations for the genre of the book which we could use to train a machine learning model.\nWe can load a dataset hosted on the Hugging Face hub by using the load_dataset function.\n\nfrom datasets import load_dataset\n\n\nds = load_dataset(\"blbooksgenre\", \"annotated_raw\", split=\"train\")\n\nReusing dataset bl_books_genre (/Users/dvanstrien/.cache/huggingface/datasets/bl_books_genre/annotated_raw/1.1.0/1e01f82403b3d9344121c3b81e5ad7c130338b250bf95dad4c6ab342c642dbe8)\n\n\nSince we requested only the train split we get back a Dataset\n\nds\n\nDataset({\n    features: ['BL record ID', 'Name', 'Dates associated with name', 'Type of name', 'Role', 'All names', 'Title', 'Variant titles', 'Series title', 'Number within series', 'Country of publication', 'Place of publication', 'Publisher', 'Date of publication', 'Edition', 'Physical description', 'Dewey classification', 'BL shelfmark', 'Topics', 'Genre', 'Languages', 'Notes', 'BL record ID for physical resource', 'classification_id', 'user_id', 'subject_ids', 'annotator_date_pub', 'annotator_normalised_date_pub', 'annotator_edition_statement', 'annotator_FAST_genre_terms', 'annotator_FAST_subject_terms', 'annotator_comments', 'annotator_main_language', 'annotator_other_languages_summaries', 'annotator_summaries_language', 'annotator_translation', 'annotator_original_language', 'annotator_publisher', 'annotator_place_pub', 'annotator_country', 'annotator_title', 'Link to digitised book', 'annotated', 'Type of resource', 'created_at', 'annotator_genre'],\n    num_rows: 4398\n})\n\n\n\nWe can see this has a bunch of columns. One that is of interest is the Data of publication column. Since we could use this dataset to train some type of classifier we may want to check whether we have enough examples across different time periods in the dataset.\n\nds[0][\"Date of publication\"]\n\n'1879'\n\n\n\n\nUsing toolz to calculate frequencies for a column\nOne quick way we can get the frequency count for a column is using the wonderful toolz library\nIf our data fits in memory, we can simply pass in a column containing a categorical value to a frequency function to get a frequency count.\n\nfrom toolz import frequencies, topk\n\n\ndates = ds[\"Date of publication\"]\n\n\n# collapse_hide\n\nfrequencies(dates)\n\n{\n    '1879': 99,\n    '1774': 5,\n    '1765': 5,\n    '1877': 69,\n    '1893': 222,\n    '1891': 148,\n    '1827': 29,\n    '1868': 42,\n    '1878': 72,\n    '1895': 189,\n    '1897': 120,\n    '1899': 104,\n    '1896': 174,\n    '1876': 48,\n    '1812': 13,\n    '1799': 8,\n    '1830': 32,\n    '1870': 42,\n    '1894': 155,\n    '1864': 28,\n    '1855': 42,\n    '1871': 42,\n    '1836': 37,\n    '1883': 51,\n    '1880': 111,\n    '1884': 69,\n    '1822': 16,\n    '1856': 38,\n    '1872': 42,\n    '1875': 57,\n    '1844': 35,\n    '1890': 134,\n    '1886': 43,\n    '1840': 15,\n    '1888': 109,\n    '1858': 43,\n    '1867': 53,\n    '1826': 24,\n    '1800': 3,\n    '1851': 43,\n    '1838': 14,\n    '1824': 20,\n    '1887': 58,\n    '1874': 42,\n    '1857': 44,\n    '1873': 34,\n    '1837': 16,\n    '1846': 32,\n    '1881': 55,\n    '1898': 104,\n    '1906': 4,\n    '1892': 134,\n    '1869': 25,\n    '1885': 69,\n    '1882': 71,\n    '1863': 55,\n    '1865': 53,\n    '1635': 3,\n    '1859': 39,\n    '1818': 17,\n    '1845': 28,\n    '1852': 43,\n    '1841': 23,\n    '1842': 29,\n    '1848': 28,\n    '1828': 23,\n    '1850': 38,\n    '1860': 45,\n    '1889': 140,\n    '1815': 5,\n    '1861': 28,\n    '1814': 13,\n    '1843': 28,\n    '1817': 12,\n    '1819': 16,\n    '1853': 34,\n    '1833': 5,\n    '1854': 36,\n    '1839': 33,\n    '1803': 7,\n    '1835': 14,\n    '1813': 8,\n    '1695': 4,\n    '1809-1811': 5,\n    '1832': 9,\n    '1823': 17,\n    '1847': 28,\n    '1816': 8,\n    '1806': 5,\n    '1866': 26,\n    '1829': 13,\n    '1791': 5,\n    '1637': 5,\n    '1821': 4,\n    '1807': 14,\n    '1862': 22,\n    '1795': 5,\n    '1834': 12,\n    '1831': 10,\n    '1849': 13,\n    '1811': 1,\n    '1825': 1,\n    '1809': 3,\n    '1905': 1,\n    '1808': 1,\n    '1900': 5,\n    '1892-1912': 1,\n    '1804': 4,\n    '1769': 5,\n    '1910': 1,\n    '1805': 5,\n    '1802': 3,\n    '1871-': 1,\n    '1901': 5,\n    '1884-1909': 1,\n    '1873-1887': 1,\n    '1979': 1,\n    '1852-1941': 1,\n    '1903': 1,\n    '1871-1873': 1,\n    '1810': 3,\n    '1907': 1,\n    '1820': 5,\n    '1789': 5\n}"
  },
  {
    "objectID": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#make-it-parallel",
    "href": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#make-it-parallel",
    "title": "Combining Hugging Face datasets with dask",
    "section": "Make it parallel!",
    "text": "Make it parallel!\nIf our data doesnâ€™t fit in memory or we want to do things in parallel we might want to use a slightly different approach. This is where dask can play a role.\nDask offers a number of different collection abstractions that make it easier to do things in parallel. This includes dask bag.\nFirst weâ€™ll create a dask client here, I wonâ€™t dig into the details of this here but you can get a good overview in the getting started pages.\n\nfrom distributed import Client\n\n\nclient = Client()\n\nSince we donâ€™t want to load all of our data into memory we can great a generator that will yield one row at a time. In this case weâ€™ll start by exploring the Title column\n\ndef yield_titles():\n    for row in ds:\n        yield row[\"Title\"]\n\nWe can see that this returns a generator\n\nyield_titles()\n\n&lt;generator object yield_titles at 0x7ffc28fdc040&gt;\n\n\n\n\nnext(iter(yield_titles()))\n\n'The Canadian farmer. A missionary incident [Signed: W. J. H. Y, i.e. William J. H. Yates.]'\n\n\n\nWe can store this in a titles variable.\n\ntitles = yield_titles()\n\nWeâ€™ll now import dask bag.\nimport dask.bag as db\nWe can create a dask bag object using the from_sequence method.\n\nbag = db.from_sequence(titles)\n\n\nbag\n\ndask.bag&lt;from_sequence, npartitions=1&gt;\n\n\n\nWe can look at an example using the take method\n\nbag.take(1)\n\n(\n    [\n        'The',\n        'Canadian',\n        'farmer.',\n        'A',\n        'missionary',\n        'incident',\n        '[Signed:',\n        'W.',\n        'J.',\n        'H.',\n        'Y,',\n        'i.e.',\n        'William',\n        'J.',\n        'H.',\n        'Yates.]'\n    ],\n)\n\n\n\ndask bag has a bunch of handy methods for processing data (some of these we could also do in ðŸ¤— datasets but others are not available as specific methods in datasets).\nFor example we can make sure we only have unique titles using the distinct method.\n\nunique_titles = bag.distinct()\n\n\nunique_titles.take(4)\n\n(\n    'The Canadian farmer. A missionary incident [Signed: W. J. H. Y, i.e. William J. H. Yates.]',\n    'A new musical Interlude, called the Election [By M. P. Andrews.]',\n    'An Elegy written among the ruins of an Abbey. By the author of the Nun [E. Jerningham]',\n    \"The Baron's Daughter. A ballad by the author of Poetical Recreations [i.e. William C. Hazlitt] . F.P\"\n)\n\n\n\nSimilar to ðŸ¤— datasets we have a map method that we can use to apply a function to all of our examples. In this case we split the title text into individual words.\n\ntitle_words_split = unique_titles.map(lambda x: x.split(\" \"))\n\n\ntitle_words_split.take(2)\n\n(\n    [\n        'The',\n        'Canadian',\n        'farmer.',\n        'A',\n        'missionary',\n        'incident',\n        '[Signed:',\n        'W.',\n        'J.',\n        'H.',\n        'Y,',\n        'i.e.',\n        'William',\n        'J.',\n        'H.',\n        'Yates.]'\n    ],\n    [\n        'A',\n        'new',\n        'musical',\n        'Interlude,',\n        'called',\n        'the',\n        'Election',\n        '[By',\n        'M.',\n        'P.',\n        'Andrews.]'\n    ]\n)\n\n\n\nWe can see we now have all our words in a list. Helpfully dask bag has a flatten method. This will consume our lists and put all the words in a single sequence.\n\nflattend_title_words = title_words_split.flatten()\n\n\nflattend_title_words.take(2)\n\n('The', 'Canadian')\n\n\n\nWe could now use the frequencies method to get the top words.\n\nfreqs = flattend_title_words.frequencies(sort=True)\n\n\nfreqs\n\ndask.bag&lt;sorted, npartitions=1&gt;\n\n\n\nSince dask bag methods are lazy by default nothing has actually been calculated yet. We could just grab the top 10 words.\n\ntop_10_words = freqs.topk(10, key=1)\n\nIf we want the results of something we call compute which will call all of the chained methods on our bag.\n\ntop_10_words.compute()\n\n[\n    ('of', 808),\n    ('the', 674),\n    ('and', 550),\n    ('...', 518),\n    ('in', 402),\n    ('van', 306),\n    ('etc', 301),\n    ('de', 258),\n    ('en', 258),\n    ('a', 231)\n]\n\n\n\nWe could also do the same with lowered version\n\nlowered_title_words = flattend_title_words.map(lambda x: x.lower())\n\n\nfreqs = lowered_title_words.frequencies(sort=True)\n\nThe visualize method gives you some insights into how the computation is managed by dask.\n\nfreqs.visualize(engine=\"cytoscape\", optimize_graph=True)"
  },
  {
    "objectID": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#moving-from-datasets-to-a-dask-dataframe",
    "href": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#moving-from-datasets-to-a-dask-dataframe",
    "title": "Combining Hugging Face datasets with dask",
    "section": "Moving from datasets to a dask dataframe",
    "text": "Moving from datasets to a dask dataframe\nFor some operations, dask bag is super easy to use. Sometimes though you will hurt your brain trying to crow bar your problem into the dask bag API ðŸ˜µâ€ðŸ’« This is where dask dataframes come in! Using parquet, we can easily save our ðŸ¤— dataset as a parquet file.\n\nds.to_parquet(\"genre.parquet\")\n\n3583138\n\n\n\nimport dask.dataframe as dd\n\nand load from this file\n\nddf = dd.read_parquet(\"genre.parquet\")\n\nAs dask dataframe works quite similar to a pandas dataframe. It is lazy by default so if we just print it out\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nBL record ID\nName\nDates associated with name\nType of name\nRole\nAll names\nTitle\nVariant titles\nSeries title\nNumber within series\nCountry of publication\nPlace of publication\nPublisher\nDate of publication\nEdition\nPhysical description\nDewey classification\nBL shelfmark\nTopics\nGenre\nLanguages\nNotes\nBL record ID for physical resource\nclassification_id\nuser_id\nsubject_ids\nannotator_date_pub\nannotator_normalised_date_pub\nannotator_edition_statement\nannotator_FAST_genre_terms\nannotator_FAST_subject_terms\nannotator_comments\nannotator_main_language\nannotator_other_languages_summaries\nannotator_summaries_language\nannotator_translation\nannotator_original_language\nannotator_publisher\nannotator_place_pub\nannotator_country\nannotator_title\nLink to digitised book\nannotated\nType of resource\ncreated_at\nannotator_genre\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nbool\nint64\ndatetime64[ns]\nint64\n\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: read-parquet, 1 tasks\n\n\nYouâ€™ll see we donâ€™t actually get back any data. If we use head we get the number of examples we ask for.\n\nddf.head(3)\n\n\n\n\n\n\n\n\nBL record ID\nName\nDates associated with name\nType of name\nRole\nAll names\nTitle\nVariant titles\nSeries title\nNumber within series\n...\nannotator_original_language\nannotator_publisher\nannotator_place_pub\nannotator_country\nannotator_title\nLink to digitised book\nannotated\nType of resource\ncreated_at\nannotator_genre\n\n\n\n\n0\n014603046\nYates, William Joseph H.\n\nperson\n\n[Yates, William Joseph H. [person] , Y, W. J....\nThe Canadian farmer. A missionary incident [Si...\n\n\n\n...\n\nNONE\nLondon\nenk\nThe Canadian farmer. A missionary incident [Si...\nhttp://access.bl.uk/item/viewer/ark:/81055/vdc...\nTrue\n0\n2020-08-11 14:30:33\n0\n\n\n1\n014603046\nYates, William Joseph H.\n\nperson\n\n[Yates, William Joseph H. [person] , Y, W. J....\nThe Canadian farmer. A missionary incident [Si...\n\n\n\n...\n\nNONE\nLondon\nenk\nThe Canadian farmer. A missionary incident [Si...\nhttp://access.bl.uk/item/viewer/ark:/81055/vdc...\nTrue\n0\n2021-04-15 09:53:23\n0\n\n\n2\n014603046\nYates, William Joseph H.\n\nperson\n\n[Yates, William Joseph H. [person] , Y, W. J....\nThe Canadian farmer. A missionary incident [Si...\n\n\n\n...\n\nNONE\nLondon\nenk\nThe Canadian farmer. A missionary incident [Si...\nhttp://access.bl.uk/item/viewer/ark:/81055/vdc...\nTrue\n0\n2020-09-24 14:27:54\n0\n\n\n\n\n3 rows Ã— 46 columns\n\n\n\nWe have some familiar methods from pandas available to us\n\nddf = ddf.drop_duplicates(subset=\"Title\")\n\nAs an example of something that would be a bit tricky in datasets, we can see how to groupby the mean title length by year of publication. First we create a new column for title length\n\nddf[\"title_len\"] = ddf[\"Title\"].map(lambda x: len(x))\n\nWe can then groupby the date of publication\n\ngrouped = ddf.groupby(\"Date of publication\")\n\nand then calculate the mean title_len\n\nmean_title_len = grouped[\"title_len\"].mean()\n\nTo actually compute this value we call the compute method\n\nmean_title_len.compute()\n\nDate of publication\n1635    248.0\n1637     67.0\n1695     63.0\n1765     86.0\n1769     20.0\n        ...  \n1905    141.0\n1906    225.0\n1907    142.0\n1910     65.0\n1979     43.0\nName: title_len, Length: 124, dtype: float64\n\n\n\nWe can also create a plot in the usual way\n\nmean_title_len.compute().plot()\n\n&lt;AxesSubplot:xlabel='Date of publication'&gt;\n\n\n\n&lt;Figure size 432x288 with 1 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nThis was a very quick overview. The dask docs go into much more detail as do the Hugging Face datasets docs."
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "",
    "text": "Iâ€™m currently going through the Full Stack Deep Learning course. As part of this weâ€™ve been going through tools for different parts of the machine learning pipeline. This post talks about data annotation, and how we can combine Label Studio and the Hugging Face Datasets hub. Iâ€™ll use the example of annotating image data for an image classification task. The details of why Iâ€™m annotating this data will wait for a future post!\nnote this post assumes you already know roughly what the Hugging Face Hub is. If you donâ€™t this is a nice intro."
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#full-stack-deep-learning-annotating-data",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#full-stack-deep-learning-annotating-data",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "",
    "text": "Iâ€™m currently going through the Full Stack Deep Learning course. As part of this weâ€™ve been going through tools for different parts of the machine learning pipeline. This post talks about data annotation, and how we can combine Label Studio and the Hugging Face Datasets hub. Iâ€™ll use the example of annotating image data for an image classification task. The details of why Iâ€™m annotating this data will wait for a future post!\nnote this post assumes you already know roughly what the Hugging Face Hub is. If you donâ€™t this is a nice intro."
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#what-is-the-goal",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#what-is-the-goal",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "What is the goal?",
    "text": "What is the goal?\nWe want to have a way of easily moving from different stages of our machine learning project pipeline. For many projects, especially the weird stuff Iâ€™m likely to do, you will need to do some of your own annotating. It almost always makes sense to move quickly between annotating a first batch of data, trying to train a model and iterating. This can help:\n\nflag issues with your data\nidentify if you have ambiguous labels\nhelp you get some sense of how a model might perform on the task you are working on\nallow you to deploy a model early so you can begin iterating on the whole pipeline\nâ€¦\n\n\n\nfrom Imgflip Meme Generator\n\nThis approach can cause some challenges; how do you keep updating your annotations, how can you version the changes?"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#a-more-mundane-challenge",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#a-more-mundane-challenge",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "A more mundane challenge",
    "text": "A more mundane challenge\nIn the full stack deep learning course, one of the labs covered using Label Studio to annotate data. Label studio is a great open source tool for annotating data across a range of domains and for a variety of tasks.\nLabel studio has great support for annotating image data. One challenge we can face, however, is how to load images into label studio. This can be particularly tricky if you only have the images locally since label studio prefers images to be available via a URL. There are various ways around this but we may also be able to tackle this challenge using the datasets hub.\nWeâ€™ll start by downloading a dataset we want annotate warning this dataset is pretty big ~44GB uncompressed.\n\n%%bash\nwget https://nlsfoundry.s3.amazonaws.com/data/nls-data-encyclopaediaBritannica.zip\nunzip *.zip\n\nWeâ€™ll import some standard libraries\n\nimport pandas as pd\nfrom pathlib import Path"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#create-a-new-dataset-on-the-hub",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#create-a-new-dataset-on-the-hub",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Create a new dataset on the Hub",
    "text": "Create a new dataset on the Hub\nSince we want to upload our data to the Hugging Face hub weâ€™ll create a new dataset on the Hugging Face Hub via the CLI.\nhuggingface-cli repo create encyclopaedia_britannica --type dataset \nUnder the hood, Hugging Face hub datasets (and models) are Git repositories. Weâ€™ll clone this repo and move the downloaded dataset into this new Git repository.\ngit clone https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica\nmv nls-data-encyclopaediaBritannica encyclopaedia_britannica/\nSince the number of examples in this dataset is beyond what weâ€™re likely to annotate we do a bit of deleting of the dataset. You could also take a sample of the original but in this case Iâ€™m happy to reclaim some space on my hardrive!\n\nimport shutil\nfrom tqdm.auto import tqdm\n\nfirst we get rid of some alto folders that we donâ€™t need for the dataset weâ€™re aiming to create\n\nfor directory in tqdm(\n    list(\n        (\n            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n                \"*alto\"\n            )\n        )\n    )\n):\n    shutil.rmtree(directory)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [00:34&lt;00:00,  5.62it/s]\n\n\nthere are a few other *.xml files in this dataset we also remove\n\nfor xml_file in tqdm(\n    list(\n        (\n            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n                \"*xml\"\n            )\n        )\n    )\n):\n    xml_file.unlink()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [00:00&lt;00:00, 1464.47it/s]\n\n\nLetâ€™s take a look at how many images we have now\n\nimage_files = list(\n    (Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\"*jpg\"))\n)\n\n\nlen(image_files)\n\n155388\n\n\nWeâ€™re not likely to annotate this many images, letâ€™s aim to have a max of 10,000 images. This is also likely to be more than weâ€™ll annotate but we may use a smaller sample for unsupervised pre-training.\n\nnum_to_remove = len(image_files) - 10_000\n\nWeâ€™ll now randomly remove the extra images we donâ€™t need beyond our sample\n\nimport random\n\n\nto_remove = random.sample(image_files, num_to_remove)\nfor file in tqdm(to_remove):\n    file.unlink()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90000/90000 [00:33&lt;00:00, 2659.02it/s]\n\n\n\nlen(\n    list(\n        (\n            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n                \"*jpg\"\n            )\n        )\n    )\n)\n\n10000\n\n\n\nUploading our raw data to the hub\nWe can now upload this data to the Hugging Face Hub. Under the hood the Hub uses Git so everything you love (and hate) about Git should be familiar. The main difference between using the hub and GitHub or another Git hosting platform is that the Hugging Face hub has support for large files. This means we can more easily work with large files (like our images).\ncd encyclopaedia_britannica\ngit lfs track \"*.jpg\"\ngit add .gitattributes\ngit add nls-data-encyclopaediaBritannica\ngit commit -m \"add image files\"\ngit push"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#loading-local-files-and-metadata",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#loading-local-files-and-metadata",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Loading local files and metadata",
    "text": "Loading local files and metadata\nThe particular dataset weâ€™re working with also has a metadata file associated with it. We can grab all of the images so we can put them in a DataFrame and merge this with metadata about these images. We may not use this extra metadata but itâ€™s nice to have this additional metadata about our items alongside our annotations. This can help us debug where our model is performing badly later on.\n\nimage_files = list(\n    Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\"*.jpg\")\n)\n\n\ndf = pd.DataFrame(image_files, columns=[\"filename\"])\n\nThis dataset also comes with some metadata. Weâ€™ll load that in to another DataFrame\n\nmetadata_df = pd.read_csv(\n    \"encyclopaedia_britannica/nls-data-encyclopaediaBritannica/encyclopaediaBritannica-inventory.csv\",\n    header=None,\n    names=[\"id\", \"meta\"],\n    dtype={\"id\": \"int64\"},\n)\n\n\ndf[\"id\"] = df.filename.apply(lambda x: x.parts[-3]).astype(\"int64\")\n\n\ndf = df.merge(metadata_df, on=\"id\")\ndf\n\n\n\n\n\n\n\n\nfilename\nid\nmeta\n\n\n\n\n0\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n1\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n2\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n3\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n4\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n...\n...\n...\n...\n\n\n9995\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n9996\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n9997\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n9998\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n9999\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n\n\n10000 rows Ã— 3 columns"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#annotating-using-label-studio",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#annotating-using-label-studio",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Annotating using label studio",
    "text": "Annotating using label studio\nNow we have our images uploaded to the Hugging Face hub, how we go about annotating? As was mentioned already the Hugging Face hub is essentially a Git repo. Since we uploaded our image files individually i.e.Â not in a compressed folder, we can access each file from that repo. We mentioned before that label studio can load images from URLs. The hub has an API that we can use to interact with our repository. Letâ€™s see how we can use this to get our data ready for label studio.\n\nfrom huggingface_hub import list_repo_files, hf_hub_url\n\n\nfiles = list_repo_files(\"davanstrien/encyclopaedia_britannica\", repo_type=\"dataset\")\nfiles[:2]\n\n['.gitattributes',\n 'nls-data-encyclopaediaBritannica/144133901/image/188082865.3.jpg']\n\n\nWeâ€™ll filter out some data we are not interested in annotating\n\nfiles = [file for file in files if not file.startswith(\".\")]\nlen(files)\n\n10002\n\n\nhf_hub_url can be used to generate the URL for a particular file\n\nhf_hub_url(\n    \"davanstrien/encyclopaedia_britannica\",\n    \"192866824.3.jpg\",\n    subfolder=\"sample/nls-data-encyclopaediaBritannica/192547788/image\",\n    repo_type=\"dataset\",\n)\n\n'https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/sample/nls-data-encyclopaediaBritannica/192547788/image/192866824.3.jpg'\n\n\nWe can use this to grab all of the URLs weâ€™re interested in\n\nurls = []\nfor file in files:\n    file = Path(file)\n    urls.append(\n        hf_hub_url(\n            \"davanstrien/encyclopedia_britannica\",\n            file.name,\n            subfolder=file.parents[0],\n            repo_type=\"dataset\",\n        )\n    )\n\nWe can now load these into a DataFrame, and save this to a CSV file.\n\npd.DataFrame(urls, columns=[\"image\"]).to_csv(\"data.csv\", index=False)\n\n\npd.read_csv(\"data.csv\")\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n0\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n1\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n2\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n3\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n4\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n...\n...\n\n\n9997\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n9998\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n9999\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n10000\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n10001\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n\n\n10002 rows Ã— 1 columns\n\n\n\n\nLoading annotations into label studio\nWe can use this file to load our data into label studio \nFrom here, we need to define our annotation task. We can then begin annotating data.\n\n\nExport annotations\nYou can either wait until youâ€™ve finished doing all the labels, however, we may have a lot of data to annotate so itâ€™s likely instead that we will want to export once weâ€™ve either hit a reasonable number of labels or get too bored of annotating. There are various different export formats available in this case weâ€™ll use JSON-Min\n\n\n\nLoad annotations\nNow we have export our annotations lets load them into a new DatafFame. Weâ€™ll only select the columns weâ€™re interested in\n\nannotation_dataframe = pd.read_json(\"project-3-at-2022-09-08-15-16-4279e901.json\")[\n    [\"image\", \"choice\"]\n]\n\n\nannotation_dataframe\n\n\n\n\n\n\n\n\nimage\nchoice\n\n\n\n\n0\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n2\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n3\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n4\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n...\n...\n...\n\n\n1516\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1517\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1518\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1519\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1520\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n\n\n1521 rows Ã— 2 columns\n\n\n\nIf we take a look at the URL for one of the annotations, youâ€™ll see that we still have a nice path that mirrors the folder structure of the original data. This also means we can merge this annotations DataFrame with our previous metadata DataFrame.\n\nannotation_dataframe.loc[0, \"image\"]\n\n'https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/nls-data-encyclopaediaBritannica/192693396/image/192979378.3.jpg'\n\n\n\nannotation_dataframe.loc[0, \"image\"].split(\"/\")[-4:]\n\n['nls-data-encyclopaediaBritannica', '192693396', 'image', '192979378.3.jpg']\n\n\n\nannotation_dataframe[\"filename\"] = annotation_dataframe[\"image\"].apply(\n    lambda x: \"/\".join(x.split(\"/\")[-4:])\n)\n\n\nannotation_dataframe[\"filename\"] = annotation_dataframe[\"filename\"].astype(str)\n\n\ndf = df.merge(annotation_dataframe, on=\"filename\", how=\"outer\")\n\n\ndf\n\n\n\n\n\n\n\n\nfilename\nid\nmeta\nimage\nchoice\n\n\n\n\n0\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n1\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n2\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n3\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n4\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n11516\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n11517\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n11518\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n11519\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n11520\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n\n\n11521 rows Ã— 5 columns\n\n\n\nThis means we can keep our nice orignal metadata intact but also add our additional metadata where it exists. Letâ€™s check how many annotations we have\n\ndf.choice.value_counts()\n\ntext-only      1436\nillustrated      70\nName: choice, dtype: int64\n\n\nWe can also see how much of our dataset we have coverage for\n\nlen(df[df.choice.notna()]) / len(df)\n\n0.13071781963371235\n\n\n\n\nHow to use our annotations?\nWe now have some annoations inside a DataFrame. What should we do we these? We can also use the Hub for storing this. This comes with a few benefits: - we keep our data and annotations in the same place. - since the Hub uses Git under the hood we also get versioning for our dataset. We can use this version information to track for example how different models perform during training as we add more labels.\nAnother nice thing about the Hub is that we can create dataset loading scripts to load our data. This script can use this CSV weâ€™ve just created and only load the data we have examples for.\nFirst weâ€™ll save to a CSV file:\n\ndf.to_csv(\"annotations.csv\", index=None)\n\nWe can then copy these into the same repository used to host our dataset.\ncp annotations.csv encyclopedia_britannica/\nOnce weâ€™ve done this we can commit these and push our annotations to the hub:\ncd encyclopedia_britannica/\ngit add annotations.csv\ngit commit -m \"update annotations\"\ngit push"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#what-next",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#what-next",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "What next?",
    "text": "What next?\nWe now have a repository which contains a bunch of images, and a CSV file which contains annotations for some of these images. How do we use this for model training? From this point we can create a dataset loading script inside the same repository.\nThis dataset loading script will allow us to load the data from the hub using the datasets library. Additionally we can write this script so that it only loads data we have annotations for.\nWhat does this mean: - we have a dataset we can use to train our model - the dataset is hosted on the Hugging Face hub which means itâ€™s easy to share with other people - we can keep adding new annotations to this dataset and pushing our changes to the hub - Since the datasets library has nice caching support it will only download the dataset if there are changes. This change will be triggered by changes to our annotations.csv file."
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#loading-the-dataset",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#loading-the-dataset",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Loading the dataset",
    "text": "Loading the dataset\nOnce we have our loading script we can load our annotations using the datasets library:\n\nfrom datasets import load_dataset\nimport datasets\n\n\nds = load_dataset('davanstrien/encyclopedia_britannica')\n\nUsing custom data configuration default\nReusing dataset encyclopedia_britannica (/Users/dvanstrien/.cache/huggingface/datasets/davanstrien___encyclopedia_britannica/default/1.1.0/8dd4d7982f31fd11ed71020b79b4b11a0068c8243080066e43b9fe3980934467)\n\n\n\n\n\n\nds['train'][0]\n\n{'metadata': 'nan',\n 'image': 'https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/nls-data-encyclopaediaBritannica/192693396/image/192979378.3.jpg',\n 'label': 0}\n\n\n\nfrom datasets import load_dataset\nfrom datasets.utils.file_utils import get_datasets_user_agent\nfrom functools import partial\nfrom concurrent.futures import ThreadPoolExecutor\nimport urllib\nimport io\nimport PIL\n\nUSER_AGENT = get_datasets_user_agent()\n\n\ndef fetch_single_image(image_url, timeout=None, retries=0):\n    for _ in range(retries + 1):\n        try:\n            request = urllib.request.Request(\n                image_url,\n                data=None,\n                headers={\"user-agent\": USER_AGENT},\n            )\n            with urllib.request.urlopen(request, timeout=timeout) as req:\n                image = PIL.Image.open(io.BytesIO(req.read()))\n            break\n        except Exception:\n            image = None\n    return image\n\n\ndef fetch_images(batch, num_threads, timeout=1, retries=0):\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        batch[\"image\"] = list(executor.map(fetch_single_image_with_args, batch[\"image\"]))\n    return batch\n\n\nnum_threads = 16\nds = ds.map(fetch_images, batched=True, batch_size=64, fn_kwargs={\"num_threads\": num_threads}, writer_batch_size=64)\n\nLoading cached processed dataset at /Users/dvanstrien/.cache/huggingface/datasets/encyclopaedia_britannica/default/1.1.0/f7fb8d1f26daa72fbaf883bb1707e13d304414c1af16f02c00782c985971f87c/cache-fda9502ac5b20332.arrow\n\n\n\nds = ds.cast_column('image', datasets.Image())\n\n\nds['train'][0]['image']"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#where-wont-this-work",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#where-wont-this-work",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Where wonâ€™t this work?",
    "text": "Where wonâ€™t this work?\nThis workflow is based on the assumption that the dataset you are annotating is public from the start. This is usually possible for the domain I work in (libraries) but could be a major blocker for other people. This workflow might also break if you have lots of people annotating. There are probably ways around this but things could start becoming a bit hackyâ€¦\nThe loading script for loading this dataset does some slightly strange things to avoid loading images that donâ€™t yet have annotations. I think it would make sense to rework this script if you get to a point you are unlikely to do any more annotations."
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "",
    "text": "%pip install datasets qdrant-client --q\n\n\n[notice] A new release of pip available: 22.3.1 -&gt; 23.3.1\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#loading-our-dataset",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#loading-our-dataset",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "Loading our dataset",
    "text": "Loading our dataset\nFor this post weâ€™ll use the Cohere/wikipedia-22-12-simple-embeddings dataset which has already had embeddings generated for it. This dataset was created by Cohere and creates embeddings for millions of Wikipedia articles. See this post for more details.\nWeâ€™ll use the Hugging Face datasets library to load the dataset.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Cohere/wikipedia-22-12-simple-embeddings\", split=\"train\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n  table = cls._concat_blocks(blocks, axis=0)\n\n\nLetâ€™s take a quick look at the dataset.\n\ndataset\n\nDataset({\n    features: ['id', 'title', 'text', 'url', 'wiki_id', 'views', 'paragraph_id', 'langs', 'emb'],\n    num_rows: 485859\n})\n\n\nWe can see the dataset has a emb column which contains the embeddings for each article. Alongside this we see the title and text for the articles alongside some other metadata. Letâ€™s also take a look at the features of the dataset.\nLetâ€™s also take a quick look at the features of the dataset. Hugging Face Dataset objects have a features attribute which contains the features of the dataset. We can see that the emb column is a Sequence of float32 values. We also have some other columns with string values, int32 and float32 values.\n\ndataset.features\n\n{'id': Value(dtype='int32', id=None),\n 'title': Value(dtype='string', id=None),\n 'text': Value(dtype='string', id=None),\n 'url': Value(dtype='string', id=None),\n 'wiki_id': Value(dtype='int32', id=None),\n 'views': Value(dtype='float32', id=None),\n 'paragraph_id': Value(dtype='int32', id=None),\n 'langs': Value(dtype='int32', id=None),\n 'emb': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}\n\n\nQdrant has support for a pretty varied range of types. All of these types in our dataset are supported by Qdrant so we donâ€™t need to do any conversion.\n\nCreating a Qdrant collection\nWeâ€™ll use the Qdrant Python client for this post. This client is really nice since it allows you to create a local collection using pure Python i.e.Â no need to run a Qdrant server. This is great for testing and development. Once youâ€™re ready to deploy your collection you can use the same client to connect to a remote Qdrant server.\n\nfrom qdrant_client import QdrantClient\n\nWe first create a client, in this case using a local path for our DB.\n\nclient = QdrantClient(path=\"db\")  # Persists changes to disk\n\n\n\nConfiguring our Qdrant collection\nQdrant is very flexible but we need to let Qdrant now a few things about our collection. These include the name, and a config for the vectors we want to store. This config includes the dimensionality of the vectors and the distance metric we want to use. Letâ€™s first check out the dimensionality of our vectors.\n\nvector_size = len(dataset[0]['emb'])\n\nWeâ€™ll also store our collection in a variable so we can use it later.\n\ncollection_name = \"cohere_wikipedia\"\n\n\nfrom qdrant_client.models import Distance, VectorParams\n\nclient.recreate_collection(\n    collection_name=collection_name,\n    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n)\n\nTrue"
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#adding-our-data-to-qdrant",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#adding-our-data-to-qdrant",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "Adding our data to Qdrant",
    "text": "Adding our data to Qdrant\nNote this code can be heavily optimized but gives an idea of how easy adding data to Qdrant can be. For many datasets this naive approach will work fine.\nThe approach weâ€™ll take below is to loop through our dataset and yield each row as a PointStruct. This is a Qdrant object that contains the vector and any other data, referred to as the payload, that we want to store.\n\nfrom qdrant_client.models import PointStruct\n\n\ndef yield_rows(dataset):\n    for idx, row in enumerate(dataset, start=1):\n        vector = row[\"emb\"] # grab the vector\n        payload = {k: v for k, v in row.items() if k != \"emb\"} # grab the rest of the fields without the vector\n        yield PointStruct(id=idx, vector=vector, payload=payload)\n\nFor this post weâ€™ll use a smallish subset of the dataset. Weâ€™ll use the first 100_000 rows. Big enough to be interesting but small enough to play around with quickly.\n\nsample = dataset.select(range(100_000))\n\nWeâ€™ll use the toolz libraries partition_all function to get batches from our yield_rows function. Weâ€™ll use tqdm to show a progress bar.\n\nfrom toolz import partition_all\nfrom tqdm.auto import tqdm\n\n\n%%time\nbs = 100\nfor batch in tqdm(partition_all(bs, yield_rows(sample)), total=len(sample) // bs):\n    client.upsert(collection_name=collection_name, points=list(batch), wait=False)\n\n\n\n\nCPU times: user 30.9 s, sys: 35.7 s, total: 1min 6s\nWall time: 1min 19s\n\n\nOn my 2021 MacBook Pro with an M1 chip this takes about 90 seconds to run. As mentioned above this can be heavily optimized but this gives an idea of how easy it is to add data to Qdrant from a Hugging Face dataset."
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#searching-our-qdrant-collection",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#searching-our-qdrant-collection",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "Searching our Qdrant collection",
    "text": "Searching our Qdrant collection\nWhat can we do with our Qdrant collection? We can use our embeddings to find similar wikipedia articles. Letâ€™s see how we can do that.\nFirst weâ€™ll use the get_collection method to see some information about our collection.\n\nfrom rich import print\n\nprint(client.get_collection(collection_name))\n\nCollectionInfo(\n    status=&lt;CollectionStatus.GREEN: 'green'&gt;,\n    optimizer_status=&lt;OptimizersStatusOneOf.OK: 'ok'&gt;,\n    vectors_count=100000,\n    indexed_vectors_count=0,\n    points_count=100000,\n    segments_count=1,\n    config=CollectionConfig(\n        params=CollectionParams(\n            vectors=VectorParams(\n                size=768,\n                distance=&lt;Distance.COSINE: 'Cosine'&gt;,\n                hnsw_config=None,\n                quantization_config=None,\n                on_disk=None\n            ),\n            shard_number=None,\n            replication_factor=None,\n            write_consistency_factor=None,\n            read_fan_out_factor=None,\n            on_disk_payload=None\n        ),\n        hnsw_config=HnswConfig(\n            m=16,\n            ef_construct=100,\n            full_scan_threshold=10000,\n            max_indexing_threads=0,\n            on_disk=None,\n            payload_m=None\n        ),\n        optimizer_config=OptimizersConfig(\n            deleted_threshold=0.2,\n            vacuum_min_vector_number=1000,\n            default_segment_number=0,\n            max_segment_size=None,\n            memmap_threshold=None,\n            indexing_threshold=20000,\n            flush_interval_sec=5,\n            max_optimization_threads=1\n        ),\n        wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0),\n        quantization_config=None\n    ),\n    payload_schema={}\n)\n\n\n\nWe can see a bunch of information about our collection. Including the vector count, the dimensionality of the vectors and the distance metric weâ€™re using. Youâ€™ll see that there are plenty of knobs to turn here to optimize your collection but thatâ€™s for another post.\nWe can use the scroll method to get the first vector from our collection\n\nprint(client.scroll(collection_name,limit=1)[0][0])\n\nRecord(\n    id=1,\n    payload={\n        'id': 0,\n        'title': '24-hour clock',\n        'text': 'The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and\nis divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only\nin the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very \nrarely) as continental time. In some parts of the world, it is called railway time. Also, the international \nstandard notation of time (ISO 8601) is based on this format.',\n        'url': 'https://simple.wikipedia.org/wiki?curid=9985',\n        'wiki_id': 9985,\n        'views': 2450.62548828125,\n        'paragraph_id': 0,\n        'langs': 30\n    },\n    vector=None\n)\n\n\n\nWe can also grab items from the payload for each point.\n\nprint(client.scroll('cohere_wikipedia',limit=1)[0][0].payload['text'])\n\nThe 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into \n24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and \nthe English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as \ncontinental time. In some parts of the world, it is called railway time. Also, the international standard notation \nof time (ISO 8601) is based on this format.\n\n\n\nWe can see this article is about the 24-hour clock system. Letâ€™s see what other pages are similar to this one. We can optionally get the vector for the query point.\n\nvector = client.scroll('cohere_wikipedia',limit=1,with_vectors=True)[0][0].vector\n\nWe can use our vector as a query to find similar vectors in our collection. Weâ€™ll use the search method to do this.\n\nquery_vector = client.scroll(collection_name, limit=1, with_vectors=True)[0][0].vector\nhits = client.search(\n    collection_name=collection_name,\n    query_vector=query_vector,\n    limit=15,  # Return 5 closest points\n)\n\nLetâ€™s look at some of the results. We can see that the first result is the same article. The rest also seem to be about time/24 hour clock systems!\n\nfor hit in hits:\n    print(f\"{hit.payload['title']} | {hit.payload['text']}\")\n    print(\"---\")\n\n24-hour clock | The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and \nis divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only\nin the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very \nrarely) as continental time. In some parts of the world, it is called railway time. Also, the international \nstandard notation of time (ISO 8601) is based on this format.\n\n\n\n---\n\n\n\n24-hour clock | However, the US military prefers not to say 24:00 - they do not like to have two names for the same\nthing, so they always say \"23:59\", which is one minute before midnight.\n\n\n\n---\n\n\n\n24-hour clock | 24-hour clock time is used in computers, military, public safety, and transport. In many Asian, \nEuropean and Latin American countries people use it to write the time. Many European people use it in speaking.\n\n\n\n---\n\n\n\n24-hour clock | In railway timetables 24:00 means the \"end\" of the day. For example, a train due to arrive at a \nstation during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the \nday go at 00:00.\n\n\n\n---\n\n\n\n24-hour clock | A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or \nhours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under \nthe 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and \nends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called \n24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you \nwould say \"Tuesday at 24:00\" and \"Wednesday at 00:00\" to mean exactly the same time.\n\n\n\n---\n\n\n\n12-hour clock | The 12-hour clock is a way of dividing the 24 hours of the day into two sections. The two halves \nare called ante meridiem (a.m.) and post meridiem (p.m.).\n\n\n\n---\n\n\n\n12-hour clock | Both names are from Latin, and numbered from 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 and 12. Time from \nmidnight to noon is a.m. and from noon to midnight p.m. The table at right shows how it relates to the 24-hour \nclock.\n\n\n\n---\n\n\n\nHour | An hour (abbreviation: h or hr) is a unit of measurement used to measure time. An hour is equal to 60 \nminutes. 24 hours are equal to one day. Unlike the second, the hour is not a base SI unit.\n\n\n\n---\n\n\n\nMidnight | The time period \"00:00 - 00:01\" is midnight. On computer clocks, the day changes to the next day the \nminute(s) after midnight.\n\n\n\n---\n\n\n\nChinese zodiac | In the old days, China and Japan used a 12-hour system to tell the time of day and night (unlike \nthe 24 hour system used today). The 12 hour system divides the day of 24 hours into 12 hours, each of which has a \nsign of the zodiac:\n\n\n\n---\n\n\n\nCoordinated Universal Time | Note that UTC uses the 24-hour clock. That means there is no 'AM' or 'PM'. For \nexample, 4:00PM would be 16:00 or 1600. UTC also does not use daylight savings time - that way the time stays \nconsistent the entire year.\n\n\n\n---\n\n\n\nMidnight | In the world, midnight is the start of one day and the end of the last day. It's the dividing point \nbetween two days.\n\n\n\n---\n\n\n\nNoon | Noon is the time exactly halfway through the day (12.00-12:00 in the 24-hour clock and 12:00 PM-12:00 PM in \nthe 12-hour clock). Midday also means noon, although this also means \"around\" noon, or very early afternoon.\n\n\n\n---\n\n\n\nCoordinated Universal Time | The standard before was Greenwich Mean Time (GMT). UTC and GMT are almost the same. In\nfact, there is no practical difference which would be noticed by ordinary people.\n\n\n\n---\n\n\n\nMidnight | In the United States and Canada, digital clocks and computers usually show 12Â a.m. right at midnight. \nHowever, people have to remember that any time is actually an instant. The \"a.m.\" shown on clock displays means the\n12-hour period after the instant of midnight. So when a clock says \"12:00 a.m.\", midnight has already passed and a \nnew day has started. In other words, 11:59Â p.m.Â shows until midnight; at the instant of midnight, it changes to \n12:00. At the same time, the p.m. changes to a.m., but a.m. does not mean the instant of midnight which separates \np.m. and a.m.\n\n\n\n---"
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#conclusion",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#conclusion",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "Conclusion",
    "text": "Conclusion\nThis post showed how itâ€™s possible to easily convert a Hugging Face dataset into a Qdrant collection. We then showed how we can use this collection to find similar articles.\nThere is a lot of scope for optimization here. For example, we could use a more efficient way to add data to Qdrant. We could also use a more efficient way to search our collection. It would be very cool to directly have a from_hf_datasets method in the Qdrant Python client that would do all of this for us and include some optimizations!\nI hope this post has shown how easy it is to use Qdrant with Hugging Face datasets. If you have any questions or comments please let me know on Twitter."
  },
  {
    "objectID": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html",
    "href": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html",
    "title": "Image labeling vs classification models",
    "section": "",
    "text": "The â€˜hello worldâ€™ example for introducing deep learning based computer vision often involves classifying images as ðŸ¶ or ðŸ±. An alternative approach to classifying images is to instead apply labels. This is usually introduced in the context of multi-label classification i.e.Â where an image can have more than one label. In this blog post I discuss some of the differences between these two approaches, specifically the difference in loss functions, and how these two approaches might work better depending on the application. The post starts with a conceptual overview of the differences between these two approaches, before showing the different loss functions and then moving to a practical example of training these two different types of model."
  },
  {
    "objectID": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html#image-classification-vs-image-labeling",
    "href": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html#image-classification-vs-image-labeling",
    "title": "Image labeling vs classification models",
    "section": "Image Classification vs Image Labeling",
    "text": "Image Classification vs Image Labeling\nIn a classification model, an input can have only one label. This could be one of a few or one of a hundred, regardless of the number of potential classes, it is assumed that the input only belongs to one of these. With a model that applies labels this is not true an input can have one, multiple or no labels.\n\nSorting through family photos\nWe can use an analogy to illustrate the difference between these two approaches. Letâ€™s say you were sorting through some old family photographs. You might â€œclassifyâ€ the photos into one (and only one) of two photo albums, depending on whether they are black-and-white or colour. This would be comparable to using a classification model since each photo will go into exactly one of these two albums - a photo cannot be both simultaneously colour and black-and-white, and it cannot be neither colour nor black-and-white.\nYou may at the same time also want to make it easier to find photos of particular people in your family. You could do this by assigning labels to each photo, indicating or â€œtaggingâ€ the family members who appear in the photo. In this case, a photo may have one label (a photo of your sister), more than one label (a photo of your sister and aunt), or it may have no labels (a photograph of a landscape taken on a holiday). This would be analogous to a multi-label classification model.\nThe choice between using a model which performs classification or a model which assigns labels should be considered in relation to the role your model has. It is also useful to look a little bit more closely as how these different types of models work under the hood."
  },
  {
    "objectID": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html#crossentropyloss-vs-bcewithlogitsloss",
    "href": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html#crossentropyloss-vs-bcewithlogitsloss",
    "title": "Image labeling vs classification models",
    "section": "CrossEntropyLoss vs BCEWithLogitsLoss",
    "text": "CrossEntropyLoss vs BCEWithLogitsLoss\nWhen we create a model which does classifications or applies labels, the distinction, if using the same data is that they use different loss functions.\nA classification model will use a variant of Cross Entropy Loss whilst the label model will use a BCE with Logits Loss. Weâ€™ll see how this is inferred by fastai below but fore now take my word for itâ€¦\nLetâ€™s take a look at a snippet of the Pytorch docs for each of these loss functions\n\nCrossEntropyLoss\n\nThis criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. Read more\n\n\n\nBCEWithLogitsLoss\n\nThis loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. Read more\n\nLetâ€™s see what these do to some activations. First weâ€™ll import required packages\n\nimport torch.nn as nn\nimport numpy as np\nimport torch"
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html",
    "title": "Using the ðŸ¤— Hub for model storage",
    "section": "",
    "text": "Part of what the flyswot should take care of is handle machine learning models. The flyswot tool is essentially just a command-line wrapper for the machine learning model. However, these two things the command-line tool, and the model, are kept separate for a number of reasons:\n\nthe flyswot tool might have changes separate from the model changing i.e.Â some new functionality is added or some bug fixed\nwe might want to update the model based on new training data or a change in the labels used\n\nWe want to be able to release a new model without having to create a new release of the flyswot tool and vice-versa. As a result of this, both of these things are versioned separately.\nWe might want to keep our model separate because flyswot is made available as a Python Package. Since a computer vision model can be pretty large, we probably donâ€™t want this to be included as a part of the Python package."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#intro",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#intro",
    "title": "Using the ðŸ¤— Hub for model storage",
    "section": "",
    "text": "Part of what the flyswot should take care of is handle machine learning models. The flyswot tool is essentially just a command-line wrapper for the machine learning model. However, these two things the command-line tool, and the model, are kept separate for a number of reasons:\n\nthe flyswot tool might have changes separate from the model changing i.e.Â some new functionality is added or some bug fixed\nwe might want to update the model based on new training data or a change in the labels used\n\nWe want to be able to release a new model without having to create a new release of the flyswot tool and vice-versa. As a result of this, both of these things are versioned separately.\nWe might want to keep our model separate because flyswot is made available as a Python Package. Since a computer vision model can be pretty large, we probably donâ€™t want this to be included as a part of the Python package."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#how-is-this-currently-being-done",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#how-is-this-currently-being-done",
    "title": "Using the ðŸ¤— Hub for model storage",
    "section": "How is this currently being done",
    "text": "How is this currently being done\nCurrently models are stored in a separate GitHub repository separate from the repository used to store the flyswot code. flyswot has some functionality for checking against this GitHub repository to see if a more recent remote model has superseded a local model. If there is a more recent model available (and a CLI flag indicates that the latest model should be used), then flyswot downloads the new model and stores it in a new directory."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#what-is-wrong-with-this",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#what-is-wrong-with-this",
    "title": "Using the ðŸ¤— Hub for model storage",
    "section": "What is wrong with this",
    "text": "What is wrong with this\nWhilst this approach does work okay there is quite a surprising amount of code that is needed to take care of some of this. Currently the option to pass a specific snapshot of a model doesnâ€™t exist.\nOn the storage side although GitHub is great for storing code there are some limitations to it for storing large files. Iâ€™ve created a GitHub action to create a release when new pull requests to update the model are made. This then creates a new release with date information in the filename. Again, this works okay, but there might be a better wayâ€¦"
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#hub-to-the-rescue",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#hub-to-the-rescue",
    "title": "Using the ðŸ¤— Hub for model storage",
    "section": "ðŸ¤— hub to the rescue?",
    "text": "ðŸ¤— hub to the rescue?\nI have already been using the huggingface hub when using other peoples models and uploading fine-tuned transformer models. However, digging around the docs, it seemed like there are a few things in this ecosystem that could be useful for flyswot.\n\nWhat is the ðŸ¤— hub?\nIf you havenâ€™t come across the ðŸ¤— hub before, it is essentially a place where models can be uploaded and explored by others. So, for example, if I want a language model trained on Arabic text, I might find it in the hub. The goal is to help avoid duplication of effort and allow other people to use or adapt existing work.\nThis video gives a helpful overview of navigating the hub and finding models you might be interested in using.\n\n\nPart of the aim of sharing the flyswot models on GitHub (or the ðŸ¤— hub) is to make them available to other people to use. The ðŸ¤— hub well supports this use case. We can easily share models (including large ones) because of the underpinning use of git-lfs. However, our interest is not only in sharing a model for others to use but also in grabbing the correct model for the flyswot CLI tool easier. Some other components might help here."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#the-hub-vs-huggingface_hub-library",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#the-hub-vs-huggingface_hub-library",
    "title": "Using the ðŸ¤— Hub for model storage",
    "section": "The hub vs huggingface_hub library",
    "text": "The hub vs huggingface_hub library\nThe ðŸ¤— hub already provides a place to store the model. You can interact with this model using the web interface only but what we want is also to download models using our CLI from the hub. We already have a way to do this with GitHub, so ideally, we want something that works better than our current approach.\nThis is where the huggingface_hub Python Library might come in. This Python library provides us with various ways of interacting with the hub. This could give us enough ways of interacting with the hub that we can delete some of the code that currently does this with GitHub (and there is nothing nicer than deleting code ðŸ˜ƒ)\nIâ€™ll use the remainder of this blog to see if we can use the ðŸ¤— hub and the [huggingface_hub](https://pypi.org/project/huggingface-hub/) library for this purpose as a replacement for the current approach.\nWeâ€™ll start by installing the huggingface_hub library\n\n# hide_output\n!pip install huggingface_hub\n\nRequirement already satisfied: huggingface_hub in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (0.2.1)\nRequirement already satisfied: requests in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (2.26.0)\nRequirement already satisfied: pyyaml in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (4.0.1)\nRequirement already satisfied: tqdm in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (4.62.3)\nRequirement already satisfied: filelock in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (3.4.0)\nRequirement already satisfied: packaging&gt;=20.9 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from packaging&gt;=20.9-&gt;huggingface_hub) (3.0.6)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from requests-&gt;huggingface_hub) (1.26.7)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from requests-&gt;huggingface_hub) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from requests-&gt;huggingface_hub) (2.0.8)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from requests-&gt;huggingface_hub) (2021.10.8)"
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#getting-information-about-a-model",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#getting-information-about-a-model",
    "title": "Using the ðŸ¤— Hub for model storage",
    "section": "Getting information about a model",
    "text": "Getting information about a model\nOne of the things we need to be able to do is get the latest version of the model. One way we could try and do this is by grabbing metadata about the model. This is the current approach taken by flyswot. We can import model_info to do this:\n\nfrom huggingface_hub import model_info\n\n\ninfo = model_info(\"distilbert-base-cased\")\ninfo\n\nModelInfo: {\n    modelId: distilbert-base-cased\n    sha: 935ac13b473164bb9d578640e33d9f21144c365e\n    lastModified: 2020-12-11T21:23:53.000Z\n    tags: ['pytorch', 'tf', 'distilbert', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1910.01108', 'transformers', 'license:apache-2.0', 'infinity_compatible']\n    pipeline_tag: None\n    siblings: [ModelFile(rfilename='.gitattributes'), ModelFile(rfilename='README.md'), ModelFile(rfilename='config.json'), ModelFile(rfilename='pytorch_model.bin'), ModelFile(rfilename='tf_model.h5'), ModelFile(rfilename='tokenizer.json'), ModelFile(rfilename='tokenizer_config.json'), ModelFile(rfilename='vocab.txt')]\n    config: {'model_type': 'distilbert'}\n    id: distilbert-base-cased\n    private: False\n    downloads: 3556770\n    library_name: transformers\n    mask_token: [MASK]\n    likes: 4\n    model-index: None\n    cardData: {'language': 'en', 'license': 'apache-2.0', 'datasets': ['bookcorpus', 'wikipedia']}\n}\n\n\n\ntype(info)\n\nhuggingface_hub.hf_api.ModelInfo\n\n\nYou can see this gives us back a bunch of information about the model. We could for example grab the date the model was changed:\n\ninfo.lastModified\n\n'2020-12-11T21:23:53.000Z'\n\n\nThis already gives us what we need for checking if a model is updated in comparison to a local model already downloaded by the flyswot CLI. However we might be able to cut out some of this checking work.\nLets see if there are other ways we can do this in the library. Since huggingface_hub requires git-lfs lets start by installing this.\n\n!apt install git-lfs\n\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.3.4-1).\n0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n\n\nWe also need to make sure we have git-lfs setup\n\n!git init && git lfs install\n\nhint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint:   git config --global init.defaultBranch &lt;name&gt;\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint:   git branch -m &lt;name&gt;\nInitialized empty Git repository in /Users/dvanstrien/Documents/daniel/blog/_notebooks/.git/\nUpdated git hooks.\nGit LFS initialized.\n\n\n\nDownloading files from the hub\nWe can use hf_hub_url to get the url for a specific file from a repository\n\nfrom huggingface_hub import hf_hub_url\n\n\nonnx_model_url = hf_hub_url(\"davanstrien/flyswot-test\", \"2021-09-22.onnx\")\nonnx_model_url\n\n'https://huggingface.co/davanstrien/flyswot-test/resolve/main/2021-09-22.onnx'\n\n\nWe can pass this url to cached_download, this will download the file for us if we donâ€™t have the latest version, we can also specify a place to download the file. This is important so we can make sure we put the file somewhere flyswot can find.\n\nfrom huggingface_hub import cached_download\n\n\ncached_download(onnx_model_url, cache_dir=\".\")\n\n\n\n\n'./de9d2ce054e3e410e3fc61b5c2ad55da7861d30e3b90aa018615b7d902e6e51e.1300a5792e44de2c59f4d25c4f7efd447ef91d69971a121e6b4df8b95047ad7c'\n\n\nIf we try and download this again it wonâ€™t download, and will instead return the path to the model\n\npath = cached_download(onnx_model_url, cache_dir=\".\")\npath\n\n'./de9d2ce054e3e410e3fc61b5c2ad55da7861d30e3b90aa018615b7d902e6e51e.1300a5792e44de2c59f4d25c4f7efd447ef91d69971a121e6b4df8b95047ad7c'\n\n\n\n\nDownloading all files from the hub\nThis is quite close to what we want our current approach requires us to get a bunch of different files in a folder. To replicate this we can instead use snapshot_download\n\nfrom huggingface_hub import snapshot_download\n\nLetâ€™s see what this does\n\n?snapshot_download\n\n\nSignature:\nsnapshot_download(\n    repo_id: str,\n    revision: Union[str, NoneType] = None,\n    cache_dir: Union[str, pathlib.Path, NoneType] = None,\n    library_name: Union[str, NoneType] = None,\n    library_version: Union[str, NoneType] = None,\n    user_agent: Union[Dict, str, NoneType] = None,\n    proxies=None,\n    etag_timeout=10,\n    resume_download=False,\n    use_auth_token: Union[bool, str, NoneType] = None,\n    local_files_only=False,\n) -&gt; str\nDocstring:\nDownloads a whole snapshot of a repo's files at the specified revision.\nThis is useful when you want all files from a repo, because you don't know\nwhich ones you will need a priori.\nAll files are nested inside a folder in order to keep their actual filename\nrelative to that folder.\nAn alternative would be to just clone a repo but this would require that\nthe user always has git and git-lfs installed, and properly configured.\nNote: at some point maybe this format of storage should actually replace\nthe flat storage structure we've used so far (initially from allennlp\nif I remember correctly).\nReturn:\n    Local folder path (string) of repo snapshot\nFile:      ~/miniconda3/envs/blog/lib/python3.8/site-packages/huggingface_hub/snapshot_download.py\nType:      function\n\n\n\n\nThis will do something similar to cached_download but will instead do it for a whole model repository. If we pass our repository it will download the directory if we donâ€™t have the latest version of the files, if for example, the model has been updated.\n\nmodel = snapshot_download(\"davanstrien/flyswot-test\", cache_dir=\".\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\n\n'./davanstrien__flyswot-test.main.e54a7421f5e5eb240783452ab734288f252bb402'\n\n\nIf we look inside this directory we can see we have the files from the repository.\n\n!ls {model}\n\n2021-09-22.onnx README.md       modelcard.md    vocab.txt\n\n\nIf we try and download it again we just get back the directory path without having to download the files again.\n\nmodel = snapshot_download(\"davanstrien/flyswot-test\", cache_dir=\".\")\nmodel\n\n'./davanstrien__flyswot-test.main.e54a7421f5e5eb240783452ab734288f252bb402'\n\n\nThis gives a replication of what we currently have setup for flyswot in terms of downloading models. There are a few extra things we might want though to be able to make flyswot more flexible. First though letâ€™s look at how we can upload to the model hub.\n\n\nUploading to the hub\nAt the moment flyswot models are uploaded to a GitHub repository which then creates a release. It would be nice to simplify this and upload directly at the end of model training. For this we can use the Repository class.\n\nfrom huggingface_hub import Repository\n\n\n?Repository\n\n\nInit signature:\nRepository(\n    local_dir: str,\n    clone_from: Union[str, NoneType] = None,\n    repo_type: Union[str, NoneType] = None,\n    use_auth_token: Union[bool, str] = True,\n    git_user: Union[str, NoneType] = None,\n    git_email: Union[str, NoneType] = None,\n    revision: Union[str, NoneType] = None,\n    private: bool = False,\n    skip_lfs_files: bool = False,\n)\nDocstring:     \nHelper class to wrap the git and git-lfs commands.\nThe aim is to facilitate interacting with huggingface.co hosted model or dataset repos,\nthough not a lot here (if any) is actually specific to huggingface.co.\nInit docstring:\nInstantiate a local clone of a git repo.\nIf specifying a `clone_from`:\nwill clone an existing remote repository, for instance one\nthat was previously created using ``HfApi().create_repo(name=repo_name)``.\n``Repository`` uses the local git credentials by default, but if required, the ``huggingface_token``\nas well as the git ``user`` and the ``email`` can be explicitly specified.\nIf `clone_from` is used, and the repository is being instantiated into a non-empty directory,\ne.g. a directory with your trained model files, it will automatically merge them.\nArgs:\n    local_dir (``str``):\n        path (e.g. ``'my_trained_model/'``) to the local directory, where the ``Repository`` will be initalized.\n    clone_from (``str``, `optional`):\n        repository url (e.g. ``'https://huggingface.co/philschmid/playground-tests'``).\n    repo_type (``str``, `optional`):\n        To set when creating a repo: et to \"dataset\" or \"space\" if creating a dataset or space, default is model.\n    use_auth_token (``str`` or ``bool``, `optional`, defaults to ``True``):\n        huggingface_token can be extract from ``HfApi().login(username, password)`` and is used to authenticate against the hub\n        (useful from Google Colab for instance).\n    git_user (``str``, `optional`):\n        will override the ``git config user.name`` for committing and pushing files to the hub.\n    git_email (``str``, `optional`):\n        will override the ``git config user.email`` for committing and pushing files to the hub.\n    revision (``str``, `optional`):\n        Revision to checkout after initializing the repository. If the revision doesn't exist, a\n        branch will be created with that revision name from the default branch's current HEAD.\n    private (``bool``, `optional`, defaults to ``False``):\n        whether the repository is private or not.\n    skip_lfs_files (``bool``, `optional`, defaults to ``False``):\n        whether to skip git-LFS files or not.\nFile:           ~/miniconda3/envs/blog/lib/python3.8/site-packages/huggingface_hub/repository.py\nType:           type\nSubclasses:     \n\n\n\n\nIâ€™ll use flyswot-test as a way of playing around with this. To start with we can use Repository to clone the current version of the model.\n\nrepo = Repository(local_dir=\"flyswot-models\", clone_from=\"davanstrien/flyswot-test\")\n\nCloning https://huggingface.co/davanstrien/flyswot-test into local empty directory.\n\n\n\nrepo\n\n&lt;huggingface_hub.repository.Repository at 0x7fb10cccbc10&gt;\n\n\nWeâ€™ll need to be logged in to push changes\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n\n\n\nTo start with letâ€™s mock making a change to some of the repo files and seeing how we can upload these changes. We can use the Repository class as a context manager to make changes and have them committed to our model repository. Here we update the vocab file to add a new label.\n\nwith Repository(\n    local_dir=\"flyswot-models\",\n    clone_from=\"davanstrien/flyswot-test\",\n    git_user=\"Daniel van Strien\",\n).commit(\"update model\"):\n    with open(\"vocab.txt\", \"a\") as f:\n        f.write(\"new label\")\n\n/Users/dvanstrien/Documents/daniel/blog/_notebooks/flyswot-models is already a clone of https://huggingface.co/davanstrien/flyswot-test. Make sure you pull the latest changes with `repo.git_pull()`.\nPulling changes ...\nTo https://huggingface.co/davanstrien/flyswot-test\n   e54a742..18d149e  main -&gt; main\n\n\n\nThis could already be used at the end of our training script. Currently I have some util files that package up the model vocab, convert Pytorch to ONNX etc. This could easily be adapted to also push the updated model to the hub. There is only one thing we might still want to add.\n\n\nAdding more metadata: creating revision branches\nCurrently flyswot uses filenames to capture metadata about the model version. The models are versioned using calendar versioning. This works okay but we might be able to manage this in a slightly better way. One of the nice features that hf_hub (the Python library) offers that flyswot currently doesnâ€™t support well is being able to pass in a specific revision when using snapshot_download. This would then allow someone to run a specific older version of the model. This might be useful for various different scenarios. To do this weâ€™ll create a revision branch for the date the model was created. All that weâ€™ll do now is pass in a formatted date as the revision.\n\nfrom datetime import datetime\n\n\ndate_now = datetime.now()\nnow = date_now.strftime(\"%Y-%m-%d\")\nnow\n\n'2021-12-30'\n\n\n\nwith Repository(\n    local_dir=\"flyswot-models\",\n    clone_from=\"davanstrien/flyswot-test\",\n    git_user=\"Daniel van Strien\",\n    revision=now,\n).commit(f\"update model {now}\"):\n    for model in Path(\".\").glob(\".onnx\"):\n        model.rename(f\"{now}-model.onnx\")\n\n/Users/dvanstrien/Documents/daniel/blog/_notebooks/flyswot-models is already a clone of https://huggingface.co/davanstrien/flyswot-test. Make sure you pull the latest changes with `repo.git_pull()`.\nChecked out 2021-12-30 from 2021-12-30.\nYour branch is up to date with 'origin/2021-12-30'.\n\nPulling changes ...\nSeveral commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\nEverything up-to-date\n\n\n\nThis creates a new revision branch for the current date. Since I also want to have the default branch be the current model we would also push the same model to the default branch. This would then mean that we end up with a bunch of different branches with model snapshots that could be passed in but for the default behavior we can easily grab the latest model by not specifying a revision."
  },
  {
    "objectID": "posts/post-with-code/langfuse-tracing/Langfuse_+_HF_inference_endpoints.html",
    "href": "posts/post-with-code/langfuse-tracing/Langfuse_+_HF_inference_endpoints.html",
    "title": "Tracing Text Generation Inference calls",
    "section": "",
    "text": "%pip install openai langfuse --quiet\n\nNote: you may need to restart the kernel to use updated packages.\n\n\nLANGFUSE_SECRET_KEY=\"sk-lf-...\"\nLANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\nLANGFUSE_HOST=\"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n\nfrom google.colab import userdata\nimport os\n\n\nos.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get('LANGFUSE_SECRET_KEY')\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get('LANGFUSE_PUBLIC_KEY')\nHF_TOKEN = userdata.get('HF_TOKEN')\n\n\nfrom langfuse.decorators import observe\nfrom langfuse.openai import openai, OpenAI # OpenAI integration\n\n\nclient = OpenAI(\n        base_url=\"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1\",\n        api_key=HF_TOKEN,\n    )\n\n\nchat_completion = client.chat.completions.create(\n    model=\"tgi\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is Hugging Face?\"}\n    ],\n    stream=False\n)\n\n\nchat_completion\n\nChatCompletion(id='', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\" Hugging Face is a technology company that specializes in natural language processing (NLP) and artificial intelligence (AI). The company is best known for its development of Transformers, an open-source library that provides a wide range of pre-trained models for various NLP tasks, such as text classification, question answering, and language translation.\\n\\nHugging Face's Transformers library has gained widespread popularity among developers and researchers due to its ease of use, flexibility, and\", role='assistant', function_call=None, tool_calls=None))], created=1712314124, model='text-generation-inference/Mixtral-8x7B-Instruct-v0.1-medusa', object='text_completion', system_fingerprint='1.4.3-sha-e6bb3ff', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115))\n\n\n\n\n\nLangfuse Trace"
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "",
    "text": "There are many potential uses of computer vision in GLAM (Galleries, Libraries, Archives and Museums). These uses include:\n\nimage similarity search, i.e., given an image, find similar images\ntext search of images, i.e., given a text string â€œa picture of a dog eating an ice cream,â€ return relevant images\npage layout recognition, i.e., pull out semantically important parts of a document (articles, photos, titles, etc.)\nOptical Character Recognition (OCR)\n\nAll of these use cases require some technical work to implement or use. Usually, they need some programming knowledge too. However, there are many tasks in GLAM where computer vision could be helpful to, requiring less technical work to implement. In particular, many uses of computer vision can be framed as an image classification task (putting images into categories).\nLast year, Kaspar Beelen, Melvin Wevers, Thomas Smits, Katherine McDonough, and I shared a two-part Programming Historian lesson, Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification.\nThis lesson aimed to provide an introduction to how computer vision can be leveraged to work with images â€˜at scaleâ€™ â€“ in particular for research applications. While we tried hard to make the lesson (particularly part one) accessible, there are still barriers to getting started:\n\nYou need some Python knowledge: while we tried to keep the Python code simple (helped massively by the fastai library we use in the lesson), knowing how to code is still required. I couldnâ€™t find a good citation for this, but most estimates for the number of people who know how to program are around 0.5-1% of the global population. Of this percentage, fewer will know Python.\nNeed to have access to a GPU: whilst you can train deep learning models (the type of machine learning model introduced in our Programming Historian tutorial), it is a lot slower without them. However, setting up access to a GPU can be annoying. While â€˜freeâ€™ access is possible, this can also come with constraints.\nThe costs involved in training deep learning models can be hard to predict. You can usually get started for free, but often at some point, you need to invest some money in cloud computing. However, it can be difficult to know before you start training a model(s) how much it will cost.\n\nBeyond this, there is also a bigger question of how much energy you might want to invest in all of the above stuff involved in getting machine learning set up. This is especially true if you donâ€™t want to become a machine learning engineer and want to do something practical with machine learning.\n\n\nMany machine learning engineers will grimace at the title of this section. However, many use cases of machine learning exist where an existing machine learning architecture will work well. Training a model is not what would benefit most from human intervention.\nFor novel applications of machine learning or situations where you want to ensure a model is well suited to your domain, you may need to spend time creating training data. After training your model, there is also a step where you need to decide how to integrate machine learning into existing or new workflows. This is partially a technical question but often involves considerations beyond how I set up an API to serve my model.\nHand-training models can eat up a lot of time. Sometimes this time might be warranted but other times you might wish you could make some of this process less hands-on."
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html#introduction",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html#introduction",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "",
    "text": "There are many potential uses of computer vision in GLAM (Galleries, Libraries, Archives and Museums). These uses include:\n\nimage similarity search, i.e., given an image, find similar images\ntext search of images, i.e., given a text string â€œa picture of a dog eating an ice cream,â€ return relevant images\npage layout recognition, i.e., pull out semantically important parts of a document (articles, photos, titles, etc.)\nOptical Character Recognition (OCR)\n\nAll of these use cases require some technical work to implement or use. Usually, they need some programming knowledge too. However, there are many tasks in GLAM where computer vision could be helpful to, requiring less technical work to implement. In particular, many uses of computer vision can be framed as an image classification task (putting images into categories).\nLast year, Kaspar Beelen, Melvin Wevers, Thomas Smits, Katherine McDonough, and I shared a two-part Programming Historian lesson, Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification.\nThis lesson aimed to provide an introduction to how computer vision can be leveraged to work with images â€˜at scaleâ€™ â€“ in particular for research applications. While we tried hard to make the lesson (particularly part one) accessible, there are still barriers to getting started:\n\nYou need some Python knowledge: while we tried to keep the Python code simple (helped massively by the fastai library we use in the lesson), knowing how to code is still required. I couldnâ€™t find a good citation for this, but most estimates for the number of people who know how to program are around 0.5-1% of the global population. Of this percentage, fewer will know Python.\nNeed to have access to a GPU: whilst you can train deep learning models (the type of machine learning model introduced in our Programming Historian tutorial), it is a lot slower without them. However, setting up access to a GPU can be annoying. While â€˜freeâ€™ access is possible, this can also come with constraints.\nThe costs involved in training deep learning models can be hard to predict. You can usually get started for free, but often at some point, you need to invest some money in cloud computing. However, it can be difficult to know before you start training a model(s) how much it will cost.\n\nBeyond this, there is also a bigger question of how much energy you might want to invest in all of the above stuff involved in getting machine learning set up. This is especially true if you donâ€™t want to become a machine learning engineer and want to do something practical with machine learning.\n\n\nMany machine learning engineers will grimace at the title of this section. However, many use cases of machine learning exist where an existing machine learning architecture will work well. Training a model is not what would benefit most from human intervention.\nFor novel applications of machine learning or situations where you want to ensure a model is well suited to your domain, you may need to spend time creating training data. After training your model, there is also a step where you need to decide how to integrate machine learning into existing or new workflows. This is partially a technical question but often involves considerations beyond how I set up an API to serve my model.\nHand-training models can eat up a lot of time. Sometimes this time might be warranted but other times you might wish you could make some of this process less hands-on."
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html#can-we-approach-this-in-another-way",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html#can-we-approach-this-in-another-way",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "Can we approach this in another way?",
    "text": "Can we approach this in another way?\nAutoTrain is a tool that allow us to train machine learning models without needing to use Python, setup compute infrastructure or deal with unpredictable costs for training our models. In the rest of this blog post weâ€™ll go through the steps to using AutoTrain for a semi-realistic computer vision problem.\n\nThe dataset\nFor this project weâ€™ll use a dataset created by the Internet Archive as part of a request for help to judge a book by its cover. The blog post presents a use case for wanting to know if an image of a book cover is â€˜usefulâ€™ or â€˜not usefulâ€™. They provide some examples\nUseful image example:\n Not useful image example:\n\n\n\nA picture of a the front cover of a book which is blank\n\n\nEssentially the task is to decide whether an image of a digitized book cover is â€˜usefulâ€™ or â€˜not useful,â€™ i.e.Â whether showing this cover to Internet Archive users would give them useful information or not. The Internet Archive shared a dataset along with this blog post which contains examples for each category.\n\nWhat type of machine learning task is this?\nIf we look at the dataset shared by the Internet Archive, we have a directory structure that looks like this:\n.\nâ”œâ”€â”€ year-1923-not-very-useful-covers\nâ””â”€â”€ year-1923-useful-covers\nWe have two folders containing images. Each folder contains examples of image belonging to the name of each folder. Essentially, we want a model that learns which image belongs in each folder (based on the examples) and can put new images into the correct folder/category. This is known as an image classification task (as was mentioned in the introduction). The Hugging Face tasks page for this gives an excellent overview: https://huggingface.co/tasks/image-classification\n\n\n\nWhat are the steps involved?\nHow do we go from the dataset we started with to a trained model that we can begin to explore? For this particular example, the steps are as follows:\n\nDownload our data\nPrepare our data\nchoose our autotrain task\nUpload our data to autotrain\nTrain our models\nEvaluate our models\n\n\n\nDownload our data\nThis step will depend on where your data is and how itâ€™s arranged, but in this example, we can download the dataset from the Internet Archive. Three folders are provided in this case covering useful/not-useful for 1923 and for the year 2000 useful. Since the types of cover will have changed a fair bit in this time period weâ€™ll just download the folders for 1923.\n\n\n\nScreenshot of IA downloads\n\n\n\n\nPreparing our data\nThere isnâ€™t much prep we need to do for our data; however, we can provide data to AutoTrain in a few different ways for our image classification task. In this case weâ€™ll use the imagefolder format. This is essentially what we have already (folders containing examples of the labels weâ€™re interested in). Weâ€™ll create a top-level directory for our image data cover, which contains two subfolders with our example images.\n\n\n\nFolder screenshot\n\n\n\nResize our images (optional)\nThis step isnâ€™t strictly necessary, but itâ€™ll save time when uploading our dataset to AutoTrain. Most machine learning models expect training images to be relatively small (often 224x224 or 512x512 pixels). You can do this from the command line, but most operating systems have inbuilt tools for bulk resizing images, e.g., https://www.makeuseof.com/tag/batch-convert-resize-images-mac/\n\n\n\nSetup AutoTrain\nFrom the projects page, we can create a new project.\n\nHere we give our project a name and choose a task (image classification). We can also specify for AutoTrain to use a particular model. If you donâ€™t have a solid reason to select a model you can leave this decision to AutoTrain ðŸ¤—.\nOnce youâ€™ve created your project, youâ€™ll need to upload your data. There are different ways of doing this depending on the task. For image classification, we can use pre-arranged folders with a CSV/JSONL file with the labels or upload a dataset hosted on the Hugging Face hub.\n\nWe already have an organized folder so we can upload data.\n\nOnce weâ€™ve uploaded our images, weâ€™ll need to wait for the data to be uploaded. How long this takes depends on your internet speed. We can now click on Go to trainings.\n\nHere you will see that AutoTrain is formatting your uploaded data.\n\nOnce your data has been prepared, you can decide how many models you want AutoTrain to train for you. This decision depends on how much you want to spend on training your models and where you are in your project. If you are getting started and want to know how well a model may do, you may choose a lower number. If you want the best possible chance of getting the best-performing model, you could choose to train a more significant number of models.\n\nOnce you are ready, you can smash the start model training button!ðŸ”¥ The nice thing is that AutoTrain will ask you to confirm how much model training will cost. Once your models start training, a screen pops up with some randomly named models. Depending on the size of your dataset, it might take a bit longer to start seeing metrics for your model, but after a little while, you will begin to see scores (in this case, accuracy).\n\nAs the models train, you will see some models overtake others in performance. If you are easily amused like me, you will treat this like a fun spectator sport.\n\nYou also have a metrics overview tab for all the models you have trained. This makes it easy to sort by different metrics.\n\nEach of these models created by AutoTrain is a â€˜realâ€™ model hosted in a model repository on the Hugging Face hub. Some AutoTrain solutions hide away the actual artifacts and only allow you to interact with the models through their API. These models are available in the same way as any other model on the hub. By default, the models are made private, but you can decide to make the models openly available for others to use ðŸ¤—.\nYouâ€™ll also see in the screenshot that the models come with the outlines of a model card."
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html#why-does-our-model-suck",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html#why-does-our-model-suck",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "Why does our model suck?",
    "text": "Why does our model suck?\nFor this particular dataset, our models donâ€™t do super well (around 92% accuracy). Why is this?\n\nThe importance of training data\nStart to dig into the training data examples provided. Youâ€™ll see that quite a few images might be reasonably classified as belonging to the other category. In particular, quite a few images of the not-useful folder are similar to those in the useful folder. This is going to make it hard for our model to learn what weâ€™re after.\nThis also shows the importance of focusing on data and not over-focusing on model training. In this case, fixing our data will likely yield much better results than messing around with how we train the models. Using a tool like AutoTrain can quickly help you spot these issues early on so you can iterate on your training data.\n\n\nHow can we fix this??\nMove images between folders!!\nThere are better ways, but spending 30 mins removing examples you donâ€™t think the fit will make a big difference to the model performance. At some point, you are likely to want to use a proper annotation tool but to start with; you might be able to get quite far by using your operating systems file browser to re-arrange your training data.\nBelow is an example from another similar dataset where we get models with 99% accuracy. All of this without writing a line of code!"
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html#what-can-i-do-with-this-model",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html#what-can-i-do-with-this-model",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "What can I do with this model?",
    "text": "What can I do with this model?\nThere are various ways in which you can use the model youâ€™ve created. How you want to use it depends largely on your use case. In a follow-up blog post Iâ€™ll suggest a few options for how you can continue on the no/low-code journey to creating and using ML tools customised to your needs and data ðŸ¤—.\n\nShow me the models!\nYou can find the best models shown above here:\n\nhttps://huggingface.co/davanstrien/autotrain-ia-useful-covers-3665397856\nhttps://huggingface.co/davanstrien/autotrain-encyclopaedia-illustrations-blog-post-3327992158"
  },
  {
    "objectID": "pages/projects.html",
    "href": "pages/projects.html",
    "title": "Selected projects",
    "section": "",
    "text": "This page collects selected projects that I have worked on."
  },
  {
    "objectID": "pages/projects.html#machine-learning-projects",
    "href": "pages/projects.html#machine-learning-projects",
    "title": "Selected projects",
    "section": "Machine learning projects",
    "text": "Machine learning projects\n\nflyswot: using computer vision to detect â€˜fake flysheetsâ€™\nRelated Posts\nAn increasing challenge for libraries is managing the scale of digitised material resulting from digitisation projects and â€˜born digitalâ€™ materials. This project aims to detect mislabelled digitised manuscript pages.\n\n\n\nA manuscript page with the correct label â€œfse.ivrâ€\n\n\nAs a result of the limitations of a previous system for hosting digitised manuscript images, many images have incorrect page metadata associated with the image. An image has been correctly labelled as an â€˜end flysheetâ€™ in the example above. This label is represented by the fse label, which is included in the filename for the image. However other types of manuscript pages also have this label incorrectly assigned, i.e.Â a â€˜coverâ€™ has fse in the filename. There is around a petabyte of images to review before ingesting a new library system. This project uses computer vision to support library staff in processing this collection. At the moment, this project does the following:\n\npulls in an updated dataset of training examples\ntrains a model on these images\nthe model architecture has multiple heads to allow the model to make both a â€˜crudeâ€™ prediction for whether the image is incorrectly labelled and a â€˜fullâ€™ prediction for the true label.\nonce a new version of the model has been trained, it is pushed to the ðŸ¤— model hub.\nthe end-user uses the model through a command-line tool which is pointed at a directory of images to be checked.\n\nThe code for the command-line tool is available here: github.com/davanstrien/flyswot/\nSome of the tools used: fastai, DVC, Weights and Biases, ðŸ¤— model hub, pytest, nox, poetry\n\n\nBook Genre Detection\n\nThis project created machine learning models which would predict whether a book was â€˜fictionâ€™ or â€˜non- fictionâ€™ based on the book title:\n\nThe project was developed to address a gap in metadata in a large scale digitised book collection.\nThe project used weak supervision to generate a more extensive training set beyond the initial human-generated annotations.\nCurrently, two models are publicly available, one via the ðŸ¤— Model Hub and one via Zenodo.\nThe process of creating the models is documented in a Jupyter Book. This documentation aims to communicate the critical steps in the machine learning pipeline to aid other people in the sector develop similar models. https://huggingface.co/spaces https://huggingface.co/spaces/BritishLibraryLabs/British-Library-books-genre-classifier-v2\n\nSome of the tools used: fastai, transformers, blurr, Hugging face model hub, Jupyter Book, Snorkel, Gradio"
  },
  {
    "objectID": "pages/projects.html#datasets",
    "href": "pages/projects.html#datasets",
    "title": "Selected projects",
    "section": "Datasets",
    "text": "Datasets\n\nBritish Library books\n\nExtracted plain text and other metadata files from ALTO XML https://github.com/davanstrien/digitised-books-ocr-and-metadata\nAdded the dataset to the ðŸ¤— datasets hub\n\n\n\nBritish Library Books Genre data\n\nCreated a datasets loading script and prepared a Dataset card for a dataset supporting book genre detection using machine learning: https://huggingface.co/datasets/blbooksgenre\n\n\n\nDatasets to support programming historian lessons\nI think having more realistic datasets is important for teaching machine learning effectively. As a result, I created two datasets for two under review Programming Historian lessons.\n\n19th Century United States Newspaper Advert images with â€˜illustratedâ€™ or â€˜non illustratedâ€™ labels\n19th Century United States Newspaper images predicted as Photographs with labels for â€œhumanâ€, â€œanimalâ€, â€œhuman-structureâ€ and â€œlandscapeâ€\n\n\n\nWorkshop datasets\n\nImages from Newspaper Navigator predicted as maps, with human corrected labels"
  },
  {
    "objectID": "pages/projects.html#workshop-materials",
    "href": "pages/projects.html#workshop-materials",
    "title": "Selected projects",
    "section": "Workshop materials",
    "text": "Workshop materials\n\nComputer Vision for the Humanities workshop: This workshop aims to provide an introduction to computer vision aimed for humanities applications. In particular this workshop focuses on providing a high level overivew of machine learning based approaches to computer vision focusing on supervised learning. The workshop includes discussion on working with historical data. The materials are based on in progress Programming Historian lessons.\nWorking with maps at scale using Computer Vision and Jupyter notebooks\nIntroduction to Jupyter Notebooks: the weird and the wonderful\nImage Search: Materials for a workshop on image search with a focus on heritage data. The workshop is based on a blog post Image search with ðŸ¤— datasets but goes into a little bit more detail."
  },
  {
    "objectID": "pages/projects.html#tutorials",
    "href": "pages/projects.html#tutorials",
    "title": "Selected projects",
    "section": "Tutorials",
    "text": "Tutorials\n\nJupyter book showing how to build an ML powered book genre classifier\nA (brief) history of advertising in US Newspapers using computer vision\n(Under review) Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification, Programming Historian lessons\n(Under development) Intro to AI for GLAM, Carpentries Lesson"
  }
]