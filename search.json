[
  {
    "objectID": "pages/projects.html",
    "href": "pages/projects.html",
    "title": "Selected projects",
    "section": "",
    "text": "This page collects selected projects that I have worked on."
  },
  {
    "objectID": "pages/projects.html#machine-learning-projects",
    "href": "pages/projects.html#machine-learning-projects",
    "title": "Selected projects",
    "section": "Machine learning projects",
    "text": "Machine learning projects\n\nflyswot: using computer vision to detect ‚Äòfake flysheets‚Äô\nRelated Posts\nAn increasing challenge for libraries is managing the scale of digitised material resulting from digitisation projects and ‚Äòborn digital‚Äô materials. This project aims to detect mislabelled digitised manuscript pages.\n\n\n\nA manuscript page with the correct label ‚Äúfse.ivr‚Äù\n\n\nAs a result of the limitations of a previous system for hosting digitised manuscript images, many images have incorrect page metadata associated with the image. An image has been correctly labelled as an ‚Äòend flysheet‚Äô in the example above. This label is represented by the fse label, which is included in the filename for the image. However other types of manuscript pages also have this label incorrectly assigned, i.e.¬†a ‚Äòcover‚Äô has fse in the filename. There is around a petabyte of images to review before ingesting a new library system. This project uses computer vision to support library staff in processing this collection. At the moment, this project does the following:\n\npulls in an updated dataset of training examples\ntrains a model on these images\nthe model architecture has multiple heads to allow the model to make both a ‚Äòcrude‚Äô prediction for whether the image is incorrectly labelled and a ‚Äòfull‚Äô prediction for the true label.\nonce a new version of the model has been trained, it is pushed to the ü§ó model hub.\nthe end-user uses the model through a command-line tool which is pointed at a directory of images to be checked.\n\nThe code for the command-line tool is available here: github.com/davanstrien/flyswot/\nSome of the tools used: fastai, DVC, Weights and Biases, ü§ó model hub, pytest, nox, poetry\n\n\nBook Genre Detection\n\nThis project created machine learning models which would predict whether a book was ‚Äòfiction‚Äô or ‚Äònon- fiction‚Äô based on the book title:\n\nThe project was developed to address a gap in metadata in a large scale digitised book collection.\nThe project used weak supervision to generate a more extensive training set beyond the initial human-generated annotations.\nCurrently, two models are publicly available, one via the ü§ó Model Hub and one via Zenodo.\nThe process of creating the models is documented in a Jupyter Book. This documentation aims to communicate the critical steps in the machine learning pipeline to aid other people in the sector develop similar models. https://huggingface.co/spaces https://huggingface.co/spaces/BritishLibraryLabs/British-Library-books-genre-classifier-v2\n\nSome of the tools used: fastai, transformers, blurr, Hugging face model hub, Jupyter Book, Snorkel, Gradio"
  },
  {
    "objectID": "pages/projects.html#datasets",
    "href": "pages/projects.html#datasets",
    "title": "Selected projects",
    "section": "Datasets",
    "text": "Datasets\n\nBritish Library books\n\nExtracted plain text and other metadata files from ALTO XML https://github.com/davanstrien/digitised-books-ocr-and-metadata\nAdded the dataset to the ü§ó datasets hub\n\n\n\nBritish Library Books Genre data\n\nCreated a datasets loading script and prepared a Dataset card for a dataset supporting book genre detection using machine learning: https://huggingface.co/datasets/blbooksgenre\n\n\n\nDatasets to support programming historian lessons\nI think having more realistic datasets is important for teaching machine learning effectively. As a result, I created two datasets for two under review Programming Historian lessons.\n\n19th Century United States Newspaper Advert images with ‚Äòillustrated‚Äô or ‚Äònon illustrated‚Äô labels\n19th Century United States Newspaper images predicted as Photographs with labels for ‚Äúhuman‚Äù, ‚Äúanimal‚Äù, ‚Äúhuman-structure‚Äù and ‚Äúlandscape‚Äù\n\n\n\nWorkshop datasets\n\nImages from Newspaper Navigator predicted as maps, with human corrected labels"
  },
  {
    "objectID": "pages/projects.html#workshop-materials",
    "href": "pages/projects.html#workshop-materials",
    "title": "Selected projects",
    "section": "Workshop materials",
    "text": "Workshop materials\n\nComputer Vision for the Humanities workshop: This workshop aims to provide an introduction to computer vision aimed for humanities applications. In particular this workshop focuses on providing a high level overivew of machine learning based approaches to computer vision focusing on supervised learning. The workshop includes discussion on working with historical data. The materials are based on in progress Programming Historian lessons.\nWorking with maps at scale using Computer Vision and Jupyter notebooks\nIntroduction to Jupyter Notebooks: the weird and the wonderful\nImage Search: Materials for a workshop on image search with a focus on heritage data. The workshop is based on a blog post Image search with ü§ó datasets but goes into a little bit more detail."
  },
  {
    "objectID": "pages/projects.html#tutorials",
    "href": "pages/projects.html#tutorials",
    "title": "Selected projects",
    "section": "Tutorials",
    "text": "Tutorials\n\nJupyter book showing how to build an ML powered book genre classifier\nA (brief) history of advertising in US Newspapers using computer vision\n(Under review) Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification, Programming Historian lessons\n(Under development) Intro to AI for GLAM, Carpentries Lesson"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I‚Äôm Daniel van Strien, a Machine Learning Librarian at Hugging Face.\nI started out in libraries and digital humanities, including time at the British Library on the Living with Machines project. I‚Äôve also worked in academic libraries supporting open access and open science.\nMy work lives at the intersection of open-source technology, information management, and applied AI. Whether I‚Äôm building tools, writing tutorials, or contributing to community projects, I‚Äôm focused on making complex systems more usable, transparent, and inclusive.\nAt Hugging Face, I focus on the foundation of machine learning: data and datasets."
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "",
    "text": "%pip install datasets qdrant-client --q\n\n\n[notice] A new release of pip available: 22.3.1 -&gt; 23.3.1\n\n[notice] To update, run: pip install --upgrade pip\n\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#loading-our-dataset",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#loading-our-dataset",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "Loading our dataset",
    "text": "Loading our dataset\nFor this post we‚Äôll use the Cohere/wikipedia-22-12-simple-embeddings dataset which has already had embeddings generated for it. This dataset was created by Cohere and creates embeddings for millions of Wikipedia articles. See this post for more details.\nWe‚Äôll use the Hugging Face datasets library to load the dataset.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Cohere/wikipedia-22-12-simple-embeddings\", split=\"train\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n  table = cls._concat_blocks(blocks, axis=0)\n\n\nLet‚Äôs take a quick look at the dataset.\n\ndataset\n\nDataset({\n    features: ['id', 'title', 'text', 'url', 'wiki_id', 'views', 'paragraph_id', 'langs', 'emb'],\n    num_rows: 485859\n})\n\n\nWe can see the dataset has a emb column which contains the embeddings for each article. Alongside this we see the title and text for the articles alongside some other metadata. Let‚Äôs also take a look at the features of the dataset.\nLet‚Äôs also take a quick look at the features of the dataset. Hugging Face Dataset objects have a features attribute which contains the features of the dataset. We can see that the emb column is a Sequence of float32 values. We also have some other columns with string values, int32 and float32 values.\n\ndataset.features\n\n{'id': Value(dtype='int32', id=None),\n 'title': Value(dtype='string', id=None),\n 'text': Value(dtype='string', id=None),\n 'url': Value(dtype='string', id=None),\n 'wiki_id': Value(dtype='int32', id=None),\n 'views': Value(dtype='float32', id=None),\n 'paragraph_id': Value(dtype='int32', id=None),\n 'langs': Value(dtype='int32', id=None),\n 'emb': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}\n\n\nQdrant has support for a pretty varied range of types. All of these types in our dataset are supported by Qdrant so we don‚Äôt need to do any conversion.\n\nCreating a Qdrant collection\nWe‚Äôll use the Qdrant Python client for this post. This client is really nice since it allows you to create a local collection using pure Python i.e.¬†no need to run a Qdrant server. This is great for testing and development. Once you‚Äôre ready to deploy your collection you can use the same client to connect to a remote Qdrant server.\n\nfrom qdrant_client import QdrantClient\n\nWe first create a client, in this case using a local path for our DB.\n\nclient = QdrantClient(path=\"db\")  # Persists changes to disk\n\n\n\nConfiguring our Qdrant collection\nQdrant is very flexible but we need to let Qdrant now a few things about our collection. These include the name, and a config for the vectors we want to store. This config includes the dimensionality of the vectors and the distance metric we want to use. Let‚Äôs first check out the dimensionality of our vectors.\n\nvector_size = len(dataset[0]['emb'])\n\nWe‚Äôll also store our collection in a variable so we can use it later.\n\ncollection_name = \"cohere_wikipedia\"\n\n\nfrom qdrant_client.models import Distance, VectorParams\n\nclient.recreate_collection(\n    collection_name=collection_name,\n    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n)\n\nTrue"
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#adding-our-data-to-qdrant",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#adding-our-data-to-qdrant",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "Adding our data to Qdrant",
    "text": "Adding our data to Qdrant\nNote this code can be heavily optimized but gives an idea of how easy adding data to Qdrant can be. For many datasets this naive approach will work fine.\nThe approach we‚Äôll take below is to loop through our dataset and yield each row as a PointStruct. This is a Qdrant object that contains the vector and any other data, referred to as the payload, that we want to store.\n\nfrom qdrant_client.models import PointStruct\n\n\ndef yield_rows(dataset):\n    for idx, row in enumerate(dataset, start=1):\n        vector = row[\"emb\"] # grab the vector\n        payload = {k: v for k, v in row.items() if k != \"emb\"} # grab the rest of the fields without the vector\n        yield PointStruct(id=idx, vector=vector, payload=payload)\n\nFor this post we‚Äôll use a smallish subset of the dataset. We‚Äôll use the first 100_000 rows. Big enough to be interesting but small enough to play around with quickly.\n\nsample = dataset.select(range(100_000))\n\nWe‚Äôll use the toolz libraries partition_all function to get batches from our yield_rows function. We‚Äôll use tqdm to show a progress bar.\n\nfrom toolz import partition_all\nfrom tqdm.auto import tqdm\n\n\n%%time\nbs = 100\nfor batch in tqdm(partition_all(bs, yield_rows(sample)), total=len(sample) // bs):\n    client.upsert(collection_name=collection_name, points=list(batch), wait=False)\n\n\n\n\nCPU times: user 30.9 s, sys: 35.7 s, total: 1min 6s\nWall time: 1min 19s\n\n\nOn my 2021 MacBook Pro with an M1 chip this takes about 90 seconds to run. As mentioned above this can be heavily optimized but this gives an idea of how easy it is to add data to Qdrant from a Hugging Face dataset."
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#searching-our-qdrant-collection",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#searching-our-qdrant-collection",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "Searching our Qdrant collection",
    "text": "Searching our Qdrant collection\nWhat can we do with our Qdrant collection? We can use our embeddings to find similar wikipedia articles. Let‚Äôs see how we can do that.\nFirst we‚Äôll use the get_collection method to see some information about our collection.\n\nfrom rich import print\n\nprint(client.get_collection(collection_name))\n\nCollectionInfo(\n    status=&lt;CollectionStatus.GREEN: 'green'&gt;,\n    optimizer_status=&lt;OptimizersStatusOneOf.OK: 'ok'&gt;,\n    vectors_count=100000,\n    indexed_vectors_count=0,\n    points_count=100000,\n    segments_count=1,\n    config=CollectionConfig(\n        params=CollectionParams(\n            vectors=VectorParams(\n                size=768,\n                distance=&lt;Distance.COSINE: 'Cosine'&gt;,\n                hnsw_config=None,\n                quantization_config=None,\n                on_disk=None\n            ),\n            shard_number=None,\n            replication_factor=None,\n            write_consistency_factor=None,\n            read_fan_out_factor=None,\n            on_disk_payload=None\n        ),\n        hnsw_config=HnswConfig(\n            m=16,\n            ef_construct=100,\n            full_scan_threshold=10000,\n            max_indexing_threads=0,\n            on_disk=None,\n            payload_m=None\n        ),\n        optimizer_config=OptimizersConfig(\n            deleted_threshold=0.2,\n            vacuum_min_vector_number=1000,\n            default_segment_number=0,\n            max_segment_size=None,\n            memmap_threshold=None,\n            indexing_threshold=20000,\n            flush_interval_sec=5,\n            max_optimization_threads=1\n        ),\n        wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0),\n        quantization_config=None\n    ),\n    payload_schema={}\n)\n\n\n\nWe can see a bunch of information about our collection. Including the vector count, the dimensionality of the vectors and the distance metric we‚Äôre using. You‚Äôll see that there are plenty of knobs to turn here to optimize your collection but that‚Äôs for another post.\nWe can use the scroll method to get the first vector from our collection\n\nprint(client.scroll(collection_name,limit=1)[0][0])\n\nRecord(\n    id=1,\n    payload={\n        'id': 0,\n        'title': '24-hour clock',\n        'text': 'The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and\nis divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only\nin the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very \nrarely) as continental time. In some parts of the world, it is called railway time. Also, the international \nstandard notation of time (ISO 8601) is based on this format.',\n        'url': 'https://simple.wikipedia.org/wiki?curid=9985',\n        'wiki_id': 9985,\n        'views': 2450.62548828125,\n        'paragraph_id': 0,\n        'langs': 30\n    },\n    vector=None\n)\n\n\n\nWe can also grab items from the payload for each point.\n\nprint(client.scroll('cohere_wikipedia',limit=1)[0][0].payload['text'])\n\nThe 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into \n24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and \nthe English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as \ncontinental time. In some parts of the world, it is called railway time. Also, the international standard notation \nof time (ISO 8601) is based on this format.\n\n\n\nWe can see this article is about the 24-hour clock system. Let‚Äôs see what other pages are similar to this one. We can optionally get the vector for the query point.\n\nvector = client.scroll('cohere_wikipedia',limit=1,with_vectors=True)[0][0].vector\n\nWe can use our vector as a query to find similar vectors in our collection. We‚Äôll use the search method to do this.\n\nquery_vector = client.scroll(collection_name, limit=1, with_vectors=True)[0][0].vector\nhits = client.search(\n    collection_name=collection_name,\n    query_vector=query_vector,\n    limit=15,  # Return 5 closest points\n)\n\nLet‚Äôs look at some of the results. We can see that the first result is the same article. The rest also seem to be about time/24 hour clock systems!\n\nfor hit in hits:\n    print(f\"{hit.payload['title']} | {hit.payload['text']}\")\n    print(\"---\")\n\n24-hour clock | The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and \nis divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only\nin the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very \nrarely) as continental time. In some parts of the world, it is called railway time. Also, the international \nstandard notation of time (ISO 8601) is based on this format.\n\n\n\n---\n\n\n\n24-hour clock | However, the US military prefers not to say 24:00 - they do not like to have two names for the same\nthing, so they always say \"23:59\", which is one minute before midnight.\n\n\n\n---\n\n\n\n24-hour clock | 24-hour clock time is used in computers, military, public safety, and transport. In many Asian, \nEuropean and Latin American countries people use it to write the time. Many European people use it in speaking.\n\n\n\n---\n\n\n\n24-hour clock | In railway timetables 24:00 means the \"end\" of the day. For example, a train due to arrive at a \nstation during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the \nday go at 00:00.\n\n\n\n---\n\n\n\n24-hour clock | A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or \nhours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under \nthe 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and \nends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called \n24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you \nwould say \"Tuesday at 24:00\" and \"Wednesday at 00:00\" to mean exactly the same time.\n\n\n\n---\n\n\n\n12-hour clock | The 12-hour clock is a way of dividing the 24 hours of the day into two sections. The two halves \nare called ante meridiem (a.m.) and post meridiem (p.m.).\n\n\n\n---\n\n\n\n12-hour clock | Both names are from Latin, and numbered from 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 and 12. Time from \nmidnight to noon is a.m. and from noon to midnight p.m. The table at right shows how it relates to the 24-hour \nclock.\n\n\n\n---\n\n\n\nHour | An hour (abbreviation: h or hr) is a unit of measurement used to measure time. An hour is equal to 60 \nminutes. 24 hours are equal to one day. Unlike the second, the hour is not a base SI unit.\n\n\n\n---\n\n\n\nMidnight | The time period \"00:00 - 00:01\" is midnight. On computer clocks, the day changes to the next day the \nminute(s) after midnight.\n\n\n\n---\n\n\n\nChinese zodiac | In the old days, China and Japan used a 12-hour system to tell the time of day and night (unlike \nthe 24 hour system used today). The 12 hour system divides the day of 24 hours into 12 hours, each of which has a \nsign of the zodiac:\n\n\n\n---\n\n\n\nCoordinated Universal Time | Note that UTC uses the 24-hour clock. That means there is no 'AM' or 'PM'. For \nexample, 4:00PM would be 16:00 or 1600. UTC also does not use daylight savings time - that way the time stays \nconsistent the entire year.\n\n\n\n---\n\n\n\nMidnight | In the world, midnight is the start of one day and the end of the last day. It's the dividing point \nbetween two days.\n\n\n\n---\n\n\n\nNoon | Noon is the time exactly halfway through the day (12.00-12:00 in the 24-hour clock and 12:00 PM-12:00 PM in \nthe 12-hour clock). Midday also means noon, although this also means \"around\" noon, or very early afternoon.\n\n\n\n---\n\n\n\nCoordinated Universal Time | The standard before was Greenwich Mean Time (GMT). UTC and GMT are almost the same. In\nfact, there is no practical difference which would be noticed by ordinary people.\n\n\n\n---\n\n\n\nMidnight | In the United States and Canada, digital clocks and computers usually show 12¬†a.m. right at midnight. \nHowever, people have to remember that any time is actually an instant. The \"a.m.\" shown on clock displays means the\n12-hour period after the instant of midnight. So when a clock says \"12:00 a.m.\", midnight has already passed and a \nnew day has started. In other words, 11:59¬†p.m.¬†shows until midnight; at the instant of midnight, it changes to \n12:00. At the same time, the p.m. changes to a.m., but a.m. does not mean the instant of midnight which separates \np.m. and a.m.\n\n\n\n---"
  },
  {
    "objectID": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#conclusion",
    "href": "posts/post-with-code/datasets-with-qdrant/2023-11-08-datasets_to_qdrant.html#conclusion",
    "title": "How to load a Hugging Face dataset into Qdrant?",
    "section": "Conclusion",
    "text": "Conclusion\nThis post showed how it‚Äôs possible to easily convert a Hugging Face dataset into a Qdrant collection. We then showed how we can use this collection to find similar articles.\nThere is a lot of scope for optimization here. For example, we could use a more efficient way to add data to Qdrant. We could also use a more efficient way to search our collection. It would be very cool to directly have a from_hf_datasets method in the Qdrant Python client that would do all of this for us and include some optimizations!\nI hope this post has shown how easy it is to use Qdrant with Hugging Face datasets. If you have any questions or comments please let me know on Twitter."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "",
    "text": "updates"
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#introduction-to-colpali",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#introduction-to-colpali",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Introduction to ColPali",
    "text": "Introduction to ColPali\ntl;dr this blog post covers how to generate a dataset for training/fine-tuning ColPali models an open VLM to generate queries. You can find the dataset produced by this approach here.\nColPali is a new multimodal approach to retrieval which aims to replace existing document retrievers which often rely on an OCR step with an end-to-end multimodal approach. This approach also aims to take into account the visual content and layout of the documents, in addition to the textual content. Looking at an example of a document:\n\nwe could rely only on the text, but the page also has a table which could be relevant for retrieving the most useful document either for direct use or in a RAG pipeline. In many documents we will find that pages don‚Äôt just contain images but also other rich sources of visual information that could be relevant for retrieval."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#how-copali-works",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#how-copali-works",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "How CoPali works?",
    "text": "How CoPali works?\nColPali is a document retrieval model that leverages a Vision Language Model (VLM) to understand and retrieve documents based on their visual content. The key steps are:\n\nDocument Encoding: ColPali takes document pages as images and processes them through a VLM (specifically, PaliGemma-3B) to generate embeddings for each image patch.\nQuery Encoding: User queries are encoded using the same VLM‚Äôs text processing capabilities.\nLate Interaction: Instead of comparing a single vector per document, ColPali uses a ‚Äúlate interaction‚Äù mechanism that compares each query token embedding to all document patch embeddings.\n\nScoring: The relevance score between a query and a document is computed by summing the maximum similarities between each query token and all document patches.\nThis approach allows ColPali to understand both textual and visual elements in documents, enabling more comprehensive and accurate retrieval compared to traditional text-only methods. It eliminates the need for complex document parsing pipelines, as it works directly with document images.\nWhilst, late interaction methods have some additional computational costs, for many documents with rich visual content there is also a high potential processing cost in the document parsing pipeline ‚Äî and this pipeline can also be rather brittle.\n\nThe training data for ColPali\nLet‚Äôs take a look at what the training data looks like for training ColPali.\n\n\nYou‚Äôll see that each row contains a bunch of metadata about the source of the document and other information but the key parts for the actual training of the model are the image and the queries pairs. When training ColPali we want a dataset of images with queries that relate to the image. This will allow the model to learn how queries are related to the images. To help the model learn it can also be helpful to have negative examples.\nLet‚Äôs take a closer look at a few examples from the data before we jump into generating our own.\nFor this notebook I‚Äôm using uv to manage Python installs because I find it to be a lot quicker but you can use pip if you prefer.\n\n%pip install uv\n\n\nCollecting uv\n\n  Downloading uv-0.4.15-py3-none-macosx_11_0_arm64.whl.metadata (11 kB)\n\nDownloading uv-0.4.15-py3-none-macosx_11_0_arm64.whl (10.9 MB)\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10.9/10.9 MB 22.7 MB/s eta 0:00:001m22.2 MB/s eta 0:00:01\n\nInstalling collected packages: uv\n\nSuccessfully installed uv-0.4.15\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n\n!uv pip install accelerate qwen-vl-utils torchvision torch datasets huggingface_hub[hf_transfer] polars --system\n!uv pip install git+https://github.com/huggingface/transformers.git  --system\n\n\nAudited 6 packages in 11ms\n\nResolved 54 packages in 230ms                                        \n\nAudited 54 packages in 0.08ms\n\n\n\n\n\n!uv pip install flash-attn --no-build-isolation --system\n\n\nAudited 1 package in 6ms\n\n\n\n\nWe can take a look at a few examples from the data using Polars.\n\nimport polars as pl\n\nsplits = {\"train\": \"data/train-*.parquet\", \"test\": \"data/test-00000-of-00001.parquet\"}\ndf = pl.scan_parquet(\n    \"hf://datasets/vidore/colpali_train_set/\" + splits[\"train\"],\n)\n\nLet‚Äôs see what columns we have in the dataset.\n\ndf.columns\n\n['image',\n 'image_filename',\n 'query',\n 'answer',\n 'source',\n 'options',\n 'page',\n 'model',\n 'prompt',\n 'answer_type']\n\n\nSince we‚Äôre shortly going to turn to how we can generate our own queries, let‚Äôs take a look at a few examples from the data. We‚Äôll filter to focus on the pdf source, since these are the ones created by the authors of ColPali (the other sources are from existing datasets).\n\nfiltered_df = (\n    df.filter(pl.col(\"source\").str.contains(\"pdf\")).select([\"query\"]).head(10).collect()\n)\nquery_list = filtered_df[\"query\"].to_list()\nquery_list\n\n['What is the duration of the course mentioned in the image?',\n 'What is the primary purpose of the PTC in lithium batteries?',\n 'How is the baseline CO2 emissions calculated for affected EGUs in the low load natural gas-fired or oil-fired subcategories?',\n 'What are some suggestions Liberty Medical Group should consider to improve their accounts receivable turnover and days sales in receivables ratios?',\n 'What measures will the Secretary determine to assess the quality of care furnished by the ACO?',\n 'How is the CT kerma index measured?',\n 'What is the difference between hindsight and foresight according to the passage?',\n 'How are the vapor jets arranged during the transitional and film boiling regimes?',\n 'What types of batteries are covered by the European Union Batteries Directive?',\n 'What are some factors to consider for CAES facility development in different parts of New York?']\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne thing you might notice about these queries is that many of them are more focused on ‚Äúquestions‚Äù about documents rather than traditional search queries. We‚Äôll shortly see the prompting approach used to generate these queries but we might already want to consider, depending on our use case, whether we want to generate more ‚Äúsearch‚Äù-like queries or more ‚Äúquestion‚Äù-like queries."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#creating-queries-from-documents",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#creating-queries-from-documents",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Creating queries from documents",
    "text": "Creating queries from documents\nFor the data using in ColPali, part of the dataset was sourced from existing document question answering datasets. Another component was generated using Claude 3.0 Sonnet with the following prompt:\n\nprompt = \"\"\"\nYou are an assistant specialized in Multimodal RAG tasks.\n\nThe task is the following: given an image from a pdf page, you will have to generate questions that can be asked by a user to retrieve information from a large documentary corpus.\n\nThe question should be relevant to the page, and should not be too specific or too general. The question should be about the subject of the page, and the answer needs to be found in the page.\n\nRemember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus.\n\nGenerate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question.\n\nGenerate at most THREE pairs of questions and answers per page in a dictionary with the following format, answer ONLY this dictionary NOTHING ELSE:\n\n{\n    \"questions\": [\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        },\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        },\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        }\n    ]\n}\nwhere XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n\nNote: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n\nHere is the page:\"\"\"\n\nAs you can see this prompt is focused on generating questions that are relevant to the page and that could be asked by a user and answered by the document. One thing I noticed from the queries generated with this prompt is that I think some of the generated queries strayed a bit from the prompts request to not assume the user knew the content of the document. Some of the questions were quite specific and it seemed like they were tailored to the particular page they were generated from.\nWhilst out of the box performance of ColPali is likely to be good for many domains and use cases it it likely that fine tuning ColPali on domain specific data will lead to improved performance. We‚Äôll now turn to how we can generate our own queries for ColPali."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#ufo-colpali-creating-a-domain-specific-dataset",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#ufo-colpali-creating-a-domain-specific-dataset",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "UFO ColPali: creating a domain specific dataset",
    "text": "UFO ColPali: creating a domain specific dataset\nLet‚Äôs now turn to how we could approach generating our own query image pairs for training ‚Äî or more likely fine tuning ‚Äî a ColPali model on domain specific data.\nTo make the example slightly more interesting we‚Äôll stray away from using an existing document dataset and use a UFO dateset which has been sourced from an Internet Archive Collection of UFO newsletters. This dataset was created from a sample of PDFs from this collection which were then split into single page images using the pdf-to-page-images-dataset Hugging Face Space. If you have a collection of PDFs related to your domain you could also use this Space to quickly create a dataset for your use case.\n\nimport os\nfrom datasets import load_dataset\n\nIf you are running on a machine with quite a fast connection, the following environment variable may increase the speed of the model and dataset downloads.\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nLet‚Äôs start by loading the UFO dataset and taking a look at a few examples.\n\nds = load_dataset(\"davanstrien/ufo\", split=\"train\")\n\n\n\n\nLet‚Äôs see what a row looks like\n\nds[0]\n\n{'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=596x842&gt;}\n\n\nand look at an example document\n\nds[3][\"image\"]\n\n\n\n\n\n\n\n\nWe can see that the dataset currently just contains images which map to a single page of a document. We can also see from the example document that the document does contain some visual elements which could be relevant for retrieval.\nWhat we need to train or fine tune ColPali is at least one query for each of our the document images in our dataset."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#using-qwen2-vl-to-generate-queries",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#using-qwen2-vl-to-generate-queries",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Using Qwen2-VL to generate queries",
    "text": "Using Qwen2-VL to generate queries\nWhilst the original ColPali paper used the Claude 3.0 Sonnet model to generate queries for the UFO dataset, we‚Äôll use the Qwen2-VL model to generate queries for our UFO dataset. Specifcally we‚Äôll use the 7B variant of the model (Qwen/Qwen2-VL-7B-Instruct). This is an open VLLM (Apache 2.0 licensed) Vision Language Model which has shown strong performance on a variety of vision and language tasks. Whilst the model won‚Äôt run on a standard T4 Google Colab instance we can run the model either on a L4 or an A100 GPU on Google Colab or you can use the Hugging Face Jupyter Spaces template to run the model on an L40s as I did.\nTo start let‚Äôs load the model and get a sense of how we can use it. We‚Äôll do this through the Transformers library. It‚Äôs also possible to run the model using the vLLM library but since we‚Äôre only focusing on generating a relatively small number of queries this extra set up probably isn‚Äôt worth it in this case (I‚Äôd be interested to hear in the comments if you try it out and how it goes).\nTo start we‚Äôll load the model. We‚Äôll use flash_attention_2 which should help a bit with the performance and memory usage of the model.\n\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n    device_map=\"auto\",\n)\n\nUnrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\nYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n\n\n\n\n\nWe next define the processor. This is the component that will help us prepare the inputs for the model.\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\nAs with many more recent models, we can use a chat template to help us prepare the inputs for the model. Here is the example from the Qwen2-VL-7B-Instruct model card.\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\nWe can now pass this to the processor to get the inputs ready for the model. You‚Äôll see here we first pass in the messages to the apply_chat_template method of the processor and then use the process_vision_info helper function which comes from the qwen_vl_utils library to prepare images and videos (which are not relevant here).\n\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\n\nIf we take a look at the text we can see that the processor has applied a chat template to the messages.\n\ntext\n\nWe now pass the text, image and video inputs (in this case None) to the processor and prepare the inputs for the model.\n\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\nwe can see what this input looks like\n\ninputs\n\n{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.8501,  0.8501,  0.8647,  ...,  1.3922,  1.3922,  1.3922],\n        [ 0.9376,  0.9376,  0.9376,  ...,  1.4491,  1.4491,  1.4491],\n        [ 0.9084,  0.9376,  0.9376,  ...,  1.4065,  1.4207,  1.4207],\n        ...,\n        [-0.1280, -0.1280, -0.1426,  ..., -0.2431, -0.2715, -0.3000],\n        [-0.3324, -0.3324, -0.3032,  ..., -0.3000, -0.2715, -0.2857],\n        [-0.3762, -0.4054, -0.4054,  ..., -0.4279, -0.4422, -0.4564]],\n       device='cuda:0'), 'image_grid_thw': tensor([[  1,  98, 146]], device='cuda:0')}\n\n\nNow the inputs are ready to pass to the model. BTW, all of this inference code is copied straight from the Qwen2-VL-7B-Instruct model card on Hugging Face. There are a few things we might want to tweak but the basic examples are pretty much all we need for our use case.\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=200)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\noutput_text\n\n[\"The image depicts a serene beach scene with a woman and her dog enjoying a moment together. The woman is sitting on the sandy beach, facing the ocean, and appears to be engaging in a playful activity with her dog. She is wearing a plaid shirt and dark pants, and her hair is long and dark. The dog, which is a large breed, possibly a Labrador Retriever, is sitting in front of her, wearing a harness. The dog is extending its front paw towards the woman's hand, as if they are giving each other a high-five. The woman is smiling, indicating a joyful interaction.\\n\\nThe beach is relatively empty, with gentle waves lapping at the shore. The sky is clear, and the sun is low on the horizon, casting a warm, golden light over the scene. The lighting suggests that the photo was taken either in the early morning or late afternoon, creating a peaceful and tranquil atmosphere. The sand is smooth and lightly textured, typical of a well-m\"]"
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#building-colpali-queries",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#building-colpali-queries",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Building Colpali queries",
    "text": "Building Colpali queries\nWe now have a sense of how to generate responses using both a text and image input using the Qwen2-VL-7B-Instruct model. We‚Äôll now use this model to generate queries for our UFO dataset. To start, let‚Äôs see how the prompt from the paper looks. The only thing we modified from the original prompt was to ask for JSON output rather than dictionaries since this model seemed to work better with this approach in my (somewhat limited) testing.\nprompt = \"\"\"\nYou are an assistant specialized in Multimodal RAG tasks.\n\nThe task is the following: given an image from a pdf page, you will have to generate questions that can be asked by a user to retrieve information from a large documentary corpus.\n\nThe question should be relevant to the page, and should not be too specific or too general. The question should be about the subject of the page, and the answer needs to be found in the page.\n\nRemember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus.\n\nGenerate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question.\n\nGenerate at most THREE pairs of questions and answers per page as JSON with the following format, answer ONLY using JSON, NOTHING ELSE:\n\n{\n    \"questions\": [\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        },\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        },\n        {\n            \"question\": \"XXXXXX\",\n            \"answer\": [\"YYYYYY\"]\n        }\n    ]\n}\n\nwhere XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n\nNote: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n\nHere is the page:\n\"\"\"\nWe‚Äôll copy and paste all of the previous code to generate a response from the model for an example from our UFO dataset. We‚Äôll wrap this in a function once we‚Äôve got a better sense of what we‚Äôre looking for.\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": ds[0][\"image\"],\n            },\n            {\"type\": \"text\", \"text\": prompt},\n        ],\n    }\n]\n\n\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=200)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\noutput_text\n\n['{\\n    \"questions\": [\\n        {\\n            \"question\": \"What is the main subject of the page?\",\\n            \"answer\": [\"astronomy\", \"space exploration\", \"New York Times\"]\\n        },\\n        {\\n            \"question\": \"What is the significance of the Sun newspaper in the context of the page?\",\\n            \"answer\": [\"published a report attributed to an astronomer\", \"founded by Sir John Herschel\", \"published a report on the discovery of a race of singing aliens\"]\\n        },\\n        {\\n            \"question\": \"What is the main event mentioned in the page?\",\\n            \"answer\": [\"discovery of singing aliens on the Moon\"]\\n        }\\n    ]\\n}']\n\n\nYou‚Äôll see we get some responses like this \"What is the main event mentioned in the page?\" which is a bit too specific and tailored to the particular page. There are a few reasons this might be happening but the first thing we should play around with is changing the prompt.\n\nValidating the responses\nOne of the big challenges you can have in generating synthetic data at scale is ensuring that you get valid responses that you can use in downstream tasks for training without having to do a lot of manual verification. Let‚Äôs see if we can load the response as valid JSON\n\nimport json\n\n\njson.loads(output_text[0])\n\n{'questions': [{'question': 'What is the main subject of the page?',\n   'answer': ['astronomy', 'space exploration', 'New York Times']},\n  {'question': 'What is the significance of the Sun newspaper in the context of the page?',\n   'answer': ['published a report attributed to an astronomer',\n    'founded by Sir John Herschel',\n    'published a report on the discovery of a race of singing aliens']},\n  {'question': 'What is the main event mentioned in the page?',\n   'answer': ['discovery of singing aliens on the Moon']}]}\n\n\n\nprint(output_text[0])\n\n{\n    \"questions\": [\n        {\n            \"question\": \"What is the main subject of the page?\",\n            \"answer\": [\"astronomy\", \"space exploration\", \"New York Times\"]\n        },\n        {\n            \"question\": \"What is the significance of the Sun newspaper in the context of the page?\",\n            \"answer\": [\"published a report attributed to an astronomer\", \"founded by Sir John Herschel\", \"published a report on the discovery of a race of singing aliens\"]\n        },\n        {\n            \"question\": \"What is the main event mentioned in the page?\",\n            \"answer\": [\"discovery of singing aliens on the Moon\"]\n        }\n    ]\n}\n\n\nHaving a valid JSON is a good start but in many synthetic data generation tasks, people are increasingly using Pydantic to ensure outputs are valid in other ways. Let‚Äôs take a look at a rewritten prompt I created for generating queries. In this prompt we ask the VLLM model to generate 3 different types of retrieval queries:\n\nprompt = \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n\nPlease generate 3 different types of retrieval queries:\n\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Frame the queries as if someone is searching for this document, not asking questions about its content.\n- Make the queries diverse and representative of different search strategies.\n\nFor each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"broad_topical_query\": \"Your query here\",\n  \"broad_topical_explanation\": \"Brief explanation\",\n  \"specific_detail_query\": \"Your query here\",\n  \"specific_detail_explanation\": \"Brief explanation\",\n  \"visual_element_query\": \"Your query here\",\n  \"visual_element_explanation\": \"Brief explanation\"\n}\n\nIf there are no relevant visual elements, replace the third query with another specific detail query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\"\n\nWe ask the model for JSON output, we can represent this using a simple Pydantic model.\n\nfrom pydantic import BaseModel\nfrom typing import Tuple\n\n\nclass GeneralRetrievalQuery(BaseModel):\n    broad_topical_query: str\n    broad_topical_explanation: str\n    specific_detail_query: str\n    specific_detail_explanation: str\n    visual_element_query: str\n    visual_element_explanation: str\n\nWe could add additional constraints to our Pydantic model for example we could set a minimum and maximum length for the queries and answers. We‚Äôll get back to this at the end of the post but for now we can make a start with this simpler approach.\nWe‚Äôll now wrap this in a function to generate a response from the model using our Pydantic model."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#an-update-retrieval-focused-prompt",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#an-update-retrieval-focused-prompt",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "An update retrieval focused prompt",
    "text": "An update retrieval focused prompt\n\ndef get_retrieval_prompt(prompt_name: str) -&gt; Tuple[str, GeneralRetrievalQuery]:\n    if prompt_name != \"general\":\n        raise ValueError(\"Only 'general' prompt is available in this version\")\n\n    prompt = \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n\nPlease generate 3 different types of retrieval queries:\n\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Frame the queries as if someone is searching for this document, not asking questions about its content.\n- Make the queries diverse and representative of different search strategies.\n\nFor each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"broad_topical_query\": \"Your query here\",\n  \"broad_topical_explanation\": \"Brief explanation\",\n  \"specific_detail_query\": \"Your query here\",\n  \"specific_detail_explanation\": \"Brief explanation\",\n  \"visual_element_query\": \"Your query here\",\n  \"visual_element_explanation\": \"Brief explanation\"\n}\n\nIf there are no relevant visual elements, replace the third query with another specific detail query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\"\n\n    return prompt, GeneralRetrievalQuery\n\n\n# Example usage:\nprompt_name = \"general\"\nprompt, pydantic_model = get_retrieval_prompt(prompt_name)\nprint(f\"Prompt for '{prompt_name}':\")\nprint(prompt)\nprint(f\"\\nPydantic model for this prompt: {pydantic_model}\")\n\nPrompt for 'general':\nYou are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n\nPlease generate 3 different types of retrieval queries:\n\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Frame the queries as if someone is searching for this document, not asking questions about its content.\n- Make the queries diverse and representative of different search strategies.\n\nFor each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"broad_topical_query\": \"Your query here\",\n  \"broad_topical_explanation\": \"Brief explanation\",\n  \"specific_detail_query\": \"Your query here\",\n  \"specific_detail_explanation\": \"Brief explanation\",\n  \"visual_element_query\": \"Your query here\",\n  \"visual_element_explanation\": \"Brief explanation\"\n}\n\nIf there are no relevant visual elements, replace the third query with another specific detail query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\n\nPydantic model for this prompt: &lt;class '__main__.GeneralRetrievalQuery'&gt;\n\n\nWe‚Äôll now also wrap this in a function to generate a response from the model using our Pydantic model. We could probably do a bit more refactoring here but this will do for now.\n\ndef generate_response(prompt, image):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": image,\n                },\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    image_inputs, video_inputs = process_vision_info(messages)\n\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(\"cuda\")\n\n    generated_ids = model.generate(**inputs, max_new_tokens=200)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :]\n        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n\n    output_text = processor.batch_decode(\n        generated_ids_trimmed,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False,\n    )\n\n    return output_text\n\nLet‚Äôs now generate a response from the model for an example image from our UFO dataset.\n\ngenerate_response(prompt, ds[2][\"image\"])\n\n['{\\n  \"broad_topical_query\": \"Document discussing the possibility of a cameraman going on the record in the future\",\\n  \"broad_topical_explanation\": \"This query focuses on the main topic of the document, which is the discussion about the cameraman\\'s potential to go on the record.\",\\n  \"specific_detail_query\": \"Document mentioning Ray Santilli and his attempts to persuade the cameraman\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail in the document, which is Ray Santilli\\'s efforts to contact the cameraman.\",\\n  \"visual_element_query\": \"Document containing images of a damaged leg and an alien\\'s foot\",\\n  \"visual_element_explanation\": \"This query refers to the visual elements present in the document, which are images of a damaged leg and an alien\\'s foot.\"\\n}']\n\n\nWe can see the model is doing a reasonable job of generating queries.\n['{\\n  \"broad_topical_query\": \"Document discussing the possibility of a cameraman going on the record in the future\",\\n  \"broad_topical_explanation\": \"This query focuses on the main topic of the document, which is the discussion about the cameraman\\'s potential to go on the record.\",\\n  \"specific_detail_query\": \"Document mentioning Ray Santilli and his attempts to persuade the cameraman\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail in the document, which is Ray Santilli\\'s efforts to contact the cameraman.\",\\n  \"visual_element_query\": \"Document containing images of a damaged leg and an alien\\'s foot\",\\n  \"visual_element_explanation\": \"This query refers to the visual elements present in the document, which are images of a damaged leg and an alien\\'s foot.\"\\n}']\nOne thing I have found in my experiments with generating synthetic data is that adding a request for an \"explanation\" from the model sometimes seems to help improve the quality of the generated data. I assume this is already noted somewhere in the literature (if not I'll call this `explain then generate`!). This seems to particularly helpful when generating more complex queries. Having the explanation can also give you a sense of how the model \"understands\" the task. This obviously comes with the donwside that it takes longer to generate the data and more tokens are required but it often seems worth trying."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#generating-the-full-dataset",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#generating-the-full-dataset",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Generating the full dataset",
    "text": "Generating the full dataset\nAs we play with the prompts and refine the queries we will often iterate quite quickly on a few examples. Once we‚Äôre reasonably confident in the queries we can generate a larger dataset to see how well our prompts work across a larger set of examples.\n\nds\n\nDataset({\n    features: ['image'],\n    num_rows: 212\n})\n\n\n\nsample = ds.take(100)\n\nTo generate our full dataset we just wrap our previous code in a loop and run it for all the examples in our dataset. We add a very broad exception handler to catch any errors and continue with the next example. This is obviously not production code but it‚Äôs good enough to get started with. If we scale to a much bigger dataset we might want to add some more robust error handling.\n\nfrom tqdm.auto import tqdm\n\nresponses = []\nfor row in tqdm(sample):\n    try:\n        resp = generate_response(prompt, row[\"image\"])\n        responses.append(resp)\n    except Exception as e:\n        responses.append(None)\n\n\n\n\n\nresponses[0]\n\n['{\\n  \"broad_topical_query\": \"Document about the Sun and its influence on life\",\\n  \"broad_topical_explanation\": \"This query focuses on the main subject of the document, which is the Sun and its impact on life.\",\\n  \"specific_detail_query\": \"New York Times article on the Sun\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail mentioned in the document, which is the New York Times article on the Sun.\",\\n  \"visual_element_query\": \"Document with a black and white image\",\\n  \"visual_element_explanation\": \"This query refers to the visual element present in the document, which is a black and white image.\"\\n}']\n\n\nWe can see how many errors we have in our dataset.\n\nlen([r for r in responses if r is None])\n\n0\n\n\nNo bad generations!\nWe can also look at the first response to see what it looks like.\n\nresponses[0][0]\n\n'{\\n  \"broad_topical_query\": \"Document about the Sun and its influence on life\",\\n  \"broad_topical_explanation\": \"This query focuses on the main subject of the document, which is the Sun and its impact on life.\",\\n  \"specific_detail_query\": \"New York Times article on the Sun\",\\n  \"specific_detail_explanation\": \"This query targets a specific detail mentioned in the document, which is the New York Times article on the Sun.\",\\n  \"visual_element_query\": \"Document with a black and white image\",\\n  \"visual_element_explanation\": \"This query refers to the visual element present in the document, which is a black and white image.\"\\n}'\n\n\nLet‚Äôs see if we can parse this into a JSON object.\n\njson.loads(responses[0][0])\n\n{'broad_topical_query': 'Document about the Sun and its influence on life',\n 'broad_topical_explanation': 'This query focuses on the main subject of the document, which is the Sun and its impact on life.',\n 'specific_detail_query': 'New York Times article on the Sun',\n 'specific_detail_explanation': 'This query targets a specific detail mentioned in the document, which is the New York Times article on the Sun.',\n 'visual_element_query': 'Document with a black and white image',\n 'visual_element_explanation': 'This query refers to the visual element present in the document, which is a black and white image.'}\n\n\nfirst example seems to work, now let‚Äôs add this to our dataset and see how many we can parse.\n\nsample = sample.add_column(\"raw_queries\", responses)\n\n\nsample\n\nDataset({\n    features: ['image', 'raw_queries'],\n    num_rows: 100\n})\n\n\nTo deal with bad generations we‚Äôll create just fill out these column with None values. We can grab all the required keys from the valid first response.\n\nkeys = list(json.loads(responses[0][0]).keys())\nkeys\n\n['broad_topical_query',\n 'broad_topical_explanation',\n 'specific_detail_query',\n 'specific_detail_explanation',\n 'visual_element_query',\n 'visual_element_explanation']\n\n\nand do something like this to fill out this row\n\n{k: None for k in keys}\n\n{'broad_topical_query': None,\n 'broad_topical_explanation': None,\n 'specific_detail_query': None,\n 'specific_detail_explanation': None,\n 'visual_element_query': None,\n 'visual_element_explanation': None}\n\n\nWe create a function to extract the data from the raw queries and parse them into a JSON object.\n\ndef extract_data(row):\n    try:\n        data = json.loads(row[\"raw_queries\"][0])\n        data[\"parsed_into_json\"] = True\n        return data\n    except Exception:\n        data = {k: None for k in keys}\n        data[\"parsed_into_json\"] = False\n        return data\n\nWe can now use the map method to apply this function to our dataset.\n\nparsed_ds = sample.map(extract_data)\n\nWe can then see how many we‚Äôve successfully parsed.\n\nfrom collections import Counter\n\nCounter(parsed_ds[\"parsed_into_json\"])\n\nCounter({True: 95, False: 5})\n\n\nSo in this case 5% of the responses were not parseable. This isn‚Äôt too bad and we might be able to live with this."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#pushing-to-the-hub",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#pushing-to-the-hub",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Pushing to the Hub",
    "text": "Pushing to the Hub\nWe can now push this dataset to the Hugging Face Hub. This will also allow us to view the dataset in the Dataset Viewer. This can often be a very nice way of quickly checking through examples in a dataset to see how the quality looks.\nIf you are not authenticated you can use the login function to authenticate with the Hub.\n\nfrom huggingface_hub import login\n\n\nlogin()\n\n\n\n\n\nparsed_ds.push_to_hub(\"davanstrien/ufo-ColPali\")\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/datasets/davanstrien/ufo-ColPali/commit/c50c8e42cd01d2dde17ef64e66ea1b826c0ca4ec', commit_message='Upload dataset', commit_description='', oid='c50c8e42cd01d2dde17ef64e66ea1b826c0ca4ec', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/davanstrien/ufo-ColPali', endpoint='https://huggingface.co', repo_type='dataset', repo_id='davanstrien/ufo-ColPali'), pr_revision=None, pr_num=None)\n\n\nHere is what the dataset looks like in the Hugging Face Hub. You‚Äôll see there are actually more than a 100 examples since I did a larger generation of data since doing the first batch of a 100. You can be your own judge but my sense it that the queries are looking pretty good already."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#conclusion-improvements-and-next-steps",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#conclusion-improvements-and-next-steps",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Conclusion, improvements and next steps",
    "text": "Conclusion, improvements and next steps\nThere are a few improvements that we could make to this process.\n\nStructured Generation\nOne of the first things I think would be worth exploring further is using structured generation to improve the quality of the generated queries. This would allow us to properly use the Pydantic models to constrain the outputs. The Outlines library has functionality for doing this with for VLMs. Once I am more satisified with the quality of the queries I‚Äôll come back to this.\n\n\nMore diverse queries\nI focused on generating a single type of query for each example in the UFO dataset. I think for a dataset of this size it would be worth taking a more diverse set of generations. Below you can see an apendix with a few options for these kinds of queries.\n\n\nNext Steps\nI am keen to test this approach with a few more domains and also work on the actual fine-tuning of ColPali models."
  },
  {
    "objectID": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#appendix-more-diverse-queries",
    "href": "posts/post-with-code/colpali/2024-09-23-generate_colpali_dataset.html#appendix-more-diverse-queries",
    "title": "Generating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset",
    "section": "Appendix more diverse queries",
    "text": "Appendix more diverse queries\nHere are a few more examples of prompts I am experimenting with.\n\nfrom pydantic import BaseModel\nfrom typing import List, Optional, Tuple, Union\n\n# Pydantic models for each prompt type\n\n\nclass GeneralRetrievalQuery(BaseModel):\n    broad_topical_query: str\n    broad_topical_explanation: str\n    specific_detail_query: str\n    specific_detail_explanation: str\n    visual_element_query: str\n    visual_element_explanation: str\n\n\nclass MultiDocumentComparisonQuery(BaseModel):\n    comparison_query: str\n    comparison_explanation: str\n    corroboration_contradiction_query: str\n    corroboration_contradiction_explanation: str\n\n\nclass DomainSpecificQuery(BaseModel):\n    identified_domain: str\n    domain_specific_query: str\n    domain_specific_explanation: str\n    data_findings_query: str\n    data_findings_explanation: str\n    applications_implications_query: str\n    applications_implications_explanation: str\n\n\nclass VisualElementFocusQuery(BaseModel):\n    similar_visual_element_query: str\n    similar_visual_element_explanation: str\n    text_visual_combination_query: str\n    text_visual_combination_explanation: str\n    visual_content_understanding_query: str\n    visual_content_understanding_explanation: str\n\n\nclass TemporalMetadataQuery(BaseModel):\n    temporal_query: str\n    temporal_explanation: str\n    topic_metadata_combination_query: str\n    topic_metadata_combination_explanation: str\n    update_related_document_query: str\n    update_related_document_explanation: str\n\n\nclass DifficultyAmbiguityQuery(BaseModel):\n    simple_query: str\n    simple_explanation: str\n    complex_query: str\n    complex_explanation: str\n    ambiguous_query: str\n    ambiguous_explanation: str\n\n\nclass MultilingualMultimodalQuery(BaseModel):\n    multilingual_query: str\n    multilingual_explanation: str\n    multimodal_combination_query: str\n    multimodal_combination_explanation: str\n    text_visual_understanding_query: str\n    text_visual_understanding_explanation: str\n\n\ndef get_retrieval_prompt(\n    prompt_name: str,\n) -&gt; Tuple[\n    str,\n    Union[\n        GeneralRetrievalQuery,\n        MultiDocumentComparisonQuery,\n        DomainSpecificQuery,\n        VisualElementFocusQuery,\n        TemporalMetadataQuery,\n        DifficultyAmbiguityQuery,\n        MultilingualMultimodalQuery,\n    ],\n]:\n    prompts = {\n        \"general\": (\n            \"\"\"You are an AI assistant specialized in document retrieval tasks. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this document in a large corpus.\n\nPlease generate 3 different types of retrieval queries:\n\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should focus on a particular fact, figure, or point made in the document.\n3. A visual element query: This should reference a chart, graph, image, or other visual component in the document, if present.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Frame the queries as if someone is searching for this document, not asking questions about its content.\n- Make the queries diverse and representative of different search strategies.\n\nFor each query, also provide a brief explanation of why this query would be effective in retrieving this document.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"broad_topical_query\": \"Your query here\",\n  \"broad_topical_explanation\": \"Brief explanation\",\n  \"specific_detail_query\": \"Your query here\",\n  \"specific_detail_explanation\": \"Brief explanation\",\n  \"visual_element_query\": \"Your query here\",\n  \"visual_element_explanation\": \"Brief explanation\"\n}\n\nIf there are no relevant visual elements, replace the third query with another specific detail query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            GeneralRetrievalQuery,\n        ),\n        \"comparison\": (\n            \"\"\"Imagine this document page is part of a larger corpus. Your task is to generate retrieval queries that would require comparing this document with others in the corpus.\n\nPlease generate 2 retrieval queries:\n\n1. A query comparing this document's topic with a related subject\n2. A query seeking documents that contradict or support the main point of this page\n\nFor each query, provide a brief explanation of how it encourages document comparison and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"comparison_query\": \"Your query here\",\n  \"comparison_explanation\": \"Brief explanation\",\n  \"corroboration_contradiction_query\": \"Your query here\",\n  \"corroboration_contradiction_explanation\": \"Brief explanation\"\n}\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            MultiDocumentComparisonQuery,\n        ),\n        \"domain\": (\n            \"\"\"Your task is to create retrieval queries that a professional in the document's domain might use to find this document in a large corpus.\n\nFirst, identify the domain of the document (e.g., scientific, financial, legal, medical, technical).\n\nThen, generate 3 retrieval queries:\n\n1. A query using domain-specific terminology\n2. A query seeking specific data or findings presented in the document\n3. A query related to the document's potential applications or implications\n\nFor each query, provide a brief explanation of its relevance to the domain and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"identified_domain\": \"Domain name\",\n  \"domain_specific_query\": \"Your query here\",\n  \"domain_specific_explanation\": \"Brief explanation\",\n  \"data_findings_query\": \"Your query here\",\n  \"data_findings_explanation\": \"Brief explanation\",\n  \"applications_implications_query\": \"Your query here\",\n  \"applications_implications_explanation\": \"Brief explanation\"\n}\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            DomainSpecificQuery,\n        ),\n        \"visual\": (\n            \"\"\"Your task is to generate retrieval queries focusing on the visual elements in this document page (charts, tables, images, diagrams).\n\nPlease generate 3 retrieval queries:\n\n1. A query specifically asking for documents with similar visual elements\n2. A query combining textual and visual information\n3. A query that would require understanding the content of the visual element to retrieve this document\n\nFor each query, provide a brief explanation of how it incorporates visual elements and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"similar_visual_element_query\": \"Your query here\",\n  \"similar_visual_element_explanation\": \"Brief explanation\",\n  \"text_visual_combination_query\": \"Your query here\",\n  \"text_visual_combination_explanation\": \"Brief explanation\",\n  \"visual_content_understanding_query\": \"Your query here\",\n  \"visual_content_understanding_explanation\": \"Brief explanation\"\n}\n\nIf the document lacks significant visual elements, explain this and generate alternative queries focusing on the document's structure or layout.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            VisualElementFocusQuery,\n        ),\n        \"temporal\": (\n            \"\"\"Assuming this document is part of a large, diverse corpus, your task is to generate retrieval queries that incorporate metadata or temporal aspects.\n\nPlease generate 3 retrieval queries:\n\n1. A query specifying a likely time frame for this document\n2. A query combining topical information with a metadata element (e.g., author, publication type)\n3. A query seeking updated or related documents on the same topic\n\nFor each query, provide a brief explanation of how it uses temporal or metadata information and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"temporal_query\": \"Your query here\",\n  \"temporal_explanation\": \"Brief explanation\",\n  \"topic_metadata_combination_query\": \"Your query here\",\n  \"topic_metadata_combination_explanation\": \"Brief explanation\",\n  \"update_related_document_query\": \"Your query here\",\n  \"update_related_document_explanation\": \"Brief explanation\"\n}\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            TemporalMetadataQuery,\n        ),\n        \"difficulty\": (\n            \"\"\"Your task is to create retrieval queries for this document at different levels of complexity and ambiguity.\n\nPlease generate 3 retrieval queries:\n\n1. A simple, straightforward query\n2. A complex query requiring understanding of multiple aspects of the document\n3. An ambiguous query that could retrieve this document among others\n\nFor each query, provide a brief explanation of its complexity level or ambiguity and why it would be effective or challenging for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"simple_query\": \"Your query here\",\n  \"simple_explanation\": \"Brief explanation\",\n  \"complex_query\": \"Your query here\",\n  \"complex_explanation\": \"Brief explanation\",\n  \"ambiguous_query\": \"Your query here\",\n  \"ambiguous_explanation\": \"Brief explanation\"\n}\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            DifficultyAmbiguityQuery,\n        ),\n        \"multilingual\": (\n            \"\"\"Your task is to generate retrieval queries considering potential multilingual and multi-modal aspects of the document.\n\nPlease generate 3 retrieval queries:\n\n1. A query in a different language (if applicable) that would retrieve this document\n2. A query combining textual and non-textual elements\n3. A query that requires understanding both the text and visual elements to retrieve this document accurately\n\nFor each query, provide a brief explanation of its multilingual or multi-modal nature and why it would be effective for retrieval.\n\nFormat your response as a JSON object with the following structure:\n\n{\n  \"multilingual_query\": \"Your query here\",\n  \"multilingual_explanation\": \"Brief explanation\",\n  \"multimodal_combination_query\": \"Your query here\",\n  \"multimodal_combination_explanation\": \"Brief explanation\",\n  \"text_visual_understanding_query\": \"Your query here\",\n  \"text_visual_understanding_explanation\": \"Brief explanation\"\n}\n\nIf the document is not suitable for multilingual queries, explain why and provide an alternative query.\n\nHere is the document image to analyze:\n&lt;image&gt;\n\nGenerate the queries based on this image and provide the response in the specified JSON format.\"\"\",\n            MultilingualMultimodalQuery,\n        ),\n    }\n\n    if prompt_name not in prompts:\n        raise ValueError(\n            f\"Invalid prompt name. Choose from: {', '.join(prompts.keys())}\"\n        )\n\n    return prompts[prompt_name]\n\n\n# Example usage:\nprompt_name = \"general\"  # You can change this to any of the available prompt names\nprompt, pydantic_model = get_retrieval_prompt(prompt_name)\nprint(f\"Prompt for '{prompt_name}':\")\nprint(prompt)\nprint(f\"\\nPydantic model for this prompt: {pydantic_model}\")"
  },
  {
    "objectID": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html",
    "href": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html",
    "title": "Dynamically updating a Hugging Face hub organization README",
    "section": "",
    "text": "tl;dr we can use the huggingface_hub library to auto generate a model card readme for the BigLAM organization."
  },
  {
    "objectID": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#what-are-we-aiming-to-do",
    "href": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#what-are-we-aiming-to-do",
    "title": "Dynamically updating a Hugging Face hub organization README",
    "section": "What are we aiming to do?",
    "text": "What are we aiming to do?\nThe Hugging Face hub allows organizations to create a README card to describe their organization.\n\nWhilst you can manually create this there might be some content that would be nice to auto populate. For example, for the BigLAM organization, we‚Äôre mainly focused on collecting datasets. Since we have many tasks supported by these datasets we might want to create a list of datasets organized by task. Ideally we don‚Äôt want to have to manually update this. Let‚Äôs see how we can do this!\nFirst we‚Äôll install the huggingface_hub library which allows us to interact with the hub. We‚Äôll install Jinja2 for templating and toolz because toolz makes Python infinitely more delightful!\n\n%pip install huggingface_hub toolz Jinja2\n\n\nRequirement already satisfied: huggingface_hub in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (0.11.1)\n\nRequirement already satisfied: toolz in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (0.12.0)\n\nRequirement already satisfied: Jinja2 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (3.1.2)\n\nRequirement already satisfied: pyyaml&gt;=5.1 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n\nRequirement already satisfied: packaging&gt;=20.9 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (23.0)\n\nRequirement already satisfied: requests in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (2.28.2)\n\nRequirement already satisfied: filelock in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\n\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (4.4.0)\n\nRequirement already satisfied: tqdm in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from Jinja2) (2.1.1)\n\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (1.26.14)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (3.4)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (2022.12.7)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests-&gt;huggingface_hub) (3.0.1)\n\n\n\n[notice] A new release of pip available: 22.3.1 -&gt; 23.0.1\n\n[notice] To update, run: pip install --upgrade pip\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n\nimport toolz\nfrom huggingface_hub import list_datasets\n\nWe list all the datasets under this organization\n\nbig_lam_datasets = list(iter(list_datasets(author=\"biglam\", limit=None, full=True)))\n\nWe want to check which tasks our organization currently has. If we look at an example of one dataset:\n\nbig_lam_datasets[0]\n\nDatasetInfo: {\n    id: biglam/illustrated_ads\n    sha: 688e7d96e99cd5730a17a5c55b0964d27a486904\n    lastModified: 2023-01-18T20:38:15.000Z\n    tags: ['task_categories:image-classification', 'task_ids:multi-class-image-classification', 'annotations_creators:expert-generated', 'size_categories:n&lt;1K', 'license:cc0-1.0', 'lam', 'historic newspapers']\n    private: False\n    author: biglam\n    description: The Dataset contains images derived from the Newspaper Navigator (news-navigator.labs.loc.gov/), a dataset of images drawn from the Library of Congress Chronicling America collection.\n    citation: @dataset{van_strien_daniel_2021_5838410,\n  author       = {van Strien, Daniel},\n  title        = {{19th Century United States Newspaper Advert images \n                   with 'illustrated' or 'non illustrated' labels}},\n  month        = oct,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {0.0.1},\n  doi          = {10.5281/zenodo.5838410},\n  url          = {https://doi.org/10.5281/zenodo.5838410}}\n    cardData: {'annotations_creators': ['expert-generated'], 'language': [], 'language_creators': [], 'license': ['cc0-1.0'], 'multilinguality': [], 'pretty_name': \"19th Century United States Newspaper Advert images with 'illustrated' or 'non illustrated' labels\", 'size_categories': ['n&lt;1K'], 'source_datasets': [], 'tags': ['lam', 'historic newspapers'], 'task_categories': ['image-classification'], 'task_ids': ['multi-class-image-classification']}\n    siblings: []\n    _id: 62b9bb453b3301c319d5b53e\n    disabled: False\n    gated: False\n    gitalyUid: 4a051da032bb27da0bc286b288384bb3362f56546a387b130121cd279db336e1\n    likes: 3\n    downloads: 11\n}\n\n\nWe can see the cardData attribute contains an item containing the tasks supported by a dataset\n\nbig_lam_datasets[0].cardData['task_categories']\n\n['image-classification']\n\n\n\ndef get_task_categories(dataset):\n    try:\n        yield from dataset.cardData['task_categories']\n    except KeyError:\n        return None\n\nWe can use the toolz.frequencies function to get counts of these tasks in our org.\n\ntask_frequencies = toolz.frequencies(\n    toolz.concat(map(get_task_categories, big_lam_datasets))\n)\ntask_frequencies\n\n{'image-classification': 8,\n 'text-classification': 6,\n 'image-to-text': 2,\n 'text-generation': 7,\n 'object-detection': 5,\n 'fill-mask': 2,\n 'text-to-image': 1,\n 'image-to-image': 1,\n 'token-classification': 1}\n\n\nSince we want to organize by task type, let‚Äôs grab the names of all the tasks in the BigLAM organization.\n\ntasks = task_frequencies.keys()\ntasks\n\ndict_keys(['image-classification', 'text-classification', 'image-to-text', 'text-generation', 'object-detection', 'fill-mask', 'text-to-image', 'image-to-image', 'token-classification'])\n\n\nWe now want to group together datasets by the task(s) they support. We can use a default dict to create a dictionary where the keys are the task and the values are a list of datasets supporting that task. Note some datasets support multiple tasks so may appear under more than one task key.\n\nfrom collections import defaultdict\n\n\ndatasets_by_task = defaultdict(list)\n\n\nfor dataset in big_lam_datasets:\n    tasks = get_task_categories(dataset)\n    for task in tasks:\n        datasets_by_task[task].append(dataset)\n\nWe now have a dictionary which allows us to get all datasets supporting a task, for example fill-mask\n\ndatasets_by_task[\"fill-mask\"]\n\n[DatasetInfo: {\n    id: biglam/berlin_state_library_ocr\n    sha: a890935d5bd754ddc5b85f56b6f34f6d2bb4abba\n    lastModified: 2022-08-05T09:36:24.000Z\n    tags: ['task_categories:fill-mask', 'task_categories:text-generation', 'task_ids:masked-language-modeling', 'task_ids:language-modeling', 'annotations_creators:machine-generated', 'language_creators:expert-generated', 'multilinguality:multilingual', 'size_categories:1M&lt;n&lt;10M', 'language:de', 'language:nl', 'language:en', 'language:fr', 'language:es', 'license:cc-by-4.0', 'ocr', 'library']\n    private: False\n    author: biglam\n    description: None\n    citation: None\n    cardData: {'annotations_creators': ['machine-generated'], 'language': ['de', 'nl', 'en', 'fr', 'es'], 'language_creators': ['expert-generated'], 'license': ['cc-by-4.0'], 'multilinguality': ['multilingual'], 'pretty_name': 'Berlin State Library OCR', 'size_categories': ['1M&lt;n&lt;10M'], 'source_datasets': [], 'tags': ['ocr', 'library'], 'task_categories': ['fill-mask', 'text-generation'], 'task_ids': ['masked-language-modeling', 'language-modeling']}\n    siblings: []\n    _id: 62e0431281d9ca6484efac31\n    disabled: False\n    gated: False\n    gitalyUid: 3818ba9c8b624d79f1fcfb0c79bd197fb5b3a3f9de2452aed5028e8b6435f56a\n    likes: 3\n    downloads: 5\n },\n DatasetInfo: {\n    id: biglam/bnl_newspapers1841-1879\n    sha: 588db6c242ecae417b92830d5646121c15726fea\n    lastModified: 2022-11-15T09:25:43.000Z\n    tags: ['task_categories:text-generation', 'task_categories:fill-mask', 'task_ids:language-modeling', 'task_ids:masked-language-modeling', 'annotations_creators:no-annotation', 'language_creators:expert-generated', 'multilinguality:multilingual', 'size_categories:100K&lt;n&lt;1M', 'source_datasets:original', 'language:de', 'language:fr', 'language:lb', 'language:nl', 'language:la', 'language:en', 'license:cc0-1.0', 'newspapers', '1800-1900']\n    private: False\n    author: biglam\n    description: None\n    citation: None\n    cardData: {'annotations_creators': ['no-annotation'], 'language': ['de', 'fr', 'lb', 'nl', 'la', 'en'], 'language_creators': ['expert-generated'], 'license': ['cc0-1.0'], 'multilinguality': ['multilingual'], 'pretty_name': 'BnL Newspapers 1841-1879', 'size_categories': ['100K&lt;n&lt;1M'], 'source_datasets': ['original'], 'tags': ['newspapers', '1800-1900'], 'task_categories': ['text-generation', 'fill-mask'], 'task_ids': ['language-modeling', 'masked-language-modeling']}\n    siblings: []\n    _id: 6372286ce8891da06b2a5d2f\n    disabled: False\n    gated: False\n    gitalyUid: 039f217af964cfa1317f03d58c367ba6f0e415721b107a298cd4e75cbad50e8b\n    likes: 2\n    downloads: 3\n }]"
  },
  {
    "objectID": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#how-can-we-create-a-readme-that-dynamically-updates",
    "href": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#how-can-we-create-a-readme-that-dynamically-updates",
    "title": "Dynamically updating a Hugging Face hub organization README",
    "section": "How can we create a README that dynamically updates",
    "text": "How can we create a README that dynamically updates\nWe now have our datasets organized by task. However, at the moment, this is in the form of a Python dictionary. It would be much nicer to render it a more pleasing format. This is where a templating engine can help. In this case we‚Äôll use Jinja.\nA templating engine allows us to create a template which can dynamically be updated based on values we pass in. We won‚Äôt go in depth to templating engines/Jinja in this blog post because I‚Äôm not an expert in templating engines. This Real Python article is a nice introduction to Jinja.\n\nfrom jinja2 import Environment, FileSystemLoader\n\nWe can start by taking a look at our template. Since a lot of the template I created doesn‚Äôt update, we‚Äôll use tail to look at the bottom of the template which is dynamically updating.\n\n!tail -n 12 templates/readme.jinja\n\nAn overview of datasets currently made available via BigLam organised by task type.\n\n{% for task_type, datasets in task_dictionary.items() %}\n\n&lt;details&gt;\n  &lt;summary&gt;{{ task_type }}&lt;/summary&gt;\n    {% for dataset in datasets %}\n  - [{{dataset.cardData['pretty_name']}}](https://huggingface.co/datasets/biglam/{{ dataset.id }})\n  {%- endfor %}\n\n&lt;/details&gt;\n{% endfor %}\n\n\nEven if you aren‚Äôt familiar with templating engines, you can probably see roughly what this does. We look through all the keys and values in our dictionary, create a section for that task based on the dictionary key. We next loop through the dictionary values (which in this case is a list) and create a link for that dataset. Since we‚Äôre looping through DatasetInfo objects in the list we can grab things like the pretty_name for the dataset and dynamically create a URL link.\nWe can load this template as follows\n\nenvironment = Environment(loader=FileSystemLoader(\"templates/\"))\ntemplate = environment.get_template(\"readme.jinja\")\n\nCreate a context dictionary which we use to pass through our dictionary\n\ncontext = {\n    \"task_dictionary\": datasets_by_task,\n}\n\nWe can now render this and see how it looks\n\nprint(template.render(context))\n\n---\ntitle: README\nemoji: üìö\ncolorFrom: pink\ncolorTo: gray\nsdk: static\npinned: false\n---\n\nBigScience üå∏ is an open scientific collaboration of nearly 600 researchers from 50 countries and 250 institutions who collaborate on various projects within the natural language processing (NLP) space to broaden the accessibility of language datasets while working on challenging scientific questions around training language models.\n\n\nBigLAM started as a [datasets hackathon](https://github.com/bigscience-workshop/lam) focused on making data from Libraries, Archives, and Museums (LAMS) with potential machine-learning applications accessible via the Hugging Face Hub.\nWe are continuing to work on making more datasets available via the Hugging Face hub to help make these datasets more discoverable, open them up to new audiences, and help ensure that machine-learning datasets more closely reflect the richness of human culture.\n\n\n## Dataset Overview\n\nAn overview of datasets currently made available via BigLam organised by task type.\n\n\n\n&lt;details&gt;\n  &lt;summary&gt;image-classification&lt;/summary&gt;\n    \n  - [19th Century United States Newspaper Advert images with 'illustrated' or 'non illustrated' labels](https://huggingface.co/datasets/biglam/biglam/illustrated_ads)\n  - [Brill Iconclass AI Test Set ](https://huggingface.co/datasets/biglam/biglam/brill_iconclass)\n  - [National Library of Scotland Chapbook Illustrations](https://huggingface.co/datasets/biglam/biglam/nls_chapbook_illustrations)\n  - [Encyclopaedia Britannica Illustrated](https://huggingface.co/datasets/biglam/biglam/encyclopaedia_britannica_illustrated)\n  - [V4Design Europeana style dataset](https://huggingface.co/datasets/biglam/biglam/v4design_europeana_style_dataset)\n  - [Early Printed Books Font Detection Dataset](https://huggingface.co/datasets/biglam/biglam/early_printed_books_font_detection)\n  - [Dataset of Pages from Early Printed Books with Multiple Font Groups](https://huggingface.co/datasets/biglam/biglam/early_printed_books_with_multiple_font_groups)\n  - [DEArt: Dataset of European Art](https://huggingface.co/datasets/biglam/biglam/european_art)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;text-classification&lt;/summary&gt;\n    \n  - [Annotated dataset to assess the accuracy of the textual description of cultural heritage records](https://huggingface.co/datasets/biglam/biglam/cultural_heritage_metadata_accuracy)\n  - [Atypical Animacy](https://huggingface.co/datasets/biglam/biglam/atypical_animacy)\n  - [Old Bailey Proceedings](https://huggingface.co/datasets/biglam/biglam/old_bailey_proceedings)\n  - [Lampeter Corpus](https://huggingface.co/datasets/biglam/biglam/lampeter_corpus)\n  - [Hansard Speeches](https://huggingface.co/datasets/biglam/biglam/hansard_speech)\n  - [Contentious Contexts Corpus](https://huggingface.co/datasets/biglam/biglam/contentious_contexts)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;image-to-text&lt;/summary&gt;\n    \n  - [Brill Iconclass AI Test Set ](https://huggingface.co/datasets/biglam/biglam/brill_iconclass)\n  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;text-generation&lt;/summary&gt;\n    \n  - [Old Bailey Proceedings](https://huggingface.co/datasets/biglam/biglam/old_bailey_proceedings)\n  - [Hansard Speeches](https://huggingface.co/datasets/biglam/biglam/hansard_speech)\n  - [Berlin State Library OCR](https://huggingface.co/datasets/biglam/biglam/berlin_state_library_ocr)\n  - [Literary fictions of Gallica](https://huggingface.co/datasets/biglam/biglam/gallica_literary_fictions)\n  - [Europeana Newspapers ](https://huggingface.co/datasets/biglam/biglam/europeana_newspapers)\n  - [Gutenberg Poetry Corpus](https://huggingface.co/datasets/biglam/biglam/gutenberg-poetry-corpus)\n  - [BnL Newspapers 1841-1879](https://huggingface.co/datasets/biglam/biglam/bnl_newspapers1841-1879)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;object-detection&lt;/summary&gt;\n    \n  - [National Library of Scotland Chapbook Illustrations](https://huggingface.co/datasets/biglam/biglam/nls_chapbook_illustrations)\n  - [YALTAi Tabular Dataset](https://huggingface.co/datasets/biglam/biglam/yalta_ai_tabular_dataset)\n  - [YALTAi Tabular Dataset](https://huggingface.co/datasets/biglam/biglam/yalta_ai_segmonto_manuscript_dataset)\n  - [Beyond Words](https://huggingface.co/datasets/biglam/biglam/loc_beyond_words)\n  - [DEArt: Dataset of European Art](https://huggingface.co/datasets/biglam/biglam/european_art)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;fill-mask&lt;/summary&gt;\n    \n  - [Berlin State Library OCR](https://huggingface.co/datasets/biglam/biglam/berlin_state_library_ocr)\n  - [BnL Newspapers 1841-1879](https://huggingface.co/datasets/biglam/biglam/bnl_newspapers1841-1879)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;text-to-image&lt;/summary&gt;\n    \n  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;image-to-image&lt;/summary&gt;\n    \n  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n\n&lt;/details&gt;\n\n\n&lt;details&gt;\n  &lt;summary&gt;token-classification&lt;/summary&gt;\n    \n  - [Unsilencing Colonial Archives via Automated Entity Recognition](https://huggingface.co/datasets/biglam/biglam/unsilence_voc)\n\n&lt;/details&gt;\n\n\n\n\nwith open('/tmp/README.md','w') as f:\n    f.write(template.render(context))"
  },
  {
    "objectID": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#updating-the-readme-on-the-hugging-face-hub",
    "href": "posts/post-with-code/readme_auto_generate/2023-03-07-readme-template.html#updating-the-readme-on-the-hugging-face-hub",
    "title": "Dynamically updating a Hugging Face hub organization README",
    "section": "Updating the README on the Hugging Face Hub",
    "text": "Updating the README on the Hugging Face Hub\nThis looks pretty good! It would be nice to also update the org README without having to manually edit the file. The huggingface_hub library helps us out here once again. Since the organization README is actually a special type of Hugging Face Space, we can interact with it in the same way we could for models or datasets.\n\nfrom huggingface_hub import HfApi\nfrom huggingface_hub import notebook_login\n\nWe‚Äôll create a HFApi instance.\n\napi = HfApi()\n\nSince we‚Äôre planning to write to a repo we‚Äôll need to login to the hub.\n\nnotebook_login()\n\n\n\n\nWe can now upload the rendered README file we created above to our biglam/README space.\n\napi.upload_file(\n    path_or_fileobj=\"/tmp/readme.md\",\n    path_in_repo=\"README.md\",\n    repo_id=\"biglam/README\",\n    repo_type=\"space\",\n)\n\n'https://huggingface.co/spaces/biglam/README/blob/main/README.md'\n\n\nIf we look at our updated README, we‚Äôll see we now have some nice collapsible sections for each task type containing the datasets for that task\n\n\n\nAfter README\n\n\nNext steps, whilst this was already quite useful, at the moment we still have to run this code when we want to regenerate our README. Webhooks make it possible to make this fully automated by creating a webhook that monitors any changes to repos under the BigLAM org. Would love to hear from anyone who tries this out!"
  },
  {
    "objectID": "posts/post-with-code/langfuse-tracing/Langfuse_+_HF_inference_endpoints.html",
    "href": "posts/post-with-code/langfuse-tracing/Langfuse_+_HF_inference_endpoints.html",
    "title": "Tracing Text Generation Inference calls",
    "section": "",
    "text": "%pip install openai langfuse --quiet\n\nNote: you may need to restart the kernel to use updated packages.\n\n\nLANGFUSE_SECRET_KEY=\"sk-lf-...\"\nLANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\nLANGFUSE_HOST=\"https://cloud.langfuse.com\" # üá™üá∫ EU region\n\nfrom google.colab import userdata\nimport os\n\n\nos.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get('LANGFUSE_SECRET_KEY')\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get('LANGFUSE_PUBLIC_KEY')\nHF_TOKEN = userdata.get('HF_TOKEN')\n\n\nfrom langfuse.decorators import observe\nfrom langfuse.openai import openai, OpenAI # OpenAI integration\n\n\nclient = OpenAI(\n        base_url=\"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1\",\n        api_key=HF_TOKEN,\n    )\n\n\nchat_completion = client.chat.completions.create(\n    model=\"tgi\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is Hugging Face?\"}\n    ],\n    stream=False\n)\n\n\nchat_completion\n\nChatCompletion(id='', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\" Hugging Face is a technology company that specializes in natural language processing (NLP) and artificial intelligence (AI). The company is best known for its development of Transformers, an open-source library that provides a wide range of pre-trained models for various NLP tasks, such as text classification, question answering, and language translation.\\n\\nHugging Face's Transformers library has gained widespread popularity among developers and researchers due to its ease of use, flexibility, and\", role='assistant', function_call=None, tool_calls=None))], created=1712314124, model='text-generation-inference/Mixtral-8x7B-Instruct-v0.1-medusa', object='text_completion', system_fingerprint='1.4.3-sha-e6bb3ff', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115))\n\n\n\n\n\nLangfuse Trace"
  },
  {
    "objectID": "posts/post-with-code/datasets-groupby/2023-09-18-datasets-groupby.html",
    "href": "posts/post-with-code/datasets-groupby/2023-09-18-datasets-groupby.html",
    "title": "How to do groupby for Hugging Face datasets",
    "section": "",
    "text": "There is no native support for GroupBy in the Hugging Face datasets library but since there is a polars integration, we can use that to do groupby operations. In this notebook, I will show you how to do a groupby operation on a Hugging Face dataset using polars.\n%pip install datasets polars matplotlib --quiet\n\nNote: you may need to restart the kernel to use updated packages.\nfrom datasets import load_dataset\nimport polars as pl\n\n/Users/davanstrien/Documents/daniel/new_blog/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nWe can use the datasets library to load the dataset and then convert it to a polars DataFrame. After that, we can use the group_by method to group the data by a specific column and then apply an aggregation function to the grouped data.\nds = load_dataset(\"argilla/databricks-dolly-15k-curated-en\")\nds['train'][0]\n\n{'id': '0',\n 'category': 'closed_qa',\n 'original-instruction': 'When did Virgin Australia start operating?',\n 'original-context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.[3] It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.[4]\",\n 'original-response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n 'new-instruction': {'user_id': [None],\n  'value': ['When did Virgin Australia start operating?'],\n  'status': ['submitted']},\n 'new-context': {'user_id': [None],\n  'value': [\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"],\n  'status': ['submitted']},\n 'new-response': {'user_id': [None],\n  'value': ['Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.'],\n  'status': ['submitted']},\n 'external_id': None}\ndf = ds['train'].to_polars()"
  },
  {
    "objectID": "posts/post-with-code/datasets-groupby/2023-09-18-datasets-groupby.html#big-datasets",
    "href": "posts/post-with-code/datasets-groupby/2023-09-18-datasets-groupby.html#big-datasets",
    "title": "How to do groupby for Hugging Face datasets",
    "section": "Big datasets",
    "text": "Big datasets\nSince polars supports lazy evaluation, you can work with very large datasets. This is very useful when you have a dataset that is too large to fit in memory. You can perform operations on the dataset without loading the entire dataset into memory. For a Hub dataset we can use the scan_parquet method to load the dataset lazily.\n\ndf = pl.scan_parquet('hf://datasets/BAAI/Infinity-Instruct/3M/*.parquet')\n\nYou‚Äôll see this returns a lazy DataFrame that you can use to perform operations on the dataset.\n\ntype(df)\n\npolars.lazyframe.frame.LazyFrame\n\n\nIf we do a similar groupby operation on this dataset\n\nresult = df.group_by('langdetect').agg(\n    pl.col('conversations').list.len().mean().alias('mean_conversation_length')\n)\n\nyou‚Äôll see that we don‚Äôt get back an immediate result but a lazy DataFrame that we can use to perform operations on the dataset.\n\nresult\n\nNAIVE QUERY PLANrun LazyFrame.show_graph() to see the optimized version\n\n\n\n\n\npolars_query\n\n\n\np1\n\nAGG [col(\"conversations\").list.length().mean().alias(\"mean_conversation_length\")]\nBY\n[col(\"langdetect\")]\n\n\n\np2\n\nParquet SCAN [https://huggingface.co/datasets/BAAI%2FInfinity-Instruct/resolve/main/3M%2Ftrain-00000-of-00035.parquet, ... 34 other files]\nœÄ */5;\n\n\n\np1--p2\n\n\n\n\n\n\nWhere possible polars will also add some optimizations to the query to make it faster.\n\nresult.show_graph()\n\n\n\n\n\n\n\n\nWe can then use the collect method to get the result of the query.\n\nresult.collect()\n\n\nshape: (2, 2)\n\n\n\nlangdetect\nmean_conversation_length\n\n\nstr\nf64\n\n\n\n\n\"en\"\n3.068929\n\n\n\"zh-cn\"\n3.214509"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html",
    "href": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html",
    "title": "Using ColPali with Qdrant to index and search a UFO document dataset",
    "section": "",
    "text": "updates"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html#multivector-databases-support",
    "href": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html#multivector-databases-support",
    "title": "Using ColPali with Qdrant to index and search a UFO document dataset",
    "section": "Multivector Databases support",
    "text": "Multivector Databases support\nThe last few years have seen an explosion of vector databases. These databases are optimized for storing and searching for high-dimensional data. Traditionally most of the vectors these databases have been storing are single-vector embeddings i.e a vector that looks like:\n[0.1, 0.2, 0.3, 0.4, 0.5]\nThese vectors can store rich representations of the data they are encoding. However, more recently, several new techniques have been proposed which don‚Äôt rely on a single vector but instead use a set of vectors i.e.¬†something like this:\n[[0.1, 0.2, 0.3, 0.4, 0.5], \n[0.1, 0.2, 0.3, 0.4, 0.5], \n[0.1, 0.2, 0.3, 0.4, 0.5]]\nIn the case of ColBERT, these multiple vectors represent the query and document tokens. These are then compared using a late interaction mechanism. This ‚Äúlate interaction‚Äù essentially means that rather than creating a pooled representation of the document and query, we compare each query token to each document token individually.\nThis similarity has mostly been calculated using MaxSim, which takes the element-wise maximum similarity between the query and document tokens. ColPali uses a similar late interaction mechanism to compare the query and document tokens. The tl;dr is that instead of comparing a single vector to another single vector, we compare a set of vectors to another set of vectors. This approach is borrowed from ColBERT. The diagram below shows how this works in the case of ColBERT.\n\n\n\nVisual representation of ColBERT (source: https://www.answer.ai/posts/colbert-pooling.html)\n\n\nI won‚Äôt go into the details too much here, since other posts cover this already. Instead, I will show how we can use ColPali using the Qdrant vector database.\n\nWhat is Qdrant?\nQdrant is a vector database. Many open-source and closed-source vector databases exist, and many ‚Äútraditional‚Äù databases have also added support for vectors.\nCurrently, only a small number of databases support multivectors. I am aware of the following:\n\nVespa\nQdrant\n\nFrom what I have understood, Weaviate has an example recipe for ColPali. However, they don‚Äôt currently natively support MaxSim so using this in practice might be a bit more challenging.\n\n\n\n\n\n\nTip\n\n\n\nVespa, and in particular, Jo Kristian Bergum, have done a lot of work on ColPali and are well worth following if you are interested in this area. It‚Äôs on my list to finally try Vespa!\n\n\nWe‚Äôll start by installing the requirements. For the example, we‚Äôll use a local in-memory Qdrant DB, so you don‚Äôt need to worry about any setup. You can easily swap this out for a remote DB or an on-disk DB if you want to use this approach in production.\n\n!pip install uv\n!uv pip install --system colpali_engine&gt;=0.3.1 datasets huggingface_hub[hf_transfer] qdrant-client transformers&gt;=4.45.0 stamina rich\n\n\nRequirement already satisfied: uv in /usr/local/lib/python3.10/dist-packages (0.4.18)\n\nAudited 7 packages in 69ms\n\n\n\n\n\nimport os\nimport torch\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom tqdm import tqdm\nfrom datasets import load_dataset\n\nWe can set the HF_HUB_ENABLE_HF_TRANSFER environment variable to 1 to enable faster downloads. This is optional but should help if you have a fast connection.\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nFor this example blog, we‚Äôll use the ufo-ColPali dataset that I created as part of my previous post. If you have your own PDFs you want to work with you can use the PDF to Page Images Dataset Space to convert your PDFs into a Hugging Face dataset of page images.\n\ndataset = load_dataset(\"davanstrien/ufo-ColPali\", split=\"train\")\n\nAs a reminder of what the dataset looks like\n\ndataset\n\nDataset({\n    features: ['image', 'raw_queries', 'broad_topical_query', 'broad_topical_explanation', 'specific_detail_query', 'specific_detail_explanation', 'visual_element_query', 'visual_element_explanation', 'parsed_into_json'],\n    num_rows: 2243\n})\n\n\nIn this particular example, we‚Äôll just work with the images, but for many ‚Äòreal world‚Äô document collections, you are likely to have at least some metadata associated with the documents that could also be indexed in the DB. These could include things like the title, author, date, etc. These additional metadata fields can also be used as part of the search query and for retrieval.\nLet‚Äôs look at an example image to see what sort of data we are working with.\n\ndataset[0][\"image\"]"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html#indexing-the-dataset",
    "href": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html#indexing-the-dataset",
    "title": "Using ColPali with Qdrant to index and search a UFO document dataset",
    "section": "Indexing the dataset",
    "text": "Indexing the dataset\nLet‚Äôs now walk through how to index the dataset using Qdrant.\n\nCreating a Qdrant client\nWe can use the Qdrant Python client to interact with a Qdrant database. One of the nice features of this client is its in-memory implementation, which is very useful for testing. It‚Äôs not recommended for production use, but it‚Äôs great for getting started and means we can jump right into the code without having to worry about setting up a database.\n\n# Only for colab, otherwise can load from a `.env` file or similar\n# from google.colab import userdata\n\nTo prove that switching between the in-memory and remote clients is easy, the code below is all I need to use a DB running on a Qdrant cloud free tier. Qdrant is open source, so you can run it on your infrastructure.\n\n# qdrant_client = QdrantClient(\n#     url=\"https://e25145aa-1e00-489a-948f-4633d3dd8a37.europe-west3-0.gcp.cloud.qdrant.io\",\n#     api_key=userdata.get('qdrantcloud'),\n# )\n\ncollections=[CollectionDescription(name='ufo2'), CollectionDescription(name='ufo')]\n\n\n\nqdrant_client = QdrantClient(\n    \":memory:\"\n)  # Use \":memory:\" for in-memory database or \"path/to/db\" for persistent storage\n\n\n\nSetting up the ColPali model\nWe‚Äôll use a ColPali model davanstrien/finetune_colpali_v1_2-ufo-4bit fine-tuned on the ufo-ColPali dataset. This model was trained using the excellent notebook in the ColPali cookbooks repo from Tony Wu.\nIf you want to use a standard ColPali model that is not optimized for UFO document retrieval, you may want to try the newly released vidore/colqwen2-v0.1 model instead.\n\n# from colpali_engine.models import ColQwen2, ColQwen2Processor\n\n# model = ColQwen2.from_pretrained(\n#     \"vidore/colqwen2-v0.1\",\n#     torch_dtype=torch.bfloat16,\n#     device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n# )\n# processor = ColQwen2Processor.from_pretrained(\"vidore/colqwen2-v0.1\")\n\n\nfrom colpali_engine.models import ColPali, ColPaliProcessor\n\n# Initialize ColPali model and processor\nmodel_name = (\n    \"davanstrien/finetune_colpali_v1_2-ufo-4bit\"  # Use the latest version available\n)\ncolpali_model = ColPali.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"cuda:0\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n)\ncolpali_processor = ColPaliProcessor.from_pretrained(\n    \"vidore/colpaligemma-3b-pt-448-base\"\n)\n\n\n\n\n\n\nWhat is a multivector, and how do we use it in Qdrant?\nTo use multivectors in Qdrant, we need to specify the dimensions of the vectors. From the Qdrant docs:\n\nThe length of the matrix is fixed, but the number of vectors in the matrix can be different for each point.\n\nSo, in order to use multivectors, we need to specify their length. We can check this by passing in an example image to the model and checking the shape of the output.\n\nsample_image = dataset[0][\"image\"]\nwith torch.no_grad():\n    sample_batch = colpali_processor.process_images([sample_image]).to(\n        colpali_model.device\n    )\n    sample_embedding = colpali_model(**sample_batch)\n\nSince it‚Äôs always fun to look at tensors, let‚Äôs see what they output embeddings from the ColPali model looks like\n\nsample_embedding\n\ntensor([[[ 0.0093, -0.0045,  0.1445,  ...,  0.0248, -0.0820, -0.1641],\n         [-0.0021,  0.1758,  0.1699,  ...,  0.0179, -0.0952, -0.1138],\n         [-0.0237,  0.0366,  0.0732,  ...,  0.0449, -0.0918, -0.1738],\n         ...,\n         [-0.0588, -0.0243,  0.1650,  ..., -0.0703,  0.0767,  0.0486],\n         [-0.1108,  0.0986,  0.1826,  ...,  0.0278, -0.0576, -0.0520],\n         [-0.0693,  0.1123,  0.2207,  ...,  0.0172, -0.0679, -0.0830]]],\n       device='cuda:0', dtype=torch.bfloat16)\n\n\nand check the shape\n\nsample_embedding.shape\n\ntorch.Size([1, 1030, 128])\n\n\nAs we can see, compared to a usual dense embedding, the output of the ColPali model is a multivector. We can see that the shape of the output is (1, 1030, 128). We can check the length of the vectors by looking at the last dimension of the tensor. In this case, the length of the vectors is 128.\n\nvector_size = sample_embedding.shape[2]\nvector_size\n\n128"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html#conclusion",
    "href": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html#conclusion",
    "title": "Using ColPali with Qdrant to index and search a UFO document dataset",
    "section": "Conclusion",
    "text": "Conclusion\nColPali remains a relatively novel approach to retrieval, and it‚Äôs likely to take some time for all vector DBs to implement proper support for multi-vectors. The community is likely to also discover new optimization approaches to both the ColPali model itself and its integration with vector DBs.\nI left some further resources below and am happy to get feedback on any improvements I can make to this blog post.\n\nFurther resources\n\nHugging Face Hub organization for the ColPali authors: https://huggingface.co/vidore\nBlog post on the rise of vision-driven document retrieval for RAG: https://blog.vespa.ai/the-rise-of-vision-driven-document-retrieval-for-rag/\nBlog post on scaling ColPali using the Vespa database: https://blog.vespa.ai/scaling-colpali-to-billions/\n\n\n\nBonus tips\nA few other things I think are worth exploring if you plan to use this in a production setting.\n\nPre-filtering\nIt‚Äôs likely in many real use cases that you either have, or can create, some metadata for your documents. These can also be indexed in Qdrant and may help improve the search results and performance of the a multivector approach. If for example, you can filter to remove all documents before/after a certain date, you can reduce the amount of vectors that Qdrant needs to search through.\n\n\n\nCombining vectors\nIt‚Äôs also possible to use other vectors for the same points in Qdrant. For example, you may want to use a dense vector to store a representation of the text extracted through an existing OCR pipeline (if you already have this to hand it‚Äôs worth trying to use it), you can then use these vectors do a first large search and then use the multivector embeddings to do a second stage search limited only to results from the first step. See the filtering docs for more info on how to do this. This could look something like:\n\nsearch for ‚Äútop secret‚Äù using the dense vectors and get the top 1000 results\nsearch for ‚Äútop secret‚Äù using the multivectors but only search in the top 1000 results from the first search\n\n\nBinary Quantization\nIt‚Äôs very likely to make sense to explore using Binary Quantization for storing your vectors since it can reduce the memory footprint and speed up the search process. You can set this up by doing something like this in the collection creation step\n\nquantization_config = models.BinaryQuantization(\n    binary=models.BinaryQuantizationConfig(\n        always_ram=True,\n    )\n)\n\n\n\n\nSome more search examples\n\nresults_ds = search_by_text_and_return_images(\"sightings data table\")\n\nfor row in results_ds:\n    # display image\n    display(row[\"image\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresults_ds = search_by_text_and_return_images(\"ufo experiences\")\nfor row in results_ds:\n    display(row[\"image\"])"
  },
  {
    "objectID": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html#glossary",
    "href": "posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html#glossary",
    "title": "Using ColPali with Qdrant to index and search a UFO document dataset",
    "section": "Glossary",
    "text": "Glossary\n\nColPali: A multimodal retrieval approach for document search.\nQdrant: An open-source vector database.\nVector Database: A database optimized for storing and searching high-dimensional vector data.\nMultivector: A set of vectors used to represent a single item, as opposed to a single vector.\nLate Interaction: A mechanism used in ColPali to compare query and document tokens individually at query time.\nMaxSim: A similarity calculation method used in ColPali and ColBERT.\nQuantization: A compression technique used to reduce the memory footprint of vectors.\nScalar Quantization: A specific type of quantization that reduces the number of bits used to represent vector components.\nOCR (Optical Character Recognition): Technology used to convert images of text into machine-readable text.\nVLM (Vision Language Model): A type of AI model that can process both image and text data.\nMultimodal: Relating to multiple modes of data, such as text and images.\nEmbedding: A numerical representation of data in a vector space.\nCosine Similarity: A measure of similarity between two vectors.\nIndexing: The process of organizing data in a database to optimize search and retrieval.\nBinary Quantization: A form of quantization that converts vectors to binary (0 or 1) values.\nRAG (Retrieval-Augmented Generation): A technique that combines retrieval of relevant information with text generation."
  },
  {
    "objectID": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html",
    "href": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html",
    "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
    "section": "",
    "text": "The Hugging Face Hub has become the de facto place to share machine learning models and datasets. As the number of models and datasets grow the challenge of finding the right model or dataset for your needs may become more challenging. There are various ways in which we can try and make it easier for people to find relevant models and datasets. One of these is by associating metadata with datasets and models. This blog post will (very briefly) begin to explore metadata on the Hugging Face Hub. Often you‚Äôll want to explore models and datasets via the Hub website but this isn‚Äôt the only way to explore the Hub. As part of the process of exploring metadata on the Hugging Face Hub we‚Äôll briefly look at how we can use the huggingface_hub library to programmatically interact with the Hub."
  },
  {
    "objectID": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#library-imports",
    "href": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#library-imports",
    "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
    "section": "Library imports",
    "text": "Library imports\nFor this post we‚Äôll need a few libraries, pandas, requests and matplotlib are likely old friends (or foes‚Ä¶). The huggingface_hub library might be new to you but will soon become a good friend too! The rich library is fantastically useful for quickly getting familiar with a library (i.e.¬†avoiding reading all the docs!) so we‚Äôll import that too.\n\nimport requests\nfrom huggingface_hub import hf_api\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport rich\n\n\n%matplotlib inline\nplt.style.use(\"ggplot\")\n\nWe‚Äôll instantiate an instance of the HfApi class.\n\napi = hf_api.HfApi()\n\nWe can use rich inspect to get a better sense of what a function or class instance is all about. Let‚Äôs see what methods the api has.\n\nrich.inspect(api, methods=True)\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;class 'huggingface_hub.hf_api.HfApi'&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ\n‚îÇ ‚îÇ &lt;huggingface_hub.hf_api.HfApi object at 0x136a2ce80&gt;                                                        ‚îÇ ‚îÇ\n‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                 endpoint = 'https://huggingface.co'                                                             ‚îÇ\n‚îÇ                    token = None                                                                                 ‚îÇ\n‚îÇ change_discussion_status = def change_discussion_status(repo_id: str, discussion_num: int, new_status:          ‚îÇ\n‚îÇ                            Literal['open', 'closed'], *, token: Optional[str] = None, comment: Optional[str] =  ‚îÇ\n‚îÇ                            None, repo_type: Optional[str] = None) -&gt;                                            ‚îÇ\n‚îÇ                            huggingface_hub.community.DiscussionStatusChange: Closes or re-opens a Discussion or ‚îÇ\n‚îÇ                            Pull Request.                                                                        ‚îÇ\n‚îÇ       comment_discussion = def comment_discussion(repo_id: str, discussion_num: int, comment: str, *, token:    ‚îÇ\n‚îÇ                            Optional[str] = None, repo_type: Optional[str] = None) -&gt;                            ‚îÇ\n‚îÇ                            huggingface_hub.community.DiscussionComment: Creates a new comment on the given      ‚îÇ\n‚îÇ                            Discussion.                                                                          ‚îÇ\n‚îÇ            create_branch = def create_branch(repo_id: str, *, branch: str, token: Optional[str] = None,         ‚îÇ\n‚îÇ                            repo_type: Optional[str] = None) -&gt; None: Create a new branch from `main` on a repo  ‚îÇ\n‚îÇ                            on the Hub.                                                                          ‚îÇ\n‚îÇ            create_commit = def create_commit(repo_id: str, operations:                                          ‚îÇ\n‚îÇ                            Iterable[Union[huggingface_hub._commit_api.CommitOperationAdd,                       ‚îÇ\n‚îÇ                            huggingface_hub._commit_api.CommitOperationDelete]], *, commit_message: str,         ‚îÇ\n‚îÇ                            commit_description: Optional[str] = None, token: Optional[str] = None, repo_type:    ‚îÇ\n‚îÇ                            Optional[str] = None, revision: Optional[str] = None, create_pr: Optional[bool] =    ‚îÇ\n‚îÇ                            None, num_threads: int = 5, parent_commit: Optional[str] = None) -&gt;                  ‚îÇ\n‚îÇ                            huggingface_hub.hf_api.CommitInfo: Creates a commit in the given repo, deleting &    ‚îÇ\n‚îÇ                            uploading files as needed.                                                           ‚îÇ\n‚îÇ        create_discussion = def create_discussion(repo_id: str, title: str, *, token: Optional[str] = None,      ‚îÇ\n‚îÇ                            description: Optional[str] = None, repo_type: Optional[str] = None, pull_request:    ‚îÇ\n‚îÇ                            bool = False) -&gt; huggingface_hub.community.DiscussionWithDetails: Creates a          ‚îÇ\n‚îÇ                            Discussion or Pull Request.                                                          ‚îÇ\n‚îÇ      create_pull_request = def create_pull_request(repo_id: str, title: str, *, token: Optional[str] = None,    ‚îÇ\n‚îÇ                            description: Optional[str] = None, repo_type: Optional[str] = None) -&gt;               ‚îÇ\n‚îÇ                            huggingface_hub.community.DiscussionWithDetails: Creates a Pull Request . Pull       ‚îÇ\n‚îÇ                            Requests created programmatically will be in `\"draft\"` status.                       ‚îÇ\n‚îÇ              create_repo = def create_repo(repo_id: str, *, token: Optional[str] = None, private: bool = False, ‚îÇ\n‚îÇ                            repo_type: Optional[str] = None, exist_ok: bool = False, space_sdk: Optional[str] =  ‚îÇ\n‚îÇ                            None) -&gt; str: Create an empty repo on the HuggingFace Hub.                           ‚îÇ\n‚îÇ               create_tag = def create_tag(repo_id: str, *, tag: str, tag_message: Optional[str] = None,         ‚îÇ\n‚îÇ                            revision: Optional[str] = None, token: Optional[str] = None, repo_type:              ‚îÇ\n‚îÇ                            Optional[str] = None) -&gt; None: Tag a given commit of a repo on the Hub.              ‚îÇ\n‚îÇ             dataset_info = def dataset_info(repo_id: str, *, revision: Optional[str] = None, timeout:           ‚îÇ\n‚îÇ                            Optional[float] = None, files_metadata: bool = False, token: Union[bool, str,        ‚îÇ\n‚îÇ                            NoneType] = None) -&gt; huggingface_hub.hf_api.DatasetInfo: Get info on one specific    ‚îÇ\n‚îÇ                            dataset on huggingface.co.                                                           ‚îÇ\n‚îÇ            delete_branch = def delete_branch(repo_id: str, *, branch: str, token: Optional[str] = None,         ‚îÇ\n‚îÇ                            repo_type: Optional[str] = None) -&gt; None: Delete a branch from a repo on the Hub.    ‚îÇ\n‚îÇ              delete_file = def delete_file(path_in_repo: str, repo_id: str, *, token: Optional[str] = None,     ‚îÇ\n‚îÇ                            repo_type: Optional[str] = None, revision: Optional[str] = None, commit_message:     ‚îÇ\n‚îÇ                            Optional[str] = None, commit_description: Optional[str] = None, create_pr:           ‚îÇ\n‚îÇ                            Optional[bool] = None, parent_commit: Optional[str] = None) -&gt;                       ‚îÇ\n‚îÇ                            huggingface_hub.hf_api.CommitInfo: Deletes a file in the given repo.                 ‚îÇ\n‚îÇ            delete_folder = def delete_folder(path_in_repo: str, repo_id: str, *, token: Optional[str] = None,   ‚îÇ\n‚îÇ                            repo_type: Optional[str] = None, revision: Optional[str] = None, commit_message:     ‚îÇ\n‚îÇ                            Optional[str] = None, commit_description: Optional[str] = None, create_pr:           ‚îÇ\n‚îÇ                            Optional[bool] = None, parent_commit: Optional[str] = None) -&gt;                       ‚îÇ\n‚îÇ                            huggingface_hub.hf_api.CommitInfo: Deletes a folder in the given repo.               ‚îÇ\n‚îÇ              delete_repo = def delete_repo(repo_id: str, *, token: Optional[str] = None, repo_type:             ‚îÇ\n‚îÇ                            Optional[str] = None): Delete a repo from the HuggingFace Hub. CAUTION: this is      ‚îÇ\n‚îÇ                            irreversible.                                                                        ‚îÇ\n‚îÇ               delete_tag = def delete_tag(repo_id: str, *, tag: str, token: Optional[str] = None, repo_type:    ‚îÇ\n‚îÇ                            Optional[str] = None) -&gt; None: Delete a tag from a repo on the Hub.                  ‚îÇ\n‚îÇ  edit_discussion_comment = def edit_discussion_comment(repo_id: str, discussion_num: int, comment_id: str,      ‚îÇ\n‚îÇ                            new_content: str, *, token: Optional[str] = None, repo_type: Optional[str] = None)   ‚îÇ\n‚îÇ                            -&gt; huggingface_hub.community.DiscussionComment: Edits a comment on a Discussion /    ‚îÇ\n‚îÇ                            Pull Request.                                                                        ‚îÇ\n‚îÇ         get_dataset_tags = def get_dataset_tags() -&gt; huggingface_hub.utils.endpoint_helpers.DatasetTags: Gets   ‚îÇ\n‚îÇ                            all valid dataset tags as a nested namespace object.                                 ‚îÇ\n‚îÇ   get_discussion_details = def get_discussion_details(repo_id: str, discussion_num: int, *, repo_type:          ‚îÇ\n‚îÇ                            Optional[str] = None, token: Optional[str] = None) -&gt;                                ‚îÇ\n‚îÇ                            huggingface_hub.community.DiscussionWithDetails: Fetches a Discussion's / Pull       ‚îÇ\n‚îÇ                            Request 's details from the Hub.                                                     ‚îÇ\n‚îÇ       get_full_repo_name = def get_full_repo_name(model_id: str, *, organization: Optional[str] = None, token:  ‚îÇ\n‚îÇ                            Union[bool, str, NoneType] = None):                                                  ‚îÇ\n‚îÇ                            Returns the repository name for a given model ID and optional                        ‚îÇ\n‚îÇ                            organization.                                                                        ‚îÇ\n‚îÇ           get_model_tags = def get_model_tags() -&gt; huggingface_hub.utils.endpoint_helpers.ModelTags: Gets all   ‚îÇ\n‚îÇ                            valid model tags as a nested namespace object                                        ‚îÇ\n‚îÇ     get_repo_discussions = def get_repo_discussions(repo_id: str, *, repo_type: Optional[str] = None, token:    ‚îÇ\n‚îÇ                            Optional[str] = None) -&gt; Iterator[huggingface_hub.community.Discussion]: Fetches     ‚îÇ\n‚îÇ                            Discussions and Pull Requests for the given repo.                                    ‚îÇ\n‚îÇ  hide_discussion_comment = def hide_discussion_comment(repo_id: str, discussion_num: int, comment_id: str, *,   ‚îÇ\n‚îÇ                            token: Optional[str] = None, repo_type: Optional[str] = None) -&gt;                     ‚îÇ\n‚îÇ                            huggingface_hub.community.DiscussionComment: Hides a comment on a Discussion / Pull  ‚îÇ\n‚îÇ                            Request.                                                                             ‚îÇ\n‚îÇ            list_datasets = def list_datasets(*, filter:                                                         ‚îÇ\n‚îÇ                            Union[huggingface_hub.utils.endpoint_helpers.DatasetFilter, str, Iterable[str],      ‚îÇ\n‚îÇ                            NoneType] = None, author: Optional[str] = None, search: Optional[str] = None, sort:  ‚îÇ\n‚îÇ                            Union[Literal['lastModified'], str, NoneType] = None, direction:                     ‚îÇ\n‚îÇ                            Optional[Literal[-1]] = None, limit: Optional[int] = None, cardData: Optional[bool]  ‚îÇ\n‚îÇ                            = None, full: Optional[bool] = None, token: Optional[str] = None) -&gt;                 ‚îÇ\n‚îÇ                            List[huggingface_hub.hf_api.DatasetInfo]: Get the list of all the datasets on        ‚îÇ\n‚îÇ                            huggingface.co                                                                       ‚îÇ\n‚îÇ             list_metrics = def list_metrics() -&gt; List[huggingface_hub.hf_api.MetricInfo]: Get the public list   ‚îÇ\n‚îÇ                            of all the metrics on huggingface.co                                                 ‚îÇ\n‚îÇ              list_models = def list_models(*, filter: Union[huggingface_hub.utils.endpoint_helpers.ModelFilter, ‚îÇ\n‚îÇ                            str, Iterable[str], NoneType] = None, author: Optional[str] = None, search:          ‚îÇ\n‚îÇ                            Optional[str] = None, emissions_thresholds: Optional[Tuple[float, float]] = None,    ‚îÇ\n‚îÇ                            sort: Union[Literal['lastModified'], str, NoneType] = None, direction:               ‚îÇ\n‚îÇ                            Optional[Literal[-1]] = None, limit: Optional[int] = None, full: Optional[bool] =    ‚îÇ\n‚îÇ                            None, cardData: bool = False, fetch_config: bool = False, token: Union[bool, str,    ‚îÇ\n‚îÇ                            NoneType] = None) -&gt; List[huggingface_hub.hf_api.ModelInfo]: Get the list of all the ‚îÇ\n‚îÇ                            models on huggingface.co                                                             ‚îÇ\n‚îÇ          list_repo_files = def list_repo_files(repo_id: str, *, revision: Optional[str] = None, repo_type:      ‚îÇ\n‚îÇ                            Optional[str] = None, timeout: Optional[float] = None, token: Union[bool, str,       ‚îÇ\n‚îÇ                            NoneType] = None) -&gt; List[str]: Get the list of files in a given repo.               ‚îÇ\n‚îÇ              list_spaces = def list_spaces(*, filter: Union[str, Iterable[str], NoneType] = None, author:       ‚îÇ\n‚îÇ                            Optional[str] = None, search: Optional[str] = None, sort:                            ‚îÇ\n‚îÇ                            Union[Literal['lastModified'], str, NoneType] = None, direction:                     ‚îÇ\n‚îÇ                            Optional[Literal[-1]] = None, limit: Optional[int] = None, datasets: Union[str,      ‚îÇ\n‚îÇ                            Iterable[str], NoneType] = None, models: Union[str, Iterable[str], NoneType] = None, ‚îÇ\n‚îÇ                            linked: bool = False, full: Optional[bool] = None, token: Optional[str] = None) -&gt;   ‚îÇ\n‚îÇ                            List[huggingface_hub.hf_api.SpaceInfo]: Get the public list of all Spaces on         ‚îÇ\n‚îÇ                            huggingface.co                                                                       ‚îÇ\n‚îÇ       merge_pull_request = def merge_pull_request(repo_id: str, discussion_num: int, *, token: Optional[str] =  ‚îÇ\n‚îÇ                            None, comment: Optional[str] = None, repo_type: Optional[str] = None): Merges a Pull ‚îÇ\n‚îÇ                            Request.                                                                             ‚îÇ\n‚îÇ               model_info = def model_info(repo_id: str, *, revision: Optional[str] = None, timeout:             ‚îÇ\n‚îÇ                            Optional[float] = None, securityStatus: Optional[bool] = None, files_metadata: bool  ‚îÇ\n‚îÇ                            = False, token: Union[bool, str, NoneType] = None) -&gt;                                ‚îÇ\n‚îÇ                            huggingface_hub.hf_api.ModelInfo: Get info on one specific model on huggingface.co   ‚îÇ\n‚îÇ                move_repo = def move_repo(from_id: str, to_id: str, *, repo_type: Optional[str] = None, token:   ‚îÇ\n‚îÇ                            Optional[str] = None): Moving a repository from namespace1/repo_name1 to             ‚îÇ\n‚îÇ                            namespace2/repo_name2                                                                ‚îÇ\n‚îÇ        rename_discussion = def rename_discussion(repo_id: str, discussion_num: int, new_title: str, *, token:   ‚îÇ\n‚îÇ                            Optional[str] = None, repo_type: Optional[str] = None) -&gt;                            ‚îÇ\n‚îÇ                            huggingface_hub.community.DiscussionTitleChange: Renames a Discussion.               ‚îÇ\n‚îÇ                repo_info = def repo_info(repo_id: str, *, revision: Optional[str] = None, repo_type:            ‚îÇ\n‚îÇ                            Optional[str] = None, timeout: Optional[float] = None, files_metadata: bool = False, ‚îÇ\n‚îÇ                            token: Union[bool, str, NoneType] = None) -&gt; Union[huggingface_hub.hf_api.ModelInfo, ‚îÇ\n‚îÇ                            huggingface_hub.hf_api.DatasetInfo, huggingface_hub.hf_api.SpaceInfo]: Get the info  ‚îÇ\n‚îÇ                            object for a given repo of a given type.                                             ‚îÇ\n‚îÇ         set_access_token = def set_access_token(access_token: str):                                             ‚îÇ\n‚îÇ                            Saves the passed access token so git can correctly authenticate the                  ‚îÇ\n‚îÇ                            user.                                                                                ‚îÇ\n‚îÇ               space_info = def space_info(repo_id: str, *, revision: Optional[str] = None, timeout:             ‚îÇ\n‚îÇ                            Optional[float] = None, files_metadata: bool = False, token: Union[bool, str,        ‚îÇ\n‚îÇ                            NoneType] = None) -&gt; huggingface_hub.hf_api.SpaceInfo: Get info on one specific      ‚îÇ\n‚îÇ                            Space on huggingface.co.                                                             ‚îÇ\n‚îÇ       unset_access_token = def unset_access_token(): Resets the user's access token.                            ‚îÇ\n‚îÇ   update_repo_visibility = def update_repo_visibility(repo_id: str, private: bool = False, *, token:            ‚îÇ\n‚îÇ                            Optional[str] = None, organization: Optional[str] = None, repo_type: Optional[str] = ‚îÇ\n‚îÇ                            None, name: Optional[str] = None) -&gt; Dict[str, bool]: Update the visibility setting  ‚îÇ\n‚îÇ                            of a repository.                                                                     ‚îÇ\n‚îÇ              upload_file = def upload_file(*, path_or_fileobj: Union[str, bytes, BinaryIO], path_in_repo: str,  ‚îÇ\n‚îÇ                            repo_id: str, token: Optional[str] = None, repo_type: Optional[str] = None,          ‚îÇ\n‚îÇ                            revision: Optional[str] = None, commit_message: Optional[str] = None,                ‚îÇ\n‚îÇ                            commit_description: Optional[str] = None, create_pr: Optional[bool] = None,          ‚îÇ\n‚îÇ                            parent_commit: Optional[str] = None) -&gt; str:                                         ‚îÇ\n‚îÇ                            Upload a local file (up to 50 GB) to the given repo. The upload is done              ‚îÇ\n‚îÇ                            through a HTTP post request, and doesn't require git or git-lfs to be                ‚îÇ\n‚îÇ                            installed.                                                                           ‚îÇ\n‚îÇ            upload_folder = def upload_folder(*, repo_id: str, folder_path: Union[str, pathlib.Path],            ‚îÇ\n‚îÇ                            path_in_repo: Optional[str] = None, commit_message: Optional[str] = None,            ‚îÇ\n‚îÇ                            commit_description: Optional[str] = None, token: Optional[str] = None, repo_type:    ‚îÇ\n‚îÇ                            Optional[str] = None, revision: Optional[str] = None, create_pr: Optional[bool] =    ‚îÇ\n‚îÇ                            None, parent_commit: Optional[str] = None, allow_patterns: Union[List[str], str,     ‚îÇ\n‚îÇ                            NoneType] = None, ignore_patterns: Union[List[str], str, NoneType] = None):          ‚îÇ\n‚îÇ                            Upload a local folder to the given repo. The upload is done                          ‚îÇ\n‚îÇ                            through a HTTP requests, and doesn't require git or git-lfs to be                    ‚îÇ\n‚îÇ                            installed.                                                                           ‚îÇ\n‚îÇ                   whoami = def whoami(token: Optional[str] = None) -&gt; Dict: Call HF API to know \"whoami\".       ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nYou‚Äôll see from looking through this there is a bunch of different things we can now do programmatically via the hub. For this post we‚Äôre interested in the list_datasets and list_models methods. If we look at one of these we can see it has a bunch of different options we can use when listing datasets or models.\n\nrich.inspect(api.list_models)\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;bound method HfApi.list_models of &lt;huggingface_hub.hf_api.HfApi object at 0x136a2ce80&gt;&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ def HfApi.list_models(*, filter: Union[huggingface_hub.utils.endpoint_helpers.ModelFilter, str, Iterable[str],  ‚îÇ\n‚îÇ NoneType] = None, author: Optional[str] = None, search: Optional[str] = None, emissions_thresholds:             ‚îÇ\n‚îÇ Optional[Tuple[float, float]] = None, sort: Union[Literal['lastModified'], str, NoneType] = None, direction:    ‚îÇ\n‚îÇ Optional[Literal[-1]] = None, limit: Optional[int] = None, full: Optional[bool] = None, cardData: bool = False, ‚îÇ\n‚îÇ fetch_config: bool = False, token: Union[bool, str, NoneType] = None) -&gt;                                        ‚îÇ\n‚îÇ List[huggingface_hub.hf_api.ModelInfo]:                                                                         ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Get the list of all the models on huggingface.co                                                                ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ 28 attribute(s) not shown. Run inspect(inspect) for options.                                                    ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nFor our use case we want everything, so we set limit=None, we don‚Äôt want any filters so we set this to None (this is the default behaviour, but we set them explicitly here to make it clearer for our future selves). We also set full=True so we get back more verbose information about our dataset and models. We also wrap the result in iter and list since the behaviour of these methods will change in future versions to support paging.\n\nhub_datasets = list(iter(api.list_datasets(limit=None, filter=None, full=True)))\n\n\nhub_models = list(iter(api.list_models(limit=None, filter=None, full=True)))\n\nLet‚Äôs peek at an example of what we get back\n\nhub_models[0]\n\nModelInfo: {\n    modelId: albert-base-v1\n    sha: aeffd769076a5c4f83b2546aea99ca45a15a5da4\n    lastModified: 2021-01-13T15:08:24.000Z\n    tags: ['pytorch', 'tf', 'albert', 'fill-mask', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1909.11942', 'transformers', 'exbert', 'license:apache-2.0', 'autotrain_compatible', 'has_space']\n    pipeline_tag: fill-mask\n    siblings: [RepoFile(rfilename='.gitattributes', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='README.md', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='config.json', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='pytorch_model.bin', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='spiece.model', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='tf_model.h5', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='tokenizer.json', size='None', blob_id='None', lfs='None'), RepoFile(rfilename='with-prefix-tf_model.h5', size='None', blob_id='None', lfs='None')]\n    private: False\n    author: None\n    config: None\n    securityStatus: None\n    _id: 621ffdc036468d709f174328\n    id: albert-base-v1\n    gitalyUid: 4f35551ea371da7a8762caab54319a54ade836044f0ca7690d21e86b159867eb\n    likes: 1\n    downloads: 75182\n    library_name: transformers\n}\n\n\n\nhub_datasets[0]\n\nDatasetInfo: {\n    id: acronym_identification\n    sha: 173af1516c409eb4596bc63a69626bdb5584c40c\n    lastModified: 2022-11-18T17:25:49.000Z\n    tags: ['task_categories:token-classification', 'annotations_creators:expert-generated', 'language_creators:found', 'multilinguality:monolingual', 'size_categories:10K&lt;n&lt;100K', 'source_datasets:original', 'language:en', 'license:mit', 'acronym-identification', 'arxiv:2010.14678']\n    private: False\n    author: None\n    description: Acronym identification training and development sets for the acronym identification task at SDU@AAAI-21.\n    citation: @inproceedings{veyseh-et-al-2020-what,\n   title={{What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation}},\n   author={Amir Pouran Ben Veyseh and Franck Dernoncourt and Quan Hung Tran and Thien Huu Nguyen},\n   year={2020},\n   booktitle={Proceedings of COLING},\n   link={https://arxiv.org/pdf/2010.14678v1.pdf}\n}\n    cardData: {'annotations_creators': ['expert-generated'], 'language_creators': ['found'], 'language': ['en'], 'license': ['mit'], 'multilinguality': ['monolingual'], 'size_categories': ['10K&lt;n&lt;100K'], 'source_datasets': ['original'], 'task_categories': ['token-classification'], 'task_ids': [], 'paperswithcode_id': 'acronym-identification', 'pretty_name': 'Acronym Identification Dataset', 'train-eval-index': [{'config': 'default', 'task': 'token-classification', 'task_id': 'entity_extraction', 'splits': {'eval_split': 'test'}, 'col_mapping': {'tokens': 'tokens', 'labels': 'tags'}}], 'tags': ['acronym-identification'], 'dataset_info': {'features': [{'name': 'id', 'dtype': 'string'}, {'name': 'tokens', 'sequence': 'string'}, {'name': 'labels', 'sequence': {'class_label': {'names': {'0': 'B-long', '1': 'B-short', '2': 'I-long', '3': 'I-short', '4': 'O'}}}}], 'splits': [{'name': 'train', 'num_bytes': 7792803, 'num_examples': 14006}, {'name': 'validation', 'num_bytes': 952705, 'num_examples': 1717}, {'name': 'test', 'num_bytes': 987728, 'num_examples': 1750}], 'download_size': 8556464, 'dataset_size': 9733236}}\n    siblings: []\n    _id: 621ffdd236468d709f181d58\n    disabled: False\n    gated: False\n    gitalyUid: 6570517623fa521aa189178e7c7e73d9d88c01b295204edef97f389a15eae144\n    likes: 9\n    downloads: 6074\n    paperswithcode_id: acronym-identification\n}\n\n\nSince we want both models and datasets we create a dictionary which stores the types of item i.e.¬†is it a dataset or a model.\n\nhub_data = {\"model\": hub_models, \"dataset\": hub_datasets}\n\nWe‚Äôll be putting our data inside a pandas DataFrame, so we‚Äôll grab the .__dict__ attribute for each hub item, so it‚Äôs more pandas friendly.\n\nhub_item_dict = []\nfor hub_type, hub_item in hub_data.items():\n    for item in hub_item:\n        data = item.__dict__\n        data[\"type\"] = hub_type\n        hub_item_dict.append(data)\n\n\ndf = pd.DataFrame.from_dict(hub_item_dict)\n\nHow many hub items do we have?\n\nlen(df)\n\n151343\n\n\nWhat info do we have?\n\ndf.columns\n\nIndex(['modelId', 'sha', 'lastModified', 'tags', 'pipeline_tag', 'siblings',\n       'private', 'author', 'config', 'securityStatus', '_id', 'id',\n       'gitalyUid', 'likes', 'downloads', 'library_name', 'type',\n       'description', 'citation', 'cardData', 'disabled', 'gated',\n       'paperswithcode_id'],\n      dtype='object')"
  },
  {
    "objectID": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#tags",
    "href": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#tags",
    "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
    "section": "Tags",
    "text": "Tags\nModels and datasets have a bunch of metadata i.e.¬†last modified and number of downloads. We‚Äôll focus on tags here. Let‚Äôs start by looking at a single example.\n\ndf.loc[30, \"tags\"]\n\n['pytorch',\n 'tf',\n 'rust',\n 'safetensors',\n 'distilbert',\n 'text-classification',\n 'en',\n 'dataset:sst2',\n 'dataset:glue',\n 'doi:10.57967/hf/0181',\n 'transformers',\n 'license:apache-2.0',\n 'model-index',\n 'has_space']\n\n\nWe can see that tags capture can relate to tasks i.e.¬†text-classification, libraries supported i.e.¬†tf, or the licence associated with a model or dataset. As a starting point for exploring tags we can take a look at how many tags models and datasets have. We‚Äôll add a new column to capture this number.\n\ndef calculate_number_of_tags(tags: [str]) -&gt; int:\n    return len(tags)\n\n\ndf[\"number_of_tags\"] = df[\"tags\"].apply(lambda x: calculate_number_of_tags(x))\n\nWe can now use describe to see the breakdown of this number.\n\ndf.number_of_tags.describe()\n\ncount    151343.000000\nmean          3.855566\nstd           6.878613\nmin           0.000000\n25%           0.000000\n50%           4.000000\n75%           6.000000\nmax         650.000000\nName: number_of_tags, dtype: float64\n\n\nWe can see that we have quite a range of tag numbers ranging from 0 to 650! If your brain works anything like mine you probably want to know what this high value is about!\n\ndf[df.number_of_tags &gt; 640][[\"id\", \"tags\"]]\n\n\n\n\n\n\n\n\nid\ntags\n\n\n\n\n136372\nbible-nlp/biblenlp-corpus\n[task_categories:translation, annotations_crea...\n\n\n\n\n\n\n\n\ndf[df.number_of_tags &gt; 640][\"tags\"].tolist()\n\n[['task_categories:translation',\n  'annotations_creators:no-annotation',\n  'language_creators:expert-generated',\n  'multilinguality:translation',\n  'multilinguality:multilingual',\n  'size_categories:1M&lt;n&lt;10M',\n  'source_datasets:original',\n  'language:aau',\n  'language:aaz',\n  'language:abx',\n  'language:aby',\n  'language:acf',\n  'language:acu',\n  'language:adz',\n  'language:aey',\n  'language:agd',\n  'language:agg',\n  'language:agm',\n  'language:agn',\n  'language:agr',\n  'language:agu',\n  'language:aia',\n  'language:ake',\n  'language:alp',\n  'language:alq',\n  'language:als',\n  'language:aly',\n  'language:ame',\n  'language:amk',\n  'language:amp',\n  'language:amr',\n  'language:amu',\n  'language:anh',\n  'language:anv',\n  'language:aoi',\n  'language:aoj',\n  'language:apb',\n  'language:apn',\n  'language:apu',\n  'language:apy',\n  'language:arb',\n  'language:arl',\n  'language:arn',\n  'language:arp',\n  'language:aso',\n  'language:ata',\n  'language:atb',\n  'language:atd',\n  'language:atg',\n  'language:auc',\n  'language:aui',\n  'language:auy',\n  'language:avt',\n  'language:awb',\n  'language:awk',\n  'language:awx',\n  'language:azg',\n  'language:azz',\n  'language:bao',\n  'language:bbb',\n  'language:bbr',\n  'language:bch',\n  'language:bco',\n  'language:bdd',\n  'language:bea',\n  'language:bel',\n  'language:bgs',\n  'language:bgt',\n  'language:bhg',\n  'language:bhl',\n  'language:big',\n  'language:bjr',\n  'language:bjv',\n  'language:bkd',\n  'language:bki',\n  'language:bkq',\n  'language:bkx',\n  'language:bla',\n  'language:blw',\n  'language:blz',\n  'language:bmh',\n  'language:bmk',\n  'language:bmr',\n  'language:bnp',\n  'language:boa',\n  'language:boj',\n  'language:bon',\n  'language:box',\n  'language:bqc',\n  'language:bre',\n  'language:bsn',\n  'language:bsp',\n  'language:bss',\n  'language:buk',\n  'language:bus',\n  'language:bvr',\n  'language:bxh',\n  'language:byx',\n  'language:bzd',\n  'language:bzj',\n  'language:cab',\n  'language:caf',\n  'language:cao',\n  'language:cap',\n  'language:car',\n  'language:cav',\n  'language:cax',\n  'language:cbc',\n  'language:cbi',\n  'language:cbk',\n  'language:cbr',\n  'language:cbs',\n  'language:cbt',\n  'language:cbu',\n  'language:cbv',\n  'language:cco',\n  'language:ces',\n  'language:cgc',\n  'language:cha',\n  'language:chd',\n  'language:chf',\n  'language:chk',\n  'language:chq',\n  'language:chz',\n  'language:cjo',\n  'language:cjv',\n  'language:cle',\n  'language:clu',\n  'language:cme',\n  'language:cmn',\n  'language:cni',\n  'language:cnl',\n  'language:cnt',\n  'language:cof',\n  'language:con',\n  'language:cop',\n  'language:cot',\n  'language:cpa',\n  'language:cpb',\n  'language:cpc',\n  'language:cpu',\n  'language:crn',\n  'language:crx',\n  'language:cso',\n  'language:cta',\n  'language:ctp',\n  'language:ctu',\n  'language:cub',\n  'language:cuc',\n  'language:cui',\n  'language:cut',\n  'language:cux',\n  'language:cwe',\n  'language:daa',\n  'language:dad',\n  'language:dah',\n  'language:ded',\n  'language:deu',\n  'language:dgr',\n  'language:dgz',\n  'language:dif',\n  'language:dik',\n  'language:dji',\n  'language:djk',\n  'language:dob',\n  'language:dwr',\n  'language:dww',\n  'language:dwy',\n  'language:eko',\n  'language:emi',\n  'language:emp',\n  'language:eng',\n  'language:epo',\n  'language:eri',\n  'language:ese',\n  'language:etr',\n  'language:faa',\n  'language:fai',\n  'language:far',\n  'language:for',\n  'language:fra',\n  'language:fuf',\n  'language:gai',\n  'language:gam',\n  'language:gaw',\n  'language:gdn',\n  'language:gdr',\n  'language:geb',\n  'language:gfk',\n  'language:ghs',\n  'language:gia',\n  'language:glk',\n  'language:gmv',\n  'language:gng',\n  'language:gnn',\n  'language:gnw',\n  'language:gof',\n  'language:grc',\n  'language:gub',\n  'language:guh',\n  'language:gui',\n  'language:gul',\n  'language:gum',\n  'language:guo',\n  'language:gvc',\n  'language:gvf',\n  'language:gwi',\n  'language:gym',\n  'language:gyr',\n  'language:hat',\n  'language:haw',\n  'language:hbo',\n  'language:hch',\n  'language:heb',\n  'language:heg',\n  'language:hix',\n  'language:hla',\n  'language:hlt',\n  'language:hns',\n  'language:hop',\n  'language:hrv',\n  'language:hub',\n  'language:hui',\n  'language:hus',\n  'language:huu',\n  'language:huv',\n  'language:hvn',\n  'language:ign',\n  'language:ikk',\n  'language:ikw',\n  'language:imo',\n  'language:inb',\n  'language:ind',\n  'language:ino',\n  'language:iou',\n  'language:ipi',\n  'language:ita',\n  'language:jac',\n  'language:jao',\n  'language:jic',\n  'language:jiv',\n  'language:jpn',\n  'language:jvn',\n  'language:kaq',\n  'language:kbc',\n  'language:kbh',\n  'language:kbm',\n  'language:kdc',\n  'language:kde',\n  'language:kdl',\n  'language:kek',\n  'language:ken',\n  'language:kew',\n  'language:kgk',\n  'language:kgp',\n  'language:khs',\n  'language:kje',\n  'language:kjs',\n  'language:kkc',\n  'language:kky',\n  'language:klt',\n  'language:klv',\n  'language:kms',\n  'language:kmu',\n  'language:kne',\n  'language:knf',\n  'language:knj',\n  'language:kos',\n  'language:kpf',\n  'language:kpg',\n  'language:kpj',\n  'language:kpw',\n  'language:kqa',\n  'language:kqc',\n  'language:kqf',\n  'language:kql',\n  'language:kqw',\n  'language:ksj',\n  'language:ksr',\n  'language:ktm',\n  'language:kto',\n  'language:kud',\n  'language:kue',\n  'language:kup',\n  'language:kvn',\n  'language:kwd',\n  'language:kwf',\n  'language:kwi',\n  'language:kwj',\n  'language:kyf',\n  'language:kyg',\n  'language:kyq',\n  'language:kyz',\n  'language:kze',\n  'language:lac',\n  'language:lat',\n  'language:lbb',\n  'language:leu',\n  'language:lex',\n  'language:lgl',\n  'language:lid',\n  'language:lif',\n  'language:lww',\n  'language:maa',\n  'language:maj',\n  'language:maq',\n  'language:mau',\n  'language:mav',\n  'language:maz',\n  'language:mbb',\n  'language:mbc',\n  'language:mbh',\n  'language:mbl',\n  'language:mbt',\n  'language:mca',\n  'language:mcb',\n  'language:mcd',\n  'language:mcf',\n  'language:mcp',\n  'language:mdy',\n  'language:med',\n  'language:mee',\n  'language:mek',\n  'language:meq',\n  'language:met',\n  'language:meu',\n  'language:mgh',\n  'language:mgw',\n  'language:mhl',\n  'language:mib',\n  'language:mic',\n  'language:mie',\n  'language:mig',\n  'language:mih',\n  'language:mil',\n  'language:mio',\n  'language:mir',\n  'language:mit',\n  'language:miz',\n  'language:mjc',\n  'language:mkn',\n  'language:mks',\n  'language:mlh',\n  'language:mlp',\n  'language:mmx',\n  'language:mna',\n  'language:mop',\n  'language:mox',\n  'language:mph',\n  'language:mpj',\n  'language:mpm',\n  'language:mpp',\n  'language:mps',\n  'language:mpx',\n  'language:mqb',\n  'language:mqj',\n  'language:msb',\n  'language:msc',\n  'language:msk',\n  'language:msm',\n  'language:msy',\n  'language:mti',\n  'language:muy',\n  'language:mva',\n  'language:mvn',\n  'language:mwc',\n  'language:mxb',\n  'language:mxp',\n  'language:mxq',\n  'language:mxt',\n  'language:myu',\n  'language:myw',\n  'language:myy',\n  'language:mzz',\n  'language:nab',\n  'language:naf',\n  'language:nak',\n  'language:nay',\n  'language:nbq',\n  'language:nca',\n  'language:nch',\n  'language:ncj',\n  'language:ncl',\n  'language:ncu',\n  'language:ndj',\n  'language:nfa',\n  'language:ngp',\n  'language:ngu',\n  'language:nhg',\n  'language:nhi',\n  'language:nho',\n  'language:nhr',\n  'language:nhu',\n  'language:nhw',\n  'language:nhy',\n  'language:nif',\n  'language:nin',\n  'language:nko',\n  'language:nld',\n  'language:nlg',\n  'language:nna',\n  'language:nnq',\n  'language:not',\n  'language:nou',\n  'language:npl',\n  'language:nsn',\n  'language:nss',\n  'language:ntj',\n  'language:ntp',\n  'language:nwi',\n  'language:nyu',\n  'language:obo',\n  'language:ong',\n  'language:ons',\n  'language:ood',\n  'language:opm',\n  'language:ote',\n  'language:otm',\n  'language:otn',\n  'language:otq',\n  'language:ots',\n  'language:pab',\n  'language:pad',\n  'language:pah',\n  'language:pao',\n  'language:pes',\n  'language:pib',\n  'language:pio',\n  'language:pir',\n  'language:pjt',\n  'language:plu',\n  'language:pma',\n  'language:poe',\n  'language:poi',\n  'language:pon',\n  'language:poy',\n  'language:ppo',\n  'language:prf',\n  'language:pri',\n  'language:ptp',\n  'language:ptu',\n  'language:pwg',\n  'language:quc',\n  'language:quf',\n  'language:quh',\n  'language:qul',\n  'language:qup',\n  'language:qvc',\n  'language:qve',\n  'language:qvh',\n  'language:qvm',\n  'language:qvn',\n  'language:qvs',\n  'language:qvw',\n  'language:qvz',\n  'language:qwh',\n  'language:qxh',\n  'language:qxn',\n  'language:qxo',\n  'language:rai',\n  'language:rkb',\n  'language:rmc',\n  'language:roo',\n  'language:rop',\n  'language:rro',\n  'language:ruf',\n  'language:rug',\n  'language:rus',\n  'language:sab',\n  'language:san',\n  'language:sbe',\n  'language:seh',\n  'language:sey',\n  'language:sgz',\n  'language:shj',\n  'language:shp',\n  'language:sim',\n  'language:sja',\n  'language:sll',\n  'language:smk',\n  'language:snc',\n  'language:snn',\n  'language:sny',\n  'language:som',\n  'language:soq',\n  'language:spa',\n  'language:spl',\n  'language:spm',\n  'language:sps',\n  'language:spy',\n  'language:sri',\n  'language:srm',\n  'language:srn',\n  'language:srp',\n  'language:srq',\n  'language:ssd',\n  'language:ssg',\n  'language:ssx',\n  'language:stp',\n  'language:sua',\n  'language:sue',\n  'language:sus',\n  'language:suz',\n  'language:swe',\n  'language:swh',\n  'language:swp',\n  'language:sxb',\n  'language:tac',\n  'language:tav',\n  'language:tbc',\n  'language:tbl',\n  'language:tbo',\n  'language:tbz',\n  'language:tca',\n  'language:tee',\n  'language:ter',\n  'language:tew',\n  'language:tfr',\n  'language:tgp',\n  'language:tif',\n  'language:tim',\n  'language:tiy',\n  'language:tke',\n  'language:tku',\n  'language:tna',\n  'language:tnc',\n  'language:tnn',\n  'language:tnp',\n  'language:toc',\n  'language:tod',\n  'language:toj',\n  'language:ton',\n  'language:too',\n  'language:top',\n  'language:tos',\n  'language:tpt',\n  'language:trc',\n  'language:tsw',\n  'language:ttc',\n  'language:tue',\n  'language:tuo',\n  'language:txu',\n  'language:ubr',\n  'language:udu',\n  'language:ukr',\n  'language:uli',\n  'language:ura',\n  'language:urb',\n  'language:usa',\n  'language:usp',\n  'language:uvl',\n  'language:vid',\n  'language:vie',\n  'language:viv',\n  'language:vmy',\n  'language:waj',\n  'language:wal',\n  'language:wap',\n  'language:wat',\n  'language:wbp',\n  'language:wed',\n  'language:wer',\n  'language:wim',\n  'language:wmt',\n  'language:wmw',\n  'language:wnc',\n  'language:wnu',\n  'language:wos',\n  'language:wrk',\n  'language:wro',\n  'language:wsk',\n  'language:wuv',\n  'language:xav',\n  'language:xed',\n  'language:xla',\n  'language:xnn',\n  'language:xon',\n  'language:xsi',\n  'language:xtd',\n  'language:xtm',\n  'language:yaa',\n  'language:yad',\n  'language:yal',\n  'language:yap',\n  'language:yaq',\n  'language:yby',\n  'language:ycn',\n  'language:yka',\n  'language:yml',\n  'language:yre',\n  'language:yuj',\n  'language:yut',\n  'language:yuw',\n  'language:yva',\n  'language:zaa',\n  'language:zab',\n  'language:zac',\n  'language:zad',\n  'language:zai',\n  'language:zaj',\n  'language:zam',\n  'language:zao',\n  'language:zar',\n  'language:zas',\n  'language:zat',\n  'language:zav',\n  'language:zaw',\n  'language:zca',\n  'language:zia',\n  'language:ziw',\n  'language:zos',\n  'language:zpc',\n  'language:zpl',\n  'language:zpo',\n  'language:zpq',\n  'language:zpu',\n  'language:zpv',\n  'language:zpz',\n  'language:zsr',\n  'language:ztq',\n  'language:zty',\n  'language:zyp',\n  'language:be',\n  'language:br',\n  'language:cs',\n  'language:ch',\n  'language:zh',\n  'language:de',\n  'language:en',\n  'language:eo',\n  'language:fr',\n  'language:ht',\n  'language:he',\n  'language:hr',\n  'language:id',\n  'language:it',\n  'language:ja',\n  'language:la',\n  'language:nl',\n  'language:ru',\n  'language:sa',\n  'language:so',\n  'language:es',\n  'language:sr',\n  'language:sv',\n  'language:to',\n  'language:uk',\n  'language:vi',\n  'license:cc-by-4.0',\n  'license:other']]\n\n\nWe can see that in this case many of the tags relate to language. Since the dataset is bible related and the bible has been heavily translated this might not be as surprising.\nAlthough these high-level stats are somewhat interesting we probably want to break these numbers down. At a high level we can groupby datasets vs models.\n\ndf.groupby(\"type\")[\"number_of_tags\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ntype\n\n\n\n\n\n\n\n\n\n\n\n\ndataset\n19576.0\n2.46935\n13.137220\n0.0\n0.0\n0.0\n2.0\n650.0\n\n\nmodel\n131767.0\n4.06151\n5.327066\n0.0\n0.0\n4.0\n6.0\n413.0\n\n\n\n\n\n\n\nWe can see that the mean number of tags for models is higher than datasets. We can also see at the 75% percentile models also have more tags compared to datasets. The possible reasons for this (and whether this is a problem or not) is something we may wish to explore further‚Ä¶\nSince the hub hosts models from different libraries we may want to also breakdown by library. First let‚Äôs grab only the model part of our DataFrame.\n\nmodels_df = df[df[\"type\"] == \"model\"]\n\nThe library_name column contains info about the library. Let‚Äôs see how many unique libraries we have.\n\nmodels_df.library_name.unique().shape\n\n(63,)\n\n\nThis is quite a few! We can do a groupby on this column\n\nmodels_df.groupby(\"library_name\")[\"number_of_tags\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nlibrary_name\n\n\n\n\n\n\n\n\n\n\n\n\nBERT\n1.0\n7.0\nNaN\n7.0\n7.0\n7.0\n7.0\n7.0\n\n\nDoc-UFCN\n2.0\n4.0\n0.000000\n4.0\n4.0\n4.0\n4.0\n4.0\n\n\nEveryDream\n2.0\n7.0\n0.000000\n7.0\n7.0\n7.0\n7.0\n7.0\n\n\nFastAI\n1.0\n1.0\nNaN\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nJoeyNMT\n1.0\n4.0\nNaN\n4.0\n4.0\n4.0\n4.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nultralytics\n4.0\n10.0\n1.414214\n8.0\n9.5\n10.5\n11.0\n11.0\n\n\nultralyticsplus\n1.0\n9.0\nNaN\n9.0\n9.0\n9.0\n9.0\n9.0\n\n\nyolor\n2.0\n9.0\n0.000000\n9.0\n9.0\n9.0\n9.0\n9.0\n\n\nyolov5\n36.0\n9.0\n0.000000\n9.0\n9.0\n9.0\n9.0\n9.0\n\n\nyolov6detect\n1.0\n10.0\nNaN\n10.0\n10.0\n10.0\n10.0\n10.0\n\n\n\n\n62 rows √ó 8 columns\n\n\n\nWe might find this a bit tricky to look at. We may want to only include the top n libraries since some of these libraries may be less well used.\n\nmodels_df.library_name.value_counts()[:15]\n\ntransformers             63754\nstable-baselines3         3183\ndiffusers                 2802\nsentence-transformers     1273\nml-agents                  763\nkeras                      470\ntimm                       383\nespnet                     381\nspacy                      296\nsample-factory             273\nadapter-transformers       201\nsklearn                    113\nnemo                       103\nfastai                      99\nspeechbrain                 94\nName: library_name, dtype: int64\n\n\n\ntop_libraries = models_df.library_name.value_counts()[:9].index.to_list()\n\n\ntop_libraries_df = models_df[models_df.library_name.isin(top_libraries)]\n\n\ntop_libraries_df.groupby(\"library_name\")[\"number_of_tags\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nlibrary_name\n\n\n\n\n\n\n\n\n\n\n\n\ndiffusers\n2802.0\n4.374732\n2.171226\n1.0\n3.0\n4.0\n5.0\n18.0\n\n\nespnet\n381.0\n6.965879\n0.595060\n3.0\n7.0\n7.0\n7.0\n9.0\n\n\nkeras\n470.0\n3.842553\n14.422674\n1.0\n1.0\n2.0\n5.0\n311.0\n\n\nml-agents\n763.0\n6.965924\n0.273775\n2.0\n7.0\n7.0\n7.0\n7.0\n\n\nsentence-transformers\n1273.0\n6.984289\n3.221840\n2.0\n6.0\n6.0\n7.0\n36.0\n\n\nspacy\n296.0\n4.611486\n0.985180\n2.0\n4.0\n5.0\n5.0\n10.0\n\n\nstable-baselines3\n3183.0\n4.997801\n0.163426\n3.0\n5.0\n5.0\n5.0\n8.0\n\n\ntimm\n383.0\n3.548303\n1.315291\n2.0\n3.0\n3.0\n3.0\n13.0\n\n\ntransformers\n63754.0\n6.912037\n5.262633\n1.0\n5.0\n6.0\n8.0\n240.0\n\n\n\n\n\n\n\nLet‚Äôs take a quick look at some examples from the library with the highest and lowest number or tags.\n\ntop_libraries_df[top_libraries_df.library_name == \"sentence-transformers\"].sample(15)[\n    \"tags\"\n]\n\n6123      [pytorch, gpt_neo, arxiv:2202.08904, sentence-...\n2488      [pytorch, distilbert, sentence-transformers, f...\n37669     [pytorch, distilbert, sentence-transformers, f...\n71483     [pytorch, bert, sentence-transformers, feature...\n20710     [pytorch, tf, roberta, ko, sentence-transforme...\n27073     [pytorch, tf, jax, roberta, arxiv:1908.10084, ...\n92037     [pytorch, mpnet, sentence-transformers, featur...\n90320     [pytorch, mpnet, sentence-transformers, featur...\n63555     [pytorch, bert, sentence-transformers, feature...\n87707     [pytorch, mpnet, sentence-transformers, featur...\n80570     [pytorch, bert, sentence-transformers, feature...\n111407    [pytorch, bert, sentence-transformers, feature...\n82690     [pytorch, mpnet, sentence-transformers, featur...\n36217     [pytorch, bert, pl, dataset:Wikipedia, arxiv:1...\n100086    [pytorch, roberta, sentence-transformers, feat...\nName: tags, dtype: object\n\n\n\ntop_libraries_df[top_libraries_df.library_name == \"timm\"].sample(15)[\"tags\"]\n\n104432                [pytorch, timm, image-classification]\n110296    [pytorch, arxiv:2301.00808, timm, image-classi...\n24158                 [pytorch, timm, image-classification]\n26471                 [pytorch, timm, image-classification]\n104437                [pytorch, timm, image-classification]\n61630     [pytorch, dataset:beans, timm, image-classific...\n110298    [pytorch, arxiv:2301.00808, timm, image-classi...\n104015                [pytorch, timm, image-classification]\n101124                [pytorch, timm, image-classification]\n57882     [coreml, onnx, en, dataset:imagenet-1k, arxiv:...\n83459     [pytorch, timm, image-classification, vision, ...\n99461                 [pytorch, timm, image-classification]\n104029                [pytorch, timm, image-classification]\n84402     [pytorch, timm, image-classification, vision, ...\n104428                [pytorch, timm, image-classification]\nName: tags, dtype: object\n\n\nWe can see here that some tags for sentence-transformers are very closely tied to that libraries purpose e.g.¬†the sentence-similarity tag. This tag migth be useful when a user is looking for models to do sentence-similarity but might be less useful if you are trying to choose between models for this task i.e.¬†trying to find the setence-transformer model that will be useful for you. We should be careful, therefore, in treating number of tags as a proxy for quality.\n\nGrouping by pipeline tags\nWe have a column in our dataframe pipeline tag, which refers to the type of task a model is for. We should be careful relying too much on this but we can have a quick look at how often these are used.\n\nmodels_df[\"pipeline_tag\"].value_counts()\n\ntext-classification               14479\ntext2text-generation               8102\ntext-generation                    7602\nreinforcement-learning             6885\ntoken-classification               6386\nautomatic-speech-recognition       6238\nfill-mask                          5447\nquestion-answering                 3147\nfeature-extraction                 2661\ntranslation                        1837\nconversational                     1770\nimage-classification               1760\ntext-to-image                      1604\nsentence-similarity                1248\nsummarization                       735\nunconditional-image-generation      428\ntext-to-speech                      244\naudio-classification                234\nmultiple-choice                     169\nobject-detection                    158\nimage-segmentation                  134\naudio-to-audio                      130\ntabular-classification               97\nzero-shot-classification             97\nimage-to-text                        76\nzero-shot-image-classification       56\nvideo-classification                 50\ntable-question-answering             47\ntabular-regression                   44\nimage-to-image                       43\ndepth-estimation                     37\ndocument-question-answering          18\nvisual-question-answering            13\nvoice-activity-detection              6\nother                                 4\ntime-series-forecasting               1\nName: pipeline_tag, dtype: int64\n\n\nWe may also want to see if there are some type of task that have more tags.\n\nmodels_df.groupby(\"pipeline_tag\")[\"number_of_tags\"].mean().sort_values().plot.barh()\n\n\n\n\n\n\n\n\nWe can also look at the breakdown for a particular task\n\ntext_classification_df = models_df[models_df[\"pipeline_tag\"] == \"text-classification\"]\n\n\ntext_classification_df[\"number_of_tags\"].describe()\n\ncount    14479.000000\nmean         5.948822\nstd          3.718800\nmin          1.000000\n25%          4.000000\n50%          5.000000\n75%          7.000000\nmax        240.000000\nName: number_of_tags, dtype: float64\n\n\nAgain, we have some extreme outliers\n\ntext_classification_df[text_classification_df.number_of_tags &gt; 230][[\"tags\", \"modelId\"]]\n\n\n\n\n\n\n\n\ntags\nmodelId\n\n\n\n\n22457\n[pytorch, tf, roberta, text-classification, mu...\nm3hrdadfi/zabanshenas-roberta-base-mix\n\n\n101628\n[pytorch, canine, text-classification, ace, af...\nSebOchs/canine-c-lang-id\n\n\n\n\n\n\n\nWe see that these mostly seem to relate to language. Let‚Äôs remove these outliers and look at the distribution in the number of tags without these.\n\ntext_classification_df_no_outliers = text_classification_df[\n    text_classification_df[\"number_of_tags\"]\n    &lt;= text_classification_df[\"number_of_tags\"].quantile(0.95)\n]\ntext_classification_df_no_outliers[\"number_of_tags\"].plot.hist(bins=9)"
  },
  {
    "objectID": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#why-counting-tags-might-not-make-sense",
    "href": "posts/post-with-code/metadata-explore/2023_01_16_hub_api_explore copy.html#why-counting-tags-might-not-make-sense",
    "title": "A (very brief) intro to exploring metadata on the Hugging Face Hub",
    "section": "Why counting tags might not make sense",
    "text": "Why counting tags might not make sense\nI‚Äôe already hinted at why looking at raw number of tags might not be a good idea. Let‚Äôs close this blog by briefly digging into at least one reason why. We‚Äôll use the toolz library for some of this analysis.\n\nfrom toolz import concat\n\nFirst we grab all the tags and put them in a single list.\n\nall_tags = list(concat(df.tags.tolist()))\n\nIf we look at some examples, we‚Äôll see some tags are in the form of something:somethingelse.\n\nall_tags[:10]\n\n['pytorch',\n 'tf',\n 'albert',\n 'fill-mask',\n 'en',\n 'dataset:bookcorpus',\n 'dataset:wikipedia',\n 'arxiv:1909.11942',\n 'transformers',\n 'exbert']\n\n\nfor example dataset:wikipedia, we should therefore avoid treating all tags as the same since tags can have a particular purpose. i.e.¬†indicating a dataset is associated with a model.\n\ndef is_special_tag(tag: str):\n    return \":\" in tag\n\n\nfrom toolz import countby, valmap\n\n\nspecial_tag_vs_normal = countby(is_special_tag, all_tags)\n\n\nspecial_tag_vs_normal\n\n{False: 467758, True: 115755}\n\n\n\ntotal = sum(special_tag_vs_normal.values())\nvalmap(lambda x: x / total, special_tag_vs_normal)\n\n{False: 0.8016239569641121, True: 0.1983760430358878}\n\n\nWe can see that a good chunk of tags are ‚Äòspecial‚Äô tags. i.e.¬†they have a ‚Äòtype‚Äô associated with them. If we want to explore tags on the hub more carefully we‚Äôll need to take this into account‚Ä¶"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html",
    "href": "posts/post-with-code/2022-01-13-image_search.html",
    "title": "Using ü§ó datasets for image search",
    "section": "",
    "text": "tl;dr it‚Äôs really easy to use the huggingface datasets library to create an image search application but it might not be suitable for sharing. update an updated version of this post is on the ü§ó blog!\nWhen datasets was first launched it was more usually associated with text data and nlp. However, datasets has got support for images. In particular there is now a datasets feature type for images{% fn %}. In this blog post I try and play around with this new datatype, in combination with some other nice features of the library to make an image search app.\nTo start lets take a look at the image feature. We can use the wonderful rich libary to poke around python objects (functions, classes etc.)\nfrom rich import inspect\nfrom datasets.features import features\ninspect(features.Image, help=True)\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;class 'datasets.features.image.Image'&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ def Image(id: Union[str, NoneType] = None) -&gt; None:                              ‚îÇ\n‚îÇ                                                                                  ‚îÇ\n‚îÇ Image feature to read image data from an image file.                             ‚îÇ\n‚îÇ                                                                                  ‚îÇ\n‚îÇ Input: The Image feature accepts as input:                                       ‚îÇ\n‚îÇ - A :obj:`str`: Absolute path to the image file (i.e. random access is allowed). ‚îÇ\n‚îÇ - A :obj:`dict` with the keys:                                                   ‚îÇ\n‚îÇ                                                                                  ‚îÇ\n‚îÇ     - path: String with relative path of the image file to the archive file.     ‚îÇ\n‚îÇ     - bytes: Bytes of the image file.                                            ‚îÇ\n‚îÇ                                                                                  ‚îÇ\n‚îÇ   This is useful for archived files with sequential access.                      ‚îÇ\n‚îÇ                                                                                  ‚îÇ\n‚îÇ - An :obj:`np.ndarray`: NumPy array representing an image.                       ‚îÇ\n‚îÇ - A :obj:`PIL.Image.Image`: PIL image object.                                    ‚îÇ\n‚îÇ                                                                                  ‚îÇ\n‚îÇ   dtype = 'dict'                                                                 ‚îÇ\n‚îÇ      id = None                                                                   ‚îÇ\n‚îÇ pa_type = None                                                                   ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nWe can see there a few different ways in which we can pass in our images. We‚Äôll come back to this in a little while.\nA really nice feature of the datasets library (beyond the functionality for processing data, memory mapping etc.) is that you get some nice things for free. One of these is the ability to add a faiss index. faiss is a ‚Äúlibrary for efficient similarity search and clustering of dense vectors‚Äù.\nThe datasets docs show and example of using faiss for text retrieval. What I‚Äôm curious about doing is using the faiss index to search for images. This can be super useful for a number of reasons but also comes with some potential issues."
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#the-dataset-digitised-books---images-identified-as-embellishments.-c.-1510---c.-1900.-jpg",
    "href": "posts/post-with-code/2022-01-13-image_search.html#the-dataset-digitised-books---images-identified-as-embellishments.-c.-1510---c.-1900.-jpg",
    "title": "Using ü§ó datasets for image search",
    "section": "The dataset: ‚ÄúDigitised Books - Images identified as Embellishments. c.¬†1510 - c.¬†1900. JPG‚Äù",
    "text": "The dataset: ‚ÄúDigitised Books - Images identified as Embellishments. c.¬†1510 - c.¬†1900. JPG‚Äù\nThis is a dataset of images which have been pulled from a collection of digitised books from the British Library. These images come from books across a wide time period and from a broad range of domains. These images were extracted using information in the OCR output for each book. As a result it‚Äôs known which book the images came from but not necessarily anything else about that image i.e.¬†what it is of.\nSome attempts to help overcome this have included uploading the images to flickr. This allows people to tag the images or put them into various different categories.\nThere have also been projects to tag the dataset using machine learning. This work already makes it possible to search by tags but we might want a ‚Äòricher‚Äô ability to search. For this particular experiment I will work with a subset of the collections which contain ‚Äúembellishments‚Äù. This dataset is a bit smaller so will be better for experimenting with. We can get the data from the BL repository: https://doi.org/10.21250/db17\n\n# hide_output\n!aria2c -x8 -o dig19cbooks-embellishments.zip \"https://bl.iro.bl.uk/downloads/ba1d1d12-b1bd-4a43-9696-7b29b56cdd20?locale=en\"\n\n\n01/12 20:18:42 [NOTICE] Downloading 1 item(s)\n\n\n\n01/12 20:18:42 [NOTICE] Removed the defunct control file /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip.aria2 because the download file /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip doesn't exist.\n\n\n\n01/12 20:18:42 [NOTICE] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n\n *** Download Progress Summary as of Wed Jan 12 20:19:43 2022 ***              58s](98%)]m\n\n===============================================================================\n\n[#f38a5d 553MiB/43GiB(1%) CN:5 DL:36MiB ETA:19m55s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:20:44 2022 ***              46s]\n\n===============================================================================\n\n[#f38a5d 2.6GiB/43GiB(6%) CN:5 DL:37MiB ETA:18m38s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:21:45 2022 ***              m1s]m\n\n===============================================================================\n\n[#f38a5d 4.7GiB/43GiB(10%) CN:5 DL:36MiB ETA:18m8s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:22:45 2022 ***              m46s]\n\n===============================================================================\n\n[#f38a5d 6.7GiB/43GiB(15%) CN:5 DL:31MiB ETA:19m38s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:23:46 2022 ***              m23s]\n\n===============================================================================\n\n[#f38a5d 8.8GiB/43GiB(20%) CN:5 DL:35MiB ETA:16m42s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:24:46 2022 ***              2s]mm\n\n===============================================================================\n\n[#f38a5d 10GiB/43GiB(24%) CN:5 DL:28MiB ETA:19m59s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:25:47 2022 ***              28s]\n\n===============================================================================\n\n[#f38a5d 12GiB/43GiB(29%) CN:5 DL:36MiB ETA:14m19s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:26:47 2022 ***              43s]\n\n===============================================================================\n\n[#f38a5d 14GiB/43GiB(33%) CN:5 DL:35MiB ETA:13m57s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:27:48 2022 ***              36s]\n\n===============================================================================\n\n[#f38a5d 16GiB/43GiB(37%) CN:5 DL:33MiB ETA:13m51s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:28:49 2022 ***              32s]\n\n===============================================================================\n\n[#f38a5d 18GiB/43GiB(42%) CN:5 DL:37MiB ETA:11m31s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:29:49 2022 ***              14s]\n\n===============================================================================\n\n[#f38a5d 20GiB/43GiB(46%) CN:5 DL:32MiB ETA:12m2s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:30:50 2022 ***              6s]m\n\n===============================================================================\n\n[#f38a5d 22GiB/43GiB(51%) CN:5 DL:36MiB ETA:9m55s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:31:51 2022 ***              9s]m\n\n===============================================================================\n\n[#f38a5d 24GiB/43GiB(56%) CN:5 DL:37MiB ETA:8m38s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:32:51 2022 ***              4s]\n\n===============================================================================\n\n[#f38a5d 26GiB/43GiB(61%) CN:5 DL:36MiB ETA:7m43s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:33:52 2022 ***              0s]\n\n===============================================================================\n\n[#f38a5d 28GiB/43GiB(66%) CN:5 DL:37MiB ETA:6m38s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:34:52 2022 ***              8s]\n\n===============================================================================\n\n[#f38a5d 31GiB/43GiB(71%) CN:5 DL:36MiB ETA:5m47s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:35:53 2022 ***              s]m\n\n===============================================================================\n\n[#f38a5d 32GiB/43GiB(75%) CN:5 DL:17MiB ETA:10m7s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:36:53 2022 ***              1s]m\n\n===============================================================================\n\n[#f38a5d 34GiB/43GiB(79%) CN:5 DL:27MiB ETA:5m31s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:37:54 2022 ***              4s]\n\n===============================================================================\n\n[#f38a5d 35GiB/43GiB(82%) CN:5 DL:29MiB ETA:4m22s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:38:55 2022 ***              5s]\n\n===============================================================================\n\n[#f38a5d 37GiB/43GiB(86%) CN:5 DL:26MiB ETA:3m36s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:39:55 2022 ***              5s]\n\n===============================================================================\n\n[#f38a5d 39GiB/43GiB(91%) CN:5 DL:36MiB ETA:1m44s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n *** Download Progress Summary as of Wed Jan 12 20:40:56 2022 ***              ]mm\n\n===============================================================================\n\n[#f38a5d 41GiB/43GiB(95%) CN:5 DL:34MiB ETA:52s]\n\nFILE: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n-------------------------------------------------------------------------------\n\n\n\n[#f38a5d 43GiB/43GiB(99%) CN:2 DL:35MiB]0m]m\n\n01/12 20:41:49 [NOTICE] Download complete: /Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n\n\nDownload Results:\n\ngid   |stat|avg speed  |path/URI\n\n======+====+===========+=======================================================\n\nf38a5d|OK  |    33MiB/s|/Users/dvanstrien/Documents/daniel/blog/_notebooks/dig19cbooks-embellishments.zip\n\n\n\nStatus Legend:\n\n(OK):download completed.\n\n\n\n\n\n!unzip -q dig19cbooks-embellishments.zip"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#install-required-packages",
    "href": "posts/post-with-code/2022-01-13-image_search.html#install-required-packages",
    "title": "Using ü§ó datasets for image search",
    "section": "Install required packages",
    "text": "Install required packages\nThere are a few packages we‚Äôll need for this work. To start with we‚Äôll need the datasets library.\n\n# hide output\nimport sys\n!{sys.executable} -m pip install datasets \n\nNow we have the data downloaded we‚Äôll try and load it into datasets. There are various ways of doing this. To start with we can grab all of the files we need.\n\nfrom pathlib import Path\n\n\nfiles = list(Path('embellishments/').rglob(\"*.jpg\"))\n\nSince the file path encodes the year of publication for the book the image came from let‚Äôs create a function to grab that.\n\ndef get_parts(f:Path):\n    _,year,fname =  f.parts\n    return year, fname"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#loading-the-images",
    "href": "posts/post-with-code/2022-01-13-image_search.html#loading-the-images",
    "title": "Using ü§ó datasets for image search",
    "section": "üì∏ Loading the images",
    "text": "üì∏ Loading the images\nThe images are fairly large, since this is an experiment we‚Äôll resize them a little using the thumbnail method (this makes sure we keep the same aspect ratio for our images)\n\nfrom PIL import Image\nimport io\n\n\ndef load_image(path):\n    with open(path, 'rb') as f:\n        im = Image.open(io.BytesIO(f.read()))\n        im.thumbnail((224,224))\n    return im \n\n\nim = load_image(files[0])\nim\n\n\n\n\n\n\n\n\n\nWhere is the image ü§î\nYou may have noticed that the load_image function doesn‚Äôt load the filepath into pillow directly. Often we would do Image.open(filepath.jpg). This is done deliberately. If we load it this way when we inspect the resulting image you‚Äôll see that the filepath attribute is empty.\n\n#collapse_output\ninspect(im)\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ\n‚îÇ ‚îÇ &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=200x224 at 0x7FBBB392D040&gt;     ‚îÇ ‚îÇ\n‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ\n‚îÇ                                                                                           ‚îÇ\n‚îÇ                app = {'APP0': b'JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00'}            ‚îÇ\n‚îÇ            applist = [('APP0', b'JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00')]          ‚îÇ\n‚îÇ               bits = 8                                                                    ‚îÇ\n‚îÇ    custom_mimetype = None                                                                 ‚îÇ\n‚îÇ      decoderconfig = (2, 0)                                                               ‚îÇ\n‚îÇ    decodermaxblock = 65536                                                                ‚îÇ\n‚îÇ      encoderconfig = (False, -1, -1, b'')                                                 ‚îÇ\n‚îÇ        encoderinfo = {}                                                                   ‚îÇ\n‚îÇ           filename = ''                                                                   ‚îÇ\n‚îÇ             format = 'JPEG'                                                               ‚îÇ\n‚îÇ format_description = 'JPEG (ISO 10918)'                                                   ‚îÇ\n‚îÇ                 fp = None                                                                 ‚îÇ\n‚îÇ             height = 224                                                                  ‚îÇ\n‚îÇ         huffman_ac = {}                                                                   ‚îÇ\n‚îÇ         huffman_dc = {}                                                                   ‚îÇ\n‚îÇ            icclist = []                                                                   ‚îÇ\n‚îÇ                 im = &lt;ImagingCore object at 0x7fbba120dc10&gt;                               ‚îÇ\n‚îÇ               info = {                                                                    ‚îÇ\n‚îÇ                          'jfif': 257,                                                     ‚îÇ\n‚îÇ                          'jfif_version': (1, 1),                                          ‚îÇ\n‚îÇ                          'jfif_unit': 0,                                                  ‚îÇ\n‚îÇ                          'jfif_density': (1, 1)                                           ‚îÇ\n‚îÇ                      }                                                                    ‚îÇ\n‚îÇ              layer = [(1, 2, 2, 0), (2, 1, 1, 1), (3, 1, 1, 1)]                           ‚îÇ\n‚îÇ             layers = 3                                                                    ‚îÇ\n‚îÇ                map = None                                                                 ‚îÇ\n‚îÇ               mode = 'RGB'                                                                ‚îÇ\n‚îÇ            palette = None                                                                 ‚îÇ\n‚îÇ           pyaccess = None                                                                 ‚îÇ\n‚îÇ       quantization = {                                                                    ‚îÇ\n‚îÇ                          0: [                                                             ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              1,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              6,                                                           ‚îÇ\n‚îÇ                              7,                                                           ‚îÇ\n‚îÇ                              9,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              4,                                                           ‚îÇ\n‚îÇ                              8,                                                           ‚îÇ\n‚îÇ                              8,                                                           ‚îÇ\n‚îÇ                              8,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              6,                                                           ‚îÇ\n‚îÇ                              8,                                                           ‚îÇ\n‚îÇ                              10,                                                          ‚îÇ\n‚îÇ                              8,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              4,                                                           ‚îÇ\n‚îÇ                              7,                                                           ‚îÇ\n‚îÇ                              12,                                                          ‚îÇ\n‚îÇ                              11,                                                          ‚îÇ\n‚îÇ                              9,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              5,                                                           ‚îÇ\n‚îÇ                              8,                                                           ‚îÇ\n‚îÇ                              10,                                                          ‚îÇ\n‚îÇ                              15,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              11,                                                          ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              5,                                                           ‚îÇ\n‚îÇ                              8,                                                           ‚îÇ\n‚îÇ                              9,                                                           ‚îÇ\n‚îÇ                              11,                                                          ‚îÇ\n‚îÇ                              15,                                                          ‚îÇ\n‚îÇ                              16,                                                          ‚îÇ\n‚îÇ                              13,                                                          ‚îÇ\n‚îÇ                              7,                                                           ‚îÇ\n‚îÇ                              9,                                                           ‚îÇ\n‚îÇ                              11,                                                          ‚îÇ\n‚îÇ                              12,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              17,                                                          ‚îÇ\n‚îÇ                              17,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              10,                                                          ‚îÇ\n‚îÇ                              13,                                                          ‚îÇ\n‚îÇ                              13,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              16,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14                                                           ‚îÇ\n‚îÇ                          ],                                                               ‚îÇ\n‚îÇ                          1: [                                                             ‚îÇ\n‚îÇ                              2,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              7,                                                           ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              4,                                                           ‚îÇ\n‚îÇ                              9,                                                           ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              3,                                                           ‚îÇ\n‚îÇ                              4,                                                           ‚îÇ\n‚îÇ                              8,                                                           ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              7,                                                           ‚îÇ\n‚îÇ                              9,                                                           ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14,                                                          ‚îÇ\n‚îÇ                              14                                                           ‚îÇ\n‚îÇ                          ]                                                                ‚îÇ\n‚îÇ                      }                                                                    ‚îÇ\n‚îÇ           readonly = 0                                                                    ‚îÇ\n‚îÇ               size = (200, 224)                                                           ‚îÇ\n‚îÇ               tile = []                                                                   ‚îÇ\n‚îÇ              width = 200                                                                  ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nYou can also directly see this\n\nim.filename\n\n''\n\n\nPillow usually loads images in a lazy way i.e.¬†it only opens them when they are needed. The filepath is used to access the image. We can see the filename attribute is present if we open it from the filepath\n\nim_file = Image.open(files[0])\nim_file.filename\n\n'/Users/dvanstrien/Documents/daniel/blog/_notebooks/embellishments/1855/000811462_05_000205_1_The Pictorial History of England  being a history of the people  as well as a hi_1855.jpg'\n\n\nThe reason I don‚Äôt want the filename attribute present here is because not only do I want to use datasets to process our images but also store the images. If we pass a Pillow object with the filename attribute datasets will also use this for loading the images. This is often what we‚Äôd want but we don‚Äôt want this here for reasons we‚Äôll see shortly.\n\n\nPreparing images for datasets\nWe can now load our images. What we‚Äôll do is is loop through all our images and then load the information for each image into a dictionary.\n\nfrom collections import defaultdict\n\n\ndata = defaultdict(list)\n\n\nfrom tqdm import tqdm\n\n\nfor file in tqdm(files):\n    try:\n        #load_image(file)\n        year, fname = get_parts(file)\n        data['fname'].append(fname)\n        data['year'].append(year)\n        data['path'].append(str(file))\n    except:\n        Image.UnidentifiedImageError\n        pass\n        \n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 416944/416944 [00:05&lt;00:00, 77169.45it/s]\n\n\nWe can now load the from_dict method to create a new dataset.\n\nfrom datasets import Dataset\n\n\ndataset = Dataset.from_dict(data)\n\nWe can look at one example to see what this looks like.\n\ndataset[0]\n\n{'fname': '000811462_05_000205_1_The Pictorial History of England  being a history of the people  as well as a hi_1855.jpg',\n 'year': '1855',\n 'path': 'embellishments/1855/000811462_05_000205_1_The Pictorial History of England  being a history of the people  as well as a hi_1855.jpg'}\n\n\n\n\nLoading our images\nAt the moment our dataset has the filename and full path for each image. However, we want to have an actual image loaded into our dataset. We already have a load_image function. This gets us most of the way there but we might also want to add some ability to deal with image errors. The datasets library has gained increased uspport for handling None types- this includes support for None types for images see pull request 3195.\nWe‚Äôll wrap our load_image function in a try block, catch a Image.UnidentifiedImageError error and return None if we can‚Äôt load the image.\n\ndef try_load_image(filename):\n    try:\n        image = load_image(filename)\n        if isinstance(image, Image.Image):\n            return image\n    except Image.UnidentifiedImageError:\n        return None\n\n\n%%time\ndataset = dataset.map(lambda example: {\"img\": try_load_image(example['path'])},writer_batch_size=50)\n\n\n\n\nCPU times: user 51min 42s, sys: 4min 31s, total: 56min 13s\nWall time: 1h 10min 31s\n\n\nLet‚Äôs see what this looks like\n\ndataset\n\nDataset({\n    features: ['fname', 'year', 'path', 'img'],\n    num_rows: 416944\n})\n\n\nWe have an image column, let‚Äôs check the type of all our features\n\ndataset.features\n\n{'fname': Value(dtype='string', id=None),\n 'year': Value(dtype='string', id=None),\n 'path': Value(dtype='string', id=None),\n 'img': Image(id=None)}\n\n\nThis is looking great already. Since we might have some None types for images let‚Äôs get rid of these.\n\ndataset = dataset.filter(lambda example: example['img'] is not None)\n\n\n\n\n\ndataset\n\nDataset({\n    features: ['fname', 'year', 'path', 'img'],\n    num_rows: 416935\n})\n\n\nYou‚Äôll see we lost a few rows by doing this filtering. We should now just have images which are successfully loaded.\nIf we access an example and index into the img column we‚Äôll see our image üòÉ\n\ndataset[10]['img']"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#push-all-the-things-to-the-hub",
    "href": "posts/post-with-code/2022-01-13-image_search.html#push-all-the-things-to-the-hub",
    "title": "Using ü§ó datasets for image search",
    "section": "Push all the things to the hub!",
    "text": "Push all the things to the hub!\n\n\n\nPush all the things to the hub!\n\n\nOne of the super awesome things about the huggingface ecosystem is the huggingface hub. We can use the hub to access models and datasets. Often this is used for sharing work with others but it can also be a useful tool for work in progress. The datasets library recently added a push_to_hub method that allows you to push a dataset to the hub with minimal fuss. This can be really helpful by allowing you to pass around a dataset with all the transformers etc. already done.\nWhen I started playing around with this feature I was also keen to see if it could be used as a way of ‚Äòbundling‚Äô everything together. This is where I noticed that if you push a dataset containing images which have been loaded in from filepaths by pillow the version on the hub won‚Äôt have the images attached. If you always have the image files in the same place when you work with the dataset then this doesn‚Äôt matter. If you want to have the images stored in the parquet file(s) associated with the dataset we need to load it without the filename attribute present (there might be another way of ensuring that datasets doesn‚Äôt rely on the image file being on the file system ‚Äì if you of this I‚Äôd love to hear about it).\nSince we loaded our images this way when we download the dataset from the hub onto a different machine we have the images already there ü§ó\nFor now we‚Äôll push the dataset to the hub and keep them private initially.\n\ndataset.push_to_hub('davanstrien/embellishments', private=True)\n\nThe repository already exists: the `private` keyword argument will be ignored.\n\n\n\n\n\n\n\n\n\nSwitching machines\nAt this point I‚Äôve created a dataset and moved it to the huggingface hub. This means it is possible to pickup the work/dataset elsewhere.\nIn this particular example, having access to a GPU is important. So the next parts of this notebook are run on Colab instead of locally on my laptop.\nWe‚Äôll need to login since the dataset is currently private.\n\n!huggingface-cli login\n\n\n        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n\n        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n\n        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n\n        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n\n        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n\n\n        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n\n        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n\n        \n\nToken: \n\nLogin successful\n\nYour token has been saved to /root/.huggingface/token\n\nAuthenticated through git-credential store but this isn't the helper defined on your machine.\n\nYou might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n\n\n\ngit config --global credential.helper store\n\n\n\n\nOnce we‚Äôve done this we can load our dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"davanstrien/embellishments\", use_auth_token=True)\n\n\n\n\nUsing custom data configuration davanstrien--embellishments-543da8e15e8f0242\n\n\nDownloading and preparing dataset None/None (download: 2.38 GiB, generated: 2.50 GiB, post-processed: Unknown size, total: 4.88 GiB) to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-543da8e15e8f0242/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-543da8e15e8f0242/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121. Subsequent calls will reuse this data."
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#creating-embeddings",
    "href": "posts/post-with-code/2022-01-13-image_search.html#creating-embeddings",
    "title": "Using ü§ó datasets for image search",
    "section": "Creating embeddings üï∏",
    "text": "Creating embeddings üï∏\nWe now have a dataset with a bunch of images in it. To begin creating our image search app we need to create some embeddings for these images. There are various ways in which we can try and do this but one possible way is to use the clip models via the sentence_transformers library. The clip model from OpenAI learns a joint representation for both images and text which is very useful for what we want to do since we want to be able to input text and get back an image. We can download the model using the SentenceTransformer class.\n\nfrom sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('clip-ViT-B-32')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n\n\nThis model will encode either an image or some text returning an embedding. We can use the map method to encode all our images.\n\nds_with_embeddings = dataset.map(\n    lambda example: {'embeddings':model.encode(example['img'],device='cuda')},\n                                 batch_size=32)\n\n\n\n\nWe can ‚Äúsave‚Äù our work by pushing back to the hub\n\nds_with_embeddings.push_to_hub('davanstrien/embellishments', private=True)\n\nPushing split train to the Hub.\nThe repository already exists: the `private` keyword argument will be ignored.\n\n\n\n\n\n\n\n\nIf we were to move to a different machine we could grab our work again by loading it from the hub üòÉ\n\nfrom datasets import load_dataset\n\nds_with_embeddings = load_dataset(\"davanstrien/embellishments\", use_auth_token=True)\n\n\n\n\nUsing custom data configuration davanstrien--embellishments-c2c1f142f272db02\n\n\nDownloading and preparing dataset None/None (download: 3.19 GiB, generated: 3.30 GiB, post-processed: Unknown size, total: 6.49 GiB) to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-c2c1f142f272db02/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/davanstrien--embellishments-c2c1f142f272db02/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121. Subsequent calls will reuse this data.\n\n\n\n\n\nWe now have a new column which contains the embeddings for our images. We could manually search through these and compare it to some input embedding but datasets has an add_faiss_index method. This uses the faiss library to create an efficient index for searching embeddings. For more background on this library you can watch this youtube video\n\n\n\nds_with_embeddings['train'].add_faiss_index(column='embeddings')\n\n\n\n\nDataset({\n    features: ['fname', 'year', 'path', 'img', 'embeddings'],\n    num_rows: 416935\n})"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#image-search",
    "href": "posts/post-with-code/2022-01-13-image_search.html#image-search",
    "title": "Using ü§ó datasets for image search",
    "section": "Image search",
    "text": "Image search\nWe now have everything we need to create a simple image search. We can use the same model we used to encode our images to encode some input text. This will act as the prompt we try and find close examples for. Let‚Äôs start with ‚Äòa steam engine‚Äô.\n\nprompt = model.encode(\"A steam engine\")\n\nWe can see what this looks like\n\n#collapse_output\nprompt\n\narray([-4.68399227e-02, -1.25237599e-01,  1.25164926e-01,  1.23583399e-01,\n        5.36684394e-02, -2.80560672e-01,  2.89631691e-02, -9.63450074e-01,\n       -1.52872965e-01, -3.83016393e-02,  9.01967064e-02, -5.84575422e-02,\n        1.04646191e-01,  2.44443744e-01,  1.38233244e-01, -3.97525132e-02,\n        4.35137331e-01, -4.26820181e-02, -8.48560631e-02, -6.94137365e-02,\n        6.25562131e-01,  3.68572891e-01,  3.34365219e-01, -3.37864846e-01,\n       -2.53632635e-01, -3.01467925e-01, -1.56484097e-01,  2.94869483e-01,\n       -1.89204350e-01, -1.13111593e-01, -1.46938376e-02,  2.97405511e-01,\n       -2.92487741e-01,  3.56931627e-01,  1.44009173e-01,  7.53008351e-02,\n       -1.02462962e-01,  2.26309776e-01, -3.77506733e-01,  4.75439876e-02,\n       -8.52131844e-03,  7.40285963e-03, -1.36876494e-01,  1.12041593e-01,\n        3.65501434e-01, -9.37360153e-02,  1.00782610e-01, -3.86462212e-01,\n       -1.39045209e-01, -2.31989667e-01, -2.62126565e-01,  8.75059143e-02,\n       -9.87479314e-02,  7.31039718e-02, -5.99793851e-01, -5.31058311e-01,\n        1.46116272e-01,  1.58094600e-01, -5.31955510e-02,  1.91384852e-01,\n        1.16943717e-01, -4.84316409e-01, -1.38332695e-01,  1.76510841e-01,\n       -2.17938051e-01, -1.00890748e-01, -4.45051998e-01,  2.71521568e-01,\n       -1.12926617e-01, -3.37198257e-01, -1.34169117e-01, -7.38745630e-02,\n       -1.23398125e-01,  3.62316787e-01,  9.09636840e-02, -3.20305794e-01,\n        5.82561374e-01, -3.51719618e-01, -1.05368085e-02, -3.90766770e-01,\n       -3.18382740e-01,  5.37567735e-02, -6.24650240e-01,  2.18755245e-01,\n        3.51645321e-01, -3.01214606e-02, -8.49011913e-02, -3.29971045e-01,\n        2.13861912e-01, -1.10820271e-02, -3.52595486e-02, -3.70746814e-02,\n       -1.35805202e+00,  3.35692495e-01, -2.83742435e-02, -1.39813796e-01,\n        3.66676860e-02,  2.62957454e-01,  2.52151459e-01, -6.14355244e-02,\n        2.01516539e-01, -4.14117992e-01, -2.58466527e-02,  1.06067717e-01,\n        3.14981639e-02, -1.45749748e-02, -5.94865866e-02,  2.55122900e-01,\n       -3.30369681e-01,  6.39781356e-04,  1.65513411e-01,  7.37893358e-02,\n       -4.69729975e-02,  3.36943477e-01,  4.38236594e-02, -4.21047479e-01,\n       -1.14590853e-01,  1.49240956e-01,  1.34405270e-01,  3.97198983e-02,\n       -1.20852023e-01, -7.22009778e-01,  1.17442548e-01, -7.35135227e-02,\n        5.45979321e-01,  1.76602621e-02,  6.59747049e-02,  8.00846070e-02,\n        3.87920737e-01, -3.57501693e-02,  1.19425125e-01, -2.89906412e-01,\n       -2.84183323e-02,  5.73142338e+00,  1.24172762e-01, -1.59575850e-01,\n       -5.33452034e-02, -1.77120879e-01,  2.14188576e-01, -3.49292234e-02,\n       -4.76958305e-02, -1.05941862e-01, -1.58911452e-01,  1.87136307e-02,\n       -2.16531213e-02,  1.37230158e-01,  4.62583750e-02,  2.19857365e-01,\n        3.41235586e-02, -3.29913348e-02,  9.88523886e-02, -1.30611554e-01,\n       -1.53349772e-01,  2.20886514e-01,  1.53534949e-01, -4.27889526e-01,\n       -4.12531018e-01,  2.70397663e-01,  1.88448757e-01,  4.66853082e-02,\n        2.63707846e-01, -9.56512764e-02, -3.26435685e-01, -1.24463499e-01,\n        4.49354291e-01, -4.17843968e-01, -5.27932420e-02, -1.28314078e-01,\n       -1.19249836e-01, -1.19294032e-01,  3.73742878e-01,  2.07954675e-01,\n       -1.41953439e-01,  3.89361024e-01, -1.99988037e-01,  3.62350583e-01,\n       -8.77851099e-02, -1.08132876e-01, -9.82177258e-03,  1.80039972e-01,\n        1.35815665e-02,  3.20201695e-01, -1.74580999e-02, -1.08204901e-01,\n       -2.29793668e-01, -2.09628209e-01,  4.13929313e-01, -1.73814282e-01,\n       -4.10574347e-01, -1.59104809e-01, -6.01581074e-02,  6.22577034e-02,\n       -3.67693931e-01,  1.85215116e-01, -2.03229636e-01, -8.92911255e-02,\n       -4.25831258e-01, -1.45366028e-01,  2.45514482e-01, -1.65927559e-01,\n       -2.54413635e-02, -2.91361034e-01, -8.33243579e-02, -4.79405448e-02,\n        6.35769814e-02,  8.04642588e-02,  5.31384498e-02,  2.50850171e-02,\n       -8.98692310e-02,  4.97757077e-01,  6.37893498e-01, -2.58815974e-01,\n        4.14507166e-02,  9.45882648e-02, -9.01474580e-02, -9.18833911e-02,\n       -2.48883665e-01,  9.16991904e-02, -2.93194801e-01, -1.49350330e-01,\n        7.20755905e-02, -9.76985693e-03, -4.70465049e-02, -2.78597653e-01,\n       -7.63949528e-02, -3.14843357e-01,  3.18657011e-01, -3.06758255e-01,\n       -2.06573829e-01, -2.20574200e-01,  1.81351285e-02,  2.57636189e-01,\n        2.39799708e-01, -2.31798366e-01, -8.34087562e-03,  6.13241374e-01,\n       -2.10393399e-01,  2.52263397e-01,  1.66839644e-01, -2.71174073e-01,\n        2.31348664e-01,  1.15150154e-01,  2.23357946e-01,  1.37287825e-01,\n       -8.56669843e-02,  3.43877286e-01, -1.09687179e-01,  3.24211985e-01,\n       -4.53893900e-01, -2.30711773e-01, -2.48840563e-02,  1.80964172e-01,\n        4.73472506e-01,  5.22104502e-01,  9.96741354e-02,  1.87694326e-01,\n        2.41730541e-01, -2.78556377e-01,  7.48419687e-02,  2.80560136e-01,\n       -1.25464931e-01,  1.51028201e-01,  1.39490321e-01,  5.16689643e-02,\n        5.30310348e-02,  1.61938250e-01,  3.72225225e-01, -4.49403644e-01,\n        1.19608052e-01,  2.43661910e-01,  9.89501849e-02,  2.74168640e-01,\n        4.84039634e-02, -1.19901955e-01, -1.57916725e-01, -2.20868304e-01,\n        1.03498720e-01,  3.99750322e-01,  1.03758566e-01,  8.08660090e-02,\n        1.68566346e-01, -3.42532575e-01,  2.51480471e-02,  1.23976640e-01,\n       -2.10433707e-01,  2.81242996e-01,  2.39082754e-01,  2.01786831e-02,\n        4.61297363e-01,  5.62884361e-02,  2.15039015e-01, -1.65275872e-01,\n        1.01690084e-01, -4.50959802e-03, -4.46137577e-01,  4.31368239e-02,\n       -4.51804757e-01, -2.26415813e-01,  1.31732523e-01, -2.00945437e-02,\n        1.77461311e-01, -1.64631978e-02,  4.40553159e-01,  1.41214132e-01,\n        3.42677176e-01, -2.23303795e-01, -2.10693538e-01,  1.94943929e-03,\n       -2.33348235e-01,  4.64889407e-03,  5.71020804e-02,  1.99669391e-01,\n        5.72273111e+00, -2.95036316e-01, -5.13455391e-01,  1.87334672e-01,\n        4.09545094e-01, -7.09135592e-01,  1.89325869e-01, -6.14660345e-02,\n        3.29098284e-01,  2.82059342e-01,  3.48631829e-01, -9.74263549e-02,\n       -4.83064592e-01, -1.35906041e-04,  3.44773471e-01, -3.56532484e-01,\n        5.36619090e-02, -1.85481656e+00,  3.87955368e-01, -1.83132842e-01,\n       -1.34021699e-01, -1.84214741e-01,  6.85371086e-02,  1.10808179e-01,\n       -6.64586425e-02,  6.85550272e-02,  1.81145087e-01, -2.15605676e-01,\n       -1.09192222e-01, -7.09795505e-02,  1.77813157e-01, -2.76037157e-01,\n        2.19184965e-01, -3.35977226e-01,  1.01434961e-01,  4.24576849e-02,\n        6.37579709e-04, -1.23296835e-01, -6.84914351e-01,  5.02923191e-01,\n        2.19384342e-01,  4.92008686e-01, -1.94621727e-01, -2.48740703e-01,\n       -1.32586688e-01, -1.77171156e-02, -4.71081585e-03,  1.58246011e-01,\n       -3.27363521e-01, -3.30681592e-01, -2.68038437e-02, -1.85811728e-01,\n       -1.84623767e-02, -3.22798610e-01,  3.07092518e-01,  1.06014945e-01,\n        3.20541680e-01, -2.55453944e-01, -2.30755419e-01, -1.19963072e-01,\n       -2.04865620e-01,  4.02828932e-01, -3.01321566e-01,  4.01021272e-01,\n       -3.02002877e-01,  1.42853945e-01,  2.94484437e-01, -2.06042349e-01,\n       -3.03069353e-01, -2.83185482e-01, -1.03388466e-02, -1.03018671e-01,\n        4.25990820e-02, -2.94244856e-01,  3.19168091e-01,  3.89839858e-02,\n       -1.95185751e-01, -9.88216847e-02, -4.01682496e-01,  4.60841119e-01,\n        1.40236557e-01,  1.49914265e-01, -4.25037295e-01,  2.63067722e-01,\n        1.31706342e-01,  3.21884871e-01, -2.39963964e-01,  4.01636630e-01,\n       -2.55293436e-02, -7.36447945e-02, -8.34826380e-03,  1.11923724e-01,\n       -2.71807779e-02, -3.35412771e-02,  2.33933121e-01,  3.33954431e-02,\n        3.56481314e-01, -8.09433609e-02, -1.82573602e-01,  1.75429478e-01,\n       -3.23554099e-01,  9.15928558e-03,  1.54344559e-01,  2.50909716e-01,\n        1.45193070e-01,  2.48686507e-01, -9.65276286e-02, -2.73654372e-01,\n        5.46456315e-02,  1.83476061e-02, -1.61773548e-01, -2.97708124e-01,\n       -1.74462572e-01, -1.14246726e-01,  2.32043359e-02,  1.98346555e-01,\n        2.31929243e-01, -9.74937603e-02, -2.26448864e-01, -6.31427839e-02,\n        2.23113708e-02, -3.72859359e-01,  2.47197479e-01, -3.65516663e-01,\n       -3.24409932e-01,  1.83964625e-01, -3.17104161e-03, -2.66632497e-01,\n       -1.86478943e-01,  1.11006252e-01, -3.93829793e-02, -3.11926544e-01,\n        2.88751245e-01,  2.66543150e-01, -1.74334750e-01, -4.89967108e-01,\n        3.38638097e-01,  2.47487854e-02, -3.66539627e-01,  5.78703731e-03,\n        1.11349493e-01, -2.60909855e-01, -4.34429348e-02, -4.47440267e-01,\n        2.80311018e-01, -6.46181554e-02, -2.93976814e-02, -3.02857161e-01,\n        2.10391358e-03, -3.70345414e-02,  7.15476647e-02,  4.39802915e-01,\n        2.11817563e-01,  6.87709302e-02,  5.68117499e-01, -2.40518659e-01,\n        2.59056687e-01, -1.32284269e-01,  1.28509507e-01, -1.94875181e-01,\n       -2.68568173e-02, -7.85035193e-02, -2.49556839e-01,  1.44016743e-01,\n       -2.98127495e-02, -1.41643599e-01,  1.77106410e-02,  1.83453292e-01,\n       -1.39113069e-02, -1.97993904e-01,  3.07995021e-01,  3.31339300e-01,\n        2.07652867e-01,  1.27762616e-01,  2.26422980e-01,  1.94940835e-01,\n       -4.90801185e-02, -5.35061479e-01, -2.99495637e-01,  3.68627608e-02,\n       -4.15636569e-01,  6.44698918e-01, -4.50457260e-02,  7.05935210e-02,\n       -1.11036956e-01, -1.42384216e-01, -7.05560222e-02,  2.86495592e-03,\n        3.45641613e-01, -5.66974521e-01, -1.34682715e-01, -2.59017110e-01,\n        3.27597320e-01,  1.04890786e-01, -3.11988890e-01, -2.32627541e-01,\n        3.14653963e-02,  2.76591361e-01,  1.66302443e-01, -2.39517853e-01],\n      dtype=float32)\n\n\nWe can use another method from the datasets library get_nearest_examples to get images which have an embedding close to our input prompt embedding. We can pass in a number of results we want to get back.\n\nscores, retrieved_examples = ds_with_embeddings['train'].get_nearest_examples('embeddings', prompt,k=9)\n\nWe can index into the first example this retrieves:\n\nretrieved_examples['img'][0]\n\n\n\n\n\n\n\n\nThis isn‚Äôt quite a steam engine but it‚Äôs also not a completely weird result. We can plot the other results to see what was returned.\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(20, 20))\ncolumns = 3\nfor i in range(9):\n    image = retrieved_examples['img'][i]\n    plt.subplot(9 / columns + 1, columns, i + 1)\n    plt.imshow(image)\n\n\n\n\n\n\n\n\nSome of these results look fairly close to our input prompt. We can wrap this in a function so can more easily play around with different prompts\n\ndef get_image_from_text(text_prompt, number_to_retrieve=9):\n    prompt = model.encode(text_prompt)\n    scores, retrieved_examples = ds_with_embeddings['train'].get_nearest_examples('embeddings', prompt,k=number_to_retrieve)\n    plt.figure(figsize=(20, 20))\n    columns = 3\n    for i in range(9):\n        image = retrieved_examples['img'][i]\n        plt.title(text_prompt)\n        plt.subplot(9 / columns + 1, columns, i + 1)\n        plt.imshow(image)\n\n\nget_image_from_text(\"An illustration of the sun behind a mountain\")\n\n\n\n\n\n\n\n\n\nTrying a bunch of prompts ‚ú®\nNow we have a function for getting a few results we can try a bunch of different prompts:\n\nFor some of these I‚Äôll choose prompts which are a broad ‚Äòcategory‚Äô i.e.¬†‚Äòa musical instrument‚Äô or ‚Äòan animal‚Äô, others are specific i.e.¬†‚Äòa guitar‚Äô.\nOut of interest I also tried a boolean operator: ‚ÄúAn illustration of a cat or a dog‚Äù.\nFinally I tried something a little more abstract: ‚Äúan empty abyss‚Äù\n\n\nprompts = [\"A musical instrument\", \"A guitar\", \"An animal\", \"An illustration of a cat or a dog\", \"an empty abyss\"]\n\n\nfor prompt in prompts:\n    get_image_from_text(prompt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see these results aren‚Äôt always right but they are usually some reasonable results in there. It already seems like this could be useful for searching for a the semantic content of an image in this dataset. However we might hold off on sharing this as is‚Ä¶"
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#creating-a-huggingface-space",
    "href": "posts/post-with-code/2022-01-13-image_search.html#creating-a-huggingface-space",
    "title": "Using ü§ó datasets for image search",
    "section": "Creating a huggingface space? ü§∑üèº",
    "text": "Creating a huggingface space? ü§∑üèº\nOne obvious next step for this kind of project is to create a hugginface spaces demo. This is what I‚Äôve done for other models\nIt was a fairly simple process to get a Gradio app setup from the point we got to here. Here is a screenshot of this app.\n\nHowever, I‚Äôm a little bit vary about making this public straightaway. Looking at the model card for the CLIP model we can look at the primary intended uses:\n\nPrimary intended uses\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models. source\n\nThis is fairly close to what we are interested in here. Particularly we might be interested in how well the model deals with the kinds of images in our dataset (illustrations from mostly 19th century books). The images in our dataset are (probably) fairly different from the training data. The fact that some of the images also contain text might help CLIP since it displays some OCR ability.\nHowever, looking at the out-of-scope use cases in the model card:\n\nOut-of-Scope Use Cases\nAny deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP‚Äôs performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. source\n\nsuggests that ‚Äòdeployment‚Äô is not a good idea. Whilst the results I got are interesting I haven‚Äôt played around with the model enough yet (and haven‚Äôt done anything more systematic to evaluate its performance and biases). Another additional consideration is the target dataset itself. The images are drawn from books covering a variety of subjects and time periods. There are plenty of books which represent colonial attitudes and as a result some of the images included may represent certain groups of people in a negative way. This could potentially be a bad combo with a tool which allows any arbitrary text input to be encoded as a prompt.\nThere may be ways around this issue but this will require a bit more thought."
  },
  {
    "objectID": "posts/post-with-code/2022-01-13-image_search.html#conclusion",
    "href": "posts/post-with-code/2022-01-13-image_search.html#conclusion",
    "title": "Using ü§ó datasets for image search",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough I don‚Äôt have a nice demo to show for it I did get to work out a few more details of how datasets handles images. I‚Äôve already used it to train some classification models and everything seems to be working smoothly. The ability to push images around on the hub will be super useful for many use cases too.\nI plan to spend a bit more time thinking about whether there is a better way of sharing a clip powered image search for the BL book images or not‚Ä¶\n{{ ‚ÄúIf you aren‚Äôt familiar with datasets. A feature represents the datatype for different data you can have inside a dataset. For example you my have int32, timestamps and strings. You can read more about how features work in the docs‚Äù | fndetail: 1}}"
  },
  {
    "objectID": "posts/plain-text/2024-05-15-self-instruct.html",
    "href": "posts/plain-text/2024-05-15-self-instruct.html",
    "title": "Synthetic dataset generation techniques: Self-Instruct",
    "section": "",
    "text": "This post is part of a series on synthetic data generation techniques. You may also want to check out Awesome Synthetic (text) datasets, where I will be collecting these posts.\nFor training an LLM to be better at following instructions or functioning as a chat model, you usually want a dataset with some combination of instructions and responses. Since creating this data by hand can be very time-consuming, more people are using LLMs to generate this data.\nIn its simplest form, you could create synthetic instruction following datasets using an LLM to generate responses to handwritten prompts/instructions. However, for many applications, there are a considerable number of prompts you may want to have in your final datasets. Creating all this data by hand will be challenging while ensuring its diversity. There are various ways in which you can try to remove this bottleneck.\nIn this blog post, I‚Äôll discuss the technique outlined in the paper SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions, which, as the paper‚Äôs title suggests, aims to overcome the need to generate instructions by hand.\nThe paper outlines a pipeline that bootstraps from an initial seed dataset of instructions to a larger dataset of synthetically generated instructions.\nThe authors include steps for generating instructions and a filtering step to clean up the data in the paper. Since our goal is to focus on the core technique of a particular paper, we‚Äôll focus only on the instruction generation part. This step can also be combined with other approaches to data filtering that have been introduced since this paper was published (or your own custom filters)."
  },
  {
    "objectID": "posts/plain-text/2024-05-15-self-instruct.html#instruction-generation",
    "href": "posts/plain-text/2024-05-15-self-instruct.html#instruction-generation",
    "title": "Synthetic dataset generation techniques: Self-Instruct",
    "section": "Instruction Generation",
    "text": "Instruction Generation\nReturning to our original challenge: how do we generate instructions without writing them all by hand? As you can see in the diagram above, the steps involve sampling from the original seeds, filtering the seed tasks to see if they are classification tasks or not, and then generating the new instructions. After the new instructions are generated, they are filtered and added to the task pool. In this way, you can keep creating new instructions from your initial seed tasks and growing the seed task pool. Using the data filtering steps aims to ensure you still have diversity in the prompts and avoid adding very repetitive instructions to your dataset.\n\nWhat does this look like in practice?\nLet‚Äôs take a look at an example from the 175 initial seeds task dataset:\n{\"id\": \"seed_task_0\",\n\"name\": \"breakfast_suggestion\",\n\"instruction\": \"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\",\n\"instances\": [{\"input\": \"\", \"output\": \"Yes, you can have 1 oatmeal banana protein shake and 4 strips of bacon. The oatmeal banana protein shake may contain 1/2 cup oatmeal, 60 grams whey protein powder, 1/2 medium banana, 1tbsp flaxseed oil and 1/2 cup watter, totalling about 550 calories. The 4 strips of bacon contains about 200 calories.\"}],\n\"is_classification\": false}\nAs you can see, this row contains fairly standard instructions, such as ‚ÄúIs there anything‚Ä¶‚Äù, some responses (i.e., the instances field), and a label indicating if it‚Äôs a classification task. The paper outlines two main approaches to generating new instructions from this data. If an instruction is a classification task, one prompting approach is used; if it‚Äôs a standard generation task, another prompt is used. Let‚Äôs start with how the extra nonclassification task instructions prompt looks:\n&gt; Come up with a series of tasks:\n&gt;\n&gt; Task 1: {instruction for existing task 1}\n&gt; Task 2: {instruction for existing task 2}\n&gt; Task 3: {instruction for existing task 3}\n&gt; Task 4: {instruction for existing task 4}\n&gt; Task 5: {instruction for existing task 5}\n&gt; Task 6: {instruction for existing task 6}\n&gt; Task 7: {instruction for existing task 7}\n&gt; Task 8: {instruction for existing task 8}\n&gt; Task 9:\nAs you can see, the prompt gives the llm some examples of tasks and encourages the model to generate new instructions. A crucial detail to note is in the original paper; the authors used GPT3 rather than an instruction-tuned/chat model. Since this is not an instruction-tuned model, a prompt that gives a few examples in a structured format can often be better for guiding the model toward a useful set of generations.\nWe can see what this process looks like in practice (using huggingface_hub and the BigScience Bloom model in place of the GPT-3):\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient('bigscience/bloom')\n\ndef encode_prompt(prompt_instructions, classification=False):\n    \"\"\"Encode multiple prompt instructions into a single string.\"\"\"\n    if classification:\n        prompt = \"Come up with a series of classification tasks. Try to specify the possible output labels when possible.\\n\"\n    else:\n        prompt = \"Come up with a series of tasks:\\n\"\n    for idx, instruction in enumerate(prompt_instructions):\n        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n        prompt += f\"{idx+1}. {instruction}\\n\"\n    prompt += f\"{len(prompt_instructions) + 1}.\"\n    return prompt\n\nprompt = encode_prompt(dataset['instruction']) #\nFor a non-classification task, this produces a prompt which looks like this:\nCome up with a series of tasks:\n1. Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\n2. What is the relation between the given pairs?\n3. Generate a one-sentence description for each of the following people.\n4. Describe a situation in which the given stereotype can harm you.\n5. Generate an appropriate subjective title for the following email\n6. How do you answer this question in a job interview?\n7. Brainstorm a list of possible New Year's resolutions.\n8. Explain the following idiom to me, and try to give me some examples.\n9.\nWe can then pass this prompt to an LLM.\nclient.text_generation(prompt, return_full_text=False, temperature=0.7, max_new_tokens=512)\n&gt;&gt;&gt;  Think of a time when you were incredibly confident, and explain why.\\n10. What is the difference between a real and normal friend?\nWe can see the LLM responds with new instructions (we also get some extra text from the LLM). If we wanted to use this in practice, we could do more work to optimize the generation parameters (temperature, etc.).\nThe process and prompts for the text classification tasks are slightly different. To try to avoid the LLM just responding with the label token, they put the label first and then show the text that generated that label i.e.¬†something like this:\nInstruction: Find out if the given text is positive about the company discussed.\nClass Label: Positive\nInput: Hugging Face is a wonderful platform for machine learning developers.\nnoteyou can find a notebook with these examples here."
  },
  {
    "objectID": "posts/plain-text/2024-05-15-self-instruct.html#using-self-instruct",
    "href": "posts/plain-text/2024-05-15-self-instruct.html#using-self-instruct",
    "title": "Synthetic dataset generation techniques: Self-Instruct",
    "section": "Using Self Instruct",
    "text": "Using Self Instruct\nThis paper has had¬†a very big¬†impact¬†both¬†in academic research (over 1,000 citations and in¬†the practical adoption of the method¬†by the community (you can find some datasets citing the method here.\nThere are several implementations of the Self Instruct method:\n\nOfficial GitHub repository: https://github.com/yizhongw/self-instruct\nDistilabel implementation\nairoboros: a modified version of self instruct.\n\nIn practice, most uses of this approach have moved away from strictly following the prompts/approach outlined in the paper. Since the quality of open and closed-source instruction following models has significantly improved since his paper was published, it often makes more sense to use this to prompt a model to generate new instructions more directly.\nWhile the exact approach outlined in the paper is often adapted, the paper is still helpful for better understanding how to approach synthetic data generation."
  },
  {
    "objectID": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html",
    "href": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html",
    "title": "Synthetic dataset generation techniques: generating custom sentence similarity data",
    "section": "",
    "text": "This post is part of a series on synthetic data generation techniques. You may also want to check out Awesome Synthetic (text) datasets, where I will be collecting these posts.\nOne of the most exciting use cases for LLMs is generating synthetic datasets that can be used to train non-LLM models. In the past, gathering enough data was one of the most significant barriers to training task-specific models, such as text classification models. LLMs can potentially help in this area."
  },
  {
    "objectID": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#creating-data-for-training-and-fine-tuning-embedding-models-using-llms",
    "href": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#creating-data-for-training-and-fine-tuning-embedding-models-using-llms",
    "title": "Synthetic dataset generation techniques: generating custom sentence similarity data",
    "section": "Creating data for training and fine-tuning embedding models using LLMs?",
    "text": "Creating data for training and fine-tuning embedding models using LLMs?\nOne area where synthetic data can be compelling is generating data for training sentence similarity models.\n\nSentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping. source\n\nWhilst some strong open embedding models can be used for sentence similarity tasks, there are times when additional data for fine-tuning a model might be helpful:\n\nWhen working in a domain where generic models don‚Äôt work well.\nWhen you want to optimize a model for a particular use, i.e.¬†retrieval vs classification\nScaling, scaling, scaling: you want to train a general embedding model and need more data.\n\nFor the latter example, LLMs are useful not only because they allow you to scale the amount of data but also because they allow you to control what data you have in your training data. Many embedding models use some weak supervision of data found ‚Äúin the wild‚Äù. While using this data allows the model to learn how to model similarity, there is also quite a lot of noise in this data. A recent paper Improving Text Embeddings with Large Language Models showed generating data that aimed to be diverse to the kinds of data an embedding model would work with reducing the amount of data needed compared to using a much larger but noisier weakly labeled dataset."
  },
  {
    "objectID": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#what-is-similar",
    "href": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#what-is-similar",
    "title": "Synthetic dataset generation techniques: generating custom sentence similarity data",
    "section": "What is similar",
    "text": "What is similar\nOne frustration I‚Äôve sometimes had when people discuss sentence similarity as a task is that what ‚Äúsimilarity‚Äù means is usually pretty poorly defined (sorry, this is my humanities training rearing its head). This is one of the reasons why I really like the paper Description-Based Text Similarity. In this papers the authors describe one of the issues with existing approaches:\n\nThe notion of similarity‚Ä¶is not explicitly defined but rather learned from vast datasets containing pairs of texts labeled as similar, often mixing various different kinds of similarity (Kaster et al., 2021; Opitz & Frank, 2022). This makes them sub-optimal for information seeking queries, as it is hard to control or predict the results of a given similarity-based query. What is a good query representation and similarity definition for a semantic-search use case?\n\nThe approach they take in their paper is to use an LLM to generate new query sentences which aim to be ‚Äúabstract descriptions of sentences‚Äù which can be trained alongside their instantiations. To make it more clear here are some examples they produce in the paper:\n\n\n\nimage/png"
  },
  {
    "objectID": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#generating-custom-sentence-similarity-data",
    "href": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#generating-custom-sentence-similarity-data",
    "title": "Synthetic dataset generation techniques: generating custom sentence similarity data",
    "section": "Generating custom sentence similarity data",
    "text": "Generating custom sentence similarity data\nWhile this paper is focused on the task of generating ‚Äòabstract‚Äô queries for sentences the approach can be adapted to other more targeted similarity datasets. In the rest of this post I will briefly give some examples of how you can generate this kind of data (the full notebook in the Awesome Synthetic Datasets repo has the full code).\n\nUsing Inference Endpoints via the huggingface_hub library.\nIn the paper the authors use GPT3.5 from OpenAI. In this post we‚Äôll switch that out with an open model meta-llama/Meta-Llama-3-70B-Instruct which we‚Äôll call via the huggingface_hub library.\nFirst we can import the required libraries:\nfrom huggingface_hub import get_token\nfrom huggingface_hub import InferenceClient\nWe can then use InferenceClient to specify which model we want to use.\nclient = InferenceClient(\"meta-llama/Meta-Llama-3-70B-Instruct\", token=get_token())\n\n\nThe prompts\nFor generating the descriptions of Wikipedia this prompt is used:\nwiki_prompt = f\"\"\"\nLet's write abstract descriptions of sentences. Example:\nSentence: Pilate's role in the events leading to the crucifixion lent themselves to melodrama , even tragedy , and Pilate often has a role in medieval mystery plays .\nDescription: A description of a historical religious figure's involvement in a significant event and its later portrayal in art.\nNote: Descriptions can differ in the level of abstraction, granularity and the part of the sentence they focus on. Some descriptions need to be abstract, while others should be concrete and detailed.\nFor the following sentence, write up 5 good and stand-alone, independent descriptions and 5 bad descriptions (which may be related, but are clearly wrong). Output a json file with keys 'good', 'bad'.\nSentence: {sentence}\nStart your answer with a curly bracket.\n\"\"\"\nLet‚Äôs generate some sentences using this prompt. We‚Äôll use this sentence as an example:\n\n‚ÄúIn Greek mythology, Achilles ( ) or Achilleus () was a hero of the Trojan War who was known as being the greatest of all the Greek warriors. A central character in Homer‚Äôs Iliad, he was the son of the Nereid Thetis and Peleus, king of Phthia and famous Argonaut. Achilles was raised in Phthia along his childhood companion Patroclus and received his education by the centaur Chiron. In the Iliad, he is presented as the commander of the mythical tribe of the Myrmidons.‚Äù\n\nresp = client.text_generation(wiki_prompt.format(sentence=sentence))\nprint(resp)\n{\n\"good\": [\n\"A description of a mythological figure's background and characteristics\",\n\"A summary of a legendary hero's life and exploits\",\n\"A passage about a character from ancient Greek literature\",\n\"A biographical sketch of a famous warrior from mythology\",\n\"A description of a central character in a famous epic poem\"\n],\n\"bad\": [\n\"A description of a real person's life\",\n\"A summary of a historical event\",\n\"A passage about a character from a novel\",\n\"A biographical sketch of a king\",\n\"A\nWe can see that we have roughly what‚Äôs requests in the prompt but let‚Äôs try and load this as JSON:\nimport json\n\njson.loads(resp)\n---------------------------------------------------------------------------\n\nJSONDecodeError                           Traceback (most recent call last)\n\nCell In[82], line 3\n      1 import json\n----&gt; 3 json.loads(resp)\n\n\nFile ~/.pyenv/versions/3.11.1/lib/python3.11/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    341     s = s.decode(detect_encoding(s), 'surrogatepass')\n    343 if (cls is None and object_hook is None and\n    344         parse_int is None and parse_float is None and\n    345         parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 346     return _default_decoder.decode(s)\n    347 if cls is None:\n    348     cls = JSONDecoder\n\n\nFile ~/.pyenv/versions/3.11.1/lib/python3.11/json/decoder.py:337, in JSONDecoder.decode(self, s, _w)\n    332 def decode(self, s, _w=WHITESPACE.match):\n    333     \"\"\"Return the Python representation of ``s`` (a ``str`` instance\n    334     containing a JSON document).\n    335\n    336     \"\"\"\n--&gt; 337     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    338     end = _w(s, end).end()\n    339     if end != len(s):\n\n\nFile ~/.pyenv/versions/3.11.1/lib/python3.11/json/decoder.py:353, in JSONDecoder.raw_decode(self, s, idx)\n    344 \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n    345 a JSON document) and return a 2-tuple of the Python\n    346 representation and the index in ``s`` where the document ended.\n   (...)\n    350\n    351 \"\"\"\n    352 try:\n--&gt; 353     obj, end = self.scan_once(s, idx)\n    354 except StopIteration as err:\n    355     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n\n\nJSONDecodeError: Unterminated string starting at: line 14 column 1 (char 489)"
  },
  {
    "objectID": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#structured-generation",
    "href": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#structured-generation",
    "title": "Synthetic dataset generation techniques: generating custom sentence similarity data",
    "section": "Structured Generation",
    "text": "Structured Generation\nOne way we could help the model generate valid JSON is to increase the number of tokens. However, we can also use another approach, Structured Text Generation. This can be used to constrain the model‚Äôs output to a more specific format.\nWe can use Structured Text Generation via Inference API models which are hosted using Text Generation Inference. We won‚Äôt discuss how this works under the hood in this post (see https://huggingface.co/docs/text-generation-inference/conceptual/guidance for a nice guide on this). We‚Äôll instead focus on how we can use this to improve the results we‚Äôre getting from our open LLM.\nWhen doing structured text generation we use something known as a ‚Äúgrammar‚Äù to specify what we want our output to look like. There are various ways of creating these but one way is to use a Pydantic model. Pydantic is a very heavily used data validation library for Python which can be used to validate data fits a certain format. This library was originally designed more for validating data coming via APIs etc but can also be very useful in the context of LLMs.\nA simple way to define our data is to create a model called Sentences and specify that we want two attributes, good and bad. Each attribute should be a list of strings. You‚Äôll notice that in this example, these are specified via standard Python types.\nfrom pydantic import BaseModel\n\nclass Sentences(BaseModel):\n    good: list[str]\n    bad: list[str]\nTo use this model via the huggingface_hub library we need to pass it as a JSON Schema. Let‚Äôs see what the schema for this model looks like:\nschema = Sentences.model_json_schema()\nschema\n{'properties': {'good': {'items': {'type': 'string'},\n   'title': 'Good',\n   'type': 'array'},\n  'bad': {'items': {'type': 'string'}, 'title': 'Bad', 'type': 'array'}},\n 'required': ['good', 'bad'],\n 'title': 'Sentences',\n 'type': 'object'}\nWe can pass this schema to the text_generation method for our client.\nresp = client.text_generation(\n    wiki_prompt.format(sentence=sentence),\n    grammar={\"type\": \"json\", \"value\": Sentences.model_json_schema()},\n    max_new_tokens=2000,\n)\nWe can see that we can now load our response into a valid JSON object\njson.loads(resp)\n{'bad': [\"Achilles' biography\",\n  'A description of a person',\n  'A passage about a book',\n  'A story about a king',\n  'A summary of a myth'],\n 'good': [\"A description of a mythological figure's background and character in ancient Greek literature\",\n  'A characterization of a legendary warrior in Greek mythology',\n  'A summary of the early life and education of a hero in ancient Greek mythology',\n  'A description of a central character in a famous epic poem',\n  \"A portrayal of a mythological hero's family and upbringing\"]}\n\nMore control\nThe focus here isn‚Äôt to dig into structured generation in great detail but you can add more control to your generations. For example, we might think that the bad examples generated above are too short. We can use StringConstraints to specify a minimum length for these strings.\nfrom pydantic.types import Annotated, StringConstraints\n\nclass Sentences(BaseModel):\n    good: Annotated[list[str], StringConstraints(min_length=30)]\n    bad: Annotated[list[str], StringConstraints(min_length=30)]\nFinally, we could also specify how many generations we want. The original prompt specified a max of five examples for good and bad. We can add this constraint to our model.\nfrom pydantic.types import Annotated, StringConstraints\nfrom pydantic import Field\n\n\nclass Sentences(BaseModel):\n    good: Annotated[\n        list[str],\n        Field(\n            max_items=5,\n            item_type=StringConstraints(\n                min_length=30,\n            ),\n        ),\n    ]\n    bad: Annotated[\n        list[str],\n        Field(\n            max_items=5,\n            item_type=StringConstraints(\n                min_length=30,\n            ),\n        ),\n    ]"
  },
  {
    "objectID": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#abstract-descriptions",
    "href": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#abstract-descriptions",
    "title": "Synthetic dataset generation techniques: generating custom sentence similarity data",
    "section": "Abstract descriptions",
    "text": "Abstract descriptions\nOne extra step the authors of the paper take is to use the descriptions generated by the first prompt along with a second prompt focused on generating a more abstract representation of the sentence. We‚Äôll quickly see one example of what this looks like using one of our examples:\nprompt_abstract = \"Sentence: in spite of excellent pediatric health care , several educational problems could be noted in this tertiary pediatric center .\\nDescription: Despite having advanced healthcare resources, certain deficiencies in education were identified at a medical center that serves children.\\nA very abstract description: The provision of care at a specialized medical center was not optimal in one particular area, despite the presence of advanced resources.\\nSentence: {sentence}\\nDescription: {description}\\nA very abstract description:\"\ndef generate_abstract_description(sentence, description):\n    return client.text_generation(\n        prompt_abstract.format(sentence=sentence, description=description),\n    )\ndescription =json.loads(resp).get('good')[1]\nOur original sentence and description look like this\nprint(f\"Sentence: {sentence}\\nDescription: {description}\\n\")\nSentence: In Greek mythology, Achilles ( ) or Achilleus () was a hero of the Trojan War who was known as being the greatest of all the Greek warriors. A central character in Homer's Iliad, he was the son of the Nereid Thetis and Peleus, king of Phthia and famous Argonaut. Achilles was raised in Phthia along his childhood companion Patroclus and received his education by the centaur Chiron. In the Iliad, he is presented as the commander of the mythical tribe of the Myrmidons.\nDescription: A characterization of a legendary hero in a famous epic poem\nprint(f\"Abstract version: {generate_abstract_description(sentence, description)}\")\nAbstract version:  A figure from ancient mythology is described in terms of their family, upbringing, and role in a famous story."
  },
  {
    "objectID": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#conclusion",
    "href": "posts/plain-text/synthetic-data-generation/2024-05-03-synethic-data-1.html#conclusion",
    "title": "Synthetic dataset generation techniques: generating custom sentence similarity data",
    "section": "Conclusion",
    "text": "Conclusion\nWhilst there would still be some details to take care of if you want to scale up the generation of this kind of data, the above example hopefully shows how an open LLM can be used to generate more tailored data for training similarity models. Whilst the prompts here are borrowed from the paper, they could, of course, be adapted to focus on generating other kinds of similarity data."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html",
    "title": "flyswot",
    "section": "",
    "text": "My previous series of posts/notes following the full stack course ended slightly abruptly. However, I didn‚Äôt give up on the dream of trying to get more experience of ‚Äòdeploying‚Äô machine learning in a GLAM setting! One of the things I have been focusing on recently is a project which gave me the chance to use machine learning in somewhat of a production context.\nThis blog post will be a very short tl;dr on this project."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html#detecting-fake-flysheets-using-computer-vision",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html#detecting-fake-flysheets-using-computer-vision",
    "title": "flyswot",
    "section": "Detecting fake flysheets using computer vision",
    "text": "Detecting fake flysheets using computer vision\nThe Library has ‚Äòlegacy‚Äô digitised content of manuscripts. Due to some limitations of the legacy system on which these manuscripts were shared many of the images have incorrect metadata. The metadata for these images is partially stored in the filename for the image. In particular many images which didn‚Äôt fit into an available category were given ‚Äòend flysheet‚Äô labels (this basically means part of the filename contains the string fse). These images may actually be other things like a frontcover, a scroll image, a box etc.\n\n\n\nA screenshot of the digitised manuscript platform showing metadata about the page type of the manuscript\n\n\nThe library is moving to a new platform which won‚Äôt have these restrictions on possible page types. As a result there is a need/desire to find all of the images which have been given a ‚Äòfake‚Äô flysheet label and correct this label with the correct label.\nThis is a task where computer vision seems like it might be helpful."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html#the-desired-outcome",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html#the-desired-outcome",
    "title": "flyswot",
    "section": "The desired outcome",
    "text": "The desired outcome\nThe desire of this project is to be able to use a computer vision model to check a bunch of image directories and see if there are any ‚Äòfake‚Äô flysheets. There are some additional constraints on the project:\n\n$$$ this isn‚Äôt a funded project so can‚Äôt involve spending a bunch of money\nrelated to the above, the approach to annotation has to be pragmatic - no Mechanical Turk here\nthe machine learning should fit into existing workflows (this is something we have/are spending a lot of time on)\n\nSince this is intended to be a tl;dr I won‚Äôt go into more detail here about all of these requirements here."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html#flyswot",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html#flyswot",
    "title": "flyswot",
    "section": "flyswot",
    "text": "flyswot\nThe approach we ended up with is to deploy a model using a command line tool that we‚Äôve called flyswot. This tool can be pointed at a directory and it will recursively check for images which contain the fse pattern in the filename. These images are then checked using a computer vision model that looks check whether an image is a ‚Äòreal‚Äô flysheet or a ‚Äòfake‚Äô flysheet."
  },
  {
    "objectID": "posts/plain-text/flyswot/2021-12-22-flyswot.html#what-i-have-learned-so-far",
    "href": "posts/plain-text/flyswot/2021-12-22-flyswot.html#what-i-have-learned-so-far",
    "title": "flyswot",
    "section": "What I have learned (so far)",
    "text": "What I have learned (so far)\nThis project has been a great way of turning some of the theory of ‚Äòproduction‚Äô ML into practice. In particular I have learned:\n\nI‚Äôm super paranoid about domain drift.\n(some) of how to use ONNX\nMore robust testing approaches\nDVC (data version control)\nand a bunch more things‚Ä¶\n\nMost of these things are being documented elsewhere and will be available to share at some point in 2022. However, I will try and use this blog to document small things I‚Äôve learned along the way too. These notes are mainly for myself. There are a lot of little things I‚Äôve picked up from doing this project that I will forget if I don‚Äôt spend a bit of time writting up."
  },
  {
    "objectID": "posts/2024/11/local-vision-language-model-lm-studio.html",
    "href": "posts/2024/11/local-vision-language-model-lm-studio.html",
    "title": "Running a Local Vision Language Model with LM Studio to sort out my screenshot mess",
    "section": "",
    "text": "tl;dr I used a VLM to sort out my screenshot mess using a local VLM.\nThere are many reasons you might prefer to run a local model, including privacy, speed, cost, or just plain old curiosity.\nRunning Local Language Models has become increasingly easy over the last year. This is in a huge part thanks to the work of George Gersonov on llama.cpp. llama.cpp is used under the hood by a growing number of UI tools like LM Studio.\nRecently LM Studio has added additonal support for running models on Macs with Apple Silicon via mlx-engine which uses Apple‚Äôs MLX library to accelerate the inference of models on Macs with Apple Silicon. The most recent release of LM Studio has also added two very exciting features:\nIn addition to these two features, LM Studio already had support for Structued Outputs (using Outlines). In this blog post I‚Äôll show how these new features can be used for a perfect local VLM task: sorting out my chaotic desktop üòÖ."
  },
  {
    "objectID": "posts/2024/11/local-vision-language-model-lm-studio.html#the-problem",
    "href": "posts/2024/11/local-vision-language-model-lm-studio.html#the-problem",
    "title": "Running a Local Vision Language Model with LM Studio to sort out my screenshot mess",
    "section": "The Problem",
    "text": "The Problem\nI have hundreds of screenshots on my desktop. Some of them are screenshots of code, some are screenshots of webpages, some are screenshots of videos, some are screenshots of stupid memes, etc.\nI‚Äôd like to be able to automatically sort these screenshots in a way that I can find them later or at least shove them into some kind of order so that my desktop doesn‚Äôt look like a screenshot graveyard.\nLet‚Äôs see how we can use VLM‚Äôs and LM Studio to tackle this problem!\n\nWhat we‚Äôre working with\nTo start with lets see what we‚Äôre working with.\n\nfrom pathlib import Path\nfrom PIL import Image\n\n\nscreenshots = list(Path(\"/Users/davanstrien/Desktop\").glob(\"Screenshot*.png\"))\nscreenshots[3]\n\nPosixPath('/Users/davanstrien/Desktop/Screenshot 2024-10-03 at 09.14.30.png')\n\n\n\n\n\nExample of the type of screenshots I have hundreds of on my desktop"
  },
  {
    "objectID": "posts/2024/11/local-vision-language-model-lm-studio.html#setting-up-a-local-vlm",
    "href": "posts/2024/11/local-vision-language-model-lm-studio.html#setting-up-a-local-vlm",
    "title": "Running a Local Vision Language Model with LM Studio to sort out my screenshot mess",
    "section": "Setting up a local VLM",
    "text": "Setting up a local VLM\nIf you haven‚Äôt already, you will need to install LM Studio. You can find the latest instructions on their website. If you already have LM Studio installed make sure you are running at least version 0.3.5.\n\nEnabling LM Studio Headless Mode\nWhilst not necessary for running a VLM, Label Studio added a new feature for running in headless mode in version 0.3.5. This allows you to run the VLM without a GUI. This can be very nice if you want to use a VLM or an LLM in the background for some task(s).\nTo enable headless mode, go to the settings menu and check the ‚ÄúEnable Local LLM Service‚Äù option.\n\nIn addition to this, we‚Äôll use the CLI to interact with Label Studio. Check out the docs for more information on how to set this up.\nLet‚Äôs see what the lms CLI has available.\n\n!lms\n\n\n   __   __  ___  ______          ___        _______   ____\n\n  / /  /  |/  / / __/ /___ _____/ (_)__    / ___/ /  /  _/\n\n / /__/ /|_/ / _\\ \\/ __/ // / _  / / _ \\  / /__/ /___/ /  \n\n/____/_/  /_/ /___/\\__/\\_,_/\\_,_/_/\\___/  \\___/____/___/  \n\n\n\nlms - LM Studio CLI - v0.0.27\n\nGitHub: https://github.com/lmstudio-ai/lmstudio-cli\n\n\n\nUsage\n\nlms &lt;subcommand&gt;\n\n\n\nwhere &lt;subcommand&gt; can be one of:\n\n\n\n- status - Prints the status of LM Studio\n\n- server - Commands for managing the local server\n\n- ls - List all downloaded models\n\n- ps - List all loaded models\n\n- get - Searching and downloading a model from online.\n\n- load - Load a model\n\n- unload - Unload a model\n\n- create - Create a new project with scaffolding\n\n- log - Log operations. Currently only supports streaming logs from LM Studio via `lms log stream`\n\n- import - Import a model file into LM Studio\n\n- version - Prints the version of the CLI\n\n- bootstrap - Bootstrap the CLI\n\n\n\nFor more help, try running `lms &lt;subcommand&gt; --help`\n\n\n\n\nAs you can see we have a bunch of commands for interacting with the local LLM service. We‚Äôll start by downloading a VLM model. For now LM Studio has support for Pixtral (specifcally using a MLX 4 bit quantised model mlx-community/pixtral-12b-4bit).\nLM studio allows us to directly download models from Hugging Face. Let‚Äôs download the Pixtral model!\n\n%%bash\nlms get mlx-community/pixtral-12b-4bit \n\n\nSearching for models with the term mlx-community/pixtral-12b-4bit\n\nBased on your hardware, choosing the recommended option: Pixtral 12B [4bit] (7.15 GB)\n\nDownloading Pixtral 12B [4bit] (7.15 GB)\n\nYou already have this model. You can load it with: \n\n\n\n    lms load pixtral-12b\n\n\n\n\n\n\n\n\n\nYou‚Äôll see above I already have the model downloaded but if you don‚Äôt the CLI will download it for you.\nOnce the model is downloaded we can load it into LM Studio.\n\n%%bash\nlms load pixtral-12b\n\n\n\n\n\nLoading model \"mlx-community/pixtral-12b-4bit\"...\n\n[LMStudioClient][LLM] Start loading model mlx-community/pixtral-12b-4bit...\n\nModel loaded successfully in 9.62s. (7.15 GB)\n\nTo use the model in the API/SDK, use the identifier \"pixtral-12b\".\n\nTo set a custom identifier, use the --identifier &lt;identifier&gt; option.\n\n\n\n\n‚†¥ [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 ] 64.16%          \n\n\nWe‚Äôll also want to start a server so we can interact with the model via an API.\n\n%%bash\nlms server start\n\nStarting server...\nSuccess! Server is now running on port 1234\n\n\nThe server will be available at http://localhost:1234. The API is largely compatible with the OpenAI API so if you‚Äôve used that before you‚Äôll feel right at home. We can start by checking what models are available.\n\n%%bash\ncurl http://localhost:1234/v1/models\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   142  100   142    0     0  59638      0 --:--:-- --:--:-- --:--:-- 71000\n\n\n{\n  \"data\": [\n    {\n      \"id\": \"pixtral-12b\",\n      \"object\": \"model\",\n      \"owned_by\": \"organization_owner\"\n    }\n  ],\n  \"object\": \"list\"\n}"
  },
  {
    "objectID": "posts/2024/11/local-vision-language-model-lm-studio.html#using-the-openai-client-to-interact-with-the-vlm",
    "href": "posts/2024/11/local-vision-language-model-lm-studio.html#using-the-openai-client-to-interact-with-the-vlm",
    "title": "Running a Local Vision Language Model with LM Studio to sort out my screenshot mess",
    "section": "Using the OpenAI Client to interact with the VLM",
    "text": "Using the OpenAI Client to interact with the VLM\nWe can use the OpenAI client to interact with the VLM. This is useful if you want to use a VLM in a script or other program. Using this Client will also make it easier for us to switch out the VLM in the future without having to change the API calls. Let‚Äôs start by seeing how we can create the client.\nThe import is exactly the same as for the OpenAI API.\n\nfrom openai import OpenAI\n\nWe need to point to our local server and use the API key lm-studio. The server will usually be available on port 1234.\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\nWe can now see what models are available via Python.\n\nclient.models.list()\n\nSyncPage[Model](data=[Model(id='pixtral-12b', created=None, object='model', owned_by='organization_owner')], object='list')\n\n\n\ncompletion = client.chat.completions.create(\n    model=\"pixtral-12b\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Always respond with a pointless anecdote.\"},\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    ],\n    temperature=0.7,\n)\n\n\nfrom rich import print\n\nprint(completion.choices[0].message.content)\n\nOh, hey there! You know, I was just thinking about the time I tried to make a sandwich with a banana instead of \nbread? It was a slippery slope, literally and metaphorically. I ended up with a mess of banana goo and a serious \ncase of the giggles. Who knew making a sandwich could be so... slippery!\n\n\n\nVery nice! Now let‚Äôs try with an image.\n\nPassing Images to the VLM\nLet‚Äôs try with an image. We need to encode the image to base64 to send it to the VLM via the API.\n\n# encode the image to base64\nimport base64\nimport io\n\nimage = Image.open(screenshots[-3])\n# Convert PIL Image to RGB mode\nimage_rgb = image.convert(\"RGB\")\n\n# Convert PIL Image to bytes\nimg_byte_arr = io.BytesIO()\nimage_rgb.save(img_byte_arr, format=\"JPEG\")\nimage_bytes = img_byte_arr.getvalue()\n\n# Encode to base64\nbase64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n\nWe can see the base64 encoded image below.\n\nbase64_image\n\n'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'\n\n\nLet‚Äôs try again with the base64 encoded image.\n\ncompletion = client.chat.completions.create(\n    model=\"pixtral-12b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What is this image about?\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        },\n    ],\n)\n\nLet‚Äôs see the response.\n\nprint(completion.choices[0].message.content)\n\nThe image appears to be a screenshot of a configuration interface for setting up an endpoint, likely in the context\nof deploying or managing a containerized application. Here's a breakdown of the various sections and fields in the \ninterface:\n\n### General Configuration\n1. **Automatic Scaling**:\n   - **Min**: 0 (Minimum number of replicas)\n   - **Max**: 1 (Maximum number of replicas)\n\n2. **Revision**:\n   - An optional field to specify a revision identifier, e.g., `b754fe8f941023b372bd6a6c17282cf4bb9c0`.\n\n### Task and Framework\n- **Task**: Set to \"Custom\".\n- **Framework**: Also set to \"Custom\".\n\n### Container Type\n- The default container type is used for deploying endpoints, which is flexible due to custom inference handlers. \nOptions include:\n  - Custom\n  - Test-Generation information\n  - Link to a custom container\n\n### Container Details\n- **Container URL**: `danielvanstreien/aria-vlim.latest`\n  - This is the registry URL for the container, which can be from various registries like Docker Hub, AWS ECR, \nAzure ACR, Google GCR, etc.\n- **Container Port**: 8000\n- **Health Route**: `/health`\n- **Username** and **Password**:\n  - These fields are optional for authentication to the container registry.\n\n### Environment Variables\n- **Default**:\n  - Key: `CLIENT_KEY`\n  - Value: (To be filled in)\n- **Secrets**:\n  - Key: `CLIENT_KEY`\n  - Value: (To be filled in)\n\n### Additional Options\n- **Add one**: Allows adding more environment variables or secrets.\n\n### Summary\nThis interface is used to configure an endpoint for a containerized application, specifying details such as the \nnumber of replicas, revision identifier, task and framework settings, container registry URL, port, health route, \nand environment variables. The configuration allows for flexibility in deploying the application with custom \nsettings and secure handling of sensitive information like client keys.\n\n\n\nVery nice! This is already pretty cool but there are still some challenges if we want to use the VLM for more than a simple chat and actually want to use the VLM to allow us to perform some other kinds of tasks or actions. For example we might want to sort screenshots into categories or tags. One way we can do this is by asking the VLM to output this kind of info as JSON. Let‚Äôs try this.\nWe never knew a few years ago that we would spend so much time looking at deeply nested lists of Dicts üòÖ so let‚Äôs extract out the prompt to make it a bit easier to see what we‚Äôre changing.\n\nprompt = \"\"\"What is this image about? Assign a maximum of 3 tags to the image, and put the image into one of the following categories: meme, documentation image, other.\nRespond in JSON format with the keys \"description\", \"category\", and \"tags\".\"\"\"\n\n\n\nShow the code\ncompletion = client.chat.completions.create(\n    model=\"pixtral-12b\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": prompt,\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        },\n    ],\n)\n\n\nLet‚Äôs see the response.\n\nprint(completion.choices[0].message.content)\n\n```json\n{\n  \"description\": \"Configuration settings for deploying a containerized application.\",\n  \"image_url\": \"https://example.com/container-image\",\n  \"endpoint_settings\": {\n    \"scale_to_zero\": true,\n    \"idle_timeout\": 300,\n    \"replicas\": {\n      \"min\": 0,\n      \"max\": 1\n    },\n    \"revision\": \"e.g. b754fe8f941023b372bd6adcba6c17282cf4bb9c0\",\n    \"task\": \"Custom\",\n    \"framework\": \"Custom\"\n  },\n  \"container_type\": {\n    \"default_container\": true,\n    \"custom_inference_handlers\": false\n  },\n  \"container_url\": {\n    \"registry_url\": \"danielvanstraten/aria-vilm:latest\",\n    \"container_port\": 8000,\n    \"health_route\": \"/health\"\n  },\n  \"authentication\": {\n    \"username\": \"\",\n    \"password\": \"\"\n  },\n  \"environment_variables\": {\n    \"default\": [\n      {\n        \"key\": \"CLIENT_KEY\",\n        \"value\": \"\"\n      }\n    ],\n    \"secrets\": [\n      {\n        \"key\": \"CLIENT_KEY\",\n        \"value\": \"\"\n      }\n    ]\n  }\n}\n```\n\n\n\nThis is sort of what we asked for but not really? Let‚Äôs try to parse the response as JSON.\n\nimport json\n\njson.loads(completion.choices[0].message.content)\n\n\n---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\nCell In[25], line 3\n      1 import json\n----&gt; 3 json.loads(completion.choices[0].message.content)\n\nFile ~/.pyenv/versions/3.11.1/lib/python3.11/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    341     s = s.decode(detect_encoding(s), 'surrogatepass')\n    343 if (cls is None and object_hook is None and\n    344         parse_int is None and parse_float is None and\n    345         parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 346     return _default_decoder.decode(s)\n    347 if cls is None:\n    348     cls = JSONDecoder\n\nFile ~/.pyenv/versions/3.11.1/lib/python3.11/json/decoder.py:337, in JSONDecoder.decode(self, s, _w)\n    332 def decode(self, s, _w=WHITESPACE.match):\n    333     \"\"\"Return the Python representation of ``s`` (a ``str`` instance\n    334     containing a JSON document).\n    335 \n    336     \"\"\"\n--&gt; 337     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    338     end = _w(s, end).end()\n    339     if end != len(s):\n\nFile ~/.pyenv/versions/3.11.1/lib/python3.11/json/decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)\n    353     obj, end = self.scan_once(s, idx)\n    354 except StopIteration as err:\n--&gt; 355     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    356 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n\n\nIn this case we can fix this by stripping the ```json from the response but it‚Äôs not a great solution.\n\njson.loads(completion.choices[0].message.content.strip(\"```json\"))\n## Structured output\n\n{'description': 'Configuration settings for deploying a containerized application.',\n 'image_url': 'https://example.com/container-image',\n 'endpoint_settings': {'scale_to_zero': True,\n  'idle_timeout': 300,\n  'replicas': {'min': 0, 'max': 1},\n  'revision': 'e.g. b754fe8f941023b372bd6adcba6c17282cf4bb9c0',\n  'task': 'Custom',\n  'framework': 'Custom'},\n 'container_type': {'default_container': True,\n  'custom_inference_handlers': False},\n 'container_url': {'registry_url': 'danielvanstraten/aria-vilm:latest',\n  'container_port': 8000,\n  'health_route': '/health'},\n 'authentication': {'username': '', 'password': ''},\n 'environment_variables': {'default': [{'key': 'CLIENT_KEY', 'value': ''}],\n  'secrets': [{'key': 'CLIENT_KEY', 'value': ''}]}}"
  },
  {
    "objectID": "posts/2024/11/local-vision-language-model-lm-studio.html#structured-output-to-the-rescue",
    "href": "posts/2024/11/local-vision-language-model-lm-studio.html#structured-output-to-the-rescue",
    "title": "Running a Local Vision Language Model with LM Studio to sort out my screenshot mess",
    "section": "Structured output to the rescue!",
    "text": "Structured output to the rescue!\nI bang on about structured generation so much it probably seems like I‚Äôve put my entire pension into a structured generation ETF but IMO it‚Äôs a massively useful tool for using LLMs to do productive work. In particular it makes it easier to use LLMs for tasks that require more than just a simple chat i.e where you want to use the LLM to do something like sort images into folders or categorize them or generate metadata or perform API calls‚Ä¶It‚Äôs also massively useful for generating synthetic data for training other models (to the point where I think I will often prefer to use a crappier model that can be used with structured generation than a better model that can‚Äôt).i\nAs we saw above the JSON returned wasn‚Äôt very close to what we asked for. We could try and fix this by iterating on the prompt but alongisde doing we can also enforce a certain output. This doesn‚Äôt mean the output will always be correct but at least it will have the correct format and be consistent. This is already very helpful to avoid having to write parsing prompts like\n\ndef try_and_parse_response(response):\n    if response.startswith(\"```json\"):\n        return json.loads(response.strip(\"```json\"))\n    else:\n        if response.startswith(\"```\"):\n            return json.loads(response.strip(\"```\"))\n        else:\n            raise ValueError(\"Response is not valid JSON\")\nThis can get annoying very quickly so let‚Äôs see how we can use structured output to enforce a certain format.\n\nDefining a Pydantic Class with Outlines\nLM Studio has support for structured outputs via Outlines for mlx models. For gguf models it uses llama.cpp‚Äôs grammar-based sampling APIs.\nLet‚Äôs see how we can use a Pydantic model to enforce a certain output. To start if we wanted to sort images into folder‚Äôs we likely don‚Äôt want the model to keeping adding new folders to the folder structure. Instead we may want to only allow a certain set of categories. We can do this by defining a Pydantic model.\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\n\nclass ScreenshotCategory(BaseModel):\n    category: Literal[\"meme\", \"documentation image\", \"other\"] = Field(\n        ..., description=\"The category of the screenshot\"\n    )\n\nThis tells the model that the category must be one of the three options and that it must be a string. We can also add more constraints and combine classes. Let‚Äôs create an overall Class to represent the output we want for each screenshot.\nWe‚Äôll ask for:\n\nA description of the screenshot: we will enforce a minimum and maximum length so we don‚Äôt get some really long or really short descriptions.\nA category: we will use our ScreenshotCategory class to enforce the category.\nWe‚Äôll give the model a bit more choice with the tags by allowing up to 3 optional tags.\n\n\nfrom typing import Annotated\nfrom pydantic.types import StringConstraints\nfrom typing import Optional, List\n\n\nclass ScreenshotInformation(BaseModel):\n    description: Annotated[str, StringConstraints(min_length=50, max_length=500)] = (\n        Field(\n            ..., description=\"A short description of the screenshot (50-200 characters)\"\n        )\n    )\n    category: ScreenshotCategory\n    tags: Optional[List[str]] = Field(\n        None, description=\"A list of tags that describe the screenshot\", max_items=3\n    )\n\nLet‚Äôs update our prompt to ask for this information and specify that the response should be in JSON format.\n\nprompt = f\"\"\"Analyze the given screenshot and provide the following information in JSON format:\n\n1. description: A short description of the screenshot (50-200 characters)\n2. category: Categorize the screenshot as one of the following:\n   - meme\n   - documentation image\n   - other\n3. tags: (Optional) Up to 3 tags that describe the screenshot\n\nEnsure your response follows this schema:\n{ScreenshotInformation.model_json_schema()}\n\nDo not include any explanations or additional text outside of the JSON structure.\"\"\"\n\n\n\nShow the code\ncompletion = client.beta.chat.completions.parse(\n    model=\"model-identifier\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are an AI assistant specialized in organizing a users screenshots into categories and tags.\n            \"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": prompt,\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        },\n    ],\n    max_tokens=2000,\n    temperature=0.7,\n    response_format=ScreenshotInformation,\n)\n\n\n\nprint(completion.choices[0].message.parsed)\n\nScreenshotInformation(\n    description='Configuration settings for deploying a container endpoint.',\n    category=ScreenshotCategory(category='documentation image'),\n    tags=['Technology', ',', 'Configuration settings']\n)\n\n\n\nWe can see that we now get back a ScreenshotInformation object rather than a string. This is already pretty cool. You might spot though that one of the tags is just a ,, whilst this is ‚Äútechnically‚Äù correct it‚Äôs not what we want. We can fix this by updating our Pydantic model. We can also add a constraint to the tags to ensure they are at least 3 characters long.\n\nfrom typing import Annotated, List, Optional\nfrom pydantic import BaseModel, Field\nfrom pydantic.types import StringConstraints\n\n\nclass ScreenshotCategory(BaseModel):\n    category: Literal[\"meme\", \"documentation image\", \"other\"] = Field(\n        ..., description=\"The category of the screenshot\"\n    )\n\n\nclass ScreenshotInformation(BaseModel):\n    description: Annotated[str, StringConstraints(min_length=50, max_length=1000)] = (\n        Field(\n            ..., description=\"A short description of the screenshot (50-200 characters)\"\n        )\n    )\n    category: ScreenshotCategory\n    tags: Optional[List[Annotated[str, StringConstraints(min_length=3)]]] = Field(\n        None, description=\"A list of tags that describe the screenshot\", max_items=3\n    )\n\n\nprompt = f\"\"\"Analyze the given screenshot and provide the following information in JSON format:\n\n1. description: A short description of the screenshot (50-200 characters)\n2. category: Categorize the screenshot as one of the following:\n   - meme\n   - documentation image\n   - other\n3. tags: (Optional) Up to 3 tags that describe the screenshot\n\nEnsure your response follows this schema:\n{ScreenshotInformation.model_json_schema()}\n\nDo not include any explanations or additional text outside of the JSON structure.\"\"\"\n\n\n\nShow the code\ncompletion = client.beta.chat.completions.parse(\n    model=\"model-identifier\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are an AI assistant specialized in organizing a users screenshots into categories and tags.\n            \"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": prompt,\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        },\n    ],\n    max_tokens=2000,\n    temperature=0.7,\n    response_format=ScreenshotInformation,\n)\n\n\nLet‚Äôs see how this changes the output.\n\nprint(completion.choices[0].message.parsed)\n\nScreenshotInformation(\n    description='Configuration page for deploying a container with various settings including security, networking,\nand environment variables.',\n    category=ScreenshotCategory(category='documentation image'),\n    tags=['.NET', 'API', 'AWS ECR']\n)\n\n\n\nThe tags could potentially be improved but we already have an output that means we can use this in a program. To make this a bit easier to use let‚Äôs wrap this up in a function.\n\nfrom typing import Union\n\n\ndef encode_image_to_base64(image):\n    img_byte_arr = io.BytesIO()\n    image.save(img_byte_arr, format=\"JPEG\")\n    image_bytes = img_byte_arr.getvalue()\n    return base64.b64encode(image_bytes).decode(\"utf-8\")\n\n\ndef analyze_screenshot(image: Image.Image, prompt, return_class):\n    image = image.convert(\"RGB\")\n    base64_image = encode_image_to_base64(image)\n    completion = client.beta.chat.completions.parse(\n        model=\"model-identifier\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an AI assistant specialized in organizing a users screenshots into categories and tags.\n            \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": prompt,\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                    },\n                ],\n            },\n        ],\n        max_tokens=2000,\n        temperature=0.7,\n        response_format=return_class,\n    )\n    return completion.choices[0].message.parsed\n\nLet‚Äôs now try with another screenshot.\n\nimage = Image.open(screenshots[0])\nimage\n\n\n\n\n\n\n\n\n\nanalyze_screenshot(screenshots[0], prompt, ScreenshotInformation)\n\nScreenshotInformation(description='This screenshot shows a list of instructions for analyzing document images.', category=ScreenshotCategory(category='documentation image'), tags=None)\n\n\nTo make sure memes are detected let‚Äôs try with a meme.\n\nurl = \"https://preview.redd.it/this-meme-only-runs-on-an-h100-v0-urpjifh14xcd1.jpeg?auto=webp&s=7be0109b54cef2cd268659cb29727f6e16462621\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n\n\n\n\n\n\n\n\n\nscreenshot_info = analyze_screenshot(image, prompt, ScreenshotInformation)\nscreenshot_info\n\nScreenshotInformation(description='The couple is discussing Llama3 400b while in bed.', category=ScreenshotCategory(category='meme'), tags=['relationship', 'humor', 'tech'])"
  },
  {
    "objectID": "posts/2024/11/local-vision-language-model-lm-studio.html#organizing-the-screenshots-aka-we-dont-need-claude-to-interact-with-our-machines",
    "href": "posts/2024/11/local-vision-language-model-lm-studio.html#organizing-the-screenshots-aka-we-dont-need-claude-to-interact-with-our-machines",
    "title": "Running a Local Vision Language Model with LM Studio to sort out my screenshot mess",
    "section": "Organizing the screenshots aka we don‚Äôt need claude to interact with our machines!",
    "text": "Organizing the screenshots aka we don‚Äôt need claude to interact with our machines!\nWe can now use this to organize our screenshots. We‚Äôll create a new folder on our desktop and move the screenshots into the appropriate folders.\n\nfolders = [\"meme\", \"other\", \"documentation_image\"]\nbase_desktop_folder = \"/Users/davanstrien/Desktop/organized_screenshots\"\nfor folder in folders:\n    Path(f\"{base_desktop_folder}/{folder}\").mkdir(exist_ok=True, parents=True)\n\n\nscreenshot_info.category.category\n\n'meme'\n\n\nWe define a mapping from the category to the folder on our desktop.\n\nimport shutil\n\ncategory_folder_map = {\n    \"meme\": Path(base_desktop_folder) / \"meme\",\n    \"other\": Path(base_desktop_folder) / \"other\",\n    \"documentation image\": Path(base_desktop_folder) / \"documentation_image\",\n}\n\ncategory_folder_map\n\n{'meme': PosixPath('/Users/davanstrien/Desktop/organized_screenshots/meme'),\n 'other': PosixPath('/Users/davanstrien/Desktop/organized_screenshots/other'),\n 'documentation image': PosixPath('/Users/davanstrien/Desktop/organized_screenshots/documentation_image')}\n\n\nand a function to move the screenshot to the appropriate folder. Let‚Äôs try with the first screenshot.\n\ndef organize_screenshot(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    analysis = analyze_screenshot(image, prompt, ScreenshotInformation)\n    category = analysis.category.category\n    print(f\"Moving {image_path} to {category_folder_map[category]}\")\n    shutil.move(image_path, category_folder_map[category] / image_path.name)\n\n\norganize_screenshot(screenshots[0])\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-22 at 18.51.23.png to \n/Users/davanstrien/Desktop/organized_screenshots/documentation_image\n\n\n\nTo prove this works here it is in the correct folder.\n\n\n\nThe screenshot folder after moving the first screenshot\n\n\nWe can now try with a bunch of screenshots.\n\nscreenshots = list(Path(\"/Users/davanstrien/Desktop\").glob(\"Screenshot*.png\"))\n\n\n%%time\nfrom tqdm.auto import tqdm\n\nten_examples = screenshots[:10]\n\nfor screenshot in tqdm(ten_examples):\n    organize_screenshot(screenshot)\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-11 at 15.17.06.png to \n/Users/davanstrien/Desktop/organized_screenshots/other\n\n\n\n 10%|‚ñà         | 1/10 [01:03&lt;09:32, 63.66s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-17 at 16.37.22.png to \n/Users/davanstrien/Desktop/organized_screenshots/other\n\n\n\n 20%|‚ñà‚ñà        | 2/10 [02:00&lt;07:56, 59.60s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-03 at 09.14.30.png to \n/Users/davanstrien/Desktop/organized_screenshots/documentation_image\n\n\n\n 30%|‚ñà‚ñà‚ñà       | 3/10 [02:58&lt;06:51, 58.74s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-03 at 09.14.26.png to \n/Users/davanstrien/Desktop/organized_screenshots/meme\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [04:26&lt;07:03, 70.56s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-11 at 10.49.02.png to \n/Users/davanstrien/Desktop/organized_screenshots/meme\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [05:13&lt;05:09, 61.97s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-17 at 16.37.08.png to \n/Users/davanstrien/Desktop/organized_screenshots/other\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [06:37&lt;04:37, 69.48s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-23 at 21.48.10.png to \n/Users/davanstrien/Desktop/organized_screenshots/documentation_image\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [08:52&lt;04:33, 91.03s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-22 at 17.26.00.png to \n/Users/davanstrien/Desktop/organized_screenshots/meme\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [09:35&lt;02:31, 75.68s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-02 at 10.31.04.png to \n/Users/davanstrien/Desktop/organized_screenshots/other\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [10:39&lt;01:12, 72.05s/it]\n\n\nMoving /Users/davanstrien/Desktop/Screenshot 2024-10-02 at 16.57.27.png to \n/Users/davanstrien/Desktop/organized_screenshots/documentation_image\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [12:20&lt;00:00, 74.08s/it]\n\n\nCPU times: user 1.29 s, sys: 500 ms, total: 1.79 s\nWall time: 12min 20s"
  },
  {
    "objectID": "posts/2024/11/local-vision-language-model-lm-studio.html#conclusion",
    "href": "posts/2024/11/local-vision-language-model-lm-studio.html#conclusion",
    "title": "Running a Local Vision Language Model with LM Studio to sort out my screenshot mess",
    "section": "Conclusion",
    "text": "Conclusion\nWithout too much effort we‚Äôve been able to use a VLM to sort out our screenshots into different folders. This probably seems like a bit of a trivial tasks but hopefully it gives you an idea of how you can use VLM‚Äôs to do all sorts of other tasks (or use an LLM to do tasks that require a bit more than just a simple chat).\nIn my opinion a very important component of this working well is the use of structured outputs. It allows us to enforce a certain format and makes it easier to use the output in a program.\nOne of the limitations with this current setup is that inference is pretty slow. To be honest, I‚Äôm quite excited at how well a local VLM can perform and for a lot of tasks it doesn‚Äôt really matter that much if it‚Äôs slow if it‚Äôs local and you don‚Äôt need to pay for cloud resources (you can just run it in the background/while you do other things).\nThe recent release of Aria has me very excited about open VLMs. At the moment it‚Äôs quite a beefy model and is unlikely to run on my mac machine but there has been a lot of progress in quantizing models and other tricks to make running these models much more accessible. For text models we now have a lot of smallish models (~8B parameters) that are pretty capable for many tasks and run very quickly even locally. It‚Äôs quite likely that we‚Äôll soon see a lot more open VLMs that are actually accessible to run on consumer hardware.\n\nResources\n\nLM Studio\nPixtral\nMLX\nmlx-vlm\nAria"
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "",
    "text": "Recently, we launched HF Jobs, a new way to run jobs on the Hugging Face platform. This post will show you how to use it to run large language model inference jobs with vLLM and uv Scripts, processing thousands of prompts with models that don‚Äôt fit on a single GPU.\nHF Jobs can be a very powerful tool for running a variety of compute jobs but I think it‚Äôs particularly useful for running LLMS for batched infernece workloads where it can make a lot of sense to try and bring the data close to the model (to remove the latency of transferring data via an API) and to use trhe powerful auto batching features of vLLM to get the most out of your GPUs."
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html#the-challenge",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html#the-challenge",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "The Challenge",
    "text": "The Challenge\nLarge language models like Qwen3-30B-A3B-Instruct (30 billion parameters) exceed single GPU memory limits. Running batch inference on datasets requires: - Multi-GPU setup with tensor parallelism - Handling prompts that exceed context limits - Managing dependencies and environment setup\nTraditional approaches involve complex Docker setups, manual dependency management, and custom scripts for GPU coordination. HF Jobs changes this."
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html#what-is-vllm",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html#what-is-vllm",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "What is vLLM?",
    "text": "What is vLLM?\nvLLM is a very well known and heavily used inference engine. It is known for its ability to scale inference for LLMs. While we can use vLLM via an OpenAI compatible API, it also has a powerful batch inference mode that allows us to process large datasets of prompts efficiently. This ‚Äúoffline inference‚Äù is particularly useful when we want to generate responses for a large number of prompts without the overhead of API calls."
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html#what-are-uv-scripts",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html#what-are-uv-scripts",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "What are uv Scripts?",
    "text": "What are uv Scripts?\nUV scripts are Python scripts with inline dependency metadata that automatically install and manage their requirements. Instead of separate requirements.txt files or complex setup instructions, everything needed to run the script is declared in the script itself:\n  # /// script\n  # requires-python = \"&gt;=3.10\"\n  # dependencies = [\n  #     \"vllm\",\n  #     \"transformers\",\n  #     \"datasets\",\n  # ]\n  # ///\n\n  # Your Python code here\nWhen you run uv run script.py, UV automatically creates an isolated environment, installs dependencies, and executes your code. No virtual env setup, no pip install commands, no version conflicts."
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html#the-solution-uv-scripts-vllm-hf-jobs",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html#the-solution-uv-scripts-vllm-hf-jobs",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "The Solution: UV Scripts + vLLM + HF Jobs",
    "text": "The Solution: UV Scripts + vLLM + HF Jobs\nHF Jobs provides managed GPU infrastructure. This is already very useful but combined with uv Scripts we can more easily distribute scripts for a variety of ML/AI tasks in a (more) reproducible way."
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html#the-uv-scripts-hugging-face-org",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html#the-uv-scripts-hugging-face-org",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "The uv scripts Hugging Face org",
    "text": "The uv scripts Hugging Face org\nSince I‚Äôm so excited about uv Scripts, I created a Hugging Face org to host them: uv scripts. This org will contain a variety of uv Scripts that you can use to run jobs on HF Jobs. For this example we‚Äôll use a script that allows us to run inference for a model using vLLM. This script exposes a bunch of parameters that allow you to control how the inference is run, including the model to use, the number of GPUs to use, and the batch size etc.\nIn this case, the script expects as input a dataset with a column containing the input prompts (as messages). It will then run inference on the model using vLLM and return the generated responses in a new dataset.\nI‚Äôm personally quite excited to see people sharing more uv scripts for things that are not complex enough to justify a full repository but that are still useful to share and run on the Hugging Face platform!\nIf you are curious, you can check out the script here or below:\n\n\nCode\nimport requests\n\nprint(requests.get(\"https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py\").text)\n\n\n# /// script\n# requires-python = \"&gt;=3.10\"\n# dependencies = [\n#     \"datasets\",\n#     \"flashinfer-python\",\n#     \"huggingface-hub[hf_transfer]\",\n#     \"torch\",\n#     \"transformers\",\n#     \"vllm&gt;=0.8.5\",\n# ]\n#\n# ///\n\"\"\"\nGenerate responses for prompts in a dataset using vLLM for efficient GPU inference.\n\nThis script loads a dataset from Hugging Face Hub containing chat-formatted messages,\napplies the model's chat template, generates responses using vLLM, and saves the\nresults back to the Hub with a comprehensive dataset card.\n\nExample usage:\n    # Local execution with auto GPU detection\n    uv run generate-responses.py \\\\\n        username/input-dataset \\\\\n        username/output-dataset \\\\\n        --messages-column messages\n\n    # With custom model and sampling parameters\n    uv run generate-responses.py \\\\\n        username/input-dataset \\\\\n        username/output-dataset \\\\\n        --model-id meta-llama/Llama-3.1-8B-Instruct \\\\\n        --temperature 0.9 \\\\\n        --top-p 0.95 \\\\\n        --max-tokens 2048\n\n    # HF Jobs execution (see script output for full command)\n    hf jobs uv run --flavor a100x4 ...\n\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport sys\nfrom datetime import datetime\nfrom typing import Optional\n\nfrom datasets import load_dataset\nfrom huggingface_hub import DatasetCard, get_token, login\nfrom torch import cuda\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\n# Enable HF Transfer for faster downloads\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n\ndef check_gpu_availability() -&gt; int:\n    \"\"\"Check if CUDA is available and return the number of GPUs.\"\"\"\n    if not cuda.is_available():\n        logger.error(\"CUDA is not available. This script requires a GPU.\")\n        logger.error(\n            \"Please run on a machine with NVIDIA GPU or use HF Jobs with GPU flavor.\"\n        )\n        sys.exit(1)\n\n    num_gpus = cuda.device_count()\n    for i in range(num_gpus):\n        gpu_name = cuda.get_device_name(i)\n        gpu_memory = cuda.get_device_properties(i).total_memory / 1024**3\n        logger.info(f\"GPU {i}: {gpu_name} with {gpu_memory:.1f} GB memory\")\n\n    return num_gpus\n\n\ndef create_dataset_card(\n    source_dataset: str,\n    model_id: str,\n    messages_column: str,\n    sampling_params: SamplingParams,\n    tensor_parallel_size: int,\n    num_examples: int,\n    generation_time: str,\n    num_skipped: int = 0,\n    max_model_len_used: Optional[int] = None,\n) -&gt; str:\n    \"\"\"Create a comprehensive dataset card documenting the generation process.\"\"\"\n    filtering_section = \"\"\n    if num_skipped &gt; 0:\n        skip_percentage = (num_skipped / num_examples) * 100\n        processed = num_examples - num_skipped\n        filtering_section = f\"\"\"\n\n### Filtering Statistics\n\n- **Total Examples**: {num_examples:,}\n- **Processed**: {processed:,} ({100 - skip_percentage:.1f}%)\n- **Skipped (too long)**: {num_skipped:,} ({skip_percentage:.1f}%)\n- **Max Model Length Used**: {max_model_len_used:,} tokens\n\nNote: Prompts exceeding the maximum model length were skipped and have empty responses.\"\"\"\n\n    return f\"\"\"---\ntags:\n- generated\n- vllm\n- uv-script\n---\n\n# Generated Responses Dataset\n\nThis dataset contains generated responses for prompts from [{source_dataset}](https://huggingface.co/datasets/{source_dataset}).\n\n## Generation Details\n\n- **Source Dataset**: [{source_dataset}](https://huggingface.co/datasets/{source_dataset})\n- **Messages Column**: `{messages_column}`\n- **Model**: [{model_id}](https://huggingface.co/{model_id})\n- **Number of Examples**: {num_examples:,}\n- **Generation Date**: {generation_time}{filtering_section}\n\n### Sampling Parameters\n\n- **Temperature**: {sampling_params.temperature}\n- **Top P**: {sampling_params.top_p}\n- **Top K**: {sampling_params.top_k}\n- **Min P**: {sampling_params.min_p}\n- **Max Tokens**: {sampling_params.max_tokens}\n- **Repetition Penalty**: {sampling_params.repetition_penalty}\n\n### Hardware Configuration\n\n- **Tensor Parallel Size**: {tensor_parallel_size}\n- **GPU Configuration**: {tensor_parallel_size} GPU(s)\n\n## Dataset Structure\n\nThe dataset contains all columns from the source dataset plus:\n- `response`: The generated response from the model\n\n## Generation Script\n\nGenerated using the vLLM inference script from [uv-scripts/vllm](https://huggingface.co/datasets/uv-scripts/vllm).\n\nTo reproduce this generation:\n\n```bash\nuv run https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py \\\\\n    {source_dataset} \\\\\n    &lt;output-dataset&gt; \\\\\n    --model-id {model_id} \\\\\n    --messages-column {messages_column} \\\\\n    --temperature {sampling_params.temperature} \\\\\n    --top-p {sampling_params.top_p} \\\\\n    --top-k {sampling_params.top_k} \\\\\n    --max-tokens {sampling_params.max_tokens}{f\" \\\\\\\\\\\\n    --max-model-len {max_model_len_used}\" if max_model_len_used else \"\"}\n```\n\"\"\"\n\n\ndef main(\n    src_dataset_hub_id: str,\n    output_dataset_hub_id: str,\n    model_id: str = \"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n    messages_column: str = \"messages\",\n    output_column: str = \"response\",\n    temperature: float = 0.7,\n    top_p: float = 0.8,\n    top_k: int = 20,\n    min_p: float = 0.0,\n    max_tokens: int = 16384,\n    repetition_penalty: float = 1.0,\n    gpu_memory_utilization: float = 0.90,\n    max_model_len: Optional[int] = None,\n    tensor_parallel_size: Optional[int] = None,\n    skip_long_prompts: bool = True,\n    hf_token: Optional[str] = None,\n):\n    \"\"\"\n    Main generation pipeline.\n\n    Args:\n        src_dataset_hub_id: Input dataset on Hugging Face Hub\n        output_dataset_hub_id: Where to save results on Hugging Face Hub\n        model_id: Hugging Face model ID for generation\n        messages_column: Column name containing chat messages\n        output_column: Column name for generated responses\n        temperature: Sampling temperature\n        top_p: Top-p sampling parameter\n        top_k: Top-k sampling parameter\n        min_p: Minimum probability threshold\n        max_tokens: Maximum tokens to generate\n        repetition_penalty: Repetition penalty parameter\n        gpu_memory_utilization: GPU memory utilization factor\n        max_model_len: Maximum model context length (None uses model default)\n        tensor_parallel_size: Number of GPUs to use (auto-detect if None)\n        skip_long_prompts: Skip prompts exceeding max_model_len instead of failing\n        hf_token: Hugging Face authentication token\n    \"\"\"\n    generation_start_time = datetime.now().isoformat()\n\n    # GPU check and configuration\n    num_gpus = check_gpu_availability()\n    if tensor_parallel_size is None:\n        tensor_parallel_size = num_gpus\n        logger.info(\n            f\"Auto-detected {num_gpus} GPU(s), using tensor_parallel_size={tensor_parallel_size}\"\n        )\n    else:\n        logger.info(f\"Using specified tensor_parallel_size={tensor_parallel_size}\")\n        if tensor_parallel_size &gt; num_gpus:\n            logger.warning(\n                f\"Requested {tensor_parallel_size} GPUs but only {num_gpus} available\"\n            )\n\n    # Authentication - try multiple methods\n    HF_TOKEN = hf_token or os.environ.get(\"HF_TOKEN\") or get_token()\n\n    if not HF_TOKEN:\n        logger.error(\"No HuggingFace token found. Please provide token via:\")\n        logger.error(\"  1. --hf-token argument\")\n        logger.error(\"  2. HF_TOKEN environment variable\")\n        logger.error(\"  3. Run 'huggingface-cli login' or use login() in Python\")\n        sys.exit(1)\n\n    logger.info(\"HuggingFace token found, authenticating...\")\n    login(token=HF_TOKEN)\n\n    # Initialize vLLM\n    logger.info(f\"Loading model: {model_id}\")\n    vllm_kwargs = {\n        \"model\": model_id,\n        \"tensor_parallel_size\": tensor_parallel_size,\n        \"gpu_memory_utilization\": gpu_memory_utilization,\n    }\n    if max_model_len is not None:\n        vllm_kwargs[\"max_model_len\"] = max_model_len\n        logger.info(f\"Using max_model_len={max_model_len}\")\n\n    llm = LLM(**vllm_kwargs)\n\n    # Load tokenizer for chat template\n    logger.info(\"Loading tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    # Create sampling parameters\n    sampling_params = SamplingParams(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        min_p=min_p,\n        max_tokens=max_tokens,\n        repetition_penalty=repetition_penalty,\n    )\n\n    # Load dataset\n    logger.info(f\"Loading dataset: {src_dataset_hub_id}\")\n    dataset = load_dataset(src_dataset_hub_id, split=\"train\")\n    total_examples = len(dataset)\n    logger.info(f\"Dataset loaded with {total_examples:,} examples\")\n\n    # Validate messages column\n    if messages_column not in dataset.column_names:\n        logger.error(\n            f\"Column '{messages_column}' not found. Available columns: {dataset.column_names}\"\n        )\n        sys.exit(1)\n\n    # Get effective max length for filtering\n    if max_model_len is not None:\n        effective_max_len = max_model_len\n    else:\n        # Get model's default max length\n        effective_max_len = llm.llm_engine.model_config.max_model_len\n    logger.info(f\"Using effective max model length: {effective_max_len}\")\n\n    # Process messages and apply chat template\n    logger.info(\"Applying chat template to messages...\")\n    all_prompts = []\n    valid_prompts = []\n    valid_indices = []\n    skipped_info = []\n\n    for i, example in enumerate(tqdm(dataset, desc=\"Processing messages\")):\n        messages = example[messages_column]\n        # Apply chat template\n        prompt = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        all_prompts.append(prompt)\n\n        # Count tokens if filtering is enabled\n        if skip_long_prompts:\n            tokens = tokenizer.encode(prompt)\n            if len(tokens) &lt;= effective_max_len:\n                valid_prompts.append(prompt)\n                valid_indices.append(i)\n            else:\n                skipped_info.append((i, len(tokens)))\n        else:\n            valid_prompts.append(prompt)\n            valid_indices.append(i)\n\n    # Log filtering results\n    if skip_long_prompts and skipped_info:\n        logger.warning(\n            f\"Skipped {len(skipped_info)} prompts that exceed max_model_len ({effective_max_len} tokens)\"\n        )\n        logger.info(\"Skipped prompt details (first 10):\")\n        for idx, (prompt_idx, token_count) in enumerate(skipped_info[:10]):\n            logger.info(\n                f\"  - Example {prompt_idx}: {token_count} tokens (exceeds by {token_count - effective_max_len})\"\n            )\n        if len(skipped_info) &gt; 10:\n            logger.info(f\"  ... and {len(skipped_info) - 10} more\")\n\n        skip_percentage = (len(skipped_info) / total_examples) * 100\n        if skip_percentage &gt; 10:\n            logger.warning(f\"WARNING: {skip_percentage:.1f}% of prompts were skipped!\")\n\n    if not valid_prompts:\n        logger.error(\"No valid prompts to process after filtering!\")\n        sys.exit(1)\n\n    # Generate responses - vLLM handles batching internally\n    logger.info(f\"Starting generation for {len(valid_prompts):,} valid prompts...\")\n    logger.info(\"vLLM will handle batching and scheduling automatically\")\n\n    outputs = llm.generate(valid_prompts, sampling_params)\n\n    # Extract generated text and create full response list\n    logger.info(\"Extracting generated responses...\")\n    responses = [\"\"] * total_examples  # Initialize with empty strings\n\n    for idx, output in enumerate(outputs):\n        original_idx = valid_indices[idx]\n        response = output.outputs[0].text.strip()\n        responses[original_idx] = response\n\n    # Add responses to dataset\n    logger.info(\"Adding responses to dataset...\")\n    dataset = dataset.add_column(output_column, responses)\n\n    # Create dataset card\n    logger.info(\"Creating dataset card...\")\n    card_content = create_dataset_card(\n        source_dataset=src_dataset_hub_id,\n        model_id=model_id,\n        messages_column=messages_column,\n        sampling_params=sampling_params,\n        tensor_parallel_size=tensor_parallel_size,\n        num_examples=total_examples,\n        generation_time=generation_start_time,\n        num_skipped=len(skipped_info) if skip_long_prompts else 0,\n        max_model_len_used=effective_max_len if skip_long_prompts else None,\n    )\n\n    # Push dataset to hub\n    logger.info(f\"Pushing dataset to: {output_dataset_hub_id}\")\n    dataset.push_to_hub(output_dataset_hub_id, token=HF_TOKEN)\n\n    # Push dataset card\n    card = DatasetCard(card_content)\n    card.push_to_hub(output_dataset_hub_id, token=HF_TOKEN)\n\n    logger.info(\"‚úÖ Generation complete!\")\n    logger.info(\n        f\"Dataset available at: https://huggingface.co/datasets/{output_dataset_hub_id}\"\n    )\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) &gt; 1:\n        parser = argparse.ArgumentParser(\n            description=\"Generate responses for dataset prompts using vLLM\",\n            formatter_class=argparse.RawDescriptionHelpFormatter,\n            epilog=\"\"\"\nExamples:\n  # Basic usage with default Qwen model\n  uv run generate-responses.py input-dataset output-dataset\n  \n  # With custom model and parameters\n  uv run generate-responses.py input-dataset output-dataset \\\\\n    --model-id meta-llama/Llama-3.1-8B-Instruct \\\\\n    --temperature 0.9 \\\\\n    --max-tokens 2048\n  \n  # Force specific GPU configuration\n  uv run generate-responses.py input-dataset output-dataset \\\\\n    --tensor-parallel-size 2 \\\\\n    --gpu-memory-utilization 0.95\n  \n  # Using environment variable for token\n  HF_TOKEN=hf_xxx uv run generate-responses.py input-dataset output-dataset\n            \"\"\",\n        )\n\n        parser.add_argument(\n            \"src_dataset_hub_id\",\n            help=\"Input dataset on Hugging Face Hub (e.g., username/dataset-name)\",\n        )\n        parser.add_argument(\n            \"output_dataset_hub_id\", help=\"Output dataset name on Hugging Face Hub\"\n        )\n        parser.add_argument(\n            \"--model-id\",\n            type=str,\n            default=\"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n            help=\"Model to use for generation (default: Qwen3-30B-A3B-Instruct-2507)\",\n        )\n        parser.add_argument(\n            \"--messages-column\",\n            type=str,\n            default=\"messages\",\n            help=\"Column containing chat messages (default: messages)\",\n        )\n        parser.add_argument(\n            \"--output-column\",\n            type=str,\n            default=\"response\",\n            help=\"Column name for generated responses (default: response)\",\n        )\n        parser.add_argument(\n            \"--temperature\",\n            type=float,\n            default=0.7,\n            help=\"Sampling temperature (default: 0.7)\",\n        )\n        parser.add_argument(\n            \"--top-p\",\n            type=float,\n            default=0.8,\n            help=\"Top-p sampling parameter (default: 0.8)\",\n        )\n        parser.add_argument(\n            \"--top-k\",\n            type=int,\n            default=20,\n            help=\"Top-k sampling parameter (default: 20)\",\n        )\n        parser.add_argument(\n            \"--min-p\",\n            type=float,\n            default=0.0,\n            help=\"Minimum probability threshold (default: 0.0)\",\n        )\n        parser.add_argument(\n            \"--max-tokens\",\n            type=int,\n            default=16384,\n            help=\"Maximum tokens to generate (default: 16384)\",\n        )\n        parser.add_argument(\n            \"--repetition-penalty\",\n            type=float,\n            default=1.0,\n            help=\"Repetition penalty (default: 1.0)\",\n        )\n        parser.add_argument(\n            \"--gpu-memory-utilization\",\n            type=float,\n            default=0.90,\n            help=\"GPU memory utilization factor (default: 0.90)\",\n        )\n        parser.add_argument(\n            \"--max-model-len\",\n            type=int,\n            help=\"Maximum model context length (default: model's default)\",\n        )\n        parser.add_argument(\n            \"--tensor-parallel-size\",\n            type=int,\n            help=\"Number of GPUs to use (default: auto-detect)\",\n        )\n        parser.add_argument(\n            \"--hf-token\",\n            type=str,\n            help=\"Hugging Face token (can also use HF_TOKEN env var)\",\n        )\n        parser.add_argument(\n            \"--skip-long-prompts\",\n            action=\"store_true\",\n            default=True,\n            help=\"Skip prompts that exceed max_model_len instead of failing (default: True)\",\n        )\n        parser.add_argument(\n            \"--no-skip-long-prompts\",\n            dest=\"skip_long_prompts\",\n            action=\"store_false\",\n            help=\"Fail on prompts that exceed max_model_len\",\n        )\n\n        args = parser.parse_args()\n\n        main(\n            src_dataset_hub_id=args.src_dataset_hub_id,\n            output_dataset_hub_id=args.output_dataset_hub_id,\n            model_id=args.model_id,\n            messages_column=args.messages_column,\n            output_column=args.output_column,\n            temperature=args.temperature,\n            top_p=args.top_p,\n            top_k=args.top_k,\n            min_p=args.min_p,\n            max_tokens=args.max_tokens,\n            repetition_penalty=args.repetition_penalty,\n            gpu_memory_utilization=args.gpu_memory_utilization,\n            max_model_len=args.max_model_len,\n            tensor_parallel_size=args.tensor_parallel_size,\n            skip_long_prompts=args.skip_long_prompts,\n            hf_token=args.hf_token,\n        )\n    else:\n        # Show HF Jobs example when run without arguments\n        print(\"\"\"\nvLLM Response Generation Script\n==============================\n\nThis script requires arguments. For usage information:\n    uv run generate-responses.py --help\n\nExample HF Jobs command with multi-GPU:\n    # If you're logged in with huggingface-cli, token will be auto-detected\n    hf jobs uv run \\\\\n        --flavor l4x4 \\\\\n        https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py \\\\\n        username/input-dataset \\\\\n        username/output-dataset \\\\\n        --messages-column messages \\\\\n        --model-id Qwen/Qwen3-30B-A3B-Instruct-2507 \\\\\n        --temperature 0.7 \\\\\n        --max-tokens 16384\n        \"\"\")"
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html#an-example-running-qwen3-30b-a3b-instruct-to-generare-summaries-of-datasets-from-2025",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html#an-example-running-qwen3-30b-a3b-instruct-to-generare-summaries-of-datasets-from-2025",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "An example: Running Qwen3-30B-A3B-Instruct to generare summaries of datasets from 2025",
    "text": "An example: Running Qwen3-30B-A3B-Instruct to generare summaries of datasets from 2025\nAs an example, let‚Äôs run Qwen3-30B-A3B-Instruct to generate summaries of datasets from 2025. We‚Äôll use the hf-jobs Python API to create a job that runs a uv Script on 4 GPUs with vLLM. First we‚Äôll quickly prepare the dataset and prompts. We‚Äôll use Polars + datasets to load the dataset and filter it down to the 2025 datasets.\n\nfrom huggingface_hub import snapshot_download\n\n\nsnapshot_download(\n    repo_id=\"librarian-bots/dataset_cards_with_metadata\",\n    local_dir=\"data\",\n    repo_type=\"dataset\",\n    allow_patterns=[\"*.parquet\"],\n)\n\n\n\n\n\n\n\n'/Users/davanstrien/Documents/daniel/blog/posts/2025/hf-jobs/data'\n\n\nWe‚Äôll do some filtering to focus on datasets where the cards are not super long or super short. We‚Äôll also filter to focus on datasets with at least one like and ten downloads.\n\nimport polars as pl\n\n\ndf = pl.scan_parquet(\"data/**/*.parquet\")\n\n\ndf.collect_schema()\n\nSchema([('datasetId', String),\n        ('author', String),\n        ('last_modified', String),\n        ('downloads', Int64),\n        ('likes', Int64),\n        ('tags', List(String)),\n        ('task_categories', List(String)),\n        ('createdAt', String),\n        ('trending_score', Float64),\n        ('card', String)])\n\n\n\ndf = df.filter(pl.col(\"card\").str.len_chars() &gt; 200)\ndf = df.filter(pl.col(\"downloads\") &gt; 2)\ndf = df.filter(pl.col(\"likes\") &gt; 1)\n\nWe make sure we have datetime for the createdAt column so we can filter by year.\n\ndf = df.with_columns(pl.col(\"createdAt\").str.to_datetime())\n\n\nfrom datetime import datetime\n\nthis_year = datetime.now().year\nthis_year\n\n2025\n\n\n\ndf_2025 = df.filter(pl.col(\"createdAt\").dt.year() == this_year)\n\nSince we‚Äôre using the LazyFrame API, we can use the collect method to execute the query and get the results we get a nice optimized query plan. This is very nice since you can be quite lazy in how you filter and transform the data and Polars will optimize the query for you!\n\ndf_2025.show_graph(optimized=True, engine=\"streaming\")\n\n\n\n\n\n\n\n\nPolars and datasets play nicely together so we can easily convert between the two. Since we‚Äôve done all the filtering we want, we can convert the Polars DataFrame to a Datasets Dataset.\n\nfrom datasets import Dataset\n\n\nds = Dataset.from_polars(df_2025.collect())\n\n\nds\n\nDataset({\n    features: ['datasetId', 'author', 'last_modified', 'downloads', 'likes', 'tags', 'task_categories', 'createdAt', 'trending_score', 'card'],\n    num_rows: 2419\n})\n\n\nWe‚Äôll do one more filter to remove datasets that don‚Äôt have a card. We could also do this in Polars but since the huggingface_hub library has a nice way of converting a string into a dataset card where we can seperate the YAML from the main content, we‚Äôll do it using the datasets library and the filter function.\n\nfrom huggingface_hub import DatasetCard\n\n\ndef is_short_card(row, min_length=200):\n    card = DatasetCard(row['card']).text\n    return len(card) &gt; min_length\n\n\nds = ds.filter(is_short_card, num_proc=4)\n\n\n\n\n\nPreparing the prompts\nSince the uv + vLLM script expecets as input a list of prompts, we‚Äôll convert the dataset to a list of prompts. We‚Äôll use the map function to create a list of prompts that we can use for inference. We‚Äôll use the card field of the dataset to create a prompt that asks the model to summarize the dataset.\n\ndef format_prompt_for_card(row, max_length=8000):\n    card = DatasetCard(row['card']).text\n    datasetId = row['datasetId']\n    return f\"\"\"You are a helpful assistant that provides concise summaries of dataset cards for datasets on the Hugging Face Hub.\nThe Hub ID of the dataset is: {datasetId}.\nThe dataset card is as follows:\n{card[:max_length]}]\nPlease write a one to two sentence summary of the dataset card.\nThe summary should be concise and informative, capturing the essence of the dataset.\nThe summary should be in English.\nThe goal of the summary is to provide a quick overview of the dataset's content and purpose. \nThis summary will be used to help users quickly understand the dataset and as input for creating embeddings for the dataset card.\n    \"\"\"\n\n\nprint(format_prompt_for_card(ds[0]))\n\nYou are a helpful assistant that provides concise summaries of dataset cards for datasets on the Hugging Face Hub.\nThe Hub ID of the dataset is: agentlans/high-quality-multilingual-sentences.\nThe dataset card is as follows:\n# High Quality Multilingual Sentences\n\n- This dataset contains multilingual sentences derived from the [agentlans/LinguaNova](https://huggingface.co/datasets/agentlans/LinguaNova) dataset.\n- It includes 1.58 million rows across 51 different languages, each in its own configuration.\n\nExample row (from the `all` config):\n```json\n{\n    \"text\": \"ÿßŸÖÿßŸÖ ÿ¨ŸÖÿπŸá ÿßÿµŸÅŸáÿßŸÜ ⁄ØŸÅÿ™: ŸÖ€åÿ≤ÿßŸÜ ŸÜ€åÿßÿ≤ ÿ¢ÿ® ÿ¥ÿ±ÿ® ÿßÿµŸÅŸáÿßŸÜ €±€±.€µ ŸÖÿ™ÿ± ŸÖ⁄©ÿπÿ® ÿßÿ≥ÿ™ ⁄©Ÿá ÿ™ŸÖÿßŸÖ ÿßÿ≥ÿ™ÿßŸÜ ÿßÿµŸÅŸáÿßŸÜ ÿ±ÿß ŸæŸàÿ¥ÿ¥ ŸÖ€åÿØŸáÿØ Ÿà ŸÜÿ≥ÿ®ÿ™ ÿ®Ÿá ŸÇÿ®ŸÑ ÿßÿ≤ ÿßŸÜŸÇŸÑÿßÿ® €å⁄©€å ÿßÿ≤ Ÿæ€åÿ¥ÿ±ŸÅÿ™Ÿáÿß ÿØÿ± ÿ≠Ÿàÿ≤Ÿá ÿ¢ÿ® ÿ®ŸàÿØŸá ÿßÿ≥ÿ™.\",\n    \"fasttext\": \"fa\",\n    \"gcld3\": \"fa\"\n}\n```\n\nFields:\n- **text**: The sentence in the original language.\n- **fasttext**, **gcld3**: Language codes determined using fastText and gcld3 Python packages.\n\n## Configurations\n\nEach individual language is available as a separate configuration, such as `ar`, `en`. These configurations contain only sentences identified to be of that specific language by both the fastText and gcld3 models.\n\nExample row (from a language-specific config):\n```json\n{\n    \"text\": \"Ne vienas asmuo yra apsaugotas nuo parazit≈≥ atsiradimo organizme.\"\n}\n```\n\n## Methods\n\n### Data Loading and Processing\n\nThe `all` split was downloaded from the [agentlans/LinguaNova](https://huggingface.co/datasets/agentlans/LinguaNova) dataset.\n1. **Text Cleaning**: Raw text was cleaned by removing HTML tags, emails, emojis, hashtags, user handles, and URLs. Unicode characters and whitespace were normalized, and hyphenated words were handled to ensure consistency.\n2. **Sentence Segmentation**: Text was segmented into individual sentences using ICU's `BreakIterator` class, which efficiently processed different languages and punctuation.\n3. **Deduplication**: Duplicate entries were removed to maintain uniqueness and prevent redundancy in the dataset.\n\n### Language Detection\n\nTwo methods were used for language identification:\n1. **gcld3**: Google's Compact Language Detector 3 was used for fast and accurate language identification.\n2. **fastText**: Facebook‚Äôs fastText model was employed, which improved accuracy by considering subword information.\n\n### Quality Assessment\n\nText quality was assessed through batch inference using the [agentlans/multilingual-e5-small-aligned-quality](https://huggingface.co/agentlans/multilingual-e5-small-aligned-quality) model.\n1. **Data Retrieval**: Entries with a quality score of 1 or higher and a minimum input length of 20 characters were retained.\n2. **Text Refinement**: Leading punctuation and spaces were removed, and balanced quotation marks were validated using regular expressions.\n\n### Dataset Configs\n\nThe filtered sentences and their annotated languages were written to the `all.jsonl` file. The file was then split into language-specific JSONL files, containing only those sentences that matched consistently with both gcld3 and fasttext in terms of language identification. Only languages with at least 100 sentences after filtering were included in these configs.\n\n## Usage\n\n### Loading the Dataset\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset('agentlans/high-quality-multilingual-sentences', 'all')\n```\n\nFor language-specific configurations:\n```python\nlanguage_config = load_dataset('agentlans/high-quality-multilingual-sentences', 'en')  # Replace with desired language code.\n```\n\n### Example Usage in Python\n```python\nfrom datasets import load_dataset\n\n# Load the dataset for all languages or a specific one\ndataset_all = load_dataset(\"agentlans/high-quality-multilingual-sentences\", \"all\")\nprint(dataset_all[\"train\"][0])\n\nlanguage_config = load_dataset(\"agentlans/high-quality-multilingual-sentences\", \"en\")  # Replace 'en' with desired language code.\nprint(language_config[\"train\"][:5])\n```\n\n## Limitations\n\n- **Multilingual content bias**: The quality classifier is biased towards educational and more formal content.\n- **Language coverage**: Limited to the 50 written languages from LinguaNova. There's a lack of African and indigenous languages.\n- **Short input issues**: Language identification accuracy can suffer when working with short inputs like single sentences.\n- **Sentence segmentation challenges**: Some languages' delimiters might not be handled correctly.\n- **Redundancy**: The filtering was only done on exact matches so some sentences may be similar (but not identical).\n\nAdditionally:\n- **Thai data imbalance**: Fewer examples are available for `th` (Thai) than expected. Could be a sentence segmentation problem.\n- **Malay and Indonesian**: There are few examples for the `ms` (Malay) subset. Consider also using the `id` (Indonesian) subset when training models.\n- **Chinese written forms**: This dataset does not distinguish between different Chinese character variations.\n\n## Licence\n\nThis dataset is released under a [Creative Commons Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/) licence, allowing for free use and distribution as long as proper attribution is given to the original source.]\nPlease write a one to two sentence summary of the dataset card.\nThe summary should be concise and informative, capturing the essence of the dataset.\nThe summary should be in English.\nThe goal of the summary is to provide a quick overview of the dataset's content and purpose. \nThis summary will be used to help users quickly understand the dataset and as input for creating embeddings for the dataset card.\n    \n\n\n\ndef create_messages(row):\n    return {\"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": format_prompt_for_card(row),\n        },\n    ]}\n\n\nds = ds.map(create_messages, num_proc=4)\n\n\n\n\n\nds\n\nDataset({\n    features: ['datasetId', 'author', 'last_modified', 'downloads', 'likes', 'tags', 'task_categories', 'createdAt', 'trending_score', 'card', 'messages'],\n    num_rows: 2082\n})\n\n\nWe remove columns we don‚Äôt need\n\nds = ds.remove_columns([c for c in ds.column_names if c not in ['messages', 'datasetId']])\n\nAnd push to the Hub!\n\nds.push_to_hub(\"davanstrien/cards_with_prompts\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/datasets/davanstrien/cards_with_prompts/commit/8e32c041eba4fbf1729e3f5a4d1536365185f7d2', commit_message='Upload dataset', commit_description='', oid='8e32c041eba4fbf1729e3f5a4d1536365185f7d2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/davanstrien/cards_with_prompts', endpoint='https://huggingface.co', repo_type='dataset', repo_id='davanstrien/cards_with_prompts'), pr_revision=None, pr_num=None)\n\n\n\n\n\n\n\n\nNote\n\n\n\nHugging Face recently moved most of the backend storage to Xet. The tl;dr of this is that it means that datasets are deduplicated at a much more granular level, this makes working with datasets which change regularly much more efficient. See  for more details. This combined with Jobs could make for a very powerful combination for running jobs on datasets that change frequently."
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html#launching-our-job",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html#launching-our-job",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "Launching our job",
    "text": "Launching our job\nWe now have the dataset with the prompts we want to use for inference.\nThe interface for Jobs should look familiar if you‚Äôve used Docker before. We can use Jobs via CLI or Python API. Via the CLI a basic command to run a job looks like this:\nhf jobs run python:3.12 python -c \"print('Hello from the cloud!')\"\nThere is also an experimental uv command thaty allows us to run uv scripts directly:\nhf jobs uv run script-url\nAs an example, we can run another simple script from the uv scripts org and just print the help for the script:\n\nhf jobs uv run https://huggingface.co/datasets/uv-scripts/deduplication/raw/main/semantic-dedupe.py --help\n\n/Users/davanstrien/Library/Application Support/uv/tools/huggingface-hub/lib/python3.13/site-packages/huggingface_hub/utils/_experimental.py:60: UserWarning: 'HfApi.run_uv_job' is experimental and might be subject to breaking changes in the future without prior notice. You can disable this warning by setting `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` as environment variable.\n  warnings.warn(\nJob started with ID: 688a31096dcd97e42f8095e7\nView at: https://huggingface.co/jobs/davanstrien/688a31096dcd97e42f8095e7\nDownloading pygments (1.2MiB)\nDownloading hf-xet (3.0MiB)\nDownloading numpy (15.9MiB)\nDownloading tokenizers (3.0MiB)\nDownloading setuptools (1.1MiB)\nDownloading aiohttp (1.6MiB)\nDownloading pandas (11.4MiB)\nDownloading pyarrow (40.8MiB)\nDownloading usearch (2.0MiB)\nDownloading hf-transfer (3.4MiB)\nDownloading simsimd (1.0MiB)\n Downloading simsimd\n Downloading usearch\n Downloading tokenizers\n Downloading hf-xet\n Downloading hf-transfer\n Downloading aiohttp\n Downloading pygments\n Downloading setuptools\n Downloading numpy\n Downloading pyarrow\n Downloading pandas\nInstalled 50 packages in 116ms\nusage: semantic-dedupekDOpug.py [-h] [--split SPLIT]\n                                [--method {duplicates,outliers,representatives}]\n                                [--threshold THRESHOLD]\n                                [--batch-size BATCH_SIZE]\n                                [--max-samples MAX_SAMPLES] [--private]\n                                [--hf-token HF_TOKEN]\n                                dataset column output_repo\n\nDeduplicate a dataset using semantic similarity\n\npositional arguments:\n  dataset               Input dataset ID (e.g., 'imdb' or 'username/dataset')\n  column                Text column to deduplicate on\n  output_repo           Output dataset repository name\n\noptions:\n  -h, --help            show this help message and exit\n  --split SPLIT         Dataset split to process (default: train)\n  --method {duplicates,outliers,representatives}\n                        Deduplication method (default: duplicates)\n  --threshold THRESHOLD\n                        Similarity threshold for duplicates (default: 0.9)\n  --batch-size BATCH_SIZE\n                        Batch size for processing (default: 64)\n  --max-samples MAX_SAMPLES\n                        Maximum number of samples to process (for testing)\n  --private             Create private dataset repository\n  --hf-token HF_TOKEN   Hugging Face API token (defaults to HF_TOKEN env var)\n\nExamples:\n  # Basic usage\n  uv run semantic-dedupe.py imdb text imdb-deduped\n\n  # With options\n  uv run semantic-dedupe.py squad question squad-deduped --threshold 0.85 --method duplicates\n\n  # Test with small sample\n  uv run semantic-dedupe.py large-dataset text test-dedup --max-samples 100\n        \n\n\nYou‚Äôll see that uv takes care of installing the dependencies and running the script. This is very convenient since we don‚Äôt have to worry about setting up a virtual environment or installing dependencies manually. This can also be very nice if you want to share a script with others and want to help them avoid getting stuck in dependency hell.\nWe can also run hf jobs via the Python API. This is very convenient if you want to run jobs programmatically or if you want to integrate Jobs into your existing Python code (i.e.¬†to run one step that requires a GPU and another step that doesn‚Äôt)."
  },
  {
    "objectID": "posts/2025/hf-jobs/vllm-batch-inference.html#running-our-inference-job-via-huggingface_hub-library",
    "href": "posts/2025/hf-jobs/vllm-batch-inference.html#running-our-inference-job-via-huggingface_hub-library",
    "title": "Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs",
    "section": "Running our inference job via huggingface_hub library",
    "text": "Running our inference job via huggingface_hub library\nWe can use the huggingface_hub library to run our inference job using run_uv_job\nWe‚Äôll grab a token to pass to our job\n\nfrom huggingface_hub import HfApi, get_token\n\n\nHF_TOKEN = get_token()\n\nWe‚Äôll create an instance of the HfApi class and use the run_uv_job method to run our job. We‚Äôll pass the URL of the script we want to run, the dataset we want to use, and the parameters we want to use for the job.\n\napi = HfApi()\n\nLet‚Äôs see what the run_uv_job method looks like:\n\n?api.run_uv_job\n\n\nSignature:\n\napi.run_uv_job(\n\n    script: 'str',\n\n    *,\n\n    script_args: 'Optional[List[str]]' = None,\n\n    dependencies: 'Optional[List[str]]' = None,\n\n    python: 'Optional[str]' = None,\n\n    image: 'Optional[str]' = None,\n\n    env: 'Optional[Dict[str, Any]]' = None,\n\n    secrets: 'Optional[Dict[str, Any]]' = None,\n\n    flavor: 'Optional[SpaceHardware]' = None,\n\n    timeout: 'Optional[Union[int, float, str]]' = None,\n\n    namespace: 'Optional[str]' = None,\n\n    token: 'Union[bool, str, None]' = None,\n\n    _repo: 'Optional[str]' = None,\n\n) -&gt; 'JobInfo'\n\nDocstring:\n\nRun a UV script Job on Hugging Face infrastructure.\n\n\n\nArgs:\n\n    script (`str`):\n\n        Path or URL of the UV script.\n\n\n\n    script_args (`List[str]`, *optional*)\n\n        Arguments to pass to the script.\n\n\n\n    dependencies (`List[str]`, *optional*)\n\n        Dependencies to use to run the UV script.\n\n\n\n    python (`str`, *optional*)\n\n        Use a specific Python version. Default is 3.12.\n\n\n\n    image (`str`, *optional*, defaults to \"ghcr.io/astral-sh/uv:python3.12-bookworm\"):\n\n        Use a custom Docker image with `uv` installed.\n\n\n\n    env (`Dict[str, Any]`, *optional*):\n\n        Defines the environment variables for the Job.\n\n\n\n    secrets (`Dict[str, Any]`, *optional*):\n\n        Defines the secret environment variables for the Job.\n\n\n\n    flavor (`str`, *optional*):\n\n        Flavor for the hardware, as in Hugging Face Spaces. See [`SpaceHardware`] for possible values.\n\n        Defaults to `\"cpu-basic\"`.\n\n\n\n    timeout (`Union[int, float, str]`, *optional*):\n\n        Max duration for the Job: int/float with s (seconds, default), m (minutes), h (hours) or d (days).\n\n        Example: `300` or `\"5m\"` for 5 minutes.\n\n\n\n    namespace (`str`, *optional*):\n\n        The namespace where the Job will be created. Defaults to the current user's namespace.\n\n\n\n    token `(Union[bool, str, None]`, *optional*):\n\n        A valid user access token. If not provided, the locally saved token will be used, which is the\n\n        recommended authentication method. Set to `False` to disable authentication.\n\n        Refer to: https://huggingface.co/docs/huggingface_hub/quick-start#authentication.\n\n\n\nExample:\n\n\n\n    ```python\n\n    &gt;&gt;&gt; from huggingface_hub import run_uv_job\n\n    &gt;&gt;&gt; script = \"https://raw.githubusercontent.com/huggingface/trl/refs/heads/main/trl/scripts/sft.py\"\n\n    &gt;&gt;&gt; run_uv_job(script, dependencies=[\"trl\"], flavor=\"a10g-small\")\n\n    ```\n\nFile:      ~/Documents/daniel/blog/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py\n\nType:      method\n\n\n\nwe can use the run_uv_job method to run our job. We‚Äôll pass the URL of the script we want to run, the dataset we want to use, and the parameters we want to use for the job. These parameters will be passed to the script as command line arguments. Since we‚Äôre using vLLM, we‚Äôll pass the vllm Docker image. This will mean that our job is run in this Docker container.\n\n\n\n\n\n\nNote\n\n\n\nThis Docker image already has uv installed but if you want to use an image + uv for an image without uv insalled you‚Äôll need to make sure uv is installed first. You can also not specify any image and hf jobs will use the default UV image which has uv installed. This will work well in many cases but for LLM inference libraries which can have quite specific requirements, it can be useful to use a specific image that has the library installed.\n\n\nWe can now run our job using the run_uv_job method. This will start the job and return a job object that we can use to monitor the job‚Äôs progress.\n\njob = api.run_uv_job(\n    script=\"https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py\",\n    script_args=[\n        \"davanstrien/cards_with_prompts\",  # Dataset with prompts\n        \"davanstrien/test-generated-responses\",  # Where to store the generated responses\n        \"--model-id\",  # Model to use for inference\n        \"Qwen/Qwen3-30B-A3B-Instruct-2507\",  # Model to use for inference\n        \"--gpu-memory-utilization\",  # GPU memory utilization\n        \"0.9\",\n        \"--max-tokens\",  # Maximum number of tokens\n        \"900\",\n        \"--max-model-len\",  # Maximum model length\n        \"8000\",\n    ],\n    flavor=\"l4x4\",  # What hardware to use\n    image=\"vllm/vllm-openai:latest\",  # Docker image to use\n    secrets={\"HF_TOKEN\": HF_TOKEN},  # Pass as secret``\n    env={\"UV_PRERELEASE\": \"if-necessary\"},  # Pass as env var\n)\n\n/Users/davanstrien/Documents/daniel/blog/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_experimental.py:60: UserWarning: 'HfApi.run_uv_job' is experimental and might be subject to breaking changes in the future without prior notice. You can disable this warning by setting `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` as environment variable.\n  warnings.warn(\n\n\nWe can get a url for our job, this will give us a page where we can monitor the job‚Äôs progress and see the logs (note this won‚Äôt URL won‚Äôt work for you unless you run this job yourself).\n\nprint(f\"Job URL: {job.url}\")\n\nJob URL: https://huggingface.co/jobs/davanstrien/688a33391c97bc486de2a232\n\n\n\nWe can also print the status of the job\n\njob.status\n\nJobStatus(stage='RUNNING', message=None)\n\n\nThere are also a bunch of other attributes from the job that can be useful when running jobs as part of a larger workflow. For example, we can get the job‚Äôs creation time, the job‚Äôs status etc.\n\njob.created_at\n\ndatetime.datetime(2025, 7, 30, 14, 59, 5, 648000, tzinfo=datetime.timezone.utc)\n\n\n\njob.flavor\n\n'l4x4'\n\n\n\njob.command\n\n['uv',\n 'run',\n 'https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py',\n 'davanstrien/cards_with_prompts',\n 'davanstrien/test-generated-responses',\n '--model-id',\n 'Qwen/Qwen3-30B-A3B-Instruct-2507',\n '--gpu-memory-utilization',\n '0.9',\n '--max-tokens',\n '900',\n '--max-model-len',\n '8000']\n\n\nWe can also grab the logs\n\napi.fetch_job_logs(\n    job_id=job.id,\n)\n\n&lt;generator object HfApi.fetch_job_logs at 0x1612df4c0&gt;\n\n\nThis returns a generator, let‚Äôs turn it into a list so we can print out a few examples of the logs\n\nprint(\n    list(\n        api.fetch_job_logs(\n            job_id=job.id,\n        )\n    )[:-10]\n)  # Print the last 10 lines of logs\n\nWe can also see the resulting dataset for the job here or below. You can see we have the original prompts and the generated responses."
  },
  {
    "objectID": "posts/2025/FineWeb-c/how-fine-is-fineweb2.html",
    "href": "posts/2025/FineWeb-c/how-fine-is-fineweb2.html",
    "title": "How Fine is FineWeb2?",
    "section": "",
    "text": "Recently FineWeb2 was released, adding multilingual data to the original FineWeb dataset. As part of the FineWeb-C community initiative, contributors have been annotating samples from this dataset to assess educational quality across different languages.\nThe goal of this effort is to reproduce something similar to the original FineWeb-Edu dataset for all languages. You can read more about why this is important in the FineWeb-C blog post."
  },
  {
    "objectID": "posts/2025/FineWeb-c/how-fine-is-fineweb2.html#number-of-annotators",
    "href": "posts/2025/FineWeb-c/how-fine-is-fineweb2.html#number-of-annotators",
    "title": "How Fine is FineWeb2?",
    "section": "Number of annotators",
    "text": "Number of annotators\nSince starting this project we‚Äôve found some languages have had more problematic data, including languages where a lot of data was identified in the wrong language. Because of this for some languages we‚Äôve set a higher or lower ‚Äúretirement‚Äù threshold before retiring a sample. This means some languages have only a single annotator labeling each example whilst others have had multiple people annotate each example.\nLet‚Äôs look at the mean number of annotators per language.\n\ndf.group_by([\"language_code\"]).agg(\n    pl.col(\"educational_value_labels\").list.len().mean().alias(\"n_annotators\")\n).sort(\"n_annotators\", descending=True).collect()\n\n\nshape: (91, 2)\n\n\n\nlanguage_code\nn_annotators\n\n\nstr\nf64\n\n\n\n\n\"vie_Latn\"\n2.869\n\n\n\"dan_Latn\"\n2.573\n\n\n\"tir_Ethi\"\n2.126174\n\n\n\"spa_Latn\"\n2.126\n\n\n\"deu_Latn\"\n2.037736\n\n\n\"ita_Latn\"\n2.006\n\n\n\"bak_Cyrl\"\n2.002262\n\n\n\"slk_Latn\"\n2.0\n\n\n\"lvs_Latn\"\n2.0\n\n\n\"eus_Latn\"\n1.166667\n\n\n\"ary_Arab\"\n1.019\n\n\n\"fin_Latn\"\n1.017\n\n\n\"fra_Latn\"\n1.012\n\n\n\"arz_Arab\"\n1.006\n\n\n\"swe_Latn\"\n1.006\n\n\n\"nds_Latn\"\n1.005141\n\n\n\"rus_Cyrl\"\n1.003\n\n\n\"tat_Cyrl\"\n1.002569\n\n\n\"arb_Arab\"\n1.002\n\n\n\"asm_Latn\"\n1.002\n\n\n\"tam_Taml\"\n1.002\n\n\n\"fas_Arab\"\n1.001\n\n\n\"nld_Latn\"\n1.0\n\n\n\"uzn_Latn\"\n1.0\n\n\n\"guj_Gujr\"\n1.0\n\n\n\"cat_Latn\"\n1.0\n\n\n\"cmn_Hani\"\n1.0\n\n\n\"uzn_Cyrl\"\n1.0\n\n\n\"ben_Beng\"\n1.0\n\n\n\"hsb_Latn\"\n1.0\n\n\n\"bul_Cyrl\"\n1.0\n\n\n\"ces_Latn\"\n1.0\n\n\n\"tel_Telu\"\n1.0\n\n\n\"bre_Latn\"\n1.0\n\n\n\"npi_Latn\"\n1.0\n\n\n\"crh_Latn\"\n1.0\n\n\n\"zsm_Latn\"\n1.0\n\n\n\"tur_Latn\"\n1.0\n\n\n\"srp_Cyrl\"\n1.0\n\n\n\"lat_Latn\"\n1.0\n\n\n\"yue_Hani\"\n1.0\n\n\n\"asm_Beng\"\n1.0\n\n\n\"pbt_Arab\"\n1.0\n\n\n\"bar_Latn\"\n1.0\n\n\n\"tha_Thai\"\n1.0\n\n\n\"kor_Hang\"\n1.0\n\n\n\"goh_Latn\"\n1.0\n\n\n\"sin_Sinh\"\n1.0\n\n\n\"lug_Latn\"\n1.0\n\n\n\"mar_Deva\"\n1.0\n\n\n\"mal_Mlym\"\n1.0\n\n\n\"ars_Arab\"\n1.0\n\n\n\"ukr_Cyrl\"\n1.0\n\n\n\"vls_Latn\"\n1.0\n\n\n\"npi_Deva\"\n1.0\n\n\n\"aeb_Arab\"\n1.0\n\n\n\"kas_Deva\"\n1.0\n\n\n\"ekk_Latn\"\n1.0\n\n\n\"som_Latn\"\n1.0\n\n\n\"lit_Latn\"\n1.0\n\n\n\"srp_Latn\"\n1.0\n\n\n\"kin_Latn\"\n1.0\n\n\n\"apc_Arab\"\n1.0\n\n\n\"ron_Latn\"\n1.0\n\n\n\"hin_Deva\"\n1.0\n\n\n\"pdc_Latn\"\n1.0\n\n\n\"bho_Deva\"\n1.0\n\n\n\"ind_Latn\"\n1.0\n\n\n\"lez_Cyrl\"\n1.0\n\n\n\"udm_Cyrl\"\n1.0\n\n\n\"sco_Latn\"\n1.0\n\n\n\"quz_Latn\"\n1.0\n\n\n\"pol_Latn\"\n1.0\n\n\n\"pcm_Latn\"\n1.0\n\n\n\"tat_Latn\"\n1.0\n\n\n\"jpn_Jpan\"\n1.0\n\n\n\"hun_Latn\"\n1.0\n\n\n\"pfl_Latn\"\n1.0\n\n\n\"lij_Latn\"\n1.0\n\n\n\"yor_Latn\"\n1.0\n\n\n\"fao_Latn\"\n1.0\n\n\n\"nan_Latn\"\n1.0\n\n\n\"ast_Latn\"\n1.0\n\n\n\"gmh_Latn\"\n1.0\n\n\n\"nob_Latn\"\n1.0\n\n\n\"fil_Latn\"\n1.0\n\n\n\"gom_Deva\"\n1.0\n\n\n\"glg_Latn\"\n1.0\n\n\n\"tok_Latn\"\n1.0\n\n\n\"por_Latn\"\n1.0\n\n\n\"gsw_Latn\"\n1.0\n\n\n\n\n\n\nIdeally as we progress with the project we‚Äôll increase the overlap at least for some time to see how much annotators agree (more on this below) but we also don‚Äôt want people to spend a lot of time annotating the same sample if it‚Äôs of low quality.\n\nLook at the distribution of labels\nNo we‚Äôve got a sense of how many annotators there are per language, let‚Äôs look at the distribution of labels. Since we sometimes have multiple labels for each sample we‚Äôll simplify things a bit to just look at the first label for each sample. For some languages this will be the only label anyway. If we work with a specific language for a longer time we‚Äôll also want to consider the agreement between annotators.\n\ndf = df.with_columns(\n    pl.col(\"educational_value_labels\").list.first().alias(\"label\")\n).collect()\ndf.head(1)\n\n\nshape: (1, 9)\n\n\n\nid\ntext\neducational_value_labels\nannotator_ids\nproblematic_content_label_present\nproblematic_content_label_agreement\nlanguage_names\nlanguage_code\nlabel\n\n\nstr\nstr\nlist[str]\nlist[str]\nbool\nf64\nstr\nstr\nstr\n\n\n\n\n\"3b69c85c-29ec-4f01-a0dd-49ed1b‚Ä¶\n\"ŸÜÿ¨ŸÖÿ© ŸÅŸä ÿßŸÑÿ≥ŸÖÿß ŸàŸÑŸÖÿ±Ÿàÿ±ŸÉ ŸÑÿØŸäŸÜÿß ŸÉÿß‚Ä¶\n[\"None\"]\n[\"73ee8b99-a3f3-484b-bccf-7874521a5bc7\"]\nfalse\n1.0\n\"aeb_Arab\"\n\"aeb_Arab\"\n\"None\"\n\n\n\n\n\n\n\n\nClick to show/hide code\nlabel_counts = (\n    df.group_by([\"language_code\", \"label\"])\n    .len()\n    .pivot(\n        values=\"len\",\n        index=\"language_code\",\n        on=\"label\",\n        aggregate_function=\"sum\",\n    )\n    .fill_null(0)\n)\n\n# Calculate row totals\nrow_totals = label_counts.select(pl.exclude(\"language_code\")).sum_horizontal()\n\n# Calculate percentages\nlabel_percentages = (\n    label_counts.with_columns(\n        pl.col(col) / row_totals * 100\n        for col in label_counts.columns\n        if col != \"language_code\"\n    )\n    .select([\"language_code\", pl.all().exclude(\"language_code\").round(2)])\n    .sort(\"language_code\")\n)\n\nlabel_percentages\n\n\n\nshape: (91, 8)\n\n\n\nlanguage_code\nGood\nBasic\n‚ùó Problematic Content ‚ùó\nNone\nMinimal\n‚ùó Wrong language ‚ùó\nExcellent\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"aeb_Arab\"\n0.57\n4.16\n12.48\n77.69\n4.73\n0.0\n0.38\n\n\n\"apc_Arab\"\n2.4\n2.4\n80.0\n12.0\n2.0\n0.0\n1.2\n\n\n\"arb_Arab\"\n2.5\n4.9\n25.3\n51.0\n15.6\n0.0\n0.7\n\n\n\"ars_Arab\"\n0.6\n1.9\n15.5\n69.9\n10.5\n1.4\n0.2\n\n\n\"ary_Arab\"\n1.4\n0.7\n94.6\n1.1\n1.2\n0.0\n1.0\n\n\n\"arz_Arab\"\n4.0\n4.1\n73.5\n12.7\n3.7\n0.0\n2.0\n\n\n\"asm_Beng\"\n12.5\n8.33\n0.0\n37.5\n12.5\n0.0\n29.17\n\n\n\"asm_Latn\"\n0.0\n0.3\n19.1\n64.7\n5.0\n10.9\n0.0\n\n\n\"ast_Latn\"\n12.5\n17.5\n7.5\n30.0\n28.75\n3.75\n0.0\n\n\n\"bak_Cyrl\"\n1.13\n4.75\n4.3\n75.11\n14.48\n0.0\n0.23\n\n\n\"bar_Latn\"\n0.0\n4.1\n77.0\n16.5\n2.4\n0.0\n0.0\n\n\n\"ben_Beng\"\n6.67\n0.0\n6.67\n60.0\n20.0\n6.67\n0.0\n\n\n\"bho_Deva\"\n3.11\n5.29\n7.93\n69.67\n13.84\n0.0\n0.16\n\n\n\"bre_Latn\"\n2.17\n2.17\n91.3\n2.17\n2.17\n0.0\n0.0\n\n\n\"bul_Cyrl\"\n11.11\n19.44\n8.33\n19.44\n33.33\n0.0\n8.33\n\n\n\"cat_Latn\"\n8.82\n17.65\n0.0\n32.35\n41.18\n0.0\n0.0\n\n\n\"ces_Latn\"\n1.96\n7.63\n1.96\n73.64\n14.38\n0.44\n0.0\n\n\n\"cmn_Hani\"\n9.7\n25.1\n2.5\n24.2\n36.2\n0.9\n1.4\n\n\n\"crh_Latn\"\n0.0\n0.53\n96.28\n0.53\n2.13\n0.0\n0.53\n\n\n\"dan_Latn\"\n1.2\n8.4\n14.5\n47.6\n28.2\n0.0\n0.1\n\n\n\"deu_Latn\"\n9.43\n14.62\n3.3\n45.28\n22.64\n0.0\n4.72\n\n\n\"ekk_Latn\"\n2.13\n12.77\n17.02\n38.3\n29.79\n0.0\n0.0\n\n\n\"eus_Latn\"\n6.91\n10.57\n12.6\n44.31\n24.39\n0.0\n1.22\n\n\n\"fao_Latn\"\n0.0\n12.5\n8.33\n45.83\n29.17\n4.17\n0.0\n\n\n\"fas_Arab\"\n8.9\n10.0\n20.5\n39.6\n18.7\n0.0\n2.3\n\n\n\"fil_Latn\"\n2.6\n5.1\n37.9\n24.1\n27.6\n1.9\n0.8\n\n\n\"fin_Latn\"\n18.2\n20.6\n8.5\n19.7\n28.5\n1.9\n2.6\n\n\n\"fra_Latn\"\n5.7\n8.6\n4.8\n54.3\n23.8\n0.0\n2.8\n\n\n\"glg_Latn\"\n5.28\n10.85\n2.05\n54.84\n24.93\n0.0\n2.05\n\n\n\"gmh_Latn\"\n0.0\n0.2\n98.3\n1.0\n0.5\n0.0\n0.0\n\n\n\"goh_Latn\"\n0.0\n0.0\n67.9\n8.4\n0.2\n23.4\n0.1\n\n\n\"gom_Deva\"\n0.0\n0.0\n100.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"gsw_Latn\"\n3.6\n2.2\n1.3\n29.3\n2.5\n60.0\n1.1\n\n\n\"guj_Gujr\"\n0.0\n27.27\n9.09\n18.18\n45.45\n0.0\n0.0\n\n\n\"hin_Deva\"\n0.2\n0.9\n0.3\n87.1\n11.2\n0.0\n0.3\n\n\n\"hsb_Latn\"\n25.93\n33.33\n0.0\n14.81\n18.52\n3.7\n3.7\n\n\n\"hun_Latn\"\n4.84\n6.45\n6.45\n50.81\n30.65\n0.81\n0.0\n\n\n\"ind_Latn\"\n6.9\n6.9\n44.83\n31.03\n10.34\n0.0\n0.0\n\n\n\"ita_Latn\"\n8.3\n14.0\n7.2\n43.9\n25.3\n0.0\n1.3\n\n\n\"jpn_Jpan\"\n0.7\n3.8\n4.6\n76.7\n13.9\n0.1\n0.2\n\n\n\"kas_Deva\"\n4.3\n6.99\n37.63\n40.32\n10.75\n0.0\n0.0\n\n\n\"kin_Latn\"\n13.49\n20.94\n0.88\n33.88\n20.83\n0.0\n9.98\n\n\n\"kor_Hang\"\n3.13\n12.99\n9.55\n39.75\n33.8\n0.0\n0.78\n\n\n\"lat_Latn\"\n0.0\n0.0\n4.03\n95.3\n0.67\n0.0\n0.0\n\n\n\"lez_Cyrl\"\n0.0\n14.55\n0.0\n16.36\n67.27\n1.82\n0.0\n\n\n\"lij_Latn\"\n0.4\n0.5\n13.2\n5.6\n1.7\n78.4\n0.2\n\n\n\"lit_Latn\"\n13.0\n14.55\n9.6\n35.6\n23.53\n1.55\n2.17\n\n\n\"lug_Latn\"\n1.52\n6.06\n27.27\n28.79\n32.58\n3.03\n0.76\n\n\n\"lvs_Latn\"\n3.75\n6.25\n15.0\n60.0\n15.0\n0.0\n0.0\n\n\n\"mal_Mlym\"\n50.0\n0.0\n10.0\n20.0\n10.0\n10.0\n0.0\n\n\n\"mar_Deva\"\n14.29\n39.29\n0.0\n14.29\n26.79\n0.0\n5.36\n\n\n\"nan_Latn\"\n0.0\n14.29\n14.29\n0.0\n14.29\n50.0\n7.14\n\n\n\"nds_Latn\"\n8.48\n14.91\n40.87\n17.48\n13.88\n0.0\n4.37\n\n\n\"nld_Latn\"\n6.37\n14.96\n3.88\n44.04\n29.09\n0.0\n1.66\n\n\n\"nob_Latn\"\n8.33\n8.33\n33.33\n25.0\n25.0\n0.0\n0.0\n\n\n\"npi_Deva\"\n70.54\n10.85\n0.26\n14.47\n2.84\n0.0\n1.03\n\n\n\"npi_Latn\"\n0.0\n8.33\n8.33\n16.67\n58.33\n8.33\n0.0\n\n\n\"pbt_Arab\"\n9.09\n0.0\n0.0\n0.0\n90.91\n0.0\n0.0\n\n\n\"pcm_Latn\"\n0.0\n0.0\n62.92\n3.33\n0.83\n32.92\n0.0\n\n\n\"pdc_Latn\"\n0.0\n0.0\n100.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"pfl_Latn\"\n0.0\n1.1\n87.2\n8.2\n3.3\n0.2\n0.0\n\n\n\"pol_Latn\"\n2.0\n8.0\n8.0\n58.0\n24.0\n0.0\n0.0\n\n\n\"por_Latn\"\n5.2\n9.9\n5.94\n60.15\n16.58\n0.0\n2.23\n\n\n\"quz_Latn\"\n0.0\n0.0\n0.0\n26.25\n60.0\n13.75\n0.0\n\n\n\"ron_Latn\"\n2.99\n5.22\n5.6\n63.81\n20.9\n0.0\n1.49\n\n\n\"rus_Cyrl\"\n3.0\n6.1\n7.6\n68.3\n13.9\n0.0\n1.1\n\n\n\"sco_Latn\"\n1.36\n4.08\n81.63\n7.82\n5.1\n0.0\n0.0\n\n\n\"sin_Sinh\"\n4.27\n12.32\n5.21\n16.59\n60.19\n1.42\n0.0\n\n\n\"slk_Latn\"\n3.17\n7.69\n4.52\n60.18\n22.17\n0.0\n2.26\n\n\n\"som_Latn\"\n36.36\n45.45\n0.0\n0.0\n9.09\n0.0\n9.09\n\n\n\"spa_Latn\"\n2.9\n7.3\n3.9\n60.7\n24.5\n0.0\n0.7\n\n\n\"srp_Cyrl\"\n16.13\n23.66\n17.2\n7.53\n34.41\n0.0\n1.08\n\n\n\"srp_Latn\"\n0.0\n18.18\n0.0\n63.64\n18.18\n0.0\n0.0\n\n\n\"swe_Latn\"\n3.3\n8.5\n8.5\n54.5\n24.1\n0.0\n1.1\n\n\n\"tam_Taml\"\n1.6\n5.0\n5.3\n73.7\n14.3\n0.0\n0.1\n\n\n\"tat_Cyrl\"\n3.85\n33.78\n2.5\n36.61\n21.97\n0.0\n1.28\n\n\n\"tat_Latn\"\n2.4\n22.6\n6.73\n43.27\n23.56\n1.44\n0.0\n\n\n\"tel_Telu\"\n22.73\n16.67\n1.52\n25.76\n27.27\n0.0\n6.06\n\n\n\"tha_Thai\"\n7.06\n10.22\n29.93\n32.6\n19.71\n0.0\n0.49\n\n\n\"tir_Ethi\"\n7.92\n13.83\n17.45\n25.5\n30.74\n0.0\n4.56\n\n\n\"tok_Latn\"\n9.09\n36.36\n18.18\n9.09\n9.09\n18.18\n0.0\n\n\n\"tur_Latn\"\n19.26\n20.05\n7.92\n9.76\n35.62\n0.0\n7.39\n\n\n\"udm_Cyrl\"\n1.82\n12.73\n0.0\n7.27\n76.36\n1.82\n0.0\n\n\n\"ukr_Cyrl\"\n7.7\n6.4\n1.4\n73.6\n8.6\n0.0\n2.3\n\n\n\"uzn_Cyrl\"\n15.0\n30.0\n0.0\n0.0\n45.0\n0.0\n10.0\n\n\n\"uzn_Latn\"\n20.45\n27.27\n4.55\n4.55\n43.18\n0.0\n0.0\n\n\n\"vie_Latn\"\n6.4\n13.1\n9.5\n35.9\n33.9\n0.0\n1.2\n\n\n\"vls_Latn\"\n0.2\n15.1\n79.1\n3.9\n1.7\n0.0\n0.0\n\n\n\"yor_Latn\"\n5.64\n9.71\n26.41\n29.12\n24.15\n0.0\n4.97\n\n\n\"yue_Hani\"\n5.5\n9.6\n0.9\n68.4\n13.8\n0.0\n1.8\n\n\n\"zsm_Latn\"\n4.5\n10.1\n0.4\n44.9\n37.3\n0.1\n2.7\n\n\n\n\n\n\nThis is a bit hard to read so let‚Äôs plot it as a heatmap.\n\n\nClick to show/hide code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Convert Polars DataFrame to pandas for seaborn\nlabel_percentages_pd = label_percentages.to_pandas()\n\n# Clean up column names by replacing the problematic symbols\nlabel_percentages_pd.columns = [\n    col.replace(\"‚ùó\", \"!\") for col in label_percentages_pd.columns\n]\n\n# Define the desired column order\ncolumn_order = [\n    \"! Problematic Content !\",\n    \"! Wrong language !\",\n    \"None\",\n    \"Basic\",\n    \"Minimal\",\n    \"Good\",\n    \"Excellent\",\n]\n\n# Reorder the columns (excluding 'language_code' which will be the index)\nlabel_percentages_pd = label_percentages_pd.set_index(\"language_code\")[column_order]\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(\n    label_percentages_pd,\n    annot=True,\n    fmt=\".1f\",  # Keep one decimal place\n    cmap=\"YlOrRd\",\n    cbar_kws={\"label\": \"Percentage (%)\"},\n    square=False,  # Changed from True to allow rectangular cells\n)\n\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\n\n# Add more padding\nplt.title(\"Distribution of Labels by Language (%)\", pad=5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also look at the distribution of labels as a bar chart.\n\n\nClick to show/hide code\nimport altair as alt\n\n# Convert the wide-format data to long format for Altair\nplot_df = label_percentages.unpivot(\n    index=[\"language_code\"],\n    on=label_percentages.columns[1:],  # All columns except language_code\n    variable_name=\"label\",\n    value_name=\"percentage\",\n)\n\n# Create the Altair chart\nchart = (\n    alt.Chart(plot_df)\n    .mark_bar()\n    .encode(\n        x=\"language_code\",\n        y=alt.Y(\"percentage\", stack=\"zero\"),\n        color=\"label\",\n        tooltip=[\"language_code\", \"label\", \"percentage\"],\n    )\n    .properties(\n        width=1000,\n        height=500,\n        title=\"Label Distribution by Language\",\n    )\n    .configure_axis(labelAngle=45)\n)\n\nchart"
  },
  {
    "objectID": "posts/2025/FineWeb-c/how-fine-is-fineweb2.html#refining-fineweb2-for-more-languages-aka-next-steps",
    "href": "posts/2025/FineWeb-c/how-fine-is-fineweb2.html#refining-fineweb2-for-more-languages-aka-next-steps",
    "title": "How Fine is FineWeb2?",
    "section": "Refining FineWeb2 for more languages aka next steps",
    "text": "Refining FineWeb2 for more languages aka next steps\nWe can see that the distribution of labels varies a fair amount between languages. Some are mostly problematic labels whilst others have a much better distribution of labels. This can also inform what is the best next step for a languages as part of the FineWeb-C project.\n\nBetter starting data for annotators aka more filtering of problematic data\nFineWeb2 already has a lot of filters to try and identify high quality data, however this is challenging to do across many many languages. This is why the community is so important.\n\nIf a language has a lot of problematic or None labels it makes sense to focus on getting higher quality data filtered for that language.\nIn a previous blog post I showed some approaches to how this can be done for a single language. The best approach will depend on the language but developing better heuristics for more languages can be very impactful. This is another area where the community can help!\nSome languages have a lot of data that has been incorrectly identified as the wrong language. This is a problem for the community as it means annotators have to spend a lot of time annotating data that is not useful. For these languages if the community has better heuristics or models for identifying the wrong language we can filter out more data.\n\n\n\nEvaluating agreement between annotators\nIf a language has a good distribution of labels the next step is to try and look at the agreement between annotators. This can help us understand how much we can trust the labels and also help us understand where the labels are ambiguous.\n\n\nEvaluating LLMs to build training data for a classifier?\nFor languages where we have fairly good agreement (or at least a subset of good agreement) we can already start to evaluate how well LLMs can do at identifying educational content. Remembering back to the overall goal of this effort, is to reproduce something similar to FineWeb-Edu. This is a subset of the original FineWeb dataset which has been filtered for educational content. This filtering was done by first training a classifier on the original FineWeb-Edu dataset and then using this classifier to filter the original FineWeb dataset. That classifier was training on labels generated by Llama. For some languages it might be possible to take a similar approach. We can do something like:\n\nEvaluate how well an LLM does in comparison to our annotators. This might require experimenting with different LLMs and prompting approaches. I‚Äôm working on a notebook to do this but this is another area where the community can help.\nIf the LLM does a good job we can then use it to create enough data to start to train a classifier.\n\n\n\nTraining educational quality classifiers\nSome languages already have quite a lot of data with a fairly good distribution of labels. For these languages we can already see if we have sufficient data to train a classifier. It‚Äôs likely for most languages we‚Äôd need more data but starting to train a classifier can be a good next step. Again this is another area where the community can help!\nAs you may have noticed, I‚Äôve been using the word ‚Äúcommunity‚Äù a lot in this post. This is because I think this is a really important part of the FineWeb-C project. We have a Discord channel where we can discuss the project and share ideas. If you‚Äôre interested in contributing to the project please join the channel and let‚Äôs chat!"
  },
  {
    "objectID": "posts/2025/grpo/trl_log_completions_to_hf_datasets.html",
    "href": "posts/2025/grpo/trl_log_completions_to_hf_datasets.html",
    "title": "Log GRPO Completions to ü§ó Datasets",
    "section": "",
    "text": "During GRPO training, it can be useful to stare at your completions and try and understand how the different reward functions are behaving. This notebook shows an experimental branch of trl which pushes completions to a ü§ó Dataset repo.\nYou can also play with this in Colab\nThis results in a dataset that looks like this:\nCode\nfrom IPython.display import HTML\n\nHTML('''\n&lt;iframe\n  src=\"https://huggingface.co/datasets/davanstrien/test-logs/embed/viewer/default/train\"\n  frameborder=\"0\"\n  width=\"100%\"\n  height=\"560px\"&gt;\n&lt;/iframe&gt;\n''')\nOnce you have this data in a HF repo, you can go on an work with it with whatever tools you prefer.\nTo try this out install from this fork of TRL\nNOTE this is an experiment so don‚Äôt expect everything to work super well!\n!pip install git+https://github.com/davanstrien/trl.git@log-data\n!pip install polars hvplot altair--upgrade\nThis is basically the same as the example code in the GRPO docs in TRL. I just add some extra reward functions so we can see what the outputs for multiple rewards look like.\nThe main things we add are: - log_completions=True - log_completions_hub_repo='davanstrien/test-logs'\nThe first option will enable the logging of completions (this goes to WandB too) and the second option will push the completions to a ü§ó Dataset repo.\nNote at the moment we don‚Äôt overwrite the dataset if it already exists on the Hub.\nfrom datasets import load_dataset\nfrom trl import GRPOConfig, GRPOTrainer\n\ndataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n\ndef reward_len(completions, **kwargs):\n    return [-abs(20 - len(completion)) for completion in completions]\n\ndef reward_shouting(completions: list[str], **kwargs) -&gt; list[float]:\n    \"\"\"Reward text completions where all alphabetic words are in uppercase letters.\"\"\"\n    results = []\n\n    for completion in completions:\n        words = completion.split()\n        all_uppercase = True\n\n        for word in words:\n            # Extract only alphabetic characters\n            alpha_only = ''.join(char for char in word if char.isalpha())\n\n            # Skip empty strings or strings with no alphabetic characters\n            if not alpha_only:\n                continue\n\n            # Check if the alphabetic part is uppercase\n            if not alpha_only.isupper():\n                all_uppercase = False\n                break\n\n        results.append(50.0 if all_uppercase else 0.0)\n\n    return results\n\ndef reward_emojis(completions: list[str], **kwargs) -&gt; list[float]:\n    \"\"\"Reward text completions that contain emojis, with extra points for ü§ó.\"\"\"\n    results = []\n\n    for completion in completions:\n        # Base score - check if any emoji exists in the completion\n        has_any_emoji = any(char for char in completion if ord(char) &gt; 127000)\n        base_score = 10.0 if has_any_emoji else 0.0\n\n        # Bonus points for ü§ó (hugging face emoji)\n        hugging_face_count = completion.count('ü§ó')\n        bonus_score = hugging_face_count * 5.0\n\n        # Total score\n        total_score = base_score + bonus_score\n        results.append(total_score)\n\n    return results\n\n\ntraining_args = GRPOConfig(output_dir=\"Qwen2-0.5B-GRPO\",\n                           logging_steps=1,\n                           per_device_train_batch_size=4,\n                           per_device_eval_batch_size=4,\n                           num_generations=2,\n                           log_completions=True, \n                           max_steps=300,\n                           log_completions_hub_repo='davanstrien/test-logs') # repo to push completions to\ntrainer = GRPOTrainer(\n    model=\"Qwen/Qwen2-0.5B-Instruct\",\n    reward_funcs=[reward_len, reward_shouting, reward_emojis],\n    args=training_args,\n    train_dataset=dataset,\n)\ntrainer.train()"
  },
  {
    "objectID": "posts/2025/grpo/trl_log_completions_to_hf_datasets.html#visualizing-the-data",
    "href": "posts/2025/grpo/trl_log_completions_to_hf_datasets.html#visualizing-the-data",
    "title": "Log GRPO Completions to ü§ó Datasets",
    "section": "Visualizing the data",
    "text": "Visualizing the data\nOnce we‚Äôve finishes training we can work with the completions dataset using Polars, Pandas, etc. This can help give us more insight into how the different reward functions are behaving and how we might modify our traing process.\n\nimport polars as pl\n\n# Login using e.g. `huggingface-cli login` to access this dataset\ndf = pl.read_parquet(\"hf://datasets/davanstrien/test-logs/**/*.parquet\")\ndf.describe()\n\nWe can already see that we need to do something different if we want more shouting. We can see that the emojis reward is triggered sometimes but not that often.\nWe can also do things like plot the length reward over time.\n\nimport polars as pl\nimport altair as alt\n\ndf_sorted = df.sort(\"step\")\ndf_with_avg = df_sorted.with_columns(\n    pl.col(\"reward_reward_len\")\n    .rolling_mean(window_size=10, min_samples=1)\n    .alias(\"rolling_avg\")\n)\n\n# Now create separate charts and combine them\nscatter_chart = (\n    alt.Chart(df_with_avg)\n    .mark_circle(opacity=0.6)\n    .encode(\n        x=alt.X(\"step:Q\", title=\"Step\"),\n        y=alt.Y(\"reward_reward_len:Q\", title=\"Length Reward Score\"),\n        tooltip=[\"step\", \"reward_reward_len\"],\n    )\n)\n\nline_chart = (\n    alt.Chart(df_with_avg)\n    .mark_line(color=\"purple\", size=2)\n    .encode(x=\"step:Q\", y=\"rolling_avg:Q\")\n)\n\ncombined_chart = (\n    (scatter_chart + line_chart)\n    .properties(\n        width=800,\n        height=400,\n        title=\"Length Rewards Over Time with 10-step Moving Average\",\n    )\n    .configure_axis(labelFontSize=12, titleFontSize=14)\n)\n\n\ncombined_chart\n\n\n\n\n\n\n\n\nCreate a heatmap showing when different reward types contribute most\n\n\nheatmap_data = df.with_columns([\n    pl.col('step').cast(pl.Int32) // 10 * 10\n]).group_by('step').agg([\n    pl.mean('reward_reward_len').alias('avg_len_reward'),\n    pl.mean('reward_reward_shouting').alias('avg_shouting_reward'),\n    pl.mean('reward_reward_emojis').alias('avg_emoji_reward')\n])\n\nheatmap_long = heatmap_data.unpivot(\n    index=['step'],\n    on=['avg_len_reward', 'avg_shouting_reward', 'avg_emoji_reward'],\n    variable_name='reward_type',\n    value_name='value'\n)\n\nheatmap = alt.Chart(heatmap_long).mark_rect().encode(\n    x='step:O',\n    y='reward_type:N',\n    color=alt.Color('value:Q', scale=alt.Scale(scheme='viridis'))\n).properties(\n    width=800,\n    height=200,\n    title='Reward Types Intensity Over Time'\n)\nheatmap\n\n\n\n\n\n\n\n\n\nCreate histograms to see distribution of rewards\n\nhistogram = alt.Chart(df).transform_fold(\n    ['reward', 'reward_reward_len', 'reward_reward_shouting', 'reward_reward_emojis'],\n    as_=['Reward Type', 'Value']\n).mark_bar(opacity=0.7).encode(\n    alt.X('Value:Q', bin=alt.Bin(maxbins=30)),\n    alt.Y('count():Q'),\n    alt.Color('Reward Type:N'),\n    alt.Row('Reward Type:N')\n).properties(\n    width=600,\n    height=150,\n    title='Distribution of Different Reward Types'\n)\nhistogram"
  },
  {
    "objectID": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html",
    "href": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html",
    "title": "Who Benefits? Rethinking Library Data in the Age of AI",
    "section": "",
    "text": "As someone who works in AI but came from libraries, I see an exciting convergence happening. Right now, as AI systems are being built that will shape information access for decades, libraries have an unprecedented opportunity to assert their role as partners in this transformation; moving beyond being just data providers to becoming co-creators of the future information ecosystem."
  },
  {
    "objectID": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#collections-as-training-data-the-current-landscape",
    "href": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#collections-as-training-data-the-current-landscape",
    "title": "Who Benefits? Rethinking Library Data in the Age of AI",
    "section": "Collections as Training Data: The Current Landscape",
    "text": "Collections as Training Data: The Current Landscape\nThere‚Äôs growing momentum around libraries contributing digitized collections to AI training datasets, for example Harvard‚Äôs Institutional Data Initiative aims to help institutions ‚Äúrefine and publish their collections as data.‚Äù This follows on from existing work on ‚Äúcollections as data‚Äù. The Institutional Data Initiative presents an exciting model for how libraries can explore what new roles for libraries in relation to data and AI could look like.\nEleutherAI‚Äôs Common Pile has taken concrete steps‚Äîreleasing an 8TB corpus of openly licensed text that includes nearly 300,000 digitized public-domain books from the Library of Congress and the Internet Archive. They‚Äôve demonstrated that models trained on this openly licensed data (their Comma models) can perform comparably to those trained on unlicensed datasets.\nCommon Pile represents genuine progress and thoughtful collaboration: they‚Äôve built tools for data extraction and license identification, collaborated with Mozilla on dataset standards, and expressed interest in partnerships with libraries to improve data quality through better OCR and collaborative dataset development. This is exactly the kind of respectful partnership approach that can benefit everyone.\nHowever, not all AI development follows this model. Many large tech companies have taken a different approach, scraping library collections without attribution or partnership. The National Library of the Netherlands captured this concern well in their AI statement, stating explicitly that they ‚Äúrestrict access of commercial AI to KB collections‚Äù.\nEven in well-intentioned collaborations, there‚Äôs often an asymmetry: libraries provide data while the tools, infrastructure, and primary benefits of AI development flow elsewhere. The real opportunity isn‚Äôt just about contributing more data: it‚Äôs about how libraries can meaningfully participate in and benefit from AI development."
  },
  {
    "objectID": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#who-benefits-libraries-as-essential-partners",
    "href": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#who-benefits-libraries-as-essential-partners",
    "title": "Who Benefits? Rethinking Library Data in the Age of AI",
    "section": "Who Benefits? Libraries as Essential Partners",
    "text": "Who Benefits? Libraries as Essential Partners\nHow can libraries benefit from actively participating in AI development?1 The answer lies in recognizing that the technical infrastructure for AI isn‚Äôt separate from library needs - it can directly enhance library services and capabilities.\n\nLibraries as Data Stewards\nLibraries have a strong track record as active data stewards and have repeatedly evolved to meet new challenges: web archiving, research data management, and open access are just recent examples. Whilst it requires libraries to actively engage with the AI community, this represents another area where libraries can play a vital role as stewards of new types of data, thinking through challenges like how to describe, preserve, and provide access to this data in a useful way for the long term.\n\n\nAddressing Language and Cultural Gaps\nCurrent AI systems under-serve many languages and cultural contexts that libraries uniquely preserve and protect. The National Library of Norway‚Äôs AI Lab ongoing work to release datasets and models for Norwegian, the Swedish National Library‚Äôs training of a Swedish speech recognition model (kb-whisper-large), and Common Corpus‚Äôs use of library collections to create a multilingual LLM pre-training dataset all demonstrate the vital role libraries play in improving linguistic diversity in AI."
  },
  {
    "objectID": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#a-practical-path-forward-better-collections-through-ml-ready-datasets",
    "href": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#a-practical-path-forward-better-collections-through-ml-ready-datasets",
    "title": "Who Benefits? Rethinking Library Data in the Age of AI",
    "section": "A Practical Path Forward: Better Collections Through ML-Ready Datasets",
    "text": "A Practical Path Forward: Better Collections Through ML-Ready Datasets\nHere‚Äôs my key pitch: the tools needed to create ML-ready datasets are the same tools that can transform how libraries serve their communities.\nTo create truly useful datasets for both AI and library purposes, we need:\n\nBetter OCR quality assessment and improvement: Move beyond crude heuristics and internal confidence scores to systematically identify and correct OCR errors at scale. Recent developments in OCR technology could significantly improve the quality of digitized GLAM collections.\nEnhanced transcription for archival audio and video: While tools like Whisper provide a starting point for speech recognition, the GLAM sector can co-create better training data for models that handle the nuances of historical recordings, dialects, and specialized terminology.\nSpecialized classifiers for GLAM-specific tasks: The recent resurgence of efficient AI classifiers opens new possibilities. These tools can automatically categorize collections, generate metadata, identify sensitive content, and enhance discovery‚Äîall while being affordable enough for library-scale deployment.\n\nThe benefits are concrete and empowering: Libraries would gain semantic search capabilities that could transform patron discovery, automated metadata generation tools that help librarians process decades-old backlogs more efficiently, and systems to identify and correct digitization errors at scale. These tools don‚Äôt replace librarians - they amplify their expertise, allowing them to focus on the complex curatorial and community work that only humans can do. These aren‚Äôt distant possibilities - they‚Äôre practical outcomes of preparing collections for ML use."
  },
  {
    "objectID": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#how-to-get-there-a-collaborative-approach",
    "href": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#how-to-get-there-a-collaborative-approach",
    "title": "Who Benefits? Rethinking Library Data in the Age of AI",
    "section": "How to Get There: A Collaborative Approach",
    "text": "How to Get There: A Collaborative Approach\nOne path forward: a collaborative effort where libraries work with AI developers to co-create the tools necessary to transform existing collections into materials useful for both AI training and library purposes.\nThe output would be a series of datasets released as machine-readable text in simple, accessible formats2. These datasets would remain under institutional control while capturing the nuances of each collection. Standardization would enable easy combination for large-scale digital humanities research, aggregation in shared library platforms, and use as training data.\n\nFineBooks: A Concrete Starting Point\nHuggingFace‚Äôs recent FineWeb dataset ‚Äî a carefully curated web dataset ‚Äî set new standards for transparency in AI training data by documenting every design choice and demonstrating how openness advances quality dataset creation. This aimed to democratize the knowledge required to create a high quality web corpus for use as LLM training data.\nFollowing this model, I propose FineBooks as a first collaborative project. Many libraries already have OCR‚Äôd book collections with partial metadata and highly variable quality. Books offer a pragmatic starting point: improving OCR quality for historic books is more tractable than newspapers, while the benefits remain substantial.\nThis shared effort would: - Create a massive new resource for digital humanities researchers and historians - Develop OCR tools specifically for historic texts, building on open-source approaches like those from Allen AI - Establish a model where libraries drive the development process - Ensure the tools and standards created serve library collections and their users, not just external AI training needs\nThe data is already open, existing tools can be tested, and the scope is manageable. Most importantly, it positions libraries as partners shaping the future of AI, not passive providers of raw material."
  },
  {
    "objectID": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#a-choice-for-libraries",
    "href": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#a-choice-for-libraries",
    "title": "Who Benefits? Rethinking Library Data in the Age of AI",
    "section": "A Choice for Libraries",
    "text": "A Choice for Libraries\nLibraries have an exciting opportunity. By becoming active partners in AI development, libraries can help shape the tools that will mediate access to human knowledge while gaining powerful new capabilities to serve their communities.\nThe convergence of library expertise and AI technology offers exciting possibilities. Libraries bring irreplaceable skills in curation, metadata, community understanding, and ethical stewardship. Combined with emerging AI tools, this expertise can unlock new ways to make collections discoverable, accessible, and useful.\nThe invitation is open: join the conversation, shape the tools, and help build an AI ecosystem that reflects library values of open access, cultural preservation, and community service. The future of information access is being written now ‚Äî and libraries have essential contributions to make."
  },
  {
    "objectID": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#footnotes",
    "href": "posts/2025/who-benefits-rethinking-library-data-ai/libraries-as-training-data.html#footnotes",
    "title": "Who Benefits? Rethinking Library Data in the Age of AI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis post focuses only on existing open data and collections. While there is legitimate discussion about whether libraries should restrict access to their collections in response to AI concerns, I believe this would be counterproductive for libraries and harmful to other users. A full argument is beyond the scope of this post.‚Ü©Ô∏é\nMarkdown is a sensible target format; it maintains structural information about collections while being directly usable in ML workflows, easily understood by non-developers, and compatible with existing tools. This allows us to move beyond requiring ALTO/METS XML as the only format for library OCR data while preserving essential metadata.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html",
    "href": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html",
    "title": "Using QwQ to generate a reasoning dataset for structured data extraction",
    "section": "",
    "text": "The past year has seen a lot of excitement about reasoning models. There are lots of nuances to what a reasoning model is, and how they are trained, but in practice, reasoning models contrast mostly from ‚Äútraditional‚Äù LLMs by using some extra tokens during inference to ‚Äúreason‚Äù.\nWhere a traditional LLM might answer:\n&gt; User: what is 2+2?\n&gt; Assistant: 4\nA reasoning model might answer:\n&gt; User: what is 2+2?\n&gt; Assistant: &lt;thinking&gt; The user is asking for the sum of 2 and 2.\nThe answer is 4 if we take this at face value but maybe...\nLet's check my answer...\nWe should answer 4&lt;/thinking&gt;\n4\nAllowing a model some extra time/tokens to ‚Äúthink‚Äù turns out to help it solve harder problems (at the cost of increased inference time and cost).\nThe release of the DeepSeek R1 model caused a lot of excitement that reached outside of the AI community. As a result, there has been a lot of work on reasoning models in the open-source AI community. This includes efforts at fully open replications of R1 and an increasing number of datasets being released focused on reasoning‚Äîmore on this below.\n\n\nTo date, reasoning models have primarily focused on math, coding, and science. One reason is that it is easier in these domains to have a verifiable answer or output to compare the LLM against.\nGroup Relative Policy Optimization (GRPO), introduced by the DeepSeek team in DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, is a variant of ‚ÄúProximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO‚Äù.\nWhen training a reasoning model using GRPO, we can use a reward function to reward correct completions. For example, we could reward correct completions based on ground truth answers. A pseudocode example of this might look like:\ndef check_answer(completions, **kwargs):\n    return [\"some reward\" if completion == \"ground truth answer\"\n    else \"no reward\" for completion in completions]\nIn this case, we have a ground-truth answer, and we want to reward completions that match this answer. When we have a single correct answer, this is straightforward. In other cases, we might need some other methods of ‚Äúverification.‚Äù For example, for a code model, we can check if the code has valid syntax, complies, or passes a test.\nHowever, nothing prevents us from using GRPO for other tasks. For example, the trl docs for the GRPOTrainer provide an example of using a reward function to reward more concise completions.\ndef reward_len(completions, **kwargs):\n    return [-abs(20 - len(completion)) for completion in completions]\nOne of the key findings of the Deepseek paper is that during training, the model learns to allocate more thinking time to a problem by reevaluating its initial answer. This allows the model to self-correct its answer. This property can be useful for tasks related to maths. If the model is able to ‚Äúthink‚Äù through a solution in steps, it can avoid the pitfalls of single-step reasoning, i.e.¬†immediately jumping to the answer. However, likely, this ‚Äúreasoning‚Äù step can also be helpful for other tasks.\nWe have some nice community examples of this. For example, Ihor Stepanov has replicated the DeepSeek R1 approach for information extraction, which I‚Äôll also cover in a series of posts starting with this one.\n\n\n\nThe DeepSeek team also showed in another paper DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning that it is possible to distill reasoning into smaller models. They showed that this works for distilling the general reasoning abilities of larger models into smaller ones, but it‚Äôs likely that we can also distil reasoning for specific tasks (and into even smaller models).\nThe steps to do this are roughly:\n\nGenerate a dataset of reasoning traces\nDo supervised finetuning (SFT) on this dataset\n\nAlternatively, we may want to use GRPO directly to train a model for a particular task. In this case, we usually still need an SFT step to train the model to understand the task and output we are looking for (although it may be possible to skip this step if the task is simple or you are starting from an instruction-tuned model that already understands the task).\nOne of the challenges of both approaches has been the cost of generating the reasoning traces. A new reasoning model from Qwen could potentially help with this.\n\n\n\nQwen recently released QwQ-32B:\n\nQwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini. source\n\nWhilst we already had access to models that could produce reasoning traces, QwQ-32B is much smaller than many of these models, making the cost and time involved in generating reasoning traces much more manageable. This may be especially important for more niche tasks where we will want to iterate on the dataset generation process. In the rest of this post, I‚Äôll show how we can use QwQ-32B to generate a dataset for the task of model card data extraction.\n\n\nModels hosted on the Hugging Face Hub often have some form of model card. These are a form of documentation that describe the model, it‚Äôs capabilities, and how to use it. The model cards on the Hub contains two parts:\n\nStructured metadata about the model stored in a YAML section of the README.md file\nA free text description of the model, which may follow a model card template but increasingly often does not\n\nWe can easily extract information from the YAML section of the README.md file. For example, we can use the pipeline_tag information to filter for object detection models on the Hub (https://huggingface.co/models?pipeline_tag=object-detection&sort=trending).\nHowever, information stored in the free text description could also be useful to know. For example, we want to know the number of parameters in the model or its context length. As an example, we may want to extract something that looks a bit like this:\nexample_output = {\n    \"base_model_mentions\": None,\n    \"model_summary\": \"An open-weight text-to-speech model with 82 million parameters that delivers quality comparable to larger models while being faster and more cost-efficient.\",\n    \"model_details\": {\n        \"name\": \"Kokoro\",\n        \"demo_url\": \"https://hf.co/spaces/hexgrad/Kokoro-TTS\",\n        \"context_length\": None,\n        \"parameter_count\": 82000000,\n    },\n    \"dataset_information\": [\n        {\"name\": \"Koniwa\", \"url\": \"https://github.com/koniwa/koniwa\", \"type\": \"GitHub\"}\n    ],\n    \"technical_specifications\": {\n        \"knowledge_cutoff\": \"2023-06\",\n        \"inference_hardware\": \"Requires at least 4GB VRAM, runs on consumer GPUs like RTX 3060\",\n        \"inference_libraries\": [\"Transformers\"],\n    },\n    \"card_quality\": \"good\",\n}\nWhile it is possible to directly use a larger LLM (particularly with structured output) to extract this information, we may often benefit from a smaller, lighter model for this kind of task. This becomes especially important if we want to use this model on large amounts of data (which will often be the case when we want to extract structured data from a large number of documents).\n\n\n\nCurator is a relatively new synthetic data generation library for generating and curating synthetic data. It can be used with both API models and via vLLM, which is how I used it in this case.\nI already have a dataset of model cards here which can be used as input to Curator. You can find the full script here but roughly the steps are:\n\nLoad the model card dataset and do some basic filtering i.e.¬†remove very short and long model cards\nDefine a prompt that asks the model to extract the structured information from the model card\nDefine how Curator should parse the output of the model.\nGenerate a dataset using Curator\n\nHere is the resulting dataset\n\n\nThe dataset generation was done on a Hugging Face Jupyter Space.\n\n\n\n\nOnce we have a dataset like this, we can use it to train a model to extract structured information from model cards. There are various ways to do this.\n\n\nFor example, we could use supervised finetuning on both the reasoning traces and the structured output.\nI will cover this in a future post, but as an example, here is an example of the output from a model trained on this dataset:\n{'base_model_mentions': [{'model_name': 'Qwen2-VL-7B-Instruct',\n   'link': 'https://huggingface.co/datasets/allenai/olmOCR-mix-0225'}],\n 'model_summary': 'A finetuned version of Qwen2-VL-7B-Instruct designed for generating images in a specific format using the olmOCR-mix-0225 dataset.',\n 'model_details': {'name': 'olmOCR-7B-0225-preview',\n  'demo_url': 'https://olmocr.allenai.org/',\n  'context_length': None,\n  'parameter_count': None},\n 'dataset_information': [{'name': 'olmOCR-mix-0225',\n   'url': 'https://huggingface.co/datasets/allenai/olmOCR-mix-0225',\n   'type': 'Hugging Face'}],\n 'technical_specifications': {'knowledge_cutoff': None,\n  'inference_hardware': None,\n  'inference_libraries': ['Transformers']},\n 'card_quality': 'good'}\nHere is a sample of some of the reasoning traces from this model\nOkay, let's tackle this query step by step. The user provided a model card for QwQ-32B and wants specific structured information extracted. Let me go through each part of the query and the model card to ensure I don't miss anything.\n\nFirst, looking at the **base_model_mentions** section. The task is to list models explicitly stated as base/foundation models used for finetuning. The card mentions that QwQ is based on Qwen2.5, whose code is from the latest Hugging Face transformers. The link is provided as https://huggingface.co/Qwen/QwQ-32B. So that should be in the list.\n\nNext, the **model_summary** needs a single-sentence summary. The primary purpose is reasoning capacity, built on Qwen2.5, with 32B parameters. It has 32.5B parameters before post-training, and its performance on downstream tasks like math problems and hard questions. The summary should include the main function (reasoning), base model, parameters, and unique features like enhanced performance and context handling.\n\nFor **model_details**, the name is \"Qwen/QwQ-32B\" as per the repo name. Demo URL is https://huggingface.co/spaces/Qwen/QwQ-32B-Demo. They mention using the transformers library, so the Python code snippet's code variable is from transformers.context_length is Full 131,072, parameter count 32.5 billion (32.5B), and non-hidden parameters count 31.0B. But wait, the model has 32.5B before post-training, so parameter_count should be 32.5B. The context_length is mentioned in the requirements as Full 131,072 tokens. So for the parameters, the non-hidden part might need to be 31.0B.\n\n**dataset_information** requires datasets used. The card mentions training stages pre-training with DeepSeek-R1 and o1-mini, but the main training used Qwen2.5's technical report. The dataset for post-training is not specified here, so maybe the datasets are already covered under the base model or other parts of the card. The training data isn't explicitly listed except mentioning post-training and the reports of those using their platforms. Since the exact training datasets aren't named here, perhaps there's no direct dataset info provided. Wait, the requirements mention enabling YaRN and using vLLM, which might involve specific datasets, but without explicit dataset names, maybe there's no direct dataset info here. So dataset_information would be null? Or do I look into the references? The card says \"Dealing with their [fellow researchers'] work, they mention their own results and not those of others on the training data. \" So it might not have explicit dataset names, so maybe leave it as an empty array?\n\nTechnical specifications: knowledge_cutoff is not explicitly stated. The training data's latest date isn't mentioned, so it's null. Inference hardware isn't specified except mentioning vLLM and the author's mention of using vLLM's setup. The requirements say to use the transformers library, but the example code uses PyTorch, but the requirements apply to the model's architecture. The card says \"transformers&lt;4.37.0\", so probably uses the current version. The inference_libraries are mentioned as \"transformers\" since it's part of the code snippet. So the libraries are transformers.\n\n**card_quality** assessment. Let's look at the criteria:\nWhile this doesn‚Äôt look terrible, it does seem quite verbose. This is potentially where using GRPO could help.\n\n\n\nWhilst SFT can help us distill reasoning into a model, we may want to use GRPO to control the output more carefully using reward functions for a specific task like structured output extraction. This may also allow us to encourage the reasoning to be more concise (whilst ensuring the output is still correct according to some criteria we define). We‚Äôll cover this in a future post!"
  },
  {
    "objectID": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html#reasoning-outside-of-math-and-science",
    "href": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html#reasoning-outside-of-math-and-science",
    "title": "Using QwQ to generate a reasoning dataset for structured data extraction",
    "section": "",
    "text": "To date, reasoning models have primarily focused on math, coding, and science. One reason is that it is easier in these domains to have a verifiable answer or output to compare the LLM against.\nGroup Relative Policy Optimization (GRPO), introduced by the DeepSeek team in DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, is a variant of ‚ÄúProximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO‚Äù.\nWhen training a reasoning model using GRPO, we can use a reward function to reward correct completions. For example, we could reward correct completions based on ground truth answers. A pseudocode example of this might look like:\ndef check_answer(completions, **kwargs):\n    return [\"some reward\" if completion == \"ground truth answer\"\n    else \"no reward\" for completion in completions]\nIn this case, we have a ground-truth answer, and we want to reward completions that match this answer. When we have a single correct answer, this is straightforward. In other cases, we might need some other methods of ‚Äúverification.‚Äù For example, for a code model, we can check if the code has valid syntax, complies, or passes a test.\nHowever, nothing prevents us from using GRPO for other tasks. For example, the trl docs for the GRPOTrainer provide an example of using a reward function to reward more concise completions.\ndef reward_len(completions, **kwargs):\n    return [-abs(20 - len(completion)) for completion in completions]\nOne of the key findings of the Deepseek paper is that during training, the model learns to allocate more thinking time to a problem by reevaluating its initial answer. This allows the model to self-correct its answer. This property can be useful for tasks related to maths. If the model is able to ‚Äúthink‚Äù through a solution in steps, it can avoid the pitfalls of single-step reasoning, i.e.¬†immediately jumping to the answer. However, likely, this ‚Äúreasoning‚Äù step can also be helpful for other tasks.\nWe have some nice community examples of this. For example, Ihor Stepanov has replicated the DeepSeek R1 approach for information extraction, which I‚Äôll also cover in a series of posts starting with this one."
  },
  {
    "objectID": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html#distiling-reasoning-models-for-information-extraction-and-other-tasks",
    "href": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html#distiling-reasoning-models-for-information-extraction-and-other-tasks",
    "title": "Using QwQ to generate a reasoning dataset for structured data extraction",
    "section": "",
    "text": "The DeepSeek team also showed in another paper DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning that it is possible to distill reasoning into smaller models. They showed that this works for distilling the general reasoning abilities of larger models into smaller ones, but it‚Äôs likely that we can also distil reasoning for specific tasks (and into even smaller models).\nThe steps to do this are roughly:\n\nGenerate a dataset of reasoning traces\nDo supervised finetuning (SFT) on this dataset\n\nAlternatively, we may want to use GRPO directly to train a model for a particular task. In this case, we usually still need an SFT step to train the model to understand the task and output we are looking for (although it may be possible to skip this step if the task is simple or you are starting from an instruction-tuned model that already understands the task).\nOne of the challenges of both approaches has been the cost of generating the reasoning traces. A new reasoning model from Qwen could potentially help with this."
  },
  {
    "objectID": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html#qwq-32b-a-reasoning-model-that-can-democratize-reasoning-dataset-generation",
    "href": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html#qwq-32b-a-reasoning-model-that-can-democratize-reasoning-dataset-generation",
    "title": "Using QwQ to generate a reasoning dataset for structured data extraction",
    "section": "",
    "text": "Qwen recently released QwQ-32B:\n\nQwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini. source\n\nWhilst we already had access to models that could produce reasoning traces, QwQ-32B is much smaller than many of these models, making the cost and time involved in generating reasoning traces much more manageable. This may be especially important for more niche tasks where we will want to iterate on the dataset generation process. In the rest of this post, I‚Äôll show how we can use QwQ-32B to generate a dataset for the task of model card data extraction.\n\n\nModels hosted on the Hugging Face Hub often have some form of model card. These are a form of documentation that describe the model, it‚Äôs capabilities, and how to use it. The model cards on the Hub contains two parts:\n\nStructured metadata about the model stored in a YAML section of the README.md file\nA free text description of the model, which may follow a model card template but increasingly often does not\n\nWe can easily extract information from the YAML section of the README.md file. For example, we can use the pipeline_tag information to filter for object detection models on the Hub (https://huggingface.co/models?pipeline_tag=object-detection&sort=trending).\nHowever, information stored in the free text description could also be useful to know. For example, we want to know the number of parameters in the model or its context length. As an example, we may want to extract something that looks a bit like this:\nexample_output = {\n    \"base_model_mentions\": None,\n    \"model_summary\": \"An open-weight text-to-speech model with 82 million parameters that delivers quality comparable to larger models while being faster and more cost-efficient.\",\n    \"model_details\": {\n        \"name\": \"Kokoro\",\n        \"demo_url\": \"https://hf.co/spaces/hexgrad/Kokoro-TTS\",\n        \"context_length\": None,\n        \"parameter_count\": 82000000,\n    },\n    \"dataset_information\": [\n        {\"name\": \"Koniwa\", \"url\": \"https://github.com/koniwa/koniwa\", \"type\": \"GitHub\"}\n    ],\n    \"technical_specifications\": {\n        \"knowledge_cutoff\": \"2023-06\",\n        \"inference_hardware\": \"Requires at least 4GB VRAM, runs on consumer GPUs like RTX 3060\",\n        \"inference_libraries\": [\"Transformers\"],\n    },\n    \"card_quality\": \"good\",\n}\nWhile it is possible to directly use a larger LLM (particularly with structured output) to extract this information, we may often benefit from a smaller, lighter model for this kind of task. This becomes especially important if we want to use this model on large amounts of data (which will often be the case when we want to extract structured data from a large number of documents).\n\n\n\nCurator is a relatively new synthetic data generation library for generating and curating synthetic data. It can be used with both API models and via vLLM, which is how I used it in this case.\nI already have a dataset of model cards here which can be used as input to Curator. You can find the full script here but roughly the steps are:\n\nLoad the model card dataset and do some basic filtering i.e.¬†remove very short and long model cards\nDefine a prompt that asks the model to extract the structured information from the model card\nDefine how Curator should parse the output of the model.\nGenerate a dataset using Curator\n\nHere is the resulting dataset\n\n\nThe dataset generation was done on a Hugging Face Jupyter Space."
  },
  {
    "objectID": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html#next-steps",
    "href": "posts/2025/reasoning-models/generating-structured-data-extraction-dataset-with-qwq-and-curator.html#next-steps",
    "title": "Using QwQ to generate a reasoning dataset for structured data extraction",
    "section": "",
    "text": "Once we have a dataset like this, we can use it to train a model to extract structured information from model cards. There are various ways to do this.\n\n\nFor example, we could use supervised finetuning on both the reasoning traces and the structured output.\nI will cover this in a future post, but as an example, here is an example of the output from a model trained on this dataset:\n{'base_model_mentions': [{'model_name': 'Qwen2-VL-7B-Instruct',\n   'link': 'https://huggingface.co/datasets/allenai/olmOCR-mix-0225'}],\n 'model_summary': 'A finetuned version of Qwen2-VL-7B-Instruct designed for generating images in a specific format using the olmOCR-mix-0225 dataset.',\n 'model_details': {'name': 'olmOCR-7B-0225-preview',\n  'demo_url': 'https://olmocr.allenai.org/',\n  'context_length': None,\n  'parameter_count': None},\n 'dataset_information': [{'name': 'olmOCR-mix-0225',\n   'url': 'https://huggingface.co/datasets/allenai/olmOCR-mix-0225',\n   'type': 'Hugging Face'}],\n 'technical_specifications': {'knowledge_cutoff': None,\n  'inference_hardware': None,\n  'inference_libraries': ['Transformers']},\n 'card_quality': 'good'}\nHere is a sample of some of the reasoning traces from this model\nOkay, let's tackle this query step by step. The user provided a model card for QwQ-32B and wants specific structured information extracted. Let me go through each part of the query and the model card to ensure I don't miss anything.\n\nFirst, looking at the **base_model_mentions** section. The task is to list models explicitly stated as base/foundation models used for finetuning. The card mentions that QwQ is based on Qwen2.5, whose code is from the latest Hugging Face transformers. The link is provided as https://huggingface.co/Qwen/QwQ-32B. So that should be in the list.\n\nNext, the **model_summary** needs a single-sentence summary. The primary purpose is reasoning capacity, built on Qwen2.5, with 32B parameters. It has 32.5B parameters before post-training, and its performance on downstream tasks like math problems and hard questions. The summary should include the main function (reasoning), base model, parameters, and unique features like enhanced performance and context handling.\n\nFor **model_details**, the name is \"Qwen/QwQ-32B\" as per the repo name. Demo URL is https://huggingface.co/spaces/Qwen/QwQ-32B-Demo. They mention using the transformers library, so the Python code snippet's code variable is from transformers.context_length is Full 131,072, parameter count 32.5 billion (32.5B), and non-hidden parameters count 31.0B. But wait, the model has 32.5B before post-training, so parameter_count should be 32.5B. The context_length is mentioned in the requirements as Full 131,072 tokens. So for the parameters, the non-hidden part might need to be 31.0B.\n\n**dataset_information** requires datasets used. The card mentions training stages pre-training with DeepSeek-R1 and o1-mini, but the main training used Qwen2.5's technical report. The dataset for post-training is not specified here, so maybe the datasets are already covered under the base model or other parts of the card. The training data isn't explicitly listed except mentioning post-training and the reports of those using their platforms. Since the exact training datasets aren't named here, perhaps there's no direct dataset info provided. Wait, the requirements mention enabling YaRN and using vLLM, which might involve specific datasets, but without explicit dataset names, maybe there's no direct dataset info here. So dataset_information would be null? Or do I look into the references? The card says \"Dealing with their [fellow researchers'] work, they mention their own results and not those of others on the training data. \" So it might not have explicit dataset names, so maybe leave it as an empty array?\n\nTechnical specifications: knowledge_cutoff is not explicitly stated. The training data's latest date isn't mentioned, so it's null. Inference hardware isn't specified except mentioning vLLM and the author's mention of using vLLM's setup. The requirements say to use the transformers library, but the example code uses PyTorch, but the requirements apply to the model's architecture. The card says \"transformers&lt;4.37.0\", so probably uses the current version. The inference_libraries are mentioned as \"transformers\" since it's part of the code snippet. So the libraries are transformers.\n\n**card_quality** assessment. Let's look at the criteria:\nWhile this doesn‚Äôt look terrible, it does seem quite verbose. This is potentially where using GRPO could help.\n\n\n\nWhilst SFT can help us distill reasoning into a model, we may want to use GRPO to control the output more carefully using reward functions for a specific task like structured output extraction. This may also allow us to encourage the reasoning to be more concise (whilst ensuring the output is still correct according to some criteria we define). We‚Äôll cover this in a future post!"
  },
  {
    "objectID": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html",
    "href": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html",
    "title": "Fine-tuning Vision-Language Models for Art History: Iconclass Classification with TRL and HF Jobs",
    "section": "",
    "text": "Open VLMs have become increasingly competitive with proprietary models but all models (including proprietary) can face challenges in adapting to specific domains like art history. Using Supervised Fine Tuning (SFT) can help improve a model‚Äôs performance on specialized tasks like this.\n\n\n\n\n\n\nWho is this for?\n\n\n\nThis is a fairly technical tutorial for developers, GLAM staff, DH researchers etc. comfortable with Python and machine learning concepts. I plan to write a post on why I think this kind of model is important for GLAM in a future post!\n\n\nRecently, TRL a very popular library for training transformers models using a variety of training approaches added support for supervised fine tuning of VLMs! Combined with the recently released HF Jobs service we can use TRL to do SFT on a VLM with no local GPU setup required!\ntl;dr in this post we see how we can train a small specialized VLM davanstrien/iconclass-vlm using the trl library + HF Jobs.\nThis is a model that given an image like this\n\nWill return:\n{\"iconclass-codes\": [\"25H213\", \"25H216\", \"25I\"]}\nWhich translates too:\n\n25H216: waterfall\n25H213: river\n25I ‚Äúcity-view, and landscape with man-made constructions‚Äù\n\nWe can see the model isn‚Äôt perfect yet but that‚Äôs what the follow up post will dig into!\n\n\nIn this post, I‚Äôll use the example of using SFT to train a model to generate Iconclass metadata for artworks.\n\nThe Iconclass system has a history reaching back into the 1940‚Äôs when Henri van de Waal began to develop ideas for a universal classification for the subject matter of works of art. 1\n\nThe Iconclass metadata system consists of a hierarchical structure of categories and subcategories, allowing for a detailed description of the content of artworks. The top level categories are:\n0 ¬∑ Abstract, Non-representational Art\n1 ¬∑ Religion and Magic\n2 ¬∑ Nature\n3 ¬∑ Human Being, Man in General\n4 ¬∑ Society, Civilization, Culture\n5 ¬∑ Abstract Ideas and Concepts\n6 ¬∑ History\n7 ¬∑ Bible\n8 ¬∑ Literature\n9 ¬∑ Classical Mythology and Ancient History\nWithin these are more specific subcategories that allow for even greater detail in the classification of artworks (you can find more detailed info on the system below)\n\n\n\n\n\n\nMore detail on how the Iconclass system works\n\n\n\n\n\nUnderneath each main category, there are numerous subcategories that further refine the classification.\nFor example under 3 (Human Being, Man in General), we find subcategories such as:\n\nWe then move down the hierarchy levels we see more specific subcategories:\n\nAs we move further ‚Äúdown‚Äù the hierarchy levels we see even more specific subcategories such as 31F12 (bones (symbol of Death) ü§ò) \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is the first in a two-part series on fine-tuning VLMs for specialized tasks. In this post, we‚Äôll cover supervised fine-tuning (SFT). The next post will explore using GRPO (Group Relative Policy Optimization) to further improve model performance!"
  },
  {
    "objectID": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#fine-tuning-a-qwen-model-to-assign-art-history-metadata",
    "href": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#fine-tuning-a-qwen-model-to-assign-art-history-metadata",
    "title": "Fine-tuning Vision-Language Models for Art History: Iconclass Classification with TRL and HF Jobs",
    "section": "",
    "text": "Open VLMs have become increasingly competitive with proprietary models but all models (including proprietary) can face challenges in adapting to specific domains like art history. Using Supervised Fine Tuning (SFT) can help improve a model‚Äôs performance on specialized tasks like this.\n\n\n\n\n\n\nWho is this for?\n\n\n\nThis is a fairly technical tutorial for developers, GLAM staff, DH researchers etc. comfortable with Python and machine learning concepts. I plan to write a post on why I think this kind of model is important for GLAM in a future post!\n\n\nRecently, TRL a very popular library for training transformers models using a variety of training approaches added support for supervised fine tuning of VLMs! Combined with the recently released HF Jobs service we can use TRL to do SFT on a VLM with no local GPU setup required!\ntl;dr in this post we see how we can train a small specialized VLM davanstrien/iconclass-vlm using the trl library + HF Jobs.\nThis is a model that given an image like this\n\nWill return:\n{\"iconclass-codes\": [\"25H213\", \"25H216\", \"25I\"]}\nWhich translates too:\n\n25H216: waterfall\n25H213: river\n25I ‚Äúcity-view, and landscape with man-made constructions‚Äù\n\nWe can see the model isn‚Äôt perfect yet but that‚Äôs what the follow up post will dig into!\n\n\nIn this post, I‚Äôll use the example of using SFT to train a model to generate Iconclass metadata for artworks.\n\nThe Iconclass system has a history reaching back into the 1940‚Äôs when Henri van de Waal began to develop ideas for a universal classification for the subject matter of works of art. 1\n\nThe Iconclass metadata system consists of a hierarchical structure of categories and subcategories, allowing for a detailed description of the content of artworks. The top level categories are:\n0 ¬∑ Abstract, Non-representational Art\n1 ¬∑ Religion and Magic\n2 ¬∑ Nature\n3 ¬∑ Human Being, Man in General\n4 ¬∑ Society, Civilization, Culture\n5 ¬∑ Abstract Ideas and Concepts\n6 ¬∑ History\n7 ¬∑ Bible\n8 ¬∑ Literature\n9 ¬∑ Classical Mythology and Ancient History\nWithin these are more specific subcategories that allow for even greater detail in the classification of artworks (you can find more detailed info on the system below)\n\n\n\n\n\n\nMore detail on how the Iconclass system works\n\n\n\n\n\nUnderneath each main category, there are numerous subcategories that further refine the classification.\nFor example under 3 (Human Being, Man in General), we find subcategories such as:\n\nWe then move down the hierarchy levels we see more specific subcategories:\n\nAs we move further ‚Äúdown‚Äù the hierarchy levels we see even more specific subcategories such as 31F12 (bones (symbol of Death) ü§ò) \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is the first in a two-part series on fine-tuning VLMs for specialized tasks. In this post, we‚Äôll cover supervised fine-tuning (SFT). The next post will explore using GRPO (Group Relative Policy Optimization) to further improve model performance!"
  },
  {
    "objectID": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#training-the-model",
    "href": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#training-the-model",
    "title": "Fine-tuning Vision-Language Models for Art History: Iconclass Classification with TRL and HF Jobs",
    "section": "Training the model",
    "text": "Training the model\nNow we have a better sense of the task we‚Äôll go through the process of fine-tuning a VLM using TRL to generate Iconclass metadata for artworks.\n\nPreparing a dataset\nThe steps to train a VLM using SFT are very minimal when using TRL. To do SFT for a VLM model we need to have a dataset with images + labels (text) output.\nFor TRL, the main thing we need to take care of is finding or preparing a dataset in the right format.\nFortunately, there is already a dataset available on Hugging Face that contains images of artworks along with their corresponding Iconclass metadata.\nThe biglam/brill_iconclass dataset, which is part of the BigLAM organization on the Hub, consists of artworks from the Brill collection, each annotated with Iconclass codes. We can see a preview of this here:\n\n\nWe‚Äôll need to do some work to get this dataset into the right format for training but this is fairly minimal.\nWe‚Äôll start with loading the dataset and inspecting its current features.\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"biglam/brill_iconclass\", split=\"train\")\nds.features\n\n\n\n\n\n\n\n{'image': Image(mode=None, decode=True), 'label': List(Value('string'))}\n\n\nWe can see we currently have two columns (image column and text column) in our dataset. We can also take a quick look at the first row\n\nds[0]\n\n{'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=390x500&gt;,\n 'label': ['31A235', '31A24(+1)', '61B(+54)', '61B:31A2212(+1)', '61B:31D14']}\n\n\nWe see that label consists of a list of iconclass codes. Let‚Äôs look at an example image:\n\nds[0]['image']\n\n\n\n\n\n\n\n\n\n\nFormatting the Dataset for TRL\nAs we can see in the TRL docs for using TRL with VLMs we need a dataset with a column of images (formated as a list of images) and a column containing messages, i.e.¬†something like this\nimage column: List[Image]\ntext scolumn: [\n  {\n    \"content\": [\n      {\n        \"text\": null,\n        \"type\": \"image\"\n      },\n      {\n        \"text\": \"Please describe this artwork thoroughly.\",\n        \"type\": \"text\"\n      }\n    ],\n    \"role\": \"user\"\n  }\n]\nWe‚Äôll see how to format our dataset accordingly in the next section so this should become clearer!\n\n\n\n\n\n\nTip\n\n\n\nFor this particular dataset I double check that the images are RGB and not too big. This isn‚Äôt always required but since GLAM (Galleries, Libraries, Archives, and Museums) datasets can contain very large images/ a lot of black and white images, I do it here as an extra precaution!\n\n\n\n\nShow pre processing code\ndef ensure_rgb(example):\n    \"\"\"Ensure image is RGB\"\"\"\n    if example['image'].mode != 'RGB':\n        example['image'] = example['image'].convert('RGB')\n    return example\n\ndef ensure_size(example, max_size):\n    \"\"\"Resize to max_size on one edge keeping aspect ratio correct\"\"\"\n    image = example['image']\n    w, h = image.size\n    if max(w, h) &gt; max_size:\n        scale = max_size / float(max(w, h))\n        new_size = (int(w * scale), int(h * scale))\n        image = image.resize(new_size, Image.LANCZOS)\n    example['image'] = image\n    return example\n\nds = ds.map(ensure_rgb, num_proc=4)\n\n\n\n\n\nWe need to convert our dataset to the messages format. We can do this using the datasets library‚Äôs map function to transform each example in the dataset. We‚Äôll create a format_as_messages function to handle the conversion for each example.\nIn this particular example I want the VLM to output JSON containing a key iconclass-codes with a list/array of iconclass codes so we use json.dumps to format a dictionary into a JSON string we can include in the messages as the assistant‚Äôs response.\n\nfrom typing import Dict, Any\nimport json\n\n\ndef format_as_messages(\n    example: Dict[str, Any], prompt: str | None = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Format single example into messages format for TRL.\"\"\"\n    if not prompt:\n        prompt = \"What is in this image?\"\n    labels = example[\"label\"]\n    response = {\"iconclass-codes\": labels}\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [{\"type\": \"text\", \"text\": json.dumps(response)}],\n        },\n    ]\n\n    return {\n        \"images\": [example[\"image\"]],  # List with single image\n        \"messages\": messages,\n    }\n\n\n\n\n\n\n\nTip\n\n\n\nIt might seem a bit weird that we pass images to a list in this case but this is done so we can also support datasets where multiple images are included in a message(s).\n\n\nWe can now apply the format_as_messages function to our dataset using the map function. We‚Äôll use a simple prompt for the VLM to follow.\n\nds = ds.map(\n    format_as_messages,\n    remove_columns=[\"image\", \"label\"],\n    num_proc=6,\n    fn_kwargs={\"prompt\": \"Extract ICONCLASS labels for this image.\"},\n)\n\n\n\n\nLet‚Äôs take a look at the dataset after formatting:\n\nds[0]\n\n{'images': [&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=390x500&gt;],\n 'messages': [{'content': [{'text': None, 'type': 'image'},\n    {'text': 'Extract ICONCLASS labels for this image.', 'type': 'text'}],\n   'role': 'user'},\n  {'content': [{'text': '{\"iconclass-codes\": [\"31A235\", \"31A24(+1)\", \"61B(+54)\", \"61B:31A2212(+1)\", \"61B:31D14\"]}',\n     'type': 'text'}],\n   'role': 'assistant'}]}\n\n\nWe now split the dataset into train, validation, and test sets. For reasons that will become clearer in a future blog post we actually want to keep quite a bit of data back for later work so we‚Äôll define these splits a bit more manually than usual!\n\nfrom datasets import DatasetDict\n\n# Step 1: Split off 5% for test\nsplit = ds.train_test_split(test_size=0.05, seed=42)\ntrain_valid = split[\"train\"]\ntest_ds = split[\"test\"]\n\n# Step 2: Split the remaining 95% into train and valid\ntrain_valid_split = train_valid.train_test_split(test_size=0.4737, seed=42)\ntrain_ds = train_valid_split[\"train\"]\nvalid_ds = train_valid_split[\"test\"]\n\n# Combine into a DatasetDict\n\nds = DatasetDict({\"train\": train_ds, \"valid\": valid_ds, \"test\": test_ds})\n\n\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['images', 'messages'],\n        num_rows: 43870\n    })\n    valid: Dataset({\n        features: ['images', 'messages'],\n        num_rows: 39486\n    })\n    test: Dataset({\n        features: ['images', 'messages'],\n        num_rows: 4388\n    })\n})\n\n\n\n\nPush to Hub\nWe‚Äôll now push the dataset to the Hugging Face Hub so that it‚Äôs easily accessible for others (and for us in the future!). We‚Äôll first use flatten_indices since we did quite a few transformations on the dataset.\n\nds = ds.flatten_indices()\nds.push_to_hub(\"davanstrien/iconclass-vlm-sft\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/datasets/davanstrien/iconclass-vlm-sft/commit/d8d2cfe679ba89346920a894c8f7d134965a43a5', commit_message='Upload dataset', commit_description='', oid='d8d2cfe679ba89346920a894c8f7d134965a43a5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/davanstrien/iconclass-vlm-sft', endpoint='https://huggingface.co', repo_type='dataset', repo_id='davanstrien/iconclass-vlm-sft'), pr_revision=None, pr_num=None)"
  },
  {
    "objectID": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#hf-jobs-x-uv-x-trl",
    "href": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#hf-jobs-x-uv-x-trl",
    "title": "Fine-tuning Vision-Language Models for Art History: Iconclass Classification with TRL and HF Jobs",
    "section": "HF Jobs x UV x TRL!",
    "text": "HF Jobs x UV x TRL!\nHugging Face Jobs allows you to run code on Hugging Face Infrastructure including GPUs. Jobs can be run using either the huggingface_hub libraries CLI interface or via the Python API. A hello world example looks like this\n\n!hf jobs run python:3.12 python -c \"print('Hello world!')\"\n\nJob started with ID: 68b86a53e824c700d00ad361\nView at: https://huggingface.co/jobs/davanstrien/68b86a53e824c700d00ad361\nHello world!\n\n\nJobs have an experimental uv run api that allow you to run uv scripts using Jobs. UV scripts are Python scripts that include their dependencies directly in the file using a special comment syntax. This makes them perfect for self-contained tasks that don‚Äôt require complex project setups. This works super well for use cases like a TRL training script!\nLet‚Äôs take a look at the script we‚Äôll be running.\n\nThe TRL SFT training script\nWith our dataset ready, we need a training script that can handle VLM fine-tuning. The beauty of TRL‚Äôs VLM support is that it makes this surprisingly straightforward - the same SFTTrainer that works for LLMs now handles vision-language models seamlessly.\n\nWhat makes this script VLM-ready?\nThe key differences from a standard LLM training script are minimal:\n\nModel loading: We use AutoModelForImageTextToText instead of the text-only variant\nDataset format: Our dataset includes an images column alongside messages\nNo tokenizer needed: The model handles both image and text processing internally\n\nThat‚Äôs it! TRL handles all the complexity of multi-modal training under the hood. Let‚Äôs look at the script:\nYou can find the full version of the script we‚Äôre running in main.py but we can also see the most important parts below.\n\n\nShow main.py training script\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForImageTextToText\n\nfrom trl import (\n    ModelConfig,\n    ScriptArguments,\n    SFTConfig,\n    SFTTrainer,\n    TrlParser,\n    get_kbit_device_map,\n    get_peft_config,\n    get_quantization_config,\n)\n\n\nif __name__ == \"__main__\":\n    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n    script_args, training_args, model_args = parser.parse_args_and_config()\n    training_args.gradient_checkpointing_kwargs = dict(use_reentrant=False)\n    training_args.max_length = None\n\n    ################\n    # Model, Tokenizer & Processor\n    ################\n    torch_dtype = (\n        model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n    )\n    quantization_config = get_quantization_config(model_args)\n    model_kwargs = dict(\n        revision=model_args.model_revision,\n        attn_implementation=model_args.attn_implementation,\n        torch_dtype=torch_dtype,\n        device_map=get_kbit_device_map() if quantization_config is not None else None,\n        quantization_config=quantization_config,\n    )\n\n    model = AutoModelForImageTextToText.from_pretrained(\n        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n    )\n\n    ################\n    # Dataset\n    ################\n    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n\n    ################\n    # Training\n    ################\n    trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[script_args.dataset_train_split],\n        eval_dataset=dataset[script_args.dataset_test_split] if training_args.eval_strategy != \"no\" else None,\n        peft_config=get_peft_config(model_args),\n    )\n\n    trainer.train()\n\n    # Save and push to hub\n    trainer.save_model(training_args.output_dir)\n    if training_args.push_to_hub:\n        trainer.push_to_hub(dataset_name=script_args.dataset_name)\n\n\n\n\nKey components explained\nLet‚Äôs break down the important parts:\n\nTRL‚Äôs argument parsing: The TrlParser handles all configuration - model settings, training hyperparameters, and dataset paths. This gives us a production-ready CLI interface for free.\nModel loading for VLMs:\nmodel = AutoModelForImageTextToText.from_pretrained(...)\nThis automatically loads the right architecture for vision-language models. It handles models like Qwen2.5-VL, SmolVLM2, and others that support image+text inputs.\nQuantization support: The script includes optional quantization (get_quantization_config) for running larger models on smaller GPUs. Perfect for experiments before scaling up.\nDataset handling: Notice how simple the dataset loading is - just point to your Hub dataset with the formatted images and messages columns.\nThe SFTTrainer: The same trainer used for text models now handles VLMs. No special configuration needed - it detects the multi-modal dataset and adjusts accordingly.\n\nThe script is designed to be flexible - you can run it locally for testing or scale to A100s on HF Jobs just by changing the hardware configuration.\n\n\nScript dependencies with UV\nThe training script includes inline script metadata that tells uv which dependencies to install. This makes the script completely self-contained:\n# /// script\n# dependencies = [\n#     \"trl\",\n#     \"Pillow&gt;=9.4.0\",\n# ]\n# ///\nWith this metadata, uv run automatically installs the exact dependencies needed before running the script - no virtual environment setup or requirements.txt needed! This is what makes HF Jobs + UV so powerful: your training environment is fully reproducible with zero manual setup.\n\n\n\n\n\n\nPro tip: Additional dependencies\n\n\n\nWhen submitting to HF Jobs, you can also specify additional dependencies at runtime (like we do with tensorboard, wandb, etc.). This is useful for optional integrations that not everyone needs.\n\n\n\n\n\nRunning uv Jobs using huggingface_hub\nWe can use the huggingface_hub library to easily manage and run our jobs on the Hugging Face platform. We‚Äôll grab our Hugging Face token from an .env file using python-dotenv. If you are running this notebook somewhere else you could also set the HF_TOKEN environment variable manually.\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\nassert HF_TOKEN\n\nWe then create a new HfApi instance.\n\nfrom huggingface_hub import HfApi\n\napi = HfApi(token=HF_TOKEN)\n\n\nModel and dataset configuration\nThe easiest way to know the options our script supports would be to check the script but if we wanted we could also use hf jobs to print out the --help for the script. We can do this using the uv run subcommand:\n\n!hf jobs uv run \"https://gist.githubusercontent.com/davanstrien/26a301d5a810746128a27b8ae49d0950/raw/141883e6ba4f5b51bc22de49e761a03564364fc5/main.py\" --help\n\nIf you run this you‚Äôll see uv installing the required dependencies for your script (in this case trl + Pillow) and then returning the available options for the script.\nSince we may want to change options later we‚Äôll define some variables to hold our configuration.\nThe main ones to note are the dataset, model and output model which refer to the dataset we want to use for training, the pre-trained model we want to fine-tune, and the output repo on the Hub where we want to save our model checkpoints respectively.\n\nDATASET = \"davanstrien/iconclass-vlm-sft\"\nMODEL = \"Qwen/Qwen2.5-VL-3B-Instruct\"\nMODEL = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\nOUTPUT_MODEL = \"davanstrien/iconclass-vlm\"\n# Training hyperparameters\nBATCH_SIZE = 10\nGRADIENT_ACCUMULATION = 8\nMAX_STEPS = None  # Adjust for full training\nLEARNING_RATE = \"2e-5\"\nEPOCHS = 1\n\nWe can then create a list to hold our script arguments that will be passed to the trl SFT fine-tuning script.\n\n# use epochs if MAX_STEPS is None\nif MAX_STEPS is None:\n    MAX_STEPS = EPOCHS * (len(ds[\"train\"]) // BATCH_SIZE // GRADIENT_ACCUMULATION)\n\n# Build the script arguments\nscript_args = [\n    \"--model_name_or_path\",\n    MODEL,\n    \"--dataset_name\",\n    DATASET,\n    \"--output_dir\",\n    \"./iconclass-vlm-outputs\",\n    \"--hub_model_id\",\n    OUTPUT_MODEL,\n    \"--push_to_hub\",\n    \"--per_device_train_batch_size\",\n    str(BATCH_SIZE),\n    \"--gradient_accumulation_steps\",\n    str(GRADIENT_ACCUMULATION),\n    \"--max_steps\",\n    str(MAX_STEPS),\n    \"--torch_dtype\",\n    \"bfloat16\",\n    \"--logging_steps\",\n    \"10\",\n    \"--save_steps\",\n    \"100\",\n    \"--eval_steps\",\n    \"100\",\n    \"--warmup_ratio\",\n    \"0.1\",\n    \"--learning_rate\",\n    LEARNING_RATE,\n    \"--attn_implementation\",\n    \"kernels-community/flash-attn\",\n]\n\nWe now have our arguments ready to be passed to the trl SFT fine-tuning script. To actually run the script we can use the run_uv_job method.\nWe‚Äôll pass a few things to this.\n\nscript: the URL or path to the trl SFT fine-tuning script.\nscript_args: the arguments to pass to the script.\ndependencies: any additional dependencies required by the script.\nflavor: the type of hardware to use i.e.¬†a100-large\nimage: the docker image to use for the job.\ntimeout: the maximum time to allow the job to run.\nenv: any environment variables to set for the job.\nsecrets: any secrets required by the job.\n\n\nTRACKIO_PROJECT=\"iconclass\"\nTRACKIO_SPACE_ID=\"trackio-trl\"\n\n\n# Submit the job using run_uv_job\njob = api.run_uv_job(\n    script=\"https://gist.githubusercontent.com/davanstrien/26a301d5a810746128a27b8ae49d0950/raw/141883e6ba4f5b51bc22de49e761a03564364fc5/main.py\",\n    script_args=script_args,\n    dependencies=[\n        \"torchvision\",\n        \"tensorboard\",\n        \"accelerate\",\n        \"wandb\",\n        \"kernels\",\n        \"trackio\",\n    ],\n    flavor=\"a100-large\",\n    image=\"vllm/vllm-openai:latest\",\n    timeout=\"12h\",  # Adjust based on MAX_STEPS\n    env={\"TRACKIO_PROJECT\": TRACKIO_PROJECT, \"TRACKIO_SPACE_ID\": TRACKIO_SPACE_ID},\n    secrets={\n        \"HF_TOKEN\": os.environ.get(\"HF_TOKEN\"),\n        \"WANDB_API_KEY\": os.environ.get(\"WANDB_API_KEY\"),\n    },\n)\n\n/Users/davanstrien/Documents/daniel/blog/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_experimental.py:60: UserWarning: 'HfApi.run_uv_job' is experimental and might be subject to breaking changes in the future without prior notice. You can disable this warning by setting `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` as environment variable.\n  warnings.warn(\n\n\nWe can now check the status of the Job.\n\njob.status\n\nJobStatus(stage='RUNNING', message=None)\n\n\nWe can also get a URL to see the logs etc for the Job on the Hub.\n\njob.url\n\n'https://huggingface.co/jobs/davanstrien/68b9c334d797b777b755e63b'"
  },
  {
    "objectID": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#exploring-the-results",
    "href": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#exploring-the-results",
    "title": "Fine-tuning Vision-Language Models for Art History: Iconclass Classification with TRL and HF Jobs",
    "section": "Exploring the Results",
    "text": "Exploring the Results\nAfter training completes, our fine-tuned model is automatically pushed to the Hub. But how well does it actually perform on generating Iconclass codes?\nI‚Äôve built an interactive viewer to explore the model‚Äôs predictions on the test set - you can compare the ground truth labels with what our fine-tuned model generates:\n\n\n\nInitial observations\nFrom exploring the predictions, a few patterns emerge:\n\nThe model successfully learns the hierarchical structure of Iconclass codes\nCommon iconographic elements (religious scenes, portraits) are well recognized\nMore specific or rare codes show room for improvement\n\nThis is a solid baseline from just one epoch of training! In the next post, we‚Äôll see how GRPO can push performance further without needing any additional labeled data.\n\n\n\n\n\n\nTry the model yourself\n\n\n\nThe fine-tuned model is available at davanstrien/iconclass-vlm if you want to experiment with it on your own art history images!\n\n\n\n\n\n\n\n\nBuilding evaluation tools\n\n\n\nInterested in how the prediction viewer was built? I‚Äôll cover that in a separate post about creating interactive model evaluation tools using vibe coding and the Hugging Face datasets viewer API!"
  },
  {
    "objectID": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#conclusion-making-vlm-fine-tuning-accessible",
    "href": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#conclusion-making-vlm-fine-tuning-accessible",
    "title": "Fine-tuning Vision-Language Models for Art History: Iconclass Classification with TRL and HF Jobs",
    "section": "Conclusion: Making VLM Fine-tuning Accessible",
    "text": "Conclusion: Making VLM Fine-tuning Accessible\nWe‚Äôve successfully fine-tuned a vision-language model to generate specialized art history metadata - and the entire process required just a few steps:\n\nFormat a dataset with images and expected responses\nWrite (or adapt) a simple TRL training script\nSubmit to HF Jobs with uv run for cloud-based training\n\nThe combination of TRL‚Äôs VLM support and HF Jobs removes the traditional barriers to specialized model training. No local GPU setup, no complex multi-modal training code, no infrastructure management.\n\nWhat‚Äôs Next: Pushing Performance with GRPO\nWhile our SFT model shows promising results, we can go further. In the next post, I‚Äôll show how to use GRPO (Group Relative Policy Optimization) to further refine the model.\nGRPO lets the model learn from its own predictions, and let‚Äôs us define more nuanced reward functions which could work very well for this kind of domain specific use case!\n\n\nResources\n\nü§ó Model: davanstrien/iconclass-vlm\nüìä Dataset: davanstrien/iconclass-vlm-sft\nüíª Training Script: main.py\nüìö TRL VLM Docs: VLM Support in TRL\n‚ö° HF Jobs Docs: Hugging Face Jobs Documentation\n\nThis kind of approach could have many benefits for GLAM institutions and other specialist domains. As usual the main thing you need is to focus on the data! Creating and sharing a cool dataset is the first (and a very crucial) step towards building effective AI models for a new domain. Thanks to Etienne Posthumus for creating and sharing the initial dataset that made this project possible!"
  },
  {
    "objectID": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#footnotes",
    "href": "posts/2025/iconclass-vlm-sft/trl-vlm-fine-tuning-iconclass.html#footnotes",
    "title": "Fine-tuning Vision-Language Models for Art History: Iconclass Classification with TRL and HF Jobs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIconclass. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Iconclass‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "",
    "text": "The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. - FineWeb blog post\n\nWhilst quality is important, the quantity of data we start with is significant. For example the original FineWeb-2 dataset contains 15 trillion tokens (44TB on disk).\nThe challenge is then: how can we curate data for LLMs in a scalable way? One possible approach is to use LLMs to help with labelling text. Whilst LLMs can do well on this kind of task (especially when using Structured Generation), scaling LLM labeling to web scale is a challenge.\nLast year, FineWeb-Edu showed that filtering data for educational quality could yield improvements for LLMs trained on this filtered data. The approach they took was to first use an LLM to label a subset of data and then use a fine-tuned a much smaller BERT-based model to filter the data.\nThe publication of ModernBert has shown that there is still a lot of excitement around the use of smaller encoder-based models for labelling tasks, indeed one of the examples they cite in their blog post is the cost of creating FineWeb-Edu if they had used a decoder-only model.\n\nAn interesting example is FineWeb-Edu, where model-based quality filtering had to be performed over 15 trillion tokens. The FineWeb-Edu team chose to generate annotations with a decoder-only model, Llama-3-70b-Instruct, and perform the bulk of the filtering with a fine-tuned BERT-based model. This filtering took 6,000 H100 hours, which, at HuggingFace Inference Endpoints‚Äô pricing of $10/hour, comes to a total of $60,000. On the other hand, feeding 15 trillion tokens to popular decoder-only models, even with the lowest-cost option of using Google‚Äôs Gemini Flash and its low inference cost of $0.075/million tokens, would cost over one million dollars! ModernBert blog post\n\nThe ideal approach then seems to be something like:\n\nUse an LLM to label a subset of data and use these labels to train a smaller model\nUse a smaller model to make prediction on a large dataset and use these predictions to filter the data\nProfit!\n\nWhile this may work well for English data, many papers have shown that the performance of LLMs for these kinds of tasks can be much worse than for English.\nThis is where the FineWeb-c dataset comes in."
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#understand-the-data",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#understand-the-data",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Understand the data",
    "text": "Understand the data\nWe‚Äôll load the dataset from the Hugging Face Hub using Polars which supports loading data from the Hub.\nIn this case, we‚Äôll load the ‚Äúdefault‚Äù config which contains all the languages that have reached the 1,000 annotations threshold.\n\ndf = pl.read_parquet(\"hf://datasets/data-is-better-together/fineweb-c/data/*.parquet\")\ndf = df.lazy()\n\n\ndf.select(\"language_code\").unique().collect().to_series().to_list()\n\n['lvs_Latn',\n 'arb_Arab',\n 'asm_Latn',\n 'swe_Latn',\n 'dan_Latn',\n 'vie_Latn',\n 'gmh_Latn',\n 'bar_Latn',\n 'tat_Cyrl',\n 'fas_Arab',\n 'cmn_Hani',\n 'slk_Latn',\n 'ukr_Cyrl',\n 'fin_Latn',\n 'arz_Arab',\n 'fra_Latn',\n 'rus_Cyrl',\n 'ary_Arab',\n 'hin_Deva',\n 'fil_Latn']"
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#can-we-train-a-model-to-work-with-a-subset-of-languages",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#can-we-train-a-model-to-work-with-a-subset-of-languages",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Can we train a model to work with a subset of languages?",
    "text": "Can we train a model to work with a subset of languages?\nSo far we have 18 languages in the dataset. Currently over a 100 languages have some level of annotation so eventually we hope the community will create a dataset for ma ne model per language.\nClaude gave me the following visual for languages groups in this dataset (which isn‚Äôt super accurate but gives a rough idea of the language families represented in the dataset).\n\n\n\n\n\n\nFigure¬†1: Color-coded grid visualization of 20 languages organized by 8 language families\n\n\n\nWe can see a few possible language groups that it could make sense to train a model on. One potential group is the Germanic languages. Let‚Äôs look at the data for these languages.\n\n\n\n\n\n\nNote\n\n\n\nI focused on Germanic languages to start with as if I squint I can get a rough understanding of the language. I would be very excited to see the community begin to explore all of the languages in the dataset.\n\n\n\ngermanic_languages = [\"gmh_Latn\", \"dan_Latn\", \"swe_Latn\", \"bar_Latn\", \"lvs_Latn\"]"
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#what-do-we-want-to-label",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#what-do-we-want-to-label",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "What do we want to label?",
    "text": "What do we want to label?\nWhile the overarching goal of the FineWeb-C project is to create a dataset for training models to label the educational quality of text, in order to effectively train this kind of model we need a reasonable distribution of labels.\n\nProblematic content?\nThe annotation interface for FineWeb-c looks something like this (using Scots as an example).\n\n\n\nAnnotation interface for Scots\n\n\nFor this example, the annotator could mostly focus on how educational the text is. However, this is the web we‚Äôre annotating so we will sometimes come across ‚Äúproblematic‚Äù content. This content is usually content in the wrong language i.e.¬†the language predicted during the FineWeb-2 extraction process is incorrect (this happens quite a lot for some languages and much less for others) or the content is garbled in some way. An example of this kind of content is shown below.\n\n\n\nProblematic content example\n\n\nWe can see that the content is in the wrong language (English) the text is also somewhat garbled\n\n\n\n\n\n\nNote\n\n\n\nWhy aren‚Äôt these just labeled as None i.e.¬†no educational value? Annotating data always comes with some ambiguity. In this case, we added a problematic label to try and make it easier for the community to flag content that was incorrect in some other way. We could have added a whole separate label for this and for the different possible types of issue but this adds extra cognitive load for the annotator. We can also deal with possible overlap in usage of these labels in other ways.\n\n\nBefore we dive into training an educational quality classifier, let‚Äôs look at the data for the Germanic languages and see how often this problematic content is present. We‚Äôre using Polars in Lazy mode but it‚Äôs not really necessary for the size of the dataset at the moment. In the future the dataset might become large enough that this can be more important.\n\ndf_germanic = df.filter(pl.col(\"language_code\").is_in(germanic_languages))\n\nLet‚Äôs start by seeing the percentage of problematic content for each language in the ‚Äúgermanic‚Äù group.\n\n(\n    df_germanic.group_by(\"language_code\")\n    .agg(\n        [\n            (\n                pl.col(\"problematic_content_label_present\").sum()\n                / pl.col(\"problematic_content_label_present\").count()\n                * 100\n            ).alias(\"problematic_percentage\")\n        ]\n    )\n    .sort(\"problematic_percentage\", descending=True)\n).collect()\n\n\nshape: (5, 2)\n\n\n\nlanguage_code\nproblematic_percentage\n\n\nstr\nf64\n\n\n\n\n\"gmh_Latn\"\n98.3\n\n\n\"bar_Latn\"\n77.0\n\n\n\"dan_Latn\"\n19.4\n\n\n\"swe_Latn\"\n8.8\n\n\n\"lvs_Latn\"\n8.7\n\n\n\n\n\n\nWe can see that for some of the languages in this group the percentage of problematic content is very high. In particular Bavarian and Middle High German have a very high percentage of problematic content. The other languages have a much lower percentage of problematic content.\n\n\nMaking the lives of annotators easier and getting more educational?\nWhile we could jump to training a model to label the educational quality of the text, we may wan to start in a more modest way. In my previous blog post I generated this plot showing the distribution of educational value for languages in the dataset.\n\n\n\nLabel distribution by language\n\n\nWhile some languages have a fairly good distribution of educational value labels, most have very few examples of ‚ÄúExcellent‚Äù educational quality content. This is not surprising ‚Äì the majority of the web is not educational‚Ä¶\nFor languages with very few examples of any educational content training a classifier to label the educational quality of the text is not going to work well. For these languages we probably want to first focus on being able to remove problematic content so we reduce the amount of ‚Äúnoise‚Äù annotators need to spend time on and then can instead focus on labelling content that is more likely to be educational.\nFor this we‚Äôll train a model to label problematic content. We‚Äôll start with the Scandinavian languages since we have Danish and Swedish datasets completed and these languages are somewhat similar (don‚Äôt come at me Danish and Swedish speaker ‚Äì I‚Äôve watched the Bridge!)\n\n\n\n\n\n\nNote\n\n\n\nThis is not the only approach we could take but it could be a good starting point. Even if we don‚Äôt end up with sufficient data to train a model to label the educational quality of the text, we can still use this model to remove problematic content and improve the quality of the data we have for some languages."
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#training-out-hygge-model",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#training-out-hygge-model",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Training out hygge model",
    "text": "Training out hygge model\nLet‚Äôs start by loading the data for the Scandinavian languages.\n\nscandinavian_languages = [\"swe_Latn\", \"dan_Latn\"]\n\n\ndf_scandinavian = df.filter(pl.col(\"language_code\").is_in(scandinavian_languages))\n\nWe can see that we have 1,000 annotations for each language.\n\ndf_scandinavian.collect().shape\n\n(2000, 8)\n\n\nNow let‚Äôs get a better understanding of the data. Swedish and Danish both have multiple annotations for many of the texts. This means multiple annotators have looked at the same text and given their assessment of the educational value. Let‚Äôs take a look at some examples where the annotators disagree i.e.¬†gave different labels.\n\ndf_scandinavian.filter(\n    pl.col(\"educational_value_labels\").list.unique().list.len() &gt; 1\n).collect()\n\n\nshape: (524, 8)\n\n\n\nid\ntext\neducational_value_labels\nannotator_ids\nproblematic_content_label_present\nproblematic_content_label_agreement\nlanguage_names\nlanguage_code\n\n\nstr\nstr\nlist[str]\nlist[str]\nbool\nf64\nstr\nstr\n\n\n\n\n\"d0347a16-14a6-40c7-a1b8-3d27b9‚Ä¶\n\"Virkelige f√•r de vil ha sj√¶l o‚Ä¶\n[\"‚ùó Problematic Content ‚ùó\", \"None\", \"‚ùó Problematic Content ‚ùó\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\", \"e9f72b47-2af5-4b06-90f2-7163de147a1d\"]\ntrue\n0.666667\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\"ec7699c9-78e2-48ef-945e-9b0a71‚Ä¶\n\"Alle dr√∏mmer om den store gevi‚Ä¶\n[\"None\", \"Minimal\", \"Basic\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\", \"9987848b-debb-4ed3-a97b-14eb9b3c4322\"]\nfalse\n1.0\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\"49de8369-2b33-47d2-a877-2fe32b‚Ä¶\n\"Der er en elektrisk forbindels‚Ä¶\n[\"Basic\", \"None\", \"None\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"b98b0144-391d-4e70-bae0-743ce94e6314\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\"]\nfalse\n1.0\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\"b20402d1-8250-410c-b177-966b8b‚Ä¶\n\"Online shopping r√•d - Levering‚Ä¶\n[\"Basic\", \"Minimal\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\"]\nfalse\n1.0\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\"dc34ee93-74e2-48ba-9b5c-9d4dfb‚Ä¶\n\"Morgenmad er det vigtigste m√•l‚Ä¶\n[\"Basic\", \"Basic\", \"None\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\", \"4e0a264e-6445-495f-ae54-8e0755b8ebd0\"]\nfalse\n1.0\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"aee9b105-18c3-455d-b07c-545eb2‚Ä¶\n\"Dronning Victoria afskyede sin‚Ä¶\n[\"Minimal\", \"None\", \"‚ùó Problematic Content ‚ùó\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"9987848b-debb-4ed3-a97b-14eb9b3c4322\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\"]\ntrue\n0.333333\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\"42abb527-d3b8-4b23-b88a-d3df06‚Ä¶\n\"Smarte Opbevaringsl√∏sninger ti‚Ä¶\n[\"Minimal\", \"None\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\"]\nfalse\n1.0\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\"7076624a-7b72-4534-bfdc-a4b6fc‚Ä¶\n\"Power Automate er under udbred‚Ä¶\n[\"Basic\", \"None\", \"Minimal\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"740270b9-61bf-4d85-a495-9e37270f7257\", \"82197ecd-6d0b-400a-834a-703da28164ae\"]\nfalse\n1.0\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\"a97595d9-61b4-4ae7-953c-654a85‚Ä¶\n\"Kl. 17.30 - 19.30 Home Concert‚Ä¶\n[\"None\", \"Minimal\", \"None\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"9987848b-debb-4ed3-a97b-14eb9b3c4322\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\"]\nfalse\n1.0\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\"d3c4b487-6976-45bf-9a81-2c8208‚Ä¶\n\"J. S. RASCH VIN & SPIRITUS J. ‚Ä¶\n[\"Basic\", \"Minimal\", \"None\"]\n[\"a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f\", \"82197ecd-6d0b-400a-834a-703da28164ae\", \"85ac8d54-89c5-4473-95c4-797366f03cd0\"]\nfalse\n1.0\n\"dan_Latn\"\n\"dan_Latn\"\n\n\n\n\n\n\nEven with a quick eyeball we can see that often ‚Äúproblematic‚Äù or None are used to label the same text. Similarly ‚Äúminimal‚Äù or ‚Äúnone‚Äù are used to label the same text. This isn‚Äôt so surprising since the educational quality is fairly subjective. The main thing we probably don‚Äôt want too much of is content with very extreme labels i.e None vs Excellent. We can take a closer look at the combinations of labels that are used. Let‚Äôs take a look at the unique combinations of non agreeing labels.\n\n\nCode\n(\n    df_scandinavian.filter(\n        pl.col(\"educational_value_labels\").list.unique().list.len() &gt; 1\n    )\n    .select(\n        pl.col(\"educational_value_labels\").list.sort().alias(\"educational_value_labels\")\n    )\n    .unique()\n).collect()\n\n\n\nshape: (37, 1)\n\n\n\neducational_value_labels\n\n\nlist[str]\n\n\n\n\n[\"Basic\", \"Good\", \"Minimal\"]\n\n\n[\"Good\", \"Minimal\"]\n\n\n[\"Basic\", \"Good\"]\n\n\n[\"Minimal\", \"Minimal\", \"‚ùó Problematic Content ‚ùó\"]\n\n\n[\"Basic\", \"None\", \"‚ùó Problematic Content ‚ùó\"]\n\n\n‚Ä¶\n\n\n[\"Good\", \"None\", \"None\"]\n\n\n[\"None\", \"‚ùó Problematic Content ‚ùó\", \"‚ùó Problematic Content ‚ùó\"]\n\n\n[\"Good\", \"Good\", \"Minimal\"]\n\n\n[\"Basic\", \"Minimal\", \"‚ùó Problematic Content ‚ùó\"]\n\n\n[\"Basic\", \"Basic\", \"Good\"]\n\n\n\n\n\n\nFrom a quick eyeball we don‚Äôt seem to have disagreement that is too extreme. Let‚Äôs get a better understanding of the co-occurrence of labels when annotators disagree.\n\n\nCode\ncombinations = (\n    (\n        df_scandinavian.filter(\n            pl.col(\"educational_value_labels\").list.unique().list.len() &gt; 1\n        )\n        .select(\n            pl.col(\"educational_value_labels\")\n            .list.unique()\n            .list.sort()\n            .alias(\"educational_value_labels\")\n        )\n        .collect()\n    )\n    .to_series()\n    .to_list()\n)\ncombinations[:10]\n\n\n[['None', '‚ùó Problematic Content ‚ùó'],\n ['Basic', 'Minimal', 'None'],\n ['Basic', 'None'],\n ['Basic', 'Minimal'],\n ['Basic', 'None'],\n ['Basic', 'None'],\n ['Basic', 'Minimal', 'None'],\n ['None', '‚ùó Problematic Content ‚ùó'],\n ['None', '‚ùó Problematic Content ‚ùó'],\n ['Minimal', 'None']]\n\n\nWe can plot this using some code Claude gave me.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# First, let's get all unique labels that appear\nall_labels = set()\nfor combo in combinations:  # combinations is your list of lists\n    all_labels.update(combo)\nall_labels = sorted(list(all_labels))\n\n# Create a co-occurrence matrix\ncooc_matrix = pd.DataFrame(0, index=all_labels, columns=all_labels)\n\n# Fill the matrix\nfor combo in combinations:\n    for label1 in combo:\n        for label2 in combo:\n            if label1 != label2:\n                cooc_matrix.loc[label1, label2] += 1\n\n# Convert to percentage of times labels co-occur\ntotal_occurrences = cooc_matrix.sum().sum()\ncooc_matrix_pct = cooc_matrix / total_occurrences * 100\n\n# Print most common co-occurrences\npairs = []\nfor i in range(len(all_labels)):\n    for j in range(i + 1, len(all_labels)):\n        label1, label2 = all_labels[i], all_labels[j]\n        count = cooc_matrix.loc[label1, label2]\n        if count &gt; 0:\n            pairs.append((label1, label2, count))\n\n# Sort by count\npairs.sort(key=lambda x: x[2], reverse=True)\n\n# Print top co-occurrences\nprint(\"Most common label combinations:\")\nfor label1, label2, count in pairs[:10]:\n    print(f\"{label1} + {label2}: {count} occurrences\")\n\n\nMost common label combinations:\nMinimal + None: 305 occurrences\nBasic + Minimal: 96 occurrences\nNone + ‚ùó Problematic Content ‚ùó: 84 occurrences\nBasic + None: 69 occurrences\nGood + Minimal: 24 occurrences\nMinimal + ‚ùó Problematic Content ‚ùó: 23 occurrences\nGood + None: 12 occurrences\nBasic + Good: 11 occurrences\nBasic + ‚ùó Problematic Content ‚ùó: 7 occurrences\nBasic + Excellent: 5 occurrences\n\n\nWe see here that Minimal and None are the most common labels when annotators disagree. We also see some ‚Äúproblematic‚Äù labels with None. In the FineWeb-c dataset the problematic_content_label_present is a boolean column that is True if any of the annotators labeled the text as problematic. We want to check that this wouldn‚Äôt capture too many examples where another annotator would rate the text highly. If we train a classifier to remove problematic content is may also remove some examples which would be labelled None or possibly minimal but since we‚Äôre mostly seeking to get higher educational quality data this isn‚Äôt really a problem."
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#preparing-the-data-for-training",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#preparing-the-data-for-training",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Preparing the data for training",
    "text": "Preparing the data for training\nLet‚Äôs remind ourselves of the percentage of problematic content for each language we‚Äôre working with.\n\n\nCode\n(\n    df_scandinavian.group_by(\"language_code\").agg(\n        [\n            (\n                pl.col(\"problematic_content_label_present\").sum()\n                / pl.col(\"problematic_content_label_present\").count()\n                * 100\n            ).alias(\"problematic_percentage\")\n        ]\n    )\n).collect()\n\n\n\nshape: (2, 2)\n\n\n\nlanguage_code\nproblematic_percentage\n\n\nstr\nf64\n\n\n\n\n\"swe_Latn\"\n8.8\n\n\n\"dan_Latn\"\n19.4\n\n\n\n\n\n\nLet‚Äôs now convert our LazyFrame to a Polars DataFrame so it‚Äôs a bit easier to pass to other libraries.\n\ndf_scandinavian = df_scandinavian.collect()\n\n\nTrain / test split\nCreating a good train/test split is important for making sure we train a model that generalizes well. We‚Äôll use a stratified split to ensure that the train and test set have a similar distribution of labels. Since we‚Äôre working with two language we probably want to stratify on language too.\n\n\nCode\n# Create stratification column\ndf_scandinavian = df_scandinavian.with_columns(\n    strat_col=pl.col(\"language_code\")\n    + \"_\"\n    + pl.col(\"problematic_content_label_present\").cast(pl.Utf8)\n)\n\n# Convert to numpy for sklearn\nX = df_scandinavian.select([\"id\", \"text\"]).to_numpy()  # Including id for tracking\ny = df_scandinavian.select(\"problematic_content_label_present\").to_numpy()\nstrat = df_scandinavian.select(\"strat_col\").to_numpy()\n\n# Create stratified split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=strat, random_state=42\n)\n\n# Convert back to Polars DataFrames with all relevant columns\ntrain_indices = set(X_train[:, 0])  # Assuming first column is id\ntest_indices = set(X_test[:, 0])\n\ntrain_df = df_scandinavian.filter(pl.col(\"id\").is_in(train_indices))\ntest_df = df_scandinavian.filter(pl.col(\"id\").is_in(test_indices))\n\n\nLet‚Äôs take a look at the distribution of labels in the train and test set by language and label (problematic or not).\n\n\nCode\nprint(\"\\nTrain Set:\")\n# Problematic content percentage by language\nprint(\"Label distribution within each language:\")\nprint(\n    (\n        train_df.group_by(\"language_code\")\n        .agg(\n            [\n                (\n                    pl.col(\"problematic_content_label_present\").sum()\n                    / pl.col(\"problematic_content_label_present\").count()\n                    * 100\n                ).alias(\"problematic_percentage\"),\n                pl.col(\"problematic_content_label_present\")\n                .count()\n                .alias(\"total_count\"),\n            ]\n        )\n        .sort(\"language_code\")\n        .with_columns(pl.col(\"problematic_percentage\").round(2))\n    )\n)\n\n# Language distribution\nprint(\"\\nLanguage distribution in train set:\")\nprint(\n    (\n        train_df.group_by(\"language_code\")\n        .agg(\n            (pl.len() / train_df.height * 100).alias(\"percentage_of_split\"),\n            pl.len().alias(\"count\"),\n        )\n        .sort(\"language_code\")\n        .with_columns(pl.col(\"percentage_of_split\").round(2))\n    )\n)\n\nprint(\"\\nTest Set:\")\n# Problematic content percentage by language\nprint(\"Label distribution within each language:\")\nprint(\n    (\n        test_df.group_by(\"language_code\")\n        .agg(\n            [\n                (\n                    pl.col(\"problematic_content_label_present\").sum()\n                    / pl.col(\"problematic_content_label_present\").count()\n                    * 100\n                ).alias(\"problematic_percentage\"),\n                pl.col(\"problematic_content_label_present\")\n                .count()\n                .alias(\"total_count\"),\n            ]\n        )\n        .sort(\"language_code\")\n        .with_columns(pl.col(\"problematic_percentage\").round(2))\n    )\n)\n\n# Language distribution\nprint(\"\\nLanguage distribution in test set:\")\nprint(\n    (\n        test_df.group_by(\"language_code\")\n        .agg(\n            (pl.len() / test_df.height * 100).alias(\"percentage_of_split\"),\n            pl.len().alias(\"count\"),\n        )\n        .sort(\"language_code\")\n        .with_columns(pl.col(\"percentage_of_split\").round(2))\n    )\n)\n\n\n\nTrain Set:\nLabel distribution within each language:\nshape: (2, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ language_code ‚îÜ problematic_percentage ‚îÜ total_count ‚îÇ\n‚îÇ ---           ‚îÜ ---                    ‚îÜ ---         ‚îÇ\n‚îÇ str           ‚îÜ f64                    ‚îÜ u32         ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ dan_Latn      ‚îÜ 19.38                  ‚îÜ 800         ‚îÇ\n‚îÇ swe_Latn      ‚îÜ 8.75                   ‚îÜ 800         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nLanguage distribution in train set:\nshape: (2, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ language_code ‚îÜ percentage_of_split ‚îÜ count ‚îÇ\n‚îÇ ---           ‚îÜ ---                 ‚îÜ ---   ‚îÇ\n‚îÇ str           ‚îÜ f64                 ‚îÜ u32   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ dan_Latn      ‚îÜ 50.0                ‚îÜ 800   ‚îÇ\n‚îÇ swe_Latn      ‚îÜ 50.0                ‚îÜ 800   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nTest Set:\nLabel distribution within each language:\nshape: (2, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ language_code ‚îÜ problematic_percentage ‚îÜ total_count ‚îÇ\n‚îÇ ---           ‚îÜ ---                    ‚îÜ ---         ‚îÇ\n‚îÇ str           ‚îÜ f64                    ‚îÜ u32         ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ dan_Latn      ‚îÜ 19.5                   ‚îÜ 200         ‚îÇ\n‚îÇ swe_Latn      ‚îÜ 9.0                    ‚îÜ 200         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nLanguage distribution in test set:\nshape: (2, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ language_code ‚îÜ percentage_of_split ‚îÜ count ‚îÇ\n‚îÇ ---           ‚îÜ ---                 ‚îÜ ---   ‚îÇ\n‚îÇ str           ‚îÜ f64                 ‚îÜ u32   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ dan_Latn      ‚îÜ 50.0                ‚îÜ 200   ‚îÇ\n‚îÇ swe_Latn      ‚îÜ 50.0                ‚îÜ 200   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#loading-as-huggingface-dataset",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#loading-as-huggingface-dataset",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Loading as HuggingFace Dataset",
    "text": "Loading as HuggingFace Dataset\nWe‚Äôll now load the data as a HuggingFace Dataset. We‚Äôll first convert the problematic_content_label_present column to an integer column.\n\n\ntrain_df = train_df.with_columns(\n    pl.col(\"problematic_content_label_present\").cast(pl.Int32)\n)\ntest_df = test_df.with_columns(\n    pl.col(\"problematic_content_label_present\").cast(pl.Int32)\n)\n\n\ntrain_ds = Dataset.from_polars(train_df)\ntest_ds = Dataset.from_polars(test_df)\n\nWe rename the problematic_content_label_present column to labels to match the expected column name for the Transformers Trainer.\n\ntrain_ds = train_ds.rename_column(\"problematic_content_label_present\", \"labels\")\ntest_ds = test_ds.rename_column(\"problematic_content_label_present\", \"labels\")"
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#fine-tuning-a-model",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#fine-tuning-a-model",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Fine tuning a model",
    "text": "Fine tuning a model\nWe‚Äôll now fine tune a model to predict the problematic_content_label_present column. To do this we‚Äôll want a fill-mask model which supports the languages we‚Äôre working with. We can find these models using the HuggingFace Hub using this url:\nhttps://huggingface.co/models?pipeline_tag=fill-mask&language=da,sv&sort=trending We can try out a few options but we‚Äôll start with the FacebookAI/xlm-roberta-base model."
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#defining-metrics",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#defining-metrics",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Defining metrics",
    "text": "Defining metrics\nWe‚Äôll define a function to compute the metrics we want to use to evaluate the model. Since we‚Äôre working with an imbalanced dataset we‚Äôll want to use a few different metrics. We‚Äôre probably going a bit overboard here but since the dataset is small it can be useful to have a few more metrics to look at to understand the model‚Äôs performance.\n\ndef compute_metrics(pred):\n    \"\"\"\n    Compute metrics including AUC-ROC for the minority class.\n    \"\"\"\n    # Get labels\n    labels = pred.label_ids\n\n    # Convert logits to probabilities using softmax \n    probs = softmax(pred.predictions, axis=1)\n    # Get probability scores for the minority class (assuming it's label 1)\n    minority_probs = probs[:, 1]\n\n    # Get predicted class (argmax of logits)\n    preds = np.argmax(pred.predictions, axis=1)\n\n    # Calculate standard metrics\n    precision = precision_score(labels, preds)\n    recall = recall_score(labels, preds)\n    f1 = f1_score(labels, preds)\n    \n    # Calculate additional metrics for imbalanced classification\n    cm = confusion_matrix(labels, preds)\n    tn, fp, fn, tp = cm.ravel()\n    specificity = tn / (tn + fp)  # True negative rate\n    balanced_acc = (recall + specificity) / 2  # Balanced accuracy\n    auc_roc = roc_auc_score(labels, minority_probs)\n    avg_precision = average_precision_score(labels, minority_probs)  # Area under PR curve\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall, \n        \"f1\": f1,\n        \"auc_roc\": auc_roc,\n        \"balanced_accuracy\": balanced_acc,\n        \"average_precision\": avg_precision\n    }"
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#setting-up-the-training",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#setting-up-the-training",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Setting up the training",
    "text": "Setting up the training\nI find it nice to have a mapping between the labels and the ids so later I don‚Äôt need to remember which label is which id.\n\npossible_labels = (\n    df_scandinavian.select(\"problematic_content_label_present\")\n    .unique()\n    .to_series()\n    .to_list()\n)\npossible_labels\n\n[False, True]\n\n\n\nlabel2id = {label: i for i, label in enumerate(possible_labels)}\nid2label = {0: \"not_problematic\", 1: \"problematic\"}\n\n\nAuthenticating with HuggingFace\nWe‚Äôll need to authenticate with HuggingFace to push the model to the Hub.\n\nfrom huggingface_hub import login\n\n\nlogin()\n\n\n\n\n\n\nLogging with Weights & Biases\nWe‚Äôll also log the training with Weights & Biases.\n\nimport wandb\n\n\nwandb.login()\n\n\nwandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\nwandb: You can find your API key in your browser here: https://wandb.ai/authorize\n\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n\n\n\n  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n\n\n\nwandb: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n\n\n\n\nTrue\n\n\nThe training code is not super interesting or particularly elegant. I just wanted to get something working.\n\n\nCode\ndef train_model(\n    train_ds,\n    test_ds,\n    hub_model_id,\n    pre_trained_model_name=\"distilbert/distilbert-base-multilingual-cased\",\n    num_epochs=20,\n    batch_size=128,\n    label2id=None,\n    id2label=None,\n):\n    \"\"\"\n    Train and evaluate the model with additional metrics for imbalanced classification.\n\n    Args:\n        train_ds: Training dataset\n        test_ds: Test dataset\n        hub_model_id: Model ID for pushing to HuggingFace Hub\n        pre_trained_model_name: Name of pretrained model to use\n        num_epochs: Number of training epochs\n        batch_size: Batch size for training\n        label2id: Dictionary mapping labels to IDs\n        id2label: Dictionary mapping IDs to labels\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pre_trained_model_name,\n        num_labels=2,  # Binary classification\n        label2id=label2id,\n        id2label=id2label,\n    )\n\n    def tokenize_function(examples):\n        \"\"\"\n        Tokenize the text data with proper padding and truncation.\n        \"\"\"\n        return tokenizer(\n            examples[\"text\"], padding=True, truncation=True, max_length=512\n        )\n\n    split_dataset = DatasetDict({\"train\": train_ds, \"test\": test_ds})\n\n    # Tokenize datasets\n    tokenized_train = split_dataset[\"train\"].map(tokenize_function, batched=True)\n    tokenized_val = split_dataset[\"test\"].map(tokenize_function, batched=True)\n\n    print(f\"Tokenized train dataset: {tokenized_train}\")\n    print(f\"Tokenized val dataset: {tokenized_val}\")\n\n    # Set up training arguments\n    training_args = TrainingArguments(\n        output_dir=\"/data/results\",\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=2e-5,\n        weight_decay=0.01,\n        push_to_hub=True,\n        eval_strategy=\"steps\",\n        eval_steps=100,\n        logging_steps=100,\n        save_strategy=\"steps\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"auc_roc\",  # Using AUC-ROC for model selection\n        greater_is_better=True,\n        save_total_limit=20,\n        hub_model_id=hub_model_id,\n        fp16=True,\n        save_safetensors=False,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        tokenizer=tokenizer,  # Using tokenizer instead of processing_class\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        compute_metrics=compute_metrics,\n        callbacks=[\n            EarlyStoppingCallback(\n                early_stopping_patience=8, early_stopping_threshold=0.001\n            )\n        ],\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate the model\n    eval_results = trainer.evaluate()\n\n    return trainer, eval_results\n\n\ndef main(\n    train_ds,\n    test_ds,\n    hub_model_id,\n    pre_trained_model_name=\"distilbert/distilbert-base-multilingual-cased\",\n    num_epochs=20,\n    batch_size=128,\n    label2id=None,\n    id2label=None,\n):\n    \"\"\"\n    Main training function that handles model training and evaluation.\n\n    Args:\n        train_ds: Training dataset\n        test_ds: Test dataset\n        hub_model_id: Model ID for pushing to HuggingFace Hub\n        pre_trained_model_name: Name of pretrained model to use\n        num_epochs: Number of training epochs\n        batch_size: Batch size for training\n        label2id: Dictionary mapping labels to IDs\n        id2label: Dictionary mapping IDs to labels\n    \"\"\"\n    # Train and evaluate the model\n    trainer, eval_results = train_model(\n        train_ds=train_ds,\n        test_ds=test_ds,\n        hub_model_id=hub_model_id,\n        pre_trained_model_name=pre_trained_model_name,\n        num_epochs=num_epochs,\n        batch_size=batch_size,\n        label2id=label2id,\n        id2label=id2label,\n    )\n\n    # Print evaluation results with all metrics\n    print(\"\\nEvaluation Results:\")\n    print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")\n    print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n    print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n    print(f\"AUC-ROC (minority class): {eval_results['eval_auc_roc']:.4f}\")\n    print(\n        f\"Average Precision (minority class): {eval_results['eval_average_precision']:.4f}\"\n    )\n    print(f\"Balanced Accuracy: {eval_results['eval_balanced_accuracy']:.4f}\")\n\n    return trainer, eval_results\n\n\n\ntrainer, results = main(\n    train_ds,\n    test_ds,\n    hub_model_id=\"davanstrien/scandi-fine-web-cleaner\",\n    pre_trained_model_name=\"FacebookAI/xlm-roberta-base\",\n    num_epochs=30,\n    batch_size=16,\n    label2id=label2id,\n    id2label=id2label,\n)\n\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\n\n\n\n\nTokenized train dataset: Dataset({\n    features: ['id', 'text', 'educational_value_labels', 'annotator_ids', 'labels', 'problematic_content_label_agreement', 'language_names', 'language_code', 'strat_col', 'input_ids', 'attention_mask'],\n    num_rows: 1600\n})\nTokenized val dataset: Dataset({\n    features: ['id', 'text', 'educational_value_labels', 'annotator_ids', 'labels', 'problematic_content_label_agreement', 'language_names', 'language_code', 'strat_col', 'input_ids', 'attention_mask'],\n    num_rows: 400\n})\n\n\n/home/user/miniconda/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_77/3658588944.py:156: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n\n\n      \n      \n      [1000/3000 02:00 &lt; 04:01, 8.29 it/s, Epoch 10/30]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\nPrecision\nRecall\nF1\nSpecificity\nNpv\nAuc Roc\nAverage Precision\nTrue Positives\nFalse Positives\nTrue Negatives\nFalse Negatives\nMinority Class Ratio\nPredicted Minority Ratio\nBest F1 Threshold\nBest F1 Score\nDefault Precision\nDefault Recall\nDefault F1\nBalanced Accuracy\n\n\n\n\n100\n0.282200\n0.233291\n0.851064\n0.701754\n0.769231\n0.979592\n0.951841\n0.912562\n0.808400\n40\n7\n336\n17\n0.142500\n0.117500\n0.710879\n0.769231\n0.800000\n0.701754\n0.747664\n0.840673\n\n\n200\n0.176000\n0.255909\n0.941176\n0.561404\n0.703297\n0.994169\n0.931694\n0.932229\n0.821460\n32\n2\n341\n25\n0.142500\n0.085000\n0.060919\n0.766355\n0.942857\n0.578947\n0.717391\n0.777786\n\n\n300\n0.155400\n0.284004\n0.948718\n0.649123\n0.770833\n0.994169\n0.944598\n0.904225\n0.812967\n37\n2\n341\n20\n0.142500\n0.097500\n0.792384\n0.778947\n0.948718\n0.649123\n0.770833\n0.821646\n\n\n400\n0.141300\n0.293026\n0.816327\n0.701754\n0.754717\n0.973761\n0.951567\n0.930004\n0.826913\n40\n9\n334\n17\n0.142500\n0.122500\n0.938463\n0.778947\n0.816327\n0.701754\n0.754717\n0.837758\n\n\n500\n0.113500\n0.284364\n0.972222\n0.614035\n0.752688\n0.997085\n0.939560\n0.927190\n0.824227\n35\n1\n342\n22\n0.142500\n0.090000\n0.013480\n0.784314\n0.945946\n0.614035\n0.744681\n0.805560\n\n\n600\n0.088200\n0.365192\n0.880952\n0.649123\n0.747475\n0.985423\n0.944134\n0.923252\n0.818148\n37\n5\n338\n20\n0.142500\n0.105000\n0.989013\n0.760870\n0.880952\n0.649123\n0.747475\n0.817273\n\n\n700\n0.078500\n0.360336\n0.972222\n0.614035\n0.752688\n0.997085\n0.939560\n0.923047\n0.819323\n35\n1\n342\n22\n0.142500\n0.090000\n0.196286\n0.770833\n0.945946\n0.614035\n0.744681\n0.805560\n\n\n800\n0.030700\n0.381501\n0.759259\n0.719298\n0.738739\n0.962099\n0.953757\n0.918853\n0.826950\n41\n13\n330\n16\n0.142500\n0.135000\n0.998579\n0.787234\n0.745455\n0.719298\n0.732143\n0.840699\n\n\n900\n0.029300\n0.486526\n0.971429\n0.596491\n0.739130\n0.997085\n0.936986\n0.912894\n0.812492\n34\n1\n342\n23\n0.142500\n0.087500\n0.006314\n0.770833\n0.972222\n0.614035\n0.752688\n0.796788\n\n\n1000\n0.015900\n0.468468\n1.000000\n0.649123\n0.787234\n1.000000\n0.944904\n0.909672\n0.804402\n37\n0\n343\n20\n0.142500\n0.092500\n0.644980\n0.787234\n1.000000\n0.649123\n0.787234\n0.824561\n\n\n\n\n\n\nCould not locate the best model at /data/results/checkpoint-200/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n\n\n\n\n\n\nEvaluation Results:\nF1 Score: 0.7872\nPrecision: 1.0000\nRecall: 0.6491\nAUC-ROC (minority class): 0.9097\nAverage Precision (minority class): 0.8044\n\n\n\n!pip install matplotlib\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nRequirement already satisfied: matplotlib in /home/user/miniconda/lib/python3.11/site-packages (3.10.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy&gt;=1.23 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (2.2.1)\nRequirement already satisfied: packaging&gt;=20.0 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (24.1)\nRequirement already satisfied: pillow&gt;=8 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/user/miniconda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six&gt;=1.5 in /home/user/miniconda/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\n\n\n\ndef analyze_thresholds(trainer, min_precision=0.9, min_threshold=0.5, fig_size=(15, 5)):\n    \"\"\"\n    Analyze model performance across different thresholds using the evaluation dataset.\n    Finds the lowest threshold that maintains the minimum precision requirement while\n    staying above a minimum threshold floor.\n\n    Args:\n        trainer: HuggingFace Trainer instance\n        min_precision: Minimum precision requirement (default: 0.9)\n        min_threshold: Minimum allowed threshold for binary classification (default: 0.5)\n        fig_size: Figure size for plots (default: (15, 5))\n\n    Returns:\n        dict: Dictionary containing optimal threshold metrics and probability statistics\n    \"\"\"\n    import numpy as np\n    from scipy.special import softmax\n    from sklearn.metrics import (\n        precision_score,\n        recall_score,\n        f1_score,\n        precision_recall_curve,\n    )\n    import matplotlib.pyplot as plt\n\n    def calculate_metrics_at_threshold(probs, true_labels, threshold):\n        \"\"\"Helper function to calculate metrics at a given threshold\"\"\"\n        preds = (probs &gt;= threshold).astype(int)\n        prec = precision_score(true_labels, preds, zero_division=0)\n        rec = recall_score(true_labels, preds, zero_division=0)\n        f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) &gt; 0 else 0\n        return prec, rec, f1\n\n    # Get predictions\n    predictions = trainer.predict(trainer.eval_dataset)\n    probs = softmax(predictions.predictions, axis=1)\n    minority_probs = probs[:, 1]  # Probabilities for positive class\n    true_labels = predictions.label_ids\n\n    # Calculate precision-recall curve\n    precisions, recalls, thresholds = precision_recall_curve(\n        true_labels, minority_probs\n    )\n\n    # Find optimal threshold meeting both minimum precision and threshold requirements\n    valid_indices = np.where(\n        (precisions[:-1] &gt;= min_precision) & (thresholds &gt;= min_threshold)\n    )[0]\n\n    if len(valid_indices) &gt; 0:\n        # Take lowest threshold that meets both criteria\n        optimal_idx = valid_indices[0]\n        optimal_threshold = thresholds[optimal_idx]\n        optimal_precision = precisions[optimal_idx]\n        optimal_recall = recalls[optimal_idx]\n    else:\n        # If no threshold meets both criteria, find best precision among valid thresholds\n        valid_thresholds_idx = np.where(thresholds &gt;= min_threshold)[0]\n        if len(valid_thresholds_idx) &gt; 0:\n            optimal_idx = valid_thresholds_idx[\n                np.argmax(precisions[valid_thresholds_idx])\n            ]\n            optimal_threshold = thresholds[optimal_idx]\n            optimal_precision = precisions[optimal_idx]\n            optimal_recall = recalls[optimal_idx]\n        else:\n            # Fallback to minimum threshold if no valid thresholds found\n            optimal_threshold = min_threshold\n            optimal_preds = (minority_probs &gt;= min_threshold).astype(int)\n            optimal_precision = precision_score(\n                true_labels, optimal_preds, zero_division=0\n            )\n            optimal_recall = recall_score(true_labels, optimal_preds, zero_division=0)\n\n    # Create plots\n    plt.figure(figsize=fig_size)\n\n    # Plot 1: Precision-Recall curve\n    plt.subplot(1, 2, 1)\n    plt.plot(recalls, precisions, label=\"Precision-Recall Curve\")\n    plt.scatter(\n        [optimal_recall],\n        [optimal_precision],\n        color=\"red\",\n        label=f\"Threshold={optimal_threshold:.2f}\\nPrecision={optimal_precision:.2f}\\nRecall={optimal_recall:.2f}\",\n    )\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(\"Precision-Recall Curve\")\n    plt.grid(True)\n    plt.legend()\n\n    # Plot 2: Metrics vs Threshold\n    max_prob = np.max(minority_probs)\n    min_prob = np.min(minority_probs)\n\n    # Create threshold range with denser sampling near optimal point\n    margin = 0.1\n    threshold_range = np.unique(\n        np.concatenate(\n            [\n                np.linspace(min_threshold, optimal_threshold - margin, 40),\n                np.linspace(optimal_threshold - margin, optimal_threshold + margin, 20),\n                np.linspace(optimal_threshold + margin, max_prob, 40),\n            ]\n        )\n    )\n    threshold_range = np.clip(threshold_range, min_threshold, max_prob)\n\n    # Calculate metrics for each threshold\n    metrics = [\n        calculate_metrics_at_threshold(minority_probs, true_labels, t)\n        for t in threshold_range\n    ]\n    precisions_plot, recalls_plot, f1_scores = zip(*metrics)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(threshold_range, precisions_plot, label=\"Precision\")\n    plt.plot(threshold_range, recalls_plot, label=\"Recall\")\n    plt.plot(threshold_range, f1_scores, label=\"F1\", linestyle=\"--\")\n    plt.axvline(\n        x=optimal_threshold,\n        color=\"red\",\n        linestyle=\"--\",\n        label=f\"Optimal Threshold={optimal_threshold:.2f}\",\n    )\n    plt.axvline(\n        x=min_threshold,\n        color=\"gray\",\n        linestyle=\":\",\n        label=f\"Min Threshold={min_threshold:.2f}\",\n    )\n    plt.xlabel(\"Threshold\")\n    plt.ylabel(\"Score\")\n    plt.title(\"Metrics vs Threshold\")\n    plt.grid(True)\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # Calculate final metrics and probability statistics\n    optimal_preds = (minority_probs &gt;= optimal_threshold).astype(int)\n    f1 = f1_score(true_labels, optimal_preds)\n    mean_prob = np.mean(minority_probs)\n\n    print(f\"\\nProbability Distribution:\")\n    print(f\"Min probability: {min_prob:.3f}\")\n    print(f\"Max probability: {max_prob:.3f}\")\n    print(f\"Mean probability: {mean_prob:.3f}\")\n\n    return {\n        \"optimal_threshold\": optimal_threshold,\n        \"optimal_precision\": optimal_precision,\n        \"optimal_recall\": optimal_recall,\n        \"optimal_f1\": f1,\n        \"min_prob\": min_prob,\n        \"max_prob\": max_prob,\n        \"mean_prob\": mean_prob,\n    }\n\n\n# Example usage\nresults = analyze_thresholds(\n    trainer,\n    min_precision=0.9,\n    min_threshold=0.5,  # Enforce minimum threshold of 0.5\n)\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution:\nMin probability: 0.000\nMax probability: 1.000\nMean probability: 0.091\n\n\n\n# Get label distribution\nfrom collections import Counter\n\nlabel_counts = Counter(trainer.eval_dataset[\"labels\"])\nprint(label_counts)  # Should show more 0s than 1s if 1 is minority class\n\nCounter({0: 343, 1: 57})"
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#using-the-model-to-filter-fineweb-2",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#using-the-model-to-filter-fineweb-2",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Using the model to filter FineWeb-2",
    "text": "Using the model to filter FineWeb-2\n\npaths = list_repo_files(\"HuggingFaceFW/fineweb-2\", repo_type=\"dataset\")\npaths[:10]\n\n['.gitattributes',\n 'README.md',\n 'data/aai_Latn/test/000_00000.parquet',\n 'data/aai_Latn/train/000_00000.parquet',\n 'data/aai_Latn_removed/train/000_00000.parquet',\n 'data/aak_Latn/test/000_00000.parquet',\n 'data/aak_Latn/train/000_00000.parquet',\n 'data/aak_Latn_removed/train/000_00000.parquet',\n 'data/aau_Latn/test/000_00000.parquet',\n 'data/aau_Latn/train/000_00000.parquet']\n\n\n\ndanish = [\n    f for f in paths if (\"dan\" in f and f.endswith(\"parquet\") and \"removed\" not in f)\n]\nswedish = [\n    f for f in paths if (\"swe\" in f and f.endswith(\"parquet\") and \"removed\" not in f)\n]\n\n\ndanish_lf = pl.scan_parquet(\n    [f\"hf://datasets/HuggingFaceFW/fineweb-2/{f}\" for f in danish]\n)\n\n\ndanish_df = danish_lf.head(10_000).collect()\ndanish_df\n\n\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"text-classification\",\n    model=\"davanstrien/scandi-fine-web-cleaner\",\n    truncation=True,  # Enable truncation\n    max_length=512,  # Set maximum length\n    batch_size=32,\n)\n\nDevice set to use cuda:0\n\n\n\ntexts = danish_df.select(\"text\").to_series().to_list()\n\n\npipe(texts[0])\n\n\nlen(texts)\n\nLet‚Äôs see how long it takes to predict on 10000 texts. While I used an A100 Hugging Face Jupyter Notebook Space for the model training, I‚Äôm using my 2021 MacBook Pro M1 for this part.\n\n%%time\npredictions = pipe(texts)\n\nCPU times: user 26.6 s, sys: 16.1 s, total: 42.7 s\nWall time: 4min 22s\n\n\n\npredictions[0]\n\n{'label': 'LABEL_0', 'score': 0.9997074007987976}\n\n\n\ndf_results = pl.DataFrame(predictions).rename(\n    {\n        \"label\": \"problematic_content_label_present\",\n        \"score\": \"problematic_content_label_present_score\",\n    }\n)\ndf_results\n\n\nshape: (10_000, 2)\n\n\n\nproblematic_content_label_present\nproblematic_content_label_present_score\n\n\nstr\nf64\n\n\n\n\n\"LABEL_0\"\n0.999707\n\n\n\"LABEL_0\"\n0.99975\n\n\n\"LABEL_0\"\n0.999737\n\n\n\"LABEL_0\"\n0.999724\n\n\n\"LABEL_0\"\n0.999745\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"LABEL_0\"\n0.999757\n\n\n\"LABEL_0\"\n0.999758\n\n\n\"LABEL_0\"\n0.999744\n\n\n\"LABEL_0\"\n0.991189\n\n\n\"LABEL_0\"\n0.999684\n\n\n\n\n\n\n\ndf_with_labels = pl.concat([danish_df, df_results], how=\"horizontal\")\ndf_with_labels.head(2)\n\n\nshape: (2, 13)\n\n\n\ntext\nid\ndump\nurl\ndate\nfile_path\nlanguage\nlanguage_score\nlanguage_script\nminhash_cluster_size\ntop_langs\nproblematic_content_label_present\nproblematic_content_label_present_score\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nf64\nstr\ni64\nstr\nstr\nf64\n\n\n\n\n\"Tema: Ankomster ‚ÄúHele tiden √•d‚Ä¶\n\"&lt;urn:uuid:0796b04c-c1bf-418b-b‚Ä¶\n\"CC-MAIN-2014-42\"\n\"http://www.copenhagen.dk/dk/de‚Ä¶\n\"2014-10-30T18:10:47Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"dan\"\n0.999933\n\"Latn\"\n26\n\"{\"dan_Latn_score\": 0.999932765‚Ä¶\n\"LABEL_0\"\n0.999707\n\n\n\"Hiddensees mangfoldige sk√∏nhed‚Ä¶\n\"&lt;urn:uuid:5f7751e9-981d-4cfe-9‚Ä¶\n\"CC-MAIN-2016-07\"\n\"http://www.germany.travel/dk/f‚Ä¶\n\"2016-02-07T03:49:50Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"dan\"\n0.999974\n\"Latn\"\n116\n\"{\"dan_Latn_score\": 0.999974370‚Ä¶\n\"LABEL_0\"\n0.99975\n\n\n\n\n\n\n\ndf_with_labels.select(\"problematic_content_label_present\").to_series().value_counts(\n    normalize=True\n)\n\n\nshape: (2, 2)\n\n\n\nproblematic_content_label_present\nproportion\n\n\nstr\nf64\n\n\n\n\n\"LABEL_1\"\n0.0698\n\n\n\"LABEL_0\"\n0.9302\n\n\n\n\n\n\nTaking a look at the problematic texts, even with my imperfect Danish I can see why these have been labelled as problematic.\n\nfrom rich import print as rprint\n\nrprint(\n    [\n        text[:1000]\n        for text in df_with_labels.filter(\n            pl.col(\"problematic_content_label_present\") == \"LABEL_1\"\n        )\n        .head(2)\n        .select(\"text\")\n        .to_series()\n        .to_list()\n    ]\n)\n\n[\n    'Layered haircuts ser altid elegant, ikke bare p√• lange h√•r, men ogs√• p√• kort h√•ret. These haircuts ser godt ud\np√• kvinder og p√• m√¶nd. Zac Efron og Keith Urban m√¶nds frisurer kan blive henvist til klippe h√•r i lag for m√¶nd. \nThese frisurer kan lette at vedligeholde og stylet p√• forskellige m√•der. Du kan ogs√• pr√∏ve forskellige h√•rfarve \nideer om lagdelte haircuts til at give et unikt look. Du kan nyde det med side feje pandeh√•r, eller en stump \nfrynser. Her er en guide til hvordan du klippe h√•ret i lag, som vil hj√¶lpe dig med at klippe h√•r i lag \nderhjemme.\\nHvordan man kan sk√¶re H√•r i Layers derhjemme\\nTing du beh√∏ver, Her er en liste over almindelige ting, \nsom du kan finde derhjemme selv:; Et godt saks, en kam, h√•r b√∏rste, f√∏nt√∏rrer, to spejle, A dyse vand flaske, for \nat opretholde fugtigt h√•r; mange h√•r skruetvinger, forberede dit h√•r; Vask dit h√•r rene med en shampoo, og \nh√•ndkl√¶de t√∏rre dem. M√• ikke helt t√∏r dem, holde dit h√•r fugtige, da det vil blive lettere at sk√¶re dem. Hvis du \nhar t√∏r',\n    'Homo bordel herrer body to body massage sj√¶llandFitness world forum √•bningstider liderlige k√¶llinger Posted by\nfitness world forum √•bningstider liderlige k√¶llinger on Fodmassage frederiksberg thai massage i hj√∏rring Posted by \nfodmassage frederiksberg thai massage i hj√∏rring on Sex Film √•lerne Jeg vil v√¶re diskret i Italien. Silkeborg \nkarina hot and sexy Massage og Escort: Islington, London Thai traditional massage. Asian escort copenhagen dansk \ngay porn Posted by asian escort copenhagen dansk gay porn on\\nJeg praktiserer den traditionelle thailandske massage\ni min klinik i Hj√∏rring. En s√•kaldt slikkelap er en ny form for pr√¶vention til kvinder, der beskytter mod \nk√∏nssygdomme ved oralsex. Vi boede p√• landet og havde en del fjerkr√¶, som vi slagtede, gjorde i stand og spiste \neller vi solgte dem til venner og bekendte. Thai-massage inkluderer ofte happy ending, og nogle steder er der \nmulighed for. Top thai massage vejle kiss porn - sammenlignede med Skriv en mail til ungtlahme gmail. Massag'\n]\n\n\n\nSince we have confidence scores, we can see how confident the model is in its predictions and potentially only use the predictions with a confidence score above a certain threshold.\n\ndf_with_labels.select(\"problematic_content_label_present_score\").describe()\n\n\nshape: (9, 2)\n\n\n\nstatistic\nproblematic_content_label_present_score\n\n\nstr\nf64\n\n\n\n\n\"count\"\n10000.0\n\n\n\"null_count\"\n0.0\n\n\n\"mean\"\n0.996547\n\n\n\"std\"\n0.027983\n\n\n\"min\"\n0.503509\n\n\n\"25%\"\n0.999564\n\n\n\"50%\"\n0.999691\n\n\n\"75%\"\n0.999733\n\n\n\"max\"\n0.999781\n\n\n\n\n\n\n\ndf_with_labels.filter(pl.col(\"problematic_content_label_present_score\") &lt; 0.9).shape\n\n(98, 13)\n\n\n\ndf_with_labels.filter(pl.col(\"problematic_content_label_present_score\") &lt; 0.8).shape\n\n(54, 13)"
  },
  {
    "objectID": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#appendix-running-on-the-full-fineweb-2-dataset-for-danish",
    "href": "posts/2025/FineWeb-c/scandinavian-content-filtering-fineweb.html#appendix-running-on-the-full-fineweb-2-dataset-for-danish",
    "title": "Hygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb",
    "section": "Appendix: Running on the full fineweb-2 dataset for Danish",
    "text": "Appendix: Running on the full fineweb-2 dataset for Danish\nBecause this is network bound I did this part on an A100 on HF which has a very fast connnection\n\ndanish_lf = pl.scan_parquet(\n    [f\"hf://datasets/HuggingFaceFW/fineweb-2/{f}\" for f in danish]\n)\n\n\ndanish_lf.head(1).collect()\n\n\nshape: (1, 11)\n\n\n\ntext\nid\ndump\nurl\ndate\nfile_path\nlanguage\nlanguage_score\nlanguage_script\nminhash_cluster_size\ntop_langs\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nf64\nstr\ni64\nstr\n\n\n\n\n\"Tema: Ankomster ‚ÄúHele tiden √•d‚Ä¶\n\"&lt;urn:uuid:0796b04c-c1bf-418b-b‚Ä¶\n\"CC-MAIN-2014-42\"\n\"http://www.copenhagen.dk/dk/de‚Ä¶\n\"2014-10-30T18:10:47Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"dan\"\n0.999933\n\"Latn\"\n26\n\"{\"dan_Latn_score\": 0.999932765‚Ä¶\n\n\n\n\n\n\n\n%%time\ndanish_lf.select(\"language_score\").describe()\n\nWe don‚Äôt need all of the column for doing inference so lets grab just the text and id.\n\ndanish_df_for_prediction = danish_lf.select([\"id\", \"text\"])\n\n\ndanish_df_for_prediction.sink_parquet(\"dan.parquet\")\n\n\ndf_pred = pl.scan_parquet(\"dan.parquet\")\n\n\ndf_pred.select(pl.len()).collect()\n\n\nshape: (1, 1)\n\n\n\nlen\n\n\nu32\n\n\n\n\n43002078\n\n\n\n\n\n\n\nfrom datasets import Dataset\n\n\nds = Dataset.from_parquet(\"dan.parquet\")\n\n\n\n\n\n\n\n\nds\n\nDataset({\n    features: ['id', 'text'],\n    num_rows: 43002078\n})\n\n\n\nsample = ds.shuffle().take(10_000)\n\n\nsample\n\nDataset({\n    features: ['id', 'text'],\n    num_rows: 10000\n})\n\n\n\nfrom tqdm.auto import tqdm\nfrom transformers.pipelines.pt_utils import KeyDataset\n\n\nresults = []\nfor out in tqdm(pipe(KeyDataset(sample, \"text\")), total=len(sample)):\n    results.append(out)\n\n\n\n\n\nresults[0]\n\n{'label': 'not_problematic', 'score': 0.9998955726623535}\n\n\n\nlabels = [x[\"label\"] for x in results]\nscores = [x[\"score\"] for x in results]\n\n\nlabels[:3], scores[:3]\n\n(['not_problematic', 'not_problematic', 'not_problematic'],\n [0.9998955726623535, 0.9998942613601685, 0.9998948574066162])\n\n\n\nsample = sample.add_column(\"problematic_label\", labels)\n\n\nsample = sample.add_column(\"problematic_label_score\", scores)\n\n\nsample[0][\"problematic_label\"]\n\n'not_problematic'\n\n\n\nclean_ds = sample.filter(lambda x: x[\"problematic_label\"] == \"not_problematic\")\n\n\n\n\n\nclean_ds.push_to_hub(\"davanstrien/fineweb2-danish-cleaned\")\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/datasets/davanstrien/fineweb2-danish-cleaned/commit/f0f86b883ee6ff82f91aed4d55ac1026e70dd473', commit_message='Upload dataset', commit_description='', oid='f0f86b883ee6ff82f91aed4d55ac1026e70dd473', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/davanstrien/fineweb2-danish-cleaned', endpoint='https://huggingface.co', repo_type='dataset', repo_id='davanstrien/fineweb2-danish-cleaned'), pr_revision=None, pr_num=None)"
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html",
    "href": "posts/2025/vllm/modern_inference_modernbert.html",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "tl;dr this blog post gives a quick overview of how to use vLLM to run inference for ModernBERT basesd classifier models.\n\nvLLM is a very well known and heavily used inference engine. It is known for its ability to scale inference for LLMs.\nvLLM has two major ways in which it can be used:\n\nOpenAI compatible inference API\nOffline batched inference\n\nWhile the OpenAI endpoint is useful, for the kinds of work I am using LLMs for I am often relying on the batched offline inference. Combined with structured generation, offline inference using vLLM can be a super useful way of generating synthetic data. See this post on using QWQ and Curator as an example of this kind of approach.\n\n\nRecently, vLLM added support for the ModernBERT arcitecture. This is great news for the community as it means that we can now use vLLM to scale inference for ModernBERT based classifier models.\n\n\n\n\n\n\nNote\n\n\n\nIf you haven‚Äôt come across ModernBERT you may want to checkout this post for a quick overview. The short verson is that ModernBERT is a modern architecture that is designed to be more efficient and effective than traditional BERT models while being useful for similar tasks to the original BERT models (including classification tasks).\n\n\nIn the rest of this post, I‚Äôll give a quick overview of how to use vLLM to scale inference for ModernBERT based classifier models.\n\n\n\nSince we are using a feature not yet in a main release, we need to install the nightly version of vLLM. Using uv makes this very easy. We‚Äôll also install the datasets and polars libraries as we‚Äôll be using them to load and process our data.\n\n%pip install uv\n!uv pip install -U vllm datasets --extra-index-url https://wheels.vllm.ai/nightly\n\nSince I orginally ran this on Google Colab, I‚Äôll also grab the HF token from the user data (we will need a token if we are working with a private dataset/model or if we want to push the results back to the Hub).\n\nfrom google.colab import userdata\n\nhf_token = userdata.get(\"HF_TOKEN\")\n\n\n\n\nSince it‚Äôs not super imporant for the post, I‚Äôll keep it very brief but the use case I am working on is using ModernBERT to classify arXiv papers as containting new datasets. You can read about how I created the dataset in this post. This results in davanstrien/ModernBERT-base-is-new-arxiv-dataset a model that classifies papers as containing a new dataset or not.\nAlthough we‚Äôll be using this model in this post, the same approach should work well with any ModernBERT based classifier model. This includes these models based on the base variant and these models based on the large variant.\n\n\n\nWe‚Äôll now load the model using vLLM. This is very similar to loading any other model in vllm, the only difference is that we specify task=\"classify\" to tell vLLM that we are using a classifier model.\n\nfrom vllm import LLM\n\nllm = LLM(model=\"davanstrien/ModernBERT-base-is-new-arxiv-dataset\", task=\"classify\")\n\nINFO 04-24 14:16:50 [__init__.py:239] Automatically detected platform cuda.\n\n\n\n\n\nINFO 04-24 14:16:59 [config.py:2888] Downcasting torch.float32 to torch.float16.\nWARNING 04-24 14:17:13 [arg_utils.py:1685] --task classify is not supported by the V1 Engine. Falling back to V0. \nINFO 04-24 14:17:13 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.5.dev195+gc0dfd9751) with config: model='davanstrien/ModernBERT-base-is-new-arxiv-dataset', speculative_config=None, tokenizer='davanstrien/ModernBERT-base-is-new-arxiv-dataset', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=davanstrien/ModernBERT-base-is-new-arxiv-dataset, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n\n\n\n\n\n\n\n\n\n\n\nINFO 04-24 14:17:20 [cuda.py:295] Using Flash Attention backend.\nINFO 04-24 14:17:20 [parallel_state.py:946] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-24 14:17:20 [model_runner.py:1120] Starting to load model davanstrien/ModernBERT-base-is-new-arxiv-dataset...\nINFO 04-24 14:17:21 [weight_utils.py:265] Using model weights format ['*.safetensors']\n\n\n\n\n\nINFO 04-24 14:17:25 [weight_utils.py:281] Time spent downloading weights for davanstrien/ModernBERT-base-is-new-arxiv-dataset: 3.888106 seconds\nINFO 04-24 14:17:25 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n\n\n\n\n\nINFO 04-24 14:17:26 [loader.py:458] Loading weights took 0.18 seconds\nINFO 04-24 14:17:26 [model_runner.py:1156] Model loading took 0.3098 GiB and 5.243792 seconds\n\n\nLet‚Äôs test the model out with a simple example.\n\n(output,) = llm.classify(\n    \"Title: This paper introduces a new benchmark dataset shared on Zenodo\"\n)\n\nprobs = output.outputs.probs\nprint(f\"Class Probabilities: {probs!r} (size={len(probs)})\")\n\n\n\n\nClass Probabilities: [1.6357421875, -1.3671875] (size=2)\n\n\nWe can also get the top predicted class and confidence score.\n\nimport torch\nimport torch.nn.functional as F\n\nlogits = torch.tensor(output.outputs.probs)\nprobabilities = F.softmax(logits, dim=0)\n\n# Get the top predicted class\ntop_idx = torch.argmax(probabilities).item()\ntop_prob = probabilities[top_idx].item()\n\nprint(f\"Top class index: {top_idx}\")\nprint(f\"Confidence: {top_prob:.4f}\")\n\nTop class index: 0\nConfidence: 0.9527\n\n\n\n\nThe outputs of vLLM don‚Äôt contain the labels but we can easily grab them from the model configuration\n\nimport httpx\nfrom huggingface_hub import hf_hub_url\n\nid2label = (\n    httpx.get(\n        hf_hub_url(\n            \"davanstrien/ModernBERT-base-is-new-arxiv-dataset\", filename=\"config.json\"\n        )\n    )\n    .json()\n    .get(\"id2label\")\n)\nid2label\n\n{'0': 'new_dataset', '1': 'no_new_dataset'}\n\n\n\nfrom toolz import keymap\n\nid2label = keymap(int, id2label)\n\n\nid2label.get(top_idx)\n\n'new_dataset'\n\n\n\n\n\n\nThere are many uses cases of classfiers but one common use case is to use a classifier to clean and curate a dataset. I.e. we want to remove datapoints that are not of a certain class or assign labels to a large corpus to make it easier to understand or organize the data. A very nice example of this kind of classifier based curation can be found in this paper.\nIn this case, we‚Äôll use an existing dataset of arxiv papers and use our classifier to label the papers as containing a new dataset or not.\nWe‚Äôll start by loading a smallish dataset which was used as the training set for the model.\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"davanstrien/arxiv-new-datasets\", split=\"train\")\n\n\n\n\n\n\n\n\n\n\n\nds.features\n\n{'id': Value(dtype='string', id=None),\n 'title': Value(dtype='string', id=None),\n 'abstract': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['new_dataset', 'no_new_dataset'], id=None),\n 'explanations': Value(dtype='string', id=None)}\n\n\nExample row\n\nds[6]\n\n{'id': '2405.00026',\n 'title': 'Enhancing Credit Card Fraud Detection A Neural Network and SMOTE\\n  Integrated Approach',\n 'abstract': '  Credit card fraud detection is a critical challenge in the financial sector,\\ndemanding sophisticated approaches to accurately identify fraudulent\\ntransactions. This research proposes an innovative methodology combining Neural\\nNetworks (NN) and Synthet ic Minority Over-sampling Technique (SMOTE) to\\nenhance the detection performance. The study addresses the inherent imbalance\\nin credit card transaction data, focusing on technical advancements for robust\\nand precise fraud detection. Results demonstrat e that the integration of NN\\nand SMOTE exhibits superior precision, recall, and F1-score compared to\\ntraditional models, highlighting its potential as an advanced solution for\\nhandling imbalanced datasets in credit card fraud detection scenarios. This\\nrese arch contributes to the ongoing efforts to develop effective and efficient\\nmechanisms for safeguarding financial transactions from fraudulent activities.\\n',\n 'labels': 1,\n 'explanations': 'The paper focuses on enhancing credit card fraud detection using a combination of neural networks and SMOTE. It evaluates its approach on existing datasets without introducing a new dataset. The study addresses imbalanced data in credit transactions but does not mention the creation of a new dataset.'}\n\n\n\n\nWhen training this model it was given both the title and abstract of the paper. We‚Äôll need to format the data in the same way when making predictions. This won‚Äôt be necessary for many classifier models.\n\ndef format_row_for_model(row):\n    return f\"TITLE: {row['title']} \\n\\nABSTRACT: {row['abstract']}\"\n\n\noutputs = llm.classify(format_row_for_model(ds[6]))\n\n\n\n\n\n\n\n\nNow we‚Äôll load the full dataset and format the data for the model. This is a dataset of arxiv papers (title, abstracts and other metadata) and we‚Äôll filter it down to only include papers that are in the computer science category.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"librarian-bots/arxiv-metadata-snapshot\", split=\"train\")\ndataset[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'id': '1305.5211',\n 'submitter': 'Hou-Biao Li',\n 'authors': 'Qian-Ping Guo, Xian-Ming Gu and Hou-biao Li',\n 'title': 'A note on the growth factor in Gaussian elimination for Higham matrices',\n 'comments': '13 pages, 1 figures;',\n 'journal-ref': None,\n 'doi': None,\n 'report-no': None,\n 'categories': 'math.NA cs.NA',\n 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n 'abstract': \"  The Higham matrix is a complex symmetric matrix A=B+iC, where both B and C\\nare real, symmetric and positive definite and $\\\\mathrm{i}=\\\\sqrt{-1}$ is the\\nimaginary unit. For any Higham matrix A, Ikramov et al. showed that the growth\\nfactor in Gaussian elimination is less than 3. In this paper, based on the\\nprevious results, a new bound of the growth factor is obtained by using the\\nmaximum of the condition numbers of matrixes B and C for the generalized Higham\\nmatrix A, which strengthens this bound to 2 and proves the Higham's conjecture.\\n\",\n 'versions': [{'version': 'v1', 'created': 'Wed, 22 May 2013 18:00:17 GMT'},\n  {'version': 'v2', 'created': 'Fri, 18 Apr 2025 14:14:30 GMT'}],\n 'update_date': datetime.datetime(2025, 4, 21, 0, 0),\n 'authors_parsed': [['Guo', 'Qian-Ping', ''],\n  ['Gu', 'Xian-Ming', ''],\n  ['Li', 'Hou-biao', '']]}\n\n\n\ndef contains_cs(row):\n    return \"cs\" in row[\"categories\"]\n\n\ndataset = dataset.filter(contains_cs, num_proc=4)\n\n\n\n\n\n\n\nI‚Äôm a big fan of Polars so in practice I would probably do the same steps in Polars.\n\n\nCode\nfrom pathlib import Path\n\nimport polars as pl\nfrom datasets import Dataset\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(\n    \"librarian-bots/arxiv-metadata-snapshot\",\n    local_dir=\"arxiv-metadata-snapshot\",\n    allow_patterns=[\"*.parquet\"],\n    repo_type=\"dataset\",\n)\nparquet_files = list(Path(\"arxiv-metadata-snapshot\").rglob(\"*.parquet\"))\ndf = pl.scan_parquet(parquet_files)\ndf = df.with_columns(\n    pl.concat_str(\n        [\n            pl.lit(\"TITLE: \"),\n            pl.col(\"title\"),\n            pl.lit(\" \\n\\nABSTRACT: \"),\n            pl.col(\"abstract\"),\n        ]\n    ).alias(\"prompt\")\n)\ndf_cs = df.filter(\n    pl.col(\"categories\")\n    .str.split(\" \")\n    .list.eval(pl.element().str.starts_with(\"cs.\"))\n    .list.any()\n)\ndf_cs_datasets = df_cs.filter(\n    pl.col(\"title\").str.contains(\"dataset\")\n    | pl.col(\"title\").str.contains(\"benchmark\")\n    | pl.col(\"abstract\").str.contains(\"dataset\")\n    | pl.col(\"abstract\").str.contains(\"benchmark\")\n)\n\ndf_cs_datasets = df_cs_datasets.collect()\n\nds = Dataset.from_polars(df_cs_datasets)\nds\n\n\nWe now end up with a dataset with a new column prompt which contains the correctly formatted input for the model.\n\nds[0][\"prompt\"]\n\n'TITLE: Good practices for a literature survey are not followed by authors while\\n  preparing scientific manuscripts \\n\\nABSTRACT:   The number of citations received by authors in scientific journals has become\\na major parameter to assess individual researchers and the journals themselves\\nthrough the impact factor. A fair assessment therefore requires that the\\ncriteria for selecting references in a given manuscript should be unbiased with\\nrespect to the authors or the journals cited. In this paper, we advocate that\\nauthors should follow two mandatory principles to select papers (later\\nreflected in the list of references) while studying the literature for a given\\nresearch: i) consider similarity of content with the topics investigated, lest\\nvery related work should be reproduced or ignored; ii) perform a systematic\\nsearch over the network of citations including seminal or very related papers.\\nWe use formalisms of complex networks for two datasets of papers from the arXiv\\nrepository to show that neither of these two criteria is fulfilled in practice.\\n'\n\n\nWe can grab all of these prompts as a list\n\nprompts = ds[\"prompt\"]\n\nLet‚Äôs see how many rows we have to classify\n\nlen(prompts)\n\n211190\n\n\n\n\nWhile in theory you should be able to pass in the entire list to the model and have it take care of batching, I ran into some issues with very big inputs so I‚Äôll use a simple batching approach. If you are working with smaller datasets you can probably skip this step.\n\nfrom toolz import partition_all\nfrom tqdm.auto import tqdm\n\n\nall_results = []\n\n\n%%time\nfor batch in tqdm(list(partition_all(50_000, prompts))):\n    all_results.append(llm.classify(batch))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 16min 22s, sys: 4.04 s, total: 16min 26s\nWall time: 16min 18s\n\n\n\n\n\n\n~16 minutes to process 211,190 isn‚Äôt bad! Besides this, there is a minimal amount of boilerplate code to maintain. For many datasets, you could just grab the relevant column and pass it directly to vLLM as a list. Even if you could shave of a bit of time doing this in another way it‚Äôs very nice to just pass the prompts and let vLLM handle the rest.\n\n\n\nWe can now join all of the results into a single list\n\nfrom toolz import concat\n\noutputs = list(concat(all_results))\n\nAt the moment the raw outputs don‚Äôt contains the labels we want so we‚Äôll create a small function that maps the outputs to the string label and returns the confidence in the prediction.\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef get_top_label(output, label_map: dict[str, str]):\n    \"\"\"\n    Given a ClassificationRequestOutput and a label_map (e.g. {'0': 'label0', ...}),\n    returns the top predicted label (or None if not found) and its confidence score.\n    \"\"\"\n    logits = torch.tensor(output.outputs.probs)\n    probs = F.softmax(logits, dim=0)\n    top_idx = torch.argmax(probs).item()\n    top_prob = probs[top_idx].item()\n    label = label_map.get(top_idx)  # Returns None if key is missing\n    return label, top_prob\n\n\nget_top_label(outputs[0], id2label)\n\n('no_new_dataset', 0.9503649473190308)\n\n\nLet‚Äôs create a new list with these results\n\nclean_results = [get_top_label(output, id2label) for output in outputs]\n\nWe can quickly count the label frequencies\n\nfrom collections import Counter\n\nCounter(result[0] for result in clean_results)\n\nCounter({'no_new_dataset': 182083, 'new_dataset': 29107})\n\n\n\n\nWe can now add the labels back to the dataset\n\nds = ds.add_column(\"is_new_dataset_label\", [result[0] for result in clean_results])\nds = ds.add_column(\"label_confidence\", [result[1] for result in clean_results])\n\n\nds[0]\n\n{'id': '1005.3063',\n 'submitter': 'Diego  Amancio Raphael',\n 'authors': 'D.R. Amancio, M. G. V. Nunes, O. N. Oliveira Jr., L. da F. Costa',\n 'title': 'Good practices for a literature survey are not followed by authors while\\n  preparing scientific manuscripts',\n 'comments': None,\n 'journal-ref': 'Scientometrics, v. 90, p. 2, (2012)',\n 'doi': '10.1007/s11192-012-0630-z',\n 'report-no': None,\n 'categories': 'physics.soc-ph cs.DL',\n 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n 'abstract': '  The number of citations received by authors in scientific journals has become\\na major parameter to assess individual researchers and the journals themselves\\nthrough the impact factor. A fair assessment therefore requires that the\\ncriteria for selecting references in a given manuscript should be unbiased with\\nrespect to the authors or the journals cited. In this paper, we advocate that\\nauthors should follow two mandatory principles to select papers (later\\nreflected in the list of references) while studying the literature for a given\\nresearch: i) consider similarity of content with the topics investigated, lest\\nvery related work should be reproduced or ignored; ii) perform a systematic\\nsearch over the network of citations including seminal or very related papers.\\nWe use formalisms of complex networks for two datasets of papers from the arXiv\\nrepository to show that neither of these two criteria is fulfilled in practice.\\n',\n 'versions': [{'version': 'v1', 'created': 'Mon, 17 May 2010 21:45:47 GMT'},\n  {'version': 'v2', 'created': 'Sun, 23 Sep 2012 00:49:13 GMT'}],\n 'update_date': datetime.datetime(2012, 9, 25, 0, 0),\n 'authors_parsed': [['Amancio', 'D. R.', ''],\n  ['Nunes', 'M. G. V.', ''],\n  ['Oliveira', 'O. N.', 'Jr.'],\n  ['Costa', 'L. da F.', '']],\n 'prompt': 'TITLE: Good practices for a literature survey are not followed by authors while\\n  preparing scientific manuscripts \\n\\nABSTRACT:   The number of citations received by authors in scientific journals has become\\na major parameter to assess individual researchers and the journals themselves\\nthrough the impact factor. A fair assessment therefore requires that the\\ncriteria for selecting references in a given manuscript should be unbiased with\\nrespect to the authors or the journals cited. In this paper, we advocate that\\nauthors should follow two mandatory principles to select papers (later\\nreflected in the list of references) while studying the literature for a given\\nresearch: i) consider similarity of content with the topics investigated, lest\\nvery related work should be reproduced or ignored; ii) perform a systematic\\nsearch over the network of citations including seminal or very related papers.\\nWe use formalisms of complex networks for two datasets of papers from the arXiv\\nrepository to show that neither of these two criteria is fulfilled in practice.\\n',\n 'is_new_dataset_label': 'no_new_dataset',\n 'label_confidence': 0.9503649473190308}\n\n\n\n\n\n\nNow we have a new column with the labels, and a second column with the confidence in the label. We can now filter the dataset as needed. You could of course combine these filters in many ways depending on your use case.\n\ndf_with_labels = ds.to_polars()\n\n\ndf_with_labels.head(2)\n\n\nshape: (2, 17)\n\n\n\nid\nsubmitter\nauthors\ntitle\ncomments\njournal-ref\ndoi\nreport-no\ncategories\nlicense\nabstract\nversions\nupdate_date\nauthors_parsed\nprompt\nis_new_dataset_label\nlabel_confidence\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nlist[struct[2]]\ndatetime[ms]\nlist[list[str]]\nstr\nstr\nf64\n\n\n\n\n\"1005.3063\"\n\"Diego¬†¬†Amancio Raphael\"\n\"D.R. Amancio, M. G. V. Nunes, ‚Ä¶\n\"Good practices for a literatur‚Ä¶\nnull\n\"Scientometrics, v. 90, p. 2, (‚Ä¶\n\"10.1007/s11192-012-0630-z\"\nnull\n\"physics.soc-ph cs.DL\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†The number of citations rece‚Ä¶\n[{\"v1\",\"Mon, 17 May 2010 21:45:47 GMT\"}, {\"v2\",\"Sun, 23 Sep 2012 00:49:13 GMT\"}]\n2012-09-25 00:00:00\n[[\"Amancio\", \"D. R.\", \"\"], [\"Nunes\", \"M. G. V.\", \"\"], ‚Ä¶ [\"Costa\", \"L. da F.\", \"\"]]\n\"TITLE: Good practices for a li‚Ä¶\n\"no_new_dataset\"\n0.950365\n\n\n\"1209.0127\"\n\"Alexandra Faynburd Mrs\"\n\"Ran El-Yaniv, Alexandra Faynbu‚Ä¶\n\"Autoregressive short-term pred‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.LG cs.CE cs.NE\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†This work is concerned with ‚Ä¶\n[{\"v1\",\"Sat, 1 Sep 2012 19:53:23 GMT\"}, {\"v2\",\"Mon, 24 Sep 2012 19:28:24 GMT\"}]\n2012-09-25 00:00:00\n[[\"El-Yaniv\", \"Ran\", \"\"], [\"Faynburd\", \"Alexandra\", \"\"]]\n\"TITLE: Autoregressive short-te‚Ä¶\n\"no_new_dataset\"\n0.950088\n\n\n\n\n\n\n\ndf_with_labels = df_with_labels.lazy()\n\n\ndf_with_labels.filter(pl.col(\"is_new_dataset_label\") == \"new_dataset\").collect().shape\n\n(29107, 17)\n\n\n\ndf_predicted_as_new_dataset = df_with_labels.filter(\n    pl.col(\"is_new_dataset_label\") == \"new_dataset\"\n)\n\nrun LazyFrame.show_graph() to see the optimized version\n\n\n\n\n\npolars_query\n\n\n\np1\n\nFILTER BY [(col(\"is_new_dataset_label\")) == (String(new_dataset))]\n\n\n\np2\n\nTABLE\nœÄ */17\n\n\n\np1--p2\n\n\n\n\n\n\n\ndf_predicted_as_new_dataset_high_confidence = df_predicted_as_new_dataset.filter(\n    pl.col(\"label_confidence\").ge(0.95)\n)\n\n\ndf_predicted_as_new_dataset_high_confidence.show_graph()\n\n\n\n\n\n\n\n\n\ndf_predicted_as_new_dataset_high_confidence.collect()\n\n\nshape: (23_018, 17)\n\n\n\nid\nsubmitter\nauthors\ntitle\ncomments\njournal-ref\ndoi\nreport-no\ncategories\nlicense\nabstract\nversions\nupdate_date\nauthors_parsed\nprompt\nis_new_dataset_label\nlabel_confidence\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nlist[struct[2]]\ndatetime[ms]\nlist[list[str]]\nstr\nstr\nf64\n\n\n\n\n\"1209.0835\"\n\"Neil Zhenqiang Gong\"\n\"Neil Zhenqiang Gong, Wenchang ‚Ä¶\n\"Evolution of Social-Attribute ‚Ä¶\n\"14 pages, 19 figures. will app‚Ä¶\nnull\nnull\nnull\n\"cs.SI cs.CY physics.soc-ph\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Understanding social network‚Ä¶\n[{\"v1\",\"Wed, 5 Sep 2012 01:01:47 GMT\"}, {\"v2\",\"Sat, 8 Sep 2012 04:12:28 GMT\"}, ‚Ä¶ {\"v4\",\"Wed, 19 Sep 2012 02:24:50 GMT\"}]\n2012-09-20 00:00:00\n[[\"Gong\", \"Neil Zhenqiang\", \"\"], [\"Xu\", \"Wenchang\", \"\"], ‚Ä¶ [\"Song\", \"Dawn\", \"\"]]\n\"TITLE: Evolution of Social-Att‚Ä¶\n\"new_dataset\"\n0.968679\n\n\n\"1207.0784\"\n\"Romain Giot\"\n\"Romain Giot (GREYC), Mohamad E‚Ä¶\n\"Web-Based Benchmark for Keystr‚Ä¶\n\"The Eighth International Confe‚Ä¶\nnull\nnull\nnull\n\"cs.LG\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Most keystroke dynamics stud‚Ä¶\n[{\"v1\",\"Tue, 3 Jul 2012 19:12:56 GMT\"}]\n2012-07-04 00:00:00\n[[\"Giot\", \"Romain\", ‚Ä¶ \"GREYC\"], [\"El-Abed\", \"Mohamad\", ‚Ä¶ \"GREYC\"], [\"Rosenberger\", \"Christophe\", ‚Ä¶ \"GREYC\"]]\n\"TITLE: Web-Based Benchmark for‚Ä¶\n\"new_dataset\"\n0.966945\n\n\n\"1205.2345\"\n\"Salah A. Aly\"\n\"Hossam Zawbaa and Salah A. Aly\"\n\"Hajj and Umrah Event Recogniti‚Ä¶\n\"4 pages, 18 figures with 33 im‚Ä¶\nnull\nnull\nnull\n\"cs.CV cs.CY\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†In this note, new Hajj and U‚Ä¶\n[{\"v1\",\"Thu, 10 May 2012 19:10:18 GMT\"}]\n2012-05-11 00:00:00\n[[\"Zawbaa\", \"Hossam\", \"\"], [\"Aly\", \"Salah A.\", \"\"]]\n\"TITLE: Hajj and Umrah Event Re‚Ä¶\n\"new_dataset\"\n0.973292\n\n\n\"1110.2096\"\n\"Philipp Herrmann\"\n\"Philipp N. Herrmann, Dennis O.‚Ä¶\n\"Beating Irrationality: Does De‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.HC cs.CY cs.SI\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†In this research, we investi‚Ä¶\n[{\"v1\",\"Mon, 10 Oct 2011 16:23:18 GMT\"}, {\"v2\",\"Tue, 3 Apr 2012 15:34:53 GMT\"}]\n2012-04-04 00:00:00\n[[\"Herrmann\", \"Philipp N.\", \"\"], [\"Kundisch\", \"Dennis O.\", \"\"], [\"Rahman\", \"Mohammad S.\", \"\"]]\n\"TITLE: Beating Irrationality: ‚Ä¶\n\"new_dataset\"\n0.960584\n\n\n\"1201.6569\"\n\"Robert Fink\"\n\"Robert Fink, Larisa Han, Dan O‚Ä¶\n\"Aggregation in Probabilistic D‚Ä¶\n\"VLDB2012\"\n\"Proceedings of the VLDB Endowm‚Ä¶\nnull\nnull\n\"cs.DB\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†This paper presents a query ‚Ä¶\n[{\"v1\",\"Tue, 31 Jan 2012 15:10:34 GMT\"}]\n2012-02-01 00:00:00\n[[\"Fink\", \"Robert\", \"\"], [\"Han\", \"Larisa\", \"\"], [\"Olteanu\", \"Dan\", \"\"]]\n\"TITLE: Aggregation in Probabil‚Ä¶\n\"new_dataset\"\n0.955361\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"1910.10470\"\n\"Hans Pinckaers\"\n\"Hans Pinckaers, Geert Litjens\"\n\"Neural Ordinary Differential E‚Ä¶\n\"Accepted to 'Medical Imaging m‚Ä¶\nnull\nnull\nnull\n\"eess.IV cs.CV\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Automated medical image segm‚Ä¶\n[{\"v1\",\"Wed, 23 Oct 2019 11:35:19 GMT\"}]\n2019-10-24 00:00:00\n[[\"Pinckaers\", \"Hans\", \"\"], [\"Litjens\", \"Geert\", \"\"]]\n\"TITLE: Neural Ordinary Differe‚Ä¶\n\"new_dataset\"\n0.960731\n\n\n\"1910.10502\"\n\"Maaike De Boer\"\n\"Hella Haanstra and Maaike H. T‚Ä¶\n\"Opinion aspect extraction in D‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.CL cs.LG\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Aspect extraction can be use‚Ä¶\n[{\"v1\",\"Mon, 21 Oct 2019 09:33:09 GMT\"}]\n2019-10-24 00:00:00\n[[\"Haanstra\", \"Hella\", \"\"], [\"de Boer\", \"Maaike H. T.\", \"\"]]\n\"TITLE: Opinion aspect extracti‚Ä¶\n\"new_dataset\"\n0.967287\n\n\n\"1908.01671\"\n\"Alice Baird\"\n\"Alice Baird and Bjoern Schulle‚Ä¶\n\"Acoustic Sounds for Wellbeing:‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.SD eess.AS\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†The field of sound healing i‚Ä¶\n[{\"v1\",\"Mon, 5 Aug 2019 15:01:17 GMT\"}, {\"v2\",\"Mon, 21 Oct 2019 20:50:51 GMT\"}]\n2019-10-23 00:00:00\n[[\"Baird\", \"Alice\", \"\"], [\"Schuller\", \"Bjoern\", \"\"]]\n\"TITLE: Acoustic Sounds for Wel‚Ä¶\n\"new_dataset\"\n0.967101\n\n\n\"cs/0211018\"\n\"Vladimir Pestov\"\n\"Vladimir Pestov and Aleksandar‚Ä¶\n\"Indexing schemes for similarit‚Ä¶\n\"19 pages, LaTeX with 8 figures‚Ä¶\n\"Fundamenta Informaticae Vol. 7‚Ä¶\nnull\nnull\n\"cs.DS\"\nnull\n\"¬†¬†We suggest a variation of th‚Ä¶\n[{\"v1\",\"Thu, 14 Nov 2002 19:10:16 GMT\"}, {\"v2\",\"Thu, 13 Oct 2005 21:06:17 GMT\"}]\n2009-09-29 00:00:00\n[[\"Pestov\", \"Vladimir\", \"\"], [\"Stojmirovic\", \"Aleksandar\", \"\"]]\n\"TITLE: Indexing schemes for si‚Ä¶\n\"new_dataset\"\n0.964954\n\n\n\"cs/0604015\"\n\"Dmitri Krioukov\"\n\"Xenofontas Dimitropoulos, Dmit‚Ä¶\n\"Revealing the Autonomous Syste‚Ä¶\nnull\n\"PAM 2006, best paper award\"\nnull\nnull\n\"cs.NI cs.LG\"\nnull\n\"¬†¬†Although the Internet AS-lev‚Ä¶\n[{\"v1\",\"Thu, 6 Apr 2006 00:08:24 GMT\"}]\n2007-05-23 00:00:00\n[[\"Dimitropoulos\", \"Xenofontas\", \"\"], [\"Krioukov\", \"Dmitri\", \"\"], ‚Ä¶ [\"claffy\", \"kc\", \"\"]]\n\"TITLE: Revealing the Autonomou‚Ä¶\n\"new_dataset\"\n0.957991\n\n\n\n\n\n\n\n\n\nIn this post we‚Äôve seen how to use vLLM to scale inference for ModernBERT based classifier models. I‚Äôm personally still very excited about the potential of smaller classifier models for a wide range of tasks and it‚Äôs really cool to see a very well known and used inference engine like vLLM extending it‚Äôs capabilities to include this kind of model. I‚Äôd be excited to see more models like this being added to vLLM and also to see the comunity shipping cool new classifiers for curating/cleaning/organizing large datasets."
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#installation",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#installation",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "Since we are using a feature not yet in a main release, we need to install the nightly version of vLLM. Using uv makes this very easy. We‚Äôll also install the datasets and polars libraries as we‚Äôll be using them to load and process our data.\n\n%pip install uv\n!uv pip install -U vllm datasets --extra-index-url https://wheels.vllm.ai/nightly\n\nSince I orginally ran this on Google Colab, I‚Äôll also grab the HF token from the user data (we will need a token if we are working with a private dataset/model or if we want to push the results back to the Hub).\n\nfrom google.colab import userdata\n\nhf_token = userdata.get(\"HF_TOKEN\")"
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#the-use-case",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#the-use-case",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "Since it‚Äôs not super imporant for the post, I‚Äôll keep it very brief but the use case I am working on is using ModernBERT to classify arXiv papers as containting new datasets. You can read about how I created the dataset in this post. This results in davanstrien/ModernBERT-base-is-new-arxiv-dataset a model that classifies papers as containing a new dataset or not.\nAlthough we‚Äôll be using this model in this post, the same approach should work well with any ModernBERT based classifier model. This includes these models based on the base variant and these models based on the large variant."
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#loading-the-model",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#loading-the-model",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "We‚Äôll now load the model using vLLM. This is very similar to loading any other model in vllm, the only difference is that we specify task=\"classify\" to tell vLLM that we are using a classifier model.\n\nfrom vllm import LLM\n\nllm = LLM(model=\"davanstrien/ModernBERT-base-is-new-arxiv-dataset\", task=\"classify\")\n\nINFO 04-24 14:16:50 [__init__.py:239] Automatically detected platform cuda.\n\n\n\n\n\nINFO 04-24 14:16:59 [config.py:2888] Downcasting torch.float32 to torch.float16.\nWARNING 04-24 14:17:13 [arg_utils.py:1685] --task classify is not supported by the V1 Engine. Falling back to V0. \nINFO 04-24 14:17:13 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.5.dev195+gc0dfd9751) with config: model='davanstrien/ModernBERT-base-is-new-arxiv-dataset', speculative_config=None, tokenizer='davanstrien/ModernBERT-base-is-new-arxiv-dataset', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=davanstrien/ModernBERT-base-is-new-arxiv-dataset, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n\n\n\n\n\n\n\n\n\n\n\nINFO 04-24 14:17:20 [cuda.py:295] Using Flash Attention backend.\nINFO 04-24 14:17:20 [parallel_state.py:946] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-24 14:17:20 [model_runner.py:1120] Starting to load model davanstrien/ModernBERT-base-is-new-arxiv-dataset...\nINFO 04-24 14:17:21 [weight_utils.py:265] Using model weights format ['*.safetensors']\n\n\n\n\n\nINFO 04-24 14:17:25 [weight_utils.py:281] Time spent downloading weights for davanstrien/ModernBERT-base-is-new-arxiv-dataset: 3.888106 seconds\nINFO 04-24 14:17:25 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n\n\n\n\n\nINFO 04-24 14:17:26 [loader.py:458] Loading weights took 0.18 seconds\nINFO 04-24 14:17:26 [model_runner.py:1156] Model loading took 0.3098 GiB and 5.243792 seconds\n\n\nLet‚Äôs test the model out with a simple example.\n\n(output,) = llm.classify(\n    \"Title: This paper introduces a new benchmark dataset shared on Zenodo\"\n)\n\nprobs = output.outputs.probs\nprint(f\"Class Probabilities: {probs!r} (size={len(probs)})\")\n\n\n\n\nClass Probabilities: [1.6357421875, -1.3671875] (size=2)\n\n\nWe can also get the top predicted class and confidence score.\n\nimport torch\nimport torch.nn.functional as F\n\nlogits = torch.tensor(output.outputs.probs)\nprobabilities = F.softmax(logits, dim=0)\n\n# Get the top predicted class\ntop_idx = torch.argmax(probabilities).item()\ntop_prob = probabilities[top_idx].item()\n\nprint(f\"Top class index: {top_idx}\")\nprint(f\"Confidence: {top_prob:.4f}\")\n\nTop class index: 0\nConfidence: 0.9527\n\n\n\n\nThe outputs of vLLM don‚Äôt contain the labels but we can easily grab them from the model configuration\n\nimport httpx\nfrom huggingface_hub import hf_hub_url\n\nid2label = (\n    httpx.get(\n        hf_hub_url(\n            \"davanstrien/ModernBERT-base-is-new-arxiv-dataset\", filename=\"config.json\"\n        )\n    )\n    .json()\n    .get(\"id2label\")\n)\nid2label\n\n{'0': 'new_dataset', '1': 'no_new_dataset'}\n\n\n\nfrom toolz import keymap\n\nid2label = keymap(int, id2label)\n\n\nid2label.get(top_idx)\n\n'new_dataset'"
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#inference-for-a-large-dataset",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#inference-for-a-large-dataset",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "There are many uses cases of classfiers but one common use case is to use a classifier to clean and curate a dataset. I.e. we want to remove datapoints that are not of a certain class or assign labels to a large corpus to make it easier to understand or organize the data. A very nice example of this kind of classifier based curation can be found in this paper.\nIn this case, we‚Äôll use an existing dataset of arxiv papers and use our classifier to label the papers as containing a new dataset or not.\nWe‚Äôll start by loading a smallish dataset which was used as the training set for the model.\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"davanstrien/arxiv-new-datasets\", split=\"train\")\n\n\n\n\n\n\n\n\n\n\n\nds.features\n\n{'id': Value(dtype='string', id=None),\n 'title': Value(dtype='string', id=None),\n 'abstract': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['new_dataset', 'no_new_dataset'], id=None),\n 'explanations': Value(dtype='string', id=None)}\n\n\nExample row\n\nds[6]\n\n{'id': '2405.00026',\n 'title': 'Enhancing Credit Card Fraud Detection A Neural Network and SMOTE\\n  Integrated Approach',\n 'abstract': '  Credit card fraud detection is a critical challenge in the financial sector,\\ndemanding sophisticated approaches to accurately identify fraudulent\\ntransactions. This research proposes an innovative methodology combining Neural\\nNetworks (NN) and Synthet ic Minority Over-sampling Technique (SMOTE) to\\nenhance the detection performance. The study addresses the inherent imbalance\\nin credit card transaction data, focusing on technical advancements for robust\\nand precise fraud detection. Results demonstrat e that the integration of NN\\nand SMOTE exhibits superior precision, recall, and F1-score compared to\\ntraditional models, highlighting its potential as an advanced solution for\\nhandling imbalanced datasets in credit card fraud detection scenarios. This\\nrese arch contributes to the ongoing efforts to develop effective and efficient\\nmechanisms for safeguarding financial transactions from fraudulent activities.\\n',\n 'labels': 1,\n 'explanations': 'The paper focuses on enhancing credit card fraud detection using a combination of neural networks and SMOTE. It evaluates its approach on existing datasets without introducing a new dataset. The study addresses imbalanced data in credit transactions but does not mention the creation of a new dataset.'}\n\n\n\n\nWhen training this model it was given both the title and abstract of the paper. We‚Äôll need to format the data in the same way when making predictions. This won‚Äôt be necessary for many classifier models.\n\ndef format_row_for_model(row):\n    return f\"TITLE: {row['title']} \\n\\nABSTRACT: {row['abstract']}\"\n\n\noutputs = llm.classify(format_row_for_model(ds[6]))"
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#full-dataset",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#full-dataset",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "Now we‚Äôll load the full dataset and format the data for the model. This is a dataset of arxiv papers (title, abstracts and other metadata) and we‚Äôll filter it down to only include papers that are in the computer science category.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"librarian-bots/arxiv-metadata-snapshot\", split=\"train\")\ndataset[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'id': '1305.5211',\n 'submitter': 'Hou-Biao Li',\n 'authors': 'Qian-Ping Guo, Xian-Ming Gu and Hou-biao Li',\n 'title': 'A note on the growth factor in Gaussian elimination for Higham matrices',\n 'comments': '13 pages, 1 figures;',\n 'journal-ref': None,\n 'doi': None,\n 'report-no': None,\n 'categories': 'math.NA cs.NA',\n 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n 'abstract': \"  The Higham matrix is a complex symmetric matrix A=B+iC, where both B and C\\nare real, symmetric and positive definite and $\\\\mathrm{i}=\\\\sqrt{-1}$ is the\\nimaginary unit. For any Higham matrix A, Ikramov et al. showed that the growth\\nfactor in Gaussian elimination is less than 3. In this paper, based on the\\nprevious results, a new bound of the growth factor is obtained by using the\\nmaximum of the condition numbers of matrixes B and C for the generalized Higham\\nmatrix A, which strengthens this bound to 2 and proves the Higham's conjecture.\\n\",\n 'versions': [{'version': 'v1', 'created': 'Wed, 22 May 2013 18:00:17 GMT'},\n  {'version': 'v2', 'created': 'Fri, 18 Apr 2025 14:14:30 GMT'}],\n 'update_date': datetime.datetime(2025, 4, 21, 0, 0),\n 'authors_parsed': [['Guo', 'Qian-Ping', ''],\n  ['Gu', 'Xian-Ming', ''],\n  ['Li', 'Hou-biao', '']]}\n\n\n\ndef contains_cs(row):\n    return \"cs\" in row[\"categories\"]\n\n\ndataset = dataset.filter(contains_cs, num_proc=4)"
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#doing-the-same-steps-with-polars",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#doing-the-same-steps-with-polars",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "I‚Äôm a big fan of Polars so in practice I would probably do the same steps in Polars.\n\n\nCode\nfrom pathlib import Path\n\nimport polars as pl\nfrom datasets import Dataset\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(\n    \"librarian-bots/arxiv-metadata-snapshot\",\n    local_dir=\"arxiv-metadata-snapshot\",\n    allow_patterns=[\"*.parquet\"],\n    repo_type=\"dataset\",\n)\nparquet_files = list(Path(\"arxiv-metadata-snapshot\").rglob(\"*.parquet\"))\ndf = pl.scan_parquet(parquet_files)\ndf = df.with_columns(\n    pl.concat_str(\n        [\n            pl.lit(\"TITLE: \"),\n            pl.col(\"title\"),\n            pl.lit(\" \\n\\nABSTRACT: \"),\n            pl.col(\"abstract\"),\n        ]\n    ).alias(\"prompt\")\n)\ndf_cs = df.filter(\n    pl.col(\"categories\")\n    .str.split(\" \")\n    .list.eval(pl.element().str.starts_with(\"cs.\"))\n    .list.any()\n)\ndf_cs_datasets = df_cs.filter(\n    pl.col(\"title\").str.contains(\"dataset\")\n    | pl.col(\"title\").str.contains(\"benchmark\")\n    | pl.col(\"abstract\").str.contains(\"dataset\")\n    | pl.col(\"abstract\").str.contains(\"benchmark\")\n)\n\ndf_cs_datasets = df_cs_datasets.collect()\n\nds = Dataset.from_polars(df_cs_datasets)\nds\n\n\nWe now end up with a dataset with a new column prompt which contains the correctly formatted input for the model.\n\nds[0][\"prompt\"]\n\n'TITLE: Good practices for a literature survey are not followed by authors while\\n  preparing scientific manuscripts \\n\\nABSTRACT:   The number of citations received by authors in scientific journals has become\\na major parameter to assess individual researchers and the journals themselves\\nthrough the impact factor. A fair assessment therefore requires that the\\ncriteria for selecting references in a given manuscript should be unbiased with\\nrespect to the authors or the journals cited. In this paper, we advocate that\\nauthors should follow two mandatory principles to select papers (later\\nreflected in the list of references) while studying the literature for a given\\nresearch: i) consider similarity of content with the topics investigated, lest\\nvery related work should be reproduced or ignored; ii) perform a systematic\\nsearch over the network of citations including seminal or very related papers.\\nWe use formalisms of complex networks for two datasets of papers from the arXiv\\nrepository to show that neither of these two criteria is fulfilled in practice.\\n'\n\n\nWe can grab all of these prompts as a list\n\nprompts = ds[\"prompt\"]\n\nLet‚Äôs see how many rows we have to classify\n\nlen(prompts)\n\n211190\n\n\n\n\nWhile in theory you should be able to pass in the entire list to the model and have it take care of batching, I ran into some issues with very big inputs so I‚Äôll use a simple batching approach. If you are working with smaller datasets you can probably skip this step.\n\nfrom toolz import partition_all\nfrom tqdm.auto import tqdm\n\n\nall_results = []\n\n\n%%time\nfor batch in tqdm(list(partition_all(50_000, prompts))):\n    all_results.append(llm.classify(batch))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 16min 22s, sys: 4.04 s, total: 16min 26s\nWall time: 16min 18s"
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#inference-performance",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#inference-performance",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "~16 minutes to process 211,190 isn‚Äôt bad! Besides this, there is a minimal amount of boilerplate code to maintain. For many datasets, you could just grab the relevant column and pass it directly to vLLM as a list. Even if you could shave of a bit of time doing this in another way it‚Äôs very nice to just pass the prompts and let vLLM handle the rest."
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#working-with-the-results",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#working-with-the-results",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "We can now join all of the results into a single list\n\nfrom toolz import concat\n\noutputs = list(concat(all_results))\n\nAt the moment the raw outputs don‚Äôt contains the labels we want so we‚Äôll create a small function that maps the outputs to the string label and returns the confidence in the prediction.\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef get_top_label(output, label_map: dict[str, str]):\n    \"\"\"\n    Given a ClassificationRequestOutput and a label_map (e.g. {'0': 'label0', ...}),\n    returns the top predicted label (or None if not found) and its confidence score.\n    \"\"\"\n    logits = torch.tensor(output.outputs.probs)\n    probs = F.softmax(logits, dim=0)\n    top_idx = torch.argmax(probs).item()\n    top_prob = probs[top_idx].item()\n    label = label_map.get(top_idx)  # Returns None if key is missing\n    return label, top_prob\n\n\nget_top_label(outputs[0], id2label)\n\n('no_new_dataset', 0.9503649473190308)\n\n\nLet‚Äôs create a new list with these results\n\nclean_results = [get_top_label(output, id2label) for output in outputs]\n\nWe can quickly count the label frequencies\n\nfrom collections import Counter\n\nCounter(result[0] for result in clean_results)\n\nCounter({'no_new_dataset': 182083, 'new_dataset': 29107})\n\n\n\n\nWe can now add the labels back to the dataset\n\nds = ds.add_column(\"is_new_dataset_label\", [result[0] for result in clean_results])\nds = ds.add_column(\"label_confidence\", [result[1] for result in clean_results])\n\n\nds[0]\n\n{'id': '1005.3063',\n 'submitter': 'Diego  Amancio Raphael',\n 'authors': 'D.R. Amancio, M. G. V. Nunes, O. N. Oliveira Jr., L. da F. Costa',\n 'title': 'Good practices for a literature survey are not followed by authors while\\n  preparing scientific manuscripts',\n 'comments': None,\n 'journal-ref': 'Scientometrics, v. 90, p. 2, (2012)',\n 'doi': '10.1007/s11192-012-0630-z',\n 'report-no': None,\n 'categories': 'physics.soc-ph cs.DL',\n 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n 'abstract': '  The number of citations received by authors in scientific journals has become\\na major parameter to assess individual researchers and the journals themselves\\nthrough the impact factor. A fair assessment therefore requires that the\\ncriteria for selecting references in a given manuscript should be unbiased with\\nrespect to the authors or the journals cited. In this paper, we advocate that\\nauthors should follow two mandatory principles to select papers (later\\nreflected in the list of references) while studying the literature for a given\\nresearch: i) consider similarity of content with the topics investigated, lest\\nvery related work should be reproduced or ignored; ii) perform a systematic\\nsearch over the network of citations including seminal or very related papers.\\nWe use formalisms of complex networks for two datasets of papers from the arXiv\\nrepository to show that neither of these two criteria is fulfilled in practice.\\n',\n 'versions': [{'version': 'v1', 'created': 'Mon, 17 May 2010 21:45:47 GMT'},\n  {'version': 'v2', 'created': 'Sun, 23 Sep 2012 00:49:13 GMT'}],\n 'update_date': datetime.datetime(2012, 9, 25, 0, 0),\n 'authors_parsed': [['Amancio', 'D. R.', ''],\n  ['Nunes', 'M. G. V.', ''],\n  ['Oliveira', 'O. N.', 'Jr.'],\n  ['Costa', 'L. da F.', '']],\n 'prompt': 'TITLE: Good practices for a literature survey are not followed by authors while\\n  preparing scientific manuscripts \\n\\nABSTRACT:   The number of citations received by authors in scientific journals has become\\na major parameter to assess individual researchers and the journals themselves\\nthrough the impact factor. A fair assessment therefore requires that the\\ncriteria for selecting references in a given manuscript should be unbiased with\\nrespect to the authors or the journals cited. In this paper, we advocate that\\nauthors should follow two mandatory principles to select papers (later\\nreflected in the list of references) while studying the literature for a given\\nresearch: i) consider similarity of content with the topics investigated, lest\\nvery related work should be reproduced or ignored; ii) perform a systematic\\nsearch over the network of citations including seminal or very related papers.\\nWe use formalisms of complex networks for two datasets of papers from the arXiv\\nrepository to show that neither of these two criteria is fulfilled in practice.\\n',\n 'is_new_dataset_label': 'no_new_dataset',\n 'label_confidence': 0.9503649473190308}"
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#filtering-the-dataset",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#filtering-the-dataset",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "Now we have a new column with the labels, and a second column with the confidence in the label. We can now filter the dataset as needed. You could of course combine these filters in many ways depending on your use case.\n\ndf_with_labels = ds.to_polars()\n\n\ndf_with_labels.head(2)\n\n\nshape: (2, 17)\n\n\n\nid\nsubmitter\nauthors\ntitle\ncomments\njournal-ref\ndoi\nreport-no\ncategories\nlicense\nabstract\nversions\nupdate_date\nauthors_parsed\nprompt\nis_new_dataset_label\nlabel_confidence\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nlist[struct[2]]\ndatetime[ms]\nlist[list[str]]\nstr\nstr\nf64\n\n\n\n\n\"1005.3063\"\n\"Diego¬†¬†Amancio Raphael\"\n\"D.R. Amancio, M. G. V. Nunes, ‚Ä¶\n\"Good practices for a literatur‚Ä¶\nnull\n\"Scientometrics, v. 90, p. 2, (‚Ä¶\n\"10.1007/s11192-012-0630-z\"\nnull\n\"physics.soc-ph cs.DL\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†The number of citations rece‚Ä¶\n[{\"v1\",\"Mon, 17 May 2010 21:45:47 GMT\"}, {\"v2\",\"Sun, 23 Sep 2012 00:49:13 GMT\"}]\n2012-09-25 00:00:00\n[[\"Amancio\", \"D. R.\", \"\"], [\"Nunes\", \"M. G. V.\", \"\"], ‚Ä¶ [\"Costa\", \"L. da F.\", \"\"]]\n\"TITLE: Good practices for a li‚Ä¶\n\"no_new_dataset\"\n0.950365\n\n\n\"1209.0127\"\n\"Alexandra Faynburd Mrs\"\n\"Ran El-Yaniv, Alexandra Faynbu‚Ä¶\n\"Autoregressive short-term pred‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.LG cs.CE cs.NE\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†This work is concerned with ‚Ä¶\n[{\"v1\",\"Sat, 1 Sep 2012 19:53:23 GMT\"}, {\"v2\",\"Mon, 24 Sep 2012 19:28:24 GMT\"}]\n2012-09-25 00:00:00\n[[\"El-Yaniv\", \"Ran\", \"\"], [\"Faynburd\", \"Alexandra\", \"\"]]\n\"TITLE: Autoregressive short-te‚Ä¶\n\"no_new_dataset\"\n0.950088\n\n\n\n\n\n\n\ndf_with_labels = df_with_labels.lazy()\n\n\ndf_with_labels.filter(pl.col(\"is_new_dataset_label\") == \"new_dataset\").collect().shape\n\n(29107, 17)\n\n\n\ndf_predicted_as_new_dataset = df_with_labels.filter(\n    pl.col(\"is_new_dataset_label\") == \"new_dataset\"\n)\n\nrun LazyFrame.show_graph() to see the optimized version\n\n\n\n\n\npolars_query\n\n\n\np1\n\nFILTER BY [(col(\"is_new_dataset_label\")) == (String(new_dataset))]\n\n\n\np2\n\nTABLE\nœÄ */17\n\n\n\np1--p2\n\n\n\n\n\n\n\ndf_predicted_as_new_dataset_high_confidence = df_predicted_as_new_dataset.filter(\n    pl.col(\"label_confidence\").ge(0.95)\n)\n\n\ndf_predicted_as_new_dataset_high_confidence.show_graph()\n\n\n\n\n\n\n\n\n\ndf_predicted_as_new_dataset_high_confidence.collect()\n\n\nshape: (23_018, 17)\n\n\n\nid\nsubmitter\nauthors\ntitle\ncomments\njournal-ref\ndoi\nreport-no\ncategories\nlicense\nabstract\nversions\nupdate_date\nauthors_parsed\nprompt\nis_new_dataset_label\nlabel_confidence\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nlist[struct[2]]\ndatetime[ms]\nlist[list[str]]\nstr\nstr\nf64\n\n\n\n\n\"1209.0835\"\n\"Neil Zhenqiang Gong\"\n\"Neil Zhenqiang Gong, Wenchang ‚Ä¶\n\"Evolution of Social-Attribute ‚Ä¶\n\"14 pages, 19 figures. will app‚Ä¶\nnull\nnull\nnull\n\"cs.SI cs.CY physics.soc-ph\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Understanding social network‚Ä¶\n[{\"v1\",\"Wed, 5 Sep 2012 01:01:47 GMT\"}, {\"v2\",\"Sat, 8 Sep 2012 04:12:28 GMT\"}, ‚Ä¶ {\"v4\",\"Wed, 19 Sep 2012 02:24:50 GMT\"}]\n2012-09-20 00:00:00\n[[\"Gong\", \"Neil Zhenqiang\", \"\"], [\"Xu\", \"Wenchang\", \"\"], ‚Ä¶ [\"Song\", \"Dawn\", \"\"]]\n\"TITLE: Evolution of Social-Att‚Ä¶\n\"new_dataset\"\n0.968679\n\n\n\"1207.0784\"\n\"Romain Giot\"\n\"Romain Giot (GREYC), Mohamad E‚Ä¶\n\"Web-Based Benchmark for Keystr‚Ä¶\n\"The Eighth International Confe‚Ä¶\nnull\nnull\nnull\n\"cs.LG\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Most keystroke dynamics stud‚Ä¶\n[{\"v1\",\"Tue, 3 Jul 2012 19:12:56 GMT\"}]\n2012-07-04 00:00:00\n[[\"Giot\", \"Romain\", ‚Ä¶ \"GREYC\"], [\"El-Abed\", \"Mohamad\", ‚Ä¶ \"GREYC\"], [\"Rosenberger\", \"Christophe\", ‚Ä¶ \"GREYC\"]]\n\"TITLE: Web-Based Benchmark for‚Ä¶\n\"new_dataset\"\n0.966945\n\n\n\"1205.2345\"\n\"Salah A. Aly\"\n\"Hossam Zawbaa and Salah A. Aly\"\n\"Hajj and Umrah Event Recogniti‚Ä¶\n\"4 pages, 18 figures with 33 im‚Ä¶\nnull\nnull\nnull\n\"cs.CV cs.CY\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†In this note, new Hajj and U‚Ä¶\n[{\"v1\",\"Thu, 10 May 2012 19:10:18 GMT\"}]\n2012-05-11 00:00:00\n[[\"Zawbaa\", \"Hossam\", \"\"], [\"Aly\", \"Salah A.\", \"\"]]\n\"TITLE: Hajj and Umrah Event Re‚Ä¶\n\"new_dataset\"\n0.973292\n\n\n\"1110.2096\"\n\"Philipp Herrmann\"\n\"Philipp N. Herrmann, Dennis O.‚Ä¶\n\"Beating Irrationality: Does De‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.HC cs.CY cs.SI\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†In this research, we investi‚Ä¶\n[{\"v1\",\"Mon, 10 Oct 2011 16:23:18 GMT\"}, {\"v2\",\"Tue, 3 Apr 2012 15:34:53 GMT\"}]\n2012-04-04 00:00:00\n[[\"Herrmann\", \"Philipp N.\", \"\"], [\"Kundisch\", \"Dennis O.\", \"\"], [\"Rahman\", \"Mohammad S.\", \"\"]]\n\"TITLE: Beating Irrationality: ‚Ä¶\n\"new_dataset\"\n0.960584\n\n\n\"1201.6569\"\n\"Robert Fink\"\n\"Robert Fink, Larisa Han, Dan O‚Ä¶\n\"Aggregation in Probabilistic D‚Ä¶\n\"VLDB2012\"\n\"Proceedings of the VLDB Endowm‚Ä¶\nnull\nnull\n\"cs.DB\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†This paper presents a query ‚Ä¶\n[{\"v1\",\"Tue, 31 Jan 2012 15:10:34 GMT\"}]\n2012-02-01 00:00:00\n[[\"Fink\", \"Robert\", \"\"], [\"Han\", \"Larisa\", \"\"], [\"Olteanu\", \"Dan\", \"\"]]\n\"TITLE: Aggregation in Probabil‚Ä¶\n\"new_dataset\"\n0.955361\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"1910.10470\"\n\"Hans Pinckaers\"\n\"Hans Pinckaers, Geert Litjens\"\n\"Neural Ordinary Differential E‚Ä¶\n\"Accepted to 'Medical Imaging m‚Ä¶\nnull\nnull\nnull\n\"eess.IV cs.CV\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Automated medical image segm‚Ä¶\n[{\"v1\",\"Wed, 23 Oct 2019 11:35:19 GMT\"}]\n2019-10-24 00:00:00\n[[\"Pinckaers\", \"Hans\", \"\"], [\"Litjens\", \"Geert\", \"\"]]\n\"TITLE: Neural Ordinary Differe‚Ä¶\n\"new_dataset\"\n0.960731\n\n\n\"1910.10502\"\n\"Maaike De Boer\"\n\"Hella Haanstra and Maaike H. T‚Ä¶\n\"Opinion aspect extraction in D‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.CL cs.LG\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Aspect extraction can be use‚Ä¶\n[{\"v1\",\"Mon, 21 Oct 2019 09:33:09 GMT\"}]\n2019-10-24 00:00:00\n[[\"Haanstra\", \"Hella\", \"\"], [\"de Boer\", \"Maaike H. T.\", \"\"]]\n\"TITLE: Opinion aspect extracti‚Ä¶\n\"new_dataset\"\n0.967287\n\n\n\"1908.01671\"\n\"Alice Baird\"\n\"Alice Baird and Bjoern Schulle‚Ä¶\n\"Acoustic Sounds for Wellbeing:‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.SD eess.AS\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†The field of sound healing i‚Ä¶\n[{\"v1\",\"Mon, 5 Aug 2019 15:01:17 GMT\"}, {\"v2\",\"Mon, 21 Oct 2019 20:50:51 GMT\"}]\n2019-10-23 00:00:00\n[[\"Baird\", \"Alice\", \"\"], [\"Schuller\", \"Bjoern\", \"\"]]\n\"TITLE: Acoustic Sounds for Wel‚Ä¶\n\"new_dataset\"\n0.967101\n\n\n\"cs/0211018\"\n\"Vladimir Pestov\"\n\"Vladimir Pestov and Aleksandar‚Ä¶\n\"Indexing schemes for similarit‚Ä¶\n\"19 pages, LaTeX with 8 figures‚Ä¶\n\"Fundamenta Informaticae Vol. 7‚Ä¶\nnull\nnull\n\"cs.DS\"\nnull\n\"¬†¬†We suggest a variation of th‚Ä¶\n[{\"v1\",\"Thu, 14 Nov 2002 19:10:16 GMT\"}, {\"v2\",\"Thu, 13 Oct 2005 21:06:17 GMT\"}]\n2009-09-29 00:00:00\n[[\"Pestov\", \"Vladimir\", \"\"], [\"Stojmirovic\", \"Aleksandar\", \"\"]]\n\"TITLE: Indexing schemes for si‚Ä¶\n\"new_dataset\"\n0.964954\n\n\n\"cs/0604015\"\n\"Dmitri Krioukov\"\n\"Xenofontas Dimitropoulos, Dmit‚Ä¶\n\"Revealing the Autonomous Syste‚Ä¶\nnull\n\"PAM 2006, best paper award\"\nnull\nnull\n\"cs.NI cs.LG\"\nnull\n\"¬†¬†Although the Internet AS-lev‚Ä¶\n[{\"v1\",\"Thu, 6 Apr 2006 00:08:24 GMT\"}]\n2007-05-23 00:00:00\n[[\"Dimitropoulos\", \"Xenofontas\", \"\"], [\"Krioukov\", \"Dmitri\", \"\"], ‚Ä¶ [\"claffy\", \"kc\", \"\"]]\n\"TITLE: Revealing the Autonomou‚Ä¶\n\"new_dataset\"\n0.957991"
  },
  {
    "objectID": "posts/2025/vllm/modern_inference_modernbert.html#conclusion",
    "href": "posts/2025/vllm/modern_inference_modernbert.html#conclusion",
    "title": "Efficient Inference for ModernBERT Classifiers Using vLLM",
    "section": "",
    "text": "In this post we‚Äôve seen how to use vLLM to scale inference for ModernBERT based classifier models. I‚Äôm personally still very excited about the potential of smaller classifier models for a wide range of tasks and it‚Äôs really cool to see a very well known and used inference engine like vLLM extending it‚Äôs capabilities to include this kind of model. I‚Äôd be excited to see more models like this being added to vLLM and also to see the comunity shipping cool new classifiers for curating/cleaning/organizing large datasets."
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "",
    "text": "%pip install polars huggingface_hub datasets openai --upgrade"
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#updates",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#updates",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "Updates",
    "text": "Updates\n\n2025-01-30: Sayak Paul has implemented this pipeline using Transformers (code). This will be a super good option if you already have sufficient GPUs for running these models."
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#how-can-we-get-the-best-of-both-worlds",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#how-can-we-get-the-best-of-both-worlds",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "How can we get the best of both worlds?",
    "text": "How can we get the best of both worlds?\ntl;dr, how can we use LLMs to generate labels to fine-tune a ModernBERT model?\nIt‚Äôs fair to say that DeepSeek-R1 has made quite an impact in the last few weeks. It‚Äôs a powerful reasoning model that excels at many tasks that require reasoning. One particularly exciting aspect of the release of this model, though, is the distilled versions of the model. These models are much smaller but still retain a lot of the reasoning ability of the larger models.\n\nClassification often requires reasoning\nWhile the interest in reasoning models often focs on use cases like mathematics and coding, there are many other use cases where reasoning can be helpful. One example is classification. Although some classification problems are very simple and mostly require ‚Äúpattern matching,‚Äù there are many other problems where reasoning is needed. This is where a reasoning model could be helpful.\n\n\nCan we distil even smaller models?\nWhile the distilled models are fairly small (the smallest is 1.5B), we may still prefer to have an even smaller model for many use cases. If you can remember all the way back to December 2024, the ModernBERT release introduced a new BERT model, which is a good candidate for this kind of efficient classification use case. The main challenge is that in order to train a classifier, we need labeled data. This is where we can use a reasoning model to generate synthetic labels."
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#the-use-case-classifying-arxiv-papers-that-introduce-a-newly-created-dataset",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#the-use-case-classifying-arxiv-papers-that-introduce-a-newly-created-dataset",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "The use case: classifying ArXiv papers that introduce a newly created dataset",
    "text": "The use case: classifying ArXiv papers that introduce a newly created dataset\nAs the Machine Learning Librarian at Hugging Face, I want to keep track of new datasets being shared on ArXiv. While you can search for ‚Äúdataset‚Äù or ‚Äúbenchmark‚Äù in the title or abstract, this returns any papers that mention datasets or benchmarks. I‚Äôm only interested in papers that introduce a newly created dataset.\nSo the goal is to give an article to classify into whether it introduces a newly created dataset.\nI‚Äôll use Polars to load the ArXiv dataset from the Hub, but you can use whichever data tool you want.\n\n\n\n\n\n\nNote\n\n\n\nFeel free to skip this section if you are not interested in the use case and just want to see how to do the labelling part.\n\n\n\nimport os\nimport polars as pl\nfrom huggingface_hub import snapshot_download\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"  # turn on HF_TRANSFER\n\n\nfiles = snapshot_download(\n    repo_id=\"librarian-bots/arxiv-metadata-snapshot\",\n    allow_patterns=[\"*.parquet\"],\n    repo_type=\"dataset\",\n)\n\n\ndf = pl.scan_parquet(files)\n\nLet‚Äôs look at the first row. We see a bunch of metadata about the paper and then the title and abstract. These are probably the column we‚Äôll want to use as input for our model.\n\ndf.head(1).collect()\n\n\nshape: (1, 14)\n\n\n\nid\nsubmitter\nauthors\ntitle\ncomments\njournal-ref\ndoi\nreport-no\ncategories\nlicense\nabstract\nversions\nupdate_date\nauthors_parsed\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nlist[struct[2]]\ndatetime[ms]\nlist[list[str]]\n\n\n\n\n\"1004.3702\"\n\"Lizhi Du\"\n\"Lizhi Du\"\n\"A Polynomial time Algorithm fo‚Ä¶\n\"26 pages. This time, I add a d‚Ä¶\nnull\nnull\nnull\n\"cs.DS\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†Based on the famous Rotation‚Ä¶\n[{\"v1\",\"Mon, 12 Apr 2010 04:39:27 GMT\"}, {\"v10\",\"Mon, 5 Nov 2012 01:44:46 GMT\"}, ‚Ä¶ {\"v9\",\"Wed, 29 Aug 2012 06:39:31 GMT\"}]\n2025-01-24 00:00:00\n[[\"Du\", \"Lizhi\", \"\"]]\n\n\n\n\n\n\nYou will see there is a categories column. This is a string that contains a list of categories that the paper belongs to. We can grab a few examples of the categories.\n\ndf.head(10).collect().select(\"categories\").to_series().to_list()\n\n['cs.DS',\n 'math.GM',\n 'math.CA math.AT math.DG math.DS',\n 'cond-mat.mtrl-sci',\n 'cond-mat.mtrl-sci',\n 'math.GT',\n 'math.GT',\n 'math.GT',\n 'math.AP',\n 'math.AP math-ph math.MP math.SP']\n\n\nFor my particular use case I‚Äôm mostly interest in papers that are in the computer science category i.e contain ‚Äúcs.‚Äù in the categories column.\n\ndf = df.filter(\n    pl.col(\"categories\")\n    .str.split(\" \")\n    .list.eval(pl.element().str.starts_with(\"cs.\"))\n    .list.any()\n)\n\nWe‚Äôll filter papers to only include those that contain the word ‚Äúdataset‚Äù in the title or abstract, again you could easily change this to use other words.\n\n\n\n\n\n\nNote\n\n\n\nOne thing to consider here is that ideally you want the distribution of data you use for training the model to be similar to the distribution of data you will use in practice. Since I will only check for ArXiV papers that contain the word ‚Äúdataset‚Äù in the title or abstract, I will filter out a lot of the data before it even gets passed to the model. For your use case, consider the distribution of data you‚Äôll be using in practice and filter the data accordingly.\n\n\n\ndf = df.filter(\n    pl.col(\"title\").str.contains(\"dataset\") | pl.col(\"abstract\").str.contains(\"dataset\")\n)\n\nSince we‚Äôre using the polars lazy api, we need to call collect() to actually get the data.\n\ndf = df.collect()"
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#generate-labels-not-synthetic-data",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#generate-labels-not-synthetic-data",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "Generate labels not synthetic data",
    "text": "Generate labels not synthetic data\nThere has been significant growth in the use of LLMs for synthetic data generation over the past couple of years. While we could generate synthetic data, i.e., developing both the ‚Äúinput‚Äù and ‚Äútarget‚Äù columns, if we already have some data we want to work with, it makes more sense to generate labels. One of the significant challenges with synethic data generation is that the data generated is often not representative of the data we want to use in practice. For generative tasks, this might matter slightly less. Since we‚Äôre focused on building classifiers, which we‚Äôll often focus on quite a narrow use case or domain, the data we use to train the model must be representative of the data we want to use in practice.\nIn this case, it might be more sensible to use a model‚Äôs reasoning ability to generate labels rather than generate synthetic data.\nLet‚Äôs grab a few examples from the data to use as a starting point.\n\nexamples = df.head(4).select(pl.col([\"abstract\", \"title\"])).to_dicts()\nexamples[0]\n\n{'abstract': '  This paper presents a new fuzzy k-means algorithm for the clustering of high\\ndimensional data in various subspaces. Since, In the case of high dimensional\\ndata, some features might be irrelevant and relevant but may have different\\nsignificance in the clustering. For a better clustering, it is crucial to\\nincorporate the contribution of these features in the clustering process. To\\ncombine these features, in this paper, we have proposed a new fuzzy k-means\\nclustering algorithm in which the objective function of the fuzzy k-means is\\nmodified using two different entropy term. The first entropy term helps to\\nminimize the within-cluster dispersion and maximize the negative entropy to\\ndetermine clusters to contribute to the association of data points. The second\\nentropy term helps to control the weight of the features because different\\nfeatures have different contributing weights in the clustering process for\\nobtaining the better partition of the data. The efficacy of the proposed method\\nis presented in terms of various clustering measures on multiple datasets and\\ncompared with various state-of-the-art methods.\\n',\n 'title': 'An Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for\\n  High Dimensional Data'}"
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#structured-generation",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#structured-generation",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "Structured generation?",
    "text": "Structured generation?\nWe‚Äôll start by using a structured generation approach to generate the labels. This means we‚Äôll define a schema for the model‚Äôs output and then use that to generate the labels. I‚Äôve written more about this in a previous blog post but the basic idea is that we define a schema for the output of the model and then use that to generate the labels. This means we don‚Äôt have to do a lot of work to parse the output of the model and can be sure we can easily train on the output.\nIn this case, we define a Pydantic model as one that has a label and an explanation.\n\nfrom enum import Enum\nfrom pydantic import BaseModel, constr\nfrom typing import Annotated\n\n\nclass DatasetLabel(str, Enum):\n    NEW = \"new_dataset\"\n    NOT_NEW = \"no_new_dataset\"\n\n\nclass IntroducesNewDataset(BaseModel):\n    explanation: constr(min_length=40)\n    label: DatasetLabel\n\nWe define a function to format the data as a prompt. This function takes a dictionary with the title and abstract and formats it as a prompt for the model.\n\ndef format_text_as_prompt(data: dict[str, str]):\n    return f\"\"\"Look at the title and abstract for the following arXiv paper. Assess whether the paper is likely to introduce a newly created dataset.\n\n\nTitle: {data['title']}\nAbstract: {data['abstract']}\n\nYour role is to decide whether the paper introduces a newly created dataset. First you should think about whether the paper is likely to introduce a newly created dataset. You should then return your reasoning and the label you've chosen. \nYou should choose out of the \"new_dataset\" or \"no_new_dataset\" labels.\n\nReturn your reasoning and the label you've chosen as a JSON object like this:\n```json\n{{\n    \"label\": \"new_dataset\" | \"no_new_dataset\",\n    \"explanation\": \"The reasoning the model used to come to its conclusion\"\n}}\n```\n\"\"\"\n\n\nprint(format_text_as_prompt(examples[0]))\n\nLook at the title and abstract for the following arXiv paper. Assess whether the paper is likely to introduce a newly created dataset.\n\n\nTitle: An Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for\n  High Dimensional Data\nAbstract:   This paper presents a new fuzzy k-means algorithm for the clustering of high\ndimensional data in various subspaces. Since, In the case of high dimensional\ndata, some features might be irrelevant and relevant but may have different\nsignificance in the clustering. For a better clustering, it is crucial to\nincorporate the contribution of these features in the clustering process. To\ncombine these features, in this paper, we have proposed a new fuzzy k-means\nclustering algorithm in which the objective function of the fuzzy k-means is\nmodified using two different entropy term. The first entropy term helps to\nminimize the within-cluster dispersion and maximize the negative entropy to\ndetermine clusters to contribute to the association of data points. The second\nentropy term helps to control the weight of the features because different\nfeatures have different contributing weights in the clustering process for\nobtaining the better partition of the data. The efficacy of the proposed method\nis presented in terms of various clustering measures on multiple datasets and\ncompared with various state-of-the-art methods.\n\n\nYour role is to decide whether the paper introduces a newly created dataset. First you should think about whether the paper is likely to introduce a newly created dataset. You should then return your reasoning and the label you've chosen. \nYou should choose out of the \"new_dataset\" or \"no_new_dataset\" labels.\n\nReturn your reasoning and the label you've chosen as a JSON object like this:\n```json\n{\n    \"label\": \"new_dataset\" | \"no_new_dataset\",\n    \"explanation\": \"The reasoning the model used to come to its conclusion\"\n}\n```"
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#using-lm-studio-to-develop-our-approach",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#using-lm-studio-to-develop-our-approach",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "Using LM Studio to develop our approach",
    "text": "Using LM Studio to develop our approach\nOne of the powerful features of open source is that it makes it easier to run models in different places. While developing our approach, we can use a smaller version of the model to test it and then switch to a hosted version once we‚Äôre happy with it.\nWe‚Äôll run the model using LM Studio. LM Studio is primarily known as a UI for running local LLMs, but it also has a server mode, which we‚Äôll use here. We can interact with the server using the CLI. To start the server, we can run the following command.\n\n!lms server start\n\nStarting server...\nSuccess! Server is now running on port 1234\n\n\nWe can use ls to see the models that are available, we‚Äôll filter these to only show the DeepSeek models.\n\n!lms ls | grep DeepSeek\n\n\nlmstudio-community/DeepSeek-R1-Distill-Qwen-1.5B-GGUF      1.12 GB          Qwen2           \n\nlmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF        4.68 GB          Qwen2           \n\nlmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF       8.99 GB          Qwen2           \n\nlmstudio-community/DeepSeek-R1-Distill-Llama-8B-GGUF       4.92 GB          Llama           \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the output here is showing models I already have locally. There are many models LM studio can download from the Hugging Face Hub.\n\n\nWe can load the model by running the following command. If the model is not already downloaded, LM Studio will download it. We‚Äôll try and see how well the 7B model does.\n\n!lms load DeepSeek-R1-Distill-Qwen-7B-GGUF\n\n\nLoading model \"lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf\"...\n\n[LMStudioClient][LLM] Start loading model lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf...\n\n‚†π [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            ] 43.04%          sl          ] 32.15%          ] 34.82%          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                ] 35.25%          Model loaded successfully in 4.20s. (4.68 GB)\n\nTo use the model in the API/SDK, use the identifier \"deepseek-r1-distill-qwen-7b\".\n\nTo set a custom identifier, use the --identifier &lt;identifier&gt; option.\n\n\n\n\nSince LM Studio has an OpenAI compatible API, we can use the OpenAI Python client to interact with the server. We just need to set the base URL to the LM Studio server and set the API key to lm-studio.\n\nfrom openai import OpenAI\n\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\nOnce we‚Äôve created the client we can interact with it in the usual way i.e.¬†to see available models we can run the following command.\n\nclient.models.list()\n\nSyncPage[Model](data=[Model(id='deepseek-r1-distill-qwen-7b', created=None, object='model', owned_by='organization_owner')], object='list')"
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#generating-labels",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#generating-labels",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "Generating labels",
    "text": "Generating labels\nWe can now generate labels for our examples. We‚Äôll use the format_text_as_prompt function to format the data as a prompt and then pass it to the model. Since we‚Äôre using a structured output, we need to use the beta.chat.completions endpoint. We pass in our Pydantic model as the response_format argument.\n\nmessages = [\n    {\"role\": \"user\", \"content\": format_text_as_prompt(examples[0])},\n]\n\n\nresponse = client.beta.chat.completions.parse(\n    model=\"deepseek-r1-distill-qwen-7b\",\n    messages=messages,\n    temperature=0.7,\n    response_format=IntroducesNewDataset,\n)\n\nWe can check that we can parse the output of the model into our Pydantic model.\n\nIntroducesNewDataset.model_validate_json(response.choices[0].message.content)\n\nIntroducesNewDataset(explanation=\"The paper discusses an entropy-based fuzzy k-means algorithm designed for high-dimensional data. While it mentions incorporating feature contributions into clustering, there's no information about introducing a new dataset.\", label=&lt;DatasetLabel.NOT_NEW: 'no_new_dataset'&gt;)\n\n\nWe‚Äôll wrap this in a function so we can easily use it for a lot of examples.\n\ndef predict_label(\n    data: dict[str, str], model: str = \"deepseek-r1-distill-qwen-1.5b\", client=client\n) -&gt; IntroducesNewDataset | None:\n    try:\n        prompt = format_text_as_prompt(data)\n        messages = [\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        response = client.beta.chat.completions.parse(\n            model=model,\n            messages=messages,\n            temperature=0.7,\n            response_format=IntroducesNewDataset,\n        )\n        return IntroducesNewDataset.model_validate_json(\n            response.choices[0].message.content\n        )\n    except Exception as e:\n        print(e)\n        return None\n\nBefore doing a big batch of predictions, let‚Äôs run the model on a few examples so we can see how it does.\n\nfrom rich import print as rich_print\n\nstructured_results = []\nfor example in examples:\n    title = example[\"title\"]\n    abstract = example[\"abstract\"]\n    prediction = predict_label(example)\n    structured_results.append(prediction)\n    rich_print(title)\n    rich_print(abstract)\n    rich_print(prediction)\n    rich_print(\"---\")\n\nAn Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for\n  High Dimensional Data\n\n\n\n  This paper presents a new fuzzy k-means algorithm for the clustering of high\ndimensional data in various subspaces. Since, In the case of high dimensional\ndata, some features might be irrelevant and relevant but may have different\nsignificance in the clustering. For a better clustering, it is crucial to\nincorporate the contribution of these features in the clustering process. To\ncombine these features, in this paper, we have proposed a new fuzzy k-means\nclustering algorithm in which the objective function of the fuzzy k-means is\nmodified using two different entropy term. The first entropy term helps to\nminimize the within-cluster dispersion and maximize the negative entropy to\ndetermine clusters to contribute to the association of data points. The second\nentropy term helps to control the weight of the features because different\nfeatures have different contributing weights in the clustering process for\nobtaining the better partition of the data. The efficacy of the proposed method\nis presented in terms of various clustering measures on multiple datasets and\ncompared with various state-of-the-art methods.\n\n\n\n\nIntroducesNewDataset(\n    explanation=\"The paper presents an algorithm for clustering high-dimensional data, focusing on feature \nweighting and entropy-based modifications to the fuzzy k-means method. The abstract mentions that their proposed \nmethod is evaluated against various datasets using different measures. Since the title doesn't suggest a new \ndataset but rather an improvement or variation in an existing one (fuzzy k-means), and the abstract emphasizes \nperformance evaluation across multiple datasets without indicating the introduction of a new one, it's reasonable \nto assume that no new dataset was created in this paper.\",\n    label=&lt;DatasetLabel.NOT_NEW: 'no_new_dataset'&gt;\n)\n\n\n\n---\n\n\n\nIdentifying Influential Brokers on Social Media from Social Network\n  Structure\n\n\n\n  Identifying influencers in a given social network has become an important\nresearch problem for various applications, including accelerating the spread of\ninformation in viral marketing and preventing the spread of fake news and\nrumors. The literature contains a rich body of studies on identifying\ninfluential source spreaders who can spread their own messages to many other\nnodes. In contrast, the identification of influential brokers who can spread\nother nodes' messages to many nodes has not been fully explored. Theoretical\nand empirical studies suggest that involvement of both influential source\nspreaders and brokers is a key to facilitating large-scale information\ndiffusion cascades. Therefore, this paper explores ways to identify influential\nbrokers from a given social network. By using three social media datasets, we\ninvestigate the characteristics of influential brokers by comparing them with\ninfluential source spreaders and central nodes obtained from centrality\nmeasures. Our results show that (i) most of the influential source spreaders\nare not influential brokers (and vice versa) and (ii) the overlap between\ncentral nodes and influential brokers is small (less than 15%) in Twitter\ndatasets. We also tackle the problem of identifying influential brokers from\ncentrality measures and node embeddings, and we examine the effectiveness of\nsocial network features in the broker identification task. Our results show\nthat (iii) although a single centrality measure cannot characterize influential\nbrokers well, prediction models using node embedding features achieve F$_1$\nscores of 0.35--0.68, suggesting the effectiveness of social network features\nfor identifying influential brokers.\n\n\n\n\nIntroducesNewDataset(\n    explanation=\"... reason ...‚ÄùÔºå... } To determine whether the paper introduces a newly created dataset, let's \nanalyze the information provided in the title and abstract. The title is \",\n    label=&lt;DatasetLabel.NEW: 'new_dataset'&gt;\n)\n\n\n\n---\n\n\n\nImproving Performance of Automatic Keyword Extraction (AKE) Methods\n  Using PoS-Tagging and Enhanced Semantic-Awareness\n\n\n\n  Automatic keyword extraction (AKE) has gained more importance with the\nincreasing amount of digital textual data that modern computing systems\nprocess. It has various applications in information retrieval (IR) and natural\nlanguage processing (NLP), including text summarisation, topic analysis and\ndocument indexing. This paper proposes a simple but effective\npost-processing-based universal approach to improve the performance of any AKE\nmethods, via an enhanced level of semantic-awareness supported by PoS-tagging.\nTo demonstrate the performance of the proposed approach, we considered word\ntypes retrieved from a PoS-tagging step and two representative sources of\nsemantic information - specialised terms defined in one or more\ncontext-dependent thesauri, and named entities in Wikipedia. The above three\nsteps can be simply added to the end of any AKE methods as part of a\npost-processor, which simply re-evaluate all candidate keywords following some\ncontext-specific and semantic-aware criteria. For five state-of-the-art (SOTA)\nAKE methods, our experimental results with 17 selected datasets showed that the\nproposed approach improved their performances both consistently (up to 100% in\nterms of improved cases) and significantly (between 10.2% and 53.8%, with an\naverage of 25.8%, in terms of F1-score and across all five methods), especially\nwhen all the three enhancement steps are used. Our results have profound\nimplications considering the ease to apply our proposed approach to any AKE\nmethods and to further extend it.\n\n\n\n\nIntroducesNewDataset(\n    explanation=\"The paper focuses on improving automatic keyword extraction methods using PoS-tagging and \nsemantic-awareness. It mentions experiments with five state-of-the-art AKE methods across 17 datasets, but there's \nno indication of introducing a new dataset.\",\n    label=&lt;DatasetLabel.NOT_NEW: 'no_new_dataset'&gt;\n)\n\n\n\n---\n\n\n\nLOCUS: LOcalization with Channel Uncertainty and Sporadic Energy\n\n\n\n  Accurate sound source localization (SSL) requires consistent multichannel\ndata for reliable degree of arrival (DoA) estimation. However, intermittently\npowered batteryless systems often suffer from incomplete sensor data due to the\nstochastic nature of energy harvesting. Existing methods struggle with missing\nchannels, leading to significant performance degradation. In this paper, we\npropose $\\textit{LOCUS}$, a novel deep learning-based system designed to\nrecover corrupted features for SSL in batteryless systems. $\\textit{LOCUS}$\naddresses missing data by leveraging information entropy estimation and\nconditional interpolation, combining three modules: (1) Information-Weighted\nFocus (InFo), which identifies and quantifies corrupted data elements, (2)\nLatent Feature Synthesizer (LaFS), which synthesizes missing features, and (3)\nGuided Replacement (GRep), which intelligently replaces missing elements while\npreserving valid data. We demonstrate significant performance improvements\nusing two datasets: DCASE and LargeSet, where $\\textit{LOCUS}$ achieves up to\n$36.91\\%$ lower DoA error compared to existing methods. Real-world evaluations\nacross three environments with intermittent power sources show a\n$25.87-59.46\\%$ improvement in performance when channels are stochastically\nmissing. Additionally, we release a 50-hour multichannel dataset to support\nfurther research in SSL.\n\n\n\n\nIntroducesNewDataset(\n    explanation=\"... reason why you think it's a new dataset or not\",\n    label=&lt;DatasetLabel.NEW: 'new_dataset'&gt;\n)\n\n\n\n---"
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#room-to-think",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#room-to-think",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "Room to think?",
    "text": "Room to think?\nOne of the features of the R1 model is that it has ‚Äúreasoning‚Äù, which is delineated by  and  tags. Since our structured output doesn‚Äôt allow for this, let‚Äôs try and see how well the model does without it.\n\ndef predict_label_without_structured_output(\n    data: dict[str, str], model: str = \"deepseek-r1-distill-qwen-1.5b\", client=client\n) -&gt; str:\n    prompt = format_text_as_prompt(data)\n    messages = [\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0.7,\n    )\n    return response.choices[0].message.content\n\nWe‚Äôll compare the results from the two approaches.\n\n# compare the results vs structured output\nfor i, example in enumerate(examples):\n    rich_print(example[\"title\"])\n    rich_print(example[\"abstract\"])\n    prediction = predict_label_without_structured_output(example)\n    print(f\"Previous: {structured_results[i].label}\")\n    print(f\"New: {prediction}\")\n    rich_print(\"---\")\n\nAn Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for\n  High Dimensional Data\n\n\n\n  This paper presents a new fuzzy k-means algorithm for the clustering of high\ndimensional data in various subspaces. Since, In the case of high dimensional\ndata, some features might be irrelevant and relevant but may have different\nsignificance in the clustering. For a better clustering, it is crucial to\nincorporate the contribution of these features in the clustering process. To\ncombine these features, in this paper, we have proposed a new fuzzy k-means\nclustering algorithm in which the objective function of the fuzzy k-means is\nmodified using two different entropy term. The first entropy term helps to\nminimize the within-cluster dispersion and maximize the negative entropy to\ndetermine clusters to contribute to the association of data points. The second\nentropy term helps to control the weight of the features because different\nfeatures have different contributing weights in the clustering process for\nobtaining the better partition of the data. The efficacy of the proposed method\nis presented in terms of various clustering measures on multiple datasets and\ncompared with various state-of-the-art methods.\n\n\n\n\nPrevious: DatasetLabel.NOT_NEW\nNew: &lt;think&gt;\nOkay, so I need to figure out whether the paper introduces a newly created dataset. The title and abstract are provided.\n\nThe title is: \"An Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for High Dimensional Data.\" It mentions an algorithm related to clustering high-dimensional data using fuzzy k-means with some entropy terms and feature weighting.\n\nLooking at the abstract, it says they've proposed a new fuzzy k-means algorithm. The focus is on modifying the objective function by adding two different entropy terms: one to minimize within-cluster dispersion and another to control feature weights because features have varying contributions in clustering.\n\nThe paper mentions that their method was tested against various datasets and compared with state-of-the-art methods, but there's no explicit mention of introducing a new dataset. They evaluate performance on multiple existing datasets without specifying any novel data creation here.\n\nSo, the key points are: they're improving an algorithm for high-dimensional clustering but don't indicate creating a new dataset; instead, they apply it to various datasets that already exist.\n&lt;/think&gt;\n\nThe paper does not introduce a newly created dataset as part of its methodology. It focuses on enhancing an existing fuzzy k-means algorithm and evaluates its performance across multiple existing datasets.\n\n```json\n{\n    \"label\": \"no_new_dataset\",\n    \"explanation\": \"The paper discusses modifications to an existing fuzzy k-means algorithm for high-dimensional data, but it does not mention introducing a new dataset. Instead, it evaluates the proposed method on various datasets that likely already exist.\"\n}\n```\n\n\n---\n\n\n\nIdentifying Influential Brokers on Social Media from Social Network\n  Structure\n\n\n\n  Identifying influencers in a given social network has become an important\nresearch problem for various applications, including accelerating the spread of\ninformation in viral marketing and preventing the spread of fake news and\nrumors. The literature contains a rich body of studies on identifying\ninfluential source spreaders who can spread their own messages to many other\nnodes. In contrast, the identification of influential brokers who can spread\nother nodes' messages to many nodes has not been fully explored. Theoretical\nand empirical studies suggest that involvement of both influential source\nspreaders and brokers is a key to facilitating large-scale information\ndiffusion cascades. Therefore, this paper explores ways to identify influential\nbrokers from a given social network. By using three social media datasets, we\ninvestigate the characteristics of influential brokers by comparing them with\ninfluential source spreaders and central nodes obtained from centrality\nmeasures. Our results show that (i) most of the influential source spreaders\nare not influential brokers (and vice versa) and (ii) the overlap between\ncentral nodes and influential brokers is small (less than 15%) in Twitter\ndatasets. We also tackle the problem of identifying influential brokers from\ncentrality measures and node embeddings, and we examine the effectiveness of\nsocial network features in the broker identification task. Our results show\nthat (iii) although a single centrality measure cannot characterize influential\nbrokers well, prediction models using node embedding features achieve F$_1$\nscores of 0.35--0.68, suggesting the effectiveness of social network features\nfor identifying influential brokers.\n\n\n\n\nPrevious: DatasetLabel.NEW\nNew: &lt;think&gt;\nOkay, so I need to figure out whether the paper titled \"Identifying Influential Brokers on Social Media from Social Network Structure\" introduces a new dataset. Let me break this down.\n\nFirst, looking at the title, it's about identifying influential brokers in social media using network structure. The abstract mentions they used three social media datasets to study these influencers. They compared brokers with source spreaders and central nodes based on centrality measures.\n\nThe abstract also talks about tackling the problem of identifying brokers from both centrality measures and node embeddings. It evaluates the effectiveness of network features, getting some F1 scores as a result.\n\nSo, I'm trying to see if they created any new dataset or used existing ones. They mention using three datasets: Twitter in their experiments. The paper doesn't seem to introduce any entirely new type of data beyond what's commonly available, like Twitter datasets. They're analyzing these existing datasets with their methods.\n\nTherefore, the paper probably uses existing social media datasets rather than creating a new one.\n&lt;/think&gt;\n\nThe paper does not introduce a newly created dataset; it utilizes existing social media datasets such as Twitter for its analysis.\n\n```json\n{\n    \"label\": \"no_new_dataset\",\n    \"explanation\": \"The paper does not introduce any new datasets. It uses three social media datasets, including Twitter, which are already available data sources.\"\n}\n```\n\n\n---\n\n\n\nImproving Performance of Automatic Keyword Extraction (AKE) Methods\n  Using PoS-Tagging and Enhanced Semantic-Awareness\n\n\n\n  Automatic keyword extraction (AKE) has gained more importance with the\nincreasing amount of digital textual data that modern computing systems\nprocess. It has various applications in information retrieval (IR) and natural\nlanguage processing (NLP), including text summarisation, topic analysis and\ndocument indexing. This paper proposes a simple but effective\npost-processing-based universal approach to improve the performance of any AKE\nmethods, via an enhanced level of semantic-awareness supported by PoS-tagging.\nTo demonstrate the performance of the proposed approach, we considered word\ntypes retrieved from a PoS-tagging step and two representative sources of\nsemantic information - specialised terms defined in one or more\ncontext-dependent thesauri, and named entities in Wikipedia. The above three\nsteps can be simply added to the end of any AKE methods as part of a\npost-processor, which simply re-evaluate all candidate keywords following some\ncontext-specific and semantic-aware criteria. For five state-of-the-art (SOTA)\nAKE methods, our experimental results with 17 selected datasets showed that the\nproposed approach improved their performances both consistently (up to 100% in\nterms of improved cases) and significantly (between 10.2% and 53.8%, with an\naverage of 25.8%, in terms of F1-score and across all five methods), especially\nwhen all the three enhancement steps are used. Our results have profound\nimplications considering the ease to apply our proposed approach to any AKE\nmethods and to further extend it.\n\n\n\n\nPrevious: DatasetLabel.NOT_NEW\nNew: &lt;think&gt;\nOkay, I'm trying to figure out whether the paper \"Improving Performance of Automatic Keyword Extraction (AKE) Methods Using PoS-Tagging and Enhanced Semantic-Awareness\" introduces a newly created dataset. \n\nFirst, looking at the title suggests that it's about improving an existing AKE method, which implies they're working with existing datasets rather than creating new ones.\n\nThe abstract mentions experiments conducted on 17 selected datasets for five SOTA AKE methods. They used these datasets to test their approach but didn't mention any new data collection or creation here. \n\nSo, there's no indication that the paper includes a newly created dataset in its methodology or results section.\n&lt;/think&gt;\n\nThe paper focuses on enhancing existing AKE methods using PoS-tagging and semantic-aware criteria without introducing new datasets.\n\n```json\n{\n    \"label\": \"no_new_dataset\",\n    \"explanation\": \"The paper does not mention any new datasets being created. It evaluates existing 17 datasets with five SOTA AKE methods.\"\n}\n```\n\n\n---\n\n\n\nLOCUS: LOcalization with Channel Uncertainty and Sporadic Energy\n\n\n\n  Accurate sound source localization (SSL) requires consistent multichannel\ndata for reliable degree of arrival (DoA) estimation. However, intermittently\npowered batteryless systems often suffer from incomplete sensor data due to the\nstochastic nature of energy harvesting. Existing methods struggle with missing\nchannels, leading to significant performance degradation. In this paper, we\npropose $\\textit{LOCUS}$, a novel deep learning-based system designed to\nrecover corrupted features for SSL in batteryless systems. $\\textit{LOCUS}$\naddresses missing data by leveraging information entropy estimation and\nconditional interpolation, combining three modules: (1) Information-Weighted\nFocus (InFo), which identifies and quantifies corrupted data elements, (2)\nLatent Feature Synthesizer (LaFS), which synthesizes missing features, and (3)\nGuided Replacement (GRep), which intelligently replaces missing elements while\npreserving valid data. We demonstrate significant performance improvements\nusing two datasets: DCASE and LargeSet, where $\\textit{LOCUS}$ achieves up to\n$36.91\\%$ lower DoA error compared to existing methods. Real-world evaluations\nacross three environments with intermittent power sources show a\n$25.87-59.46\\%$ improvement in performance when channels are stochastically\nmissing. Additionally, we release a 50-hour multichannel dataset to support\nfurther research in SSL.\n\n\n\n\nPrevious: DatasetLabel.NEW\nNew: &lt;think&gt;\nAlright, so I'm trying to figure out whether the paper introduces a newly created dataset. Let's look at the information given.\n\nFirst, the title is \"LOCUS: LOcalization with Channel Uncertainty and Sporadic Energy.\" It mentions an acronym LOCUS, which seems to be the main focus of the paper‚Äîsound source localization using deep learning methods to handle missing data in batteryless systems.\n\nNow looking at the abstract. The authors mention that their method addresses missing channels by leveraging information entropy estimation and conditional interpolation through three modules: InFo, LaFS, and GRep. They demonstrate performance improvements on two datasets called DCASE and LargeSet, achieving up to 36.91% reduction in DoA error compared to existing methods.\n\nThe abstract also includes a real-world evaluation across three environments with intermittent power sources, showing significant performance improvements when channels are stochastically missing. Additionally, the authors mention releasing a 50-hour multichannel dataset called DCASE 2023 Challenge Set v1.0.\n\nFrom this information, it's clear that the paper not only presents their method but also provides new datasets for evaluation‚Äîspecifically the DCASE and LargeSet datasets, along with the newly released DCASE challenge set. These datasets are likely used to test and validate the performance of their proposed system, LOCUS.\n\nTherefore, since the abstract explicitly states the release of these datasets as part of their contribution and they use them in experiments, this paper does introduce a newly created dataset.\n&lt;/think&gt;\n\nThe paper \"LOCUS: LOcalization with Channel Uncertainty and Sporadic Energy\" introduces new datasets for evaluating its proposed method. The authors mention releasing two datasets (DCASE and LargeSet) and the DCASE 2023 Challenge Set v1.0, which are used to validate their system's performance.\n\n```json\n{\n    \"label\": \"new_dataset\",\n    \"explanation\": \"The paper explicitly mentions the release of new datasets (DCASE, LargeSet, and DCASE 2023 Challenge Set v1.0) for evaluating the proposed method and demonstrates its effectiveness using these datasets.\"\n}\n```\n\n\n---\n\n\n\nWhile this is definitely a vibes-based assessment, it does seem like the model does better when it has room to think, so we‚Äôll proceed with this approach.\n\n\n\n\n\n\nNote\n\n\n\nThere are ways to allow for both structured generation and reasoning. I‚Äôll post more on that in the future!\n\n\nWe‚Äôll now write a function to extract the JSON from the model‚Äôs output.\n\n\nCode\nimport contextlib\nimport re\nimport json\n\nJSON_PATTERN = re.compile(r\"```json\\n(.*?)```\", re.DOTALL)\nDIRECT_JSON_PATTERN = re.compile(r\"\\{[^}]*\\}\", re.DOTALL)\n\n\ndef try_extract_json_from_text(text: str) -&gt; tuple[str, dict | None]:\n    if match := JSON_PATTERN.search(text):\n        json_results = match.group(1)\n        with contextlib.suppress(json.JSONDecodeError):\n            return text, json.loads(json_results)\n    if match := DIRECT_JSON_PATTERN.search(text):\n        json_text = match.group(0)\n        with contextlib.suppress(json.JSONDecodeError):\n            return text, json.loads(json_text)\n    return text, None\n\n\n\nprediction = predict_label_without_structured_output(examples[0])\ntry_extract_json_from_text(prediction)\n\n('&lt;think&gt;\\nOkay, so I\\'m trying to figure out whether the paper titled \"An Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for High Dimensional Data\" introduces a newly created dataset. Let me go through this step by step.\\n\\nFirst, I look at the title. The title mentions an algorithm called fuzzy k-means that\\'s been modified with entropy terms and feature weighting to handle high-dimensional data. It doesn\\'t explicitly say anything about introducing new datasets, but it does focus on improving clustering in such data environments, which often involves dealing with irrelevant or less important features.\\n\\nNow, looking at the abstract. The paper discusses modifying the fuzzy k-means algorithm by incorporating two entropy terms: one for within-cluster dispersion and negative entropy to determine clusters, and another to control feature weights because different features have varying contributions. They compare their method\\'s efficacy using various clustering measures on multiple datasets against state-of-the-art methods.\\n\\nHmm, so in both the title and abstract, I don\\'t see any mention of a new dataset being created or introduced. The focus is more on improving an existing algorithm to better handle high-dimensional data rather than introducing entirely new data for analysis.\\n\\nI should also consider whether it\\'s possible that the paper might use standard datasets without explicitly stating so. For example, many clustering algorithms are tested on common datasets like MNIST, CIFAR-10, etc., especially when dealing with high-dimensional data because these datasets have numerous features after preprocessing (like PCA or similar techniques). However, since the abstract doesn\\'t specify which datasets they used or mention any novel datasets, it\\'s more about comparing performance rather than introducing a new dataset.\\n\\nSo, putting this together: The paper presents an improved clustering algorithm but doesn\\'t introduce a new dataset. It evaluates its performance on existing ones, hence likely \"no_new_dataset.\"\\n&lt;/think&gt;\\n\\nThe paper discusses an enhanced fuzzy k-means algorithm designed for high-dimensional data, focusing on improving the clustering process by incorporating entropy terms and feature weighting. While it addresses challenges in handling irrelevant or less significant features, there is no mention of introducing a new dataset. Instead, it evaluates its method against existing datasets using various metrics.\\n\\n```json\\n{\\n    \"label\": \"no_new_dataset\",\\n    \"explanation\": \"The paper does not introduce a newly created dataset; instead, it focuses on improving an algorithm to handle high-dimensional data by modifying the objective function with entropy terms and feature weighting. It evaluates its performance on existing datasets without introducing new ones.\"\\n}\\n```',\n {'label': 'no_new_dataset',\n  'explanation': 'The paper does not introduce a newly created dataset; instead, it focuses on improving an algorithm to handle high-dimensional data by modifying the objective function with entropy terms and feature weighting. It evaluates its performance on existing datasets without introducing new ones.'})\n\n\nLet‚Äôs see how well this works on all the examples we had before\n\nresults = [predict_label_without_structured_output(example) for example in examples]\nparsed_results = [try_extract_json_from_text(result) for result in results]\n[p for p in parsed_results if p[1] is None]\n\n[]\n\n\nWe can see in this example we don‚Äôt have any examples where we don‚Äôt get a valid JSON object (this is why we get back an empty list).\nAlthough we might miss a few examples where we don‚Äôt get a valid JSON object when doing the full dataset, let‚Äôs proceed with this approach since the model does much better when given room to reason.\nWe‚Äôll now use the hosted version of the model to generate labels for the entire dataset. For this version, we‚Äôll use a dedicated Hugging Face inference endpoint, but if we wanted to use the full R1 model, we could use the new Inference Providers feature on the Hub. See this blog post for more information.\n\n\nCode\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(\n    base_url=\"https://tgtdz7g5h3sd1lov.us-east-1.aws.endpoints.huggingface.cloud/v1/\",\n    api_key=os.getenv(\"HF_TOKEN\"),\n)\n\n\n\nrich_print(\n    predict_label_without_structured_output(\n        examples[0], model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\", client=client\n    )\n)\n\n&lt;think&gt;\nAlright, I'm looking at this paper to determine if it introduces a newly created dataset. The title mentions \n\"Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models.\" The word \n\"dataset\" isn't in the title, but the abstract gives more details.\n\nIn the abstract, the authors talk about introducing NuInstruct, which they describe as a novel dataset. It has 91K \nmulti-view video-QA pairs across 17 subtasks. This indicates that they've created a new collection of data \nspecifically for their research. They also mention a SQL-based method for generating instruction-response pairs \nautomatically, which suggests they developed a systematic approach to build this dataset.\n\nFurthermore, the paper introduces a new method called BEV-InMLLM, which uses this dataset. They report experiments \non NuInstruct showing improvements, and they plan to release it for future research. This release plan is another \nindicator that they've created a new dataset intended for broader use.\n\nPutting it all together, the paper clearly states the creation of NuInstruct, its characteristics, and their \nintention to share it. Therefore, it's introducing a new dataset.\n&lt;/think&gt;\n\n```json\n{\n    \"label\": \"new_dataset\",\n    \"explanation\": \"The paper explicitly introduces NuInstruct, a novel dataset with 91K multi-view video-QA pairs \nacross 17 subtasks. The authors describe its creation method and plan to release it for future research, clearly \nindicating the introduction of a new dataset.\"\n}\n```\n\n\n\nWe‚Äôll now sample 3000 examples from the dataset and use the hosted model to generate labels for them.\n\nsample_df = df.sample(3000, seed=42)\n\n\nexamples = sample_df.select(pl.col([\"abstract\", \"title\"])).to_dicts()\n\nWe create a function to predict the labels using the hosted model. We‚Äôll use the stamina library to retry the request if it fails.\n\n\nCode\nimport stamina\nfrom openai import APIConnectionError, APIStatusError\n\n\n@stamina.retry(on=(APIConnectionError, APIStatusError), attempts=3)\ndef predict_hf_endpoint(data: dict[str, str], model: str = \"tgi\", client=client):\n    return predict_label_without_structured_output(data, model, client)\n\n\ndef predict(data):\n    try:\n        return predict_hf_endpoint(data)\n    except Exception as e:\n        print(e)\n        return None\n\n\nGet the results from the model.\n\nfrom tqdm.contrib.concurrent import thread_map\n\nresults = thread_map(predict, examples, max_workers=5)\n\nLet‚Äôs take a look at the first result\n\nrich_print(results[0])\n\n&lt;think&gt;\nOkay, so I need to figure out if the given arXiv paper introduces a newly created dataset. Let's look at the title \nand abstract carefully.\n\nThe title is \"Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models.\" That\nimmediately suggests it's about a dataset related to autonomous driving. The abstract mentions that the paper \nintroduces a dataset called NuInstruct, which has 91K multi-view video-QA pairs across 17 subtasks. Each task \nrequires holistic information like temporal, multi-view, and spatial data, making the challenges higher.\n\nThe authors propose a method using SQL to generate instruction-response pairs automatically, inspired by human \ndriving logic. They also introduce BEV-InMLLM, an end-to-end method that enhances large language models by \nintegrating BEV features, language alignment, and tasks like multi-view, spatial awareness, and temporal semantics.\nThey note that their BEV injection module is plug-and-play for existing MLLMs.\n\nExperiments on NuInstruct show significant improvements over existing MLLMs. The authors also mention releasing the\ndataset for future research.\n\nSo, from the title, abstract, and details, it's clear that NuInstruct is a new dataset they created specifically \nfor their research. They describe it in detail, including the structure and methods used, so it's definitely a \nnewly created dataset aimed at advancing autonomous driving through language models.\n&lt;/think&gt;\n\n```json\n{\n    \"label\": \"new_dataset\",\n    \"explanation\": \"The paper introduces a dataset called NuInstruct with 91K multi-view video-QA pairs across 17 \nsubtasks, each requiring holistic information for robust autonomous driving tasks. The authors detail its structure\nand methods for creation, confirming it as a newly developed dataset.\"\n}\n```\n\n\n\n\ntry_extract_json_from_text(results[0])\n\n('&lt;think&gt;\\nOkay, so I need to figure out if the given arXiv paper introduces a newly created dataset. Let\\'s look at the title and abstract carefully.\\n\\nThe title is \"Holistic Autonomous Driving Understanding by Bird\\'s-Eye-View Injected Multi-Modal Large Models.\" That immediately suggests it\\'s about a dataset related to autonomous driving. The abstract mentions that the paper introduces a dataset called NuInstruct, which has 91K multi-view video-QA pairs across 17 subtasks. Each task requires holistic information like temporal, multi-view, and spatial data, making the challenges higher.\\n\\nThe authors propose a method using SQL to generate instruction-response pairs automatically, inspired by human driving logic. They also introduce BEV-InMLLM, an end-to-end method that enhances large language models by integrating BEV features, language alignment, and tasks like multi-view, spatial awareness, and temporal semantics. They note that their BEV injection module is plug-and-play for existing MLLMs.\\n\\nExperiments on NuInstruct show significant improvements over existing MLLMs. The authors also mention releasing the dataset for future research.\\n\\nSo, from the title, abstract, and details, it\\'s clear that NuInstruct is a new dataset they created specifically for their research. They describe it in detail, including the structure and methods used, so it\\'s definitely a newly created dataset aimed at advancing autonomous driving through language models.\\n&lt;/think&gt;\\n\\n```json\\n{\\n    \"label\": \"new_dataset\",\\n    \"explanation\": \"The paper introduces a dataset called NuInstruct with 91K multi-view video-QA pairs across 17 subtasks, each requiring holistic information for robust autonomous driving tasks. The authors detail its structure and methods for creation, confirming it as a newly developed dataset.\"\\n}\\n```',\n {'label': 'new_dataset',\n  'explanation': 'The paper introduces a dataset called NuInstruct with 91K multi-view video-QA pairs across 17 subtasks, each requiring holistic information for robust autonomous driving tasks. The authors detail its structure and methods for creation, confirming it as a newly developed dataset.'})\n\n\nWe‚Äôll do a bit of cleaning up of the results to get them in a format we can add to our existing dataframe.\n\n\nCode\nparsed_results = [try_extract_json_from_text(result) for result in results]\n\n\n\nparsed_results[:3]\n\n[('&lt;think&gt;\\nOkay, so I need to figure out if the given arXiv paper introduces a newly created dataset. Let\\'s look at the title and abstract carefully.\\n\\nThe title is \"Holistic Autonomous Driving Understanding by Bird\\'s-Eye-View Injected Multi-Modal Large Models.\" That immediately suggests it\\'s about a dataset related to autonomous driving. The abstract mentions that the paper introduces a dataset called NuInstruct, which has 91K multi-view video-QA pairs across 17 subtasks. Each task requires holistic information like temporal, multi-view, and spatial data, making the challenges higher.\\n\\nThe authors propose a method using SQL to generate instruction-response pairs automatically, inspired by human driving logic. They also introduce BEV-InMLLM, an end-to-end method that enhances large language models by integrating BEV features, language alignment, and tasks like multi-view, spatial awareness, and temporal semantics. They note that their BEV injection module is plug-and-play for existing MLLMs.\\n\\nExperiments on NuInstruct show significant improvements over existing MLLMs. The authors also mention releasing the dataset for future research.\\n\\nSo, from the title, abstract, and details, it\\'s clear that NuInstruct is a new dataset they created specifically for their research. They describe it in detail, including the structure and methods used, so it\\'s definitely a newly created dataset aimed at advancing autonomous driving through language models.\\n&lt;/think&gt;\\n\\n```json\\n{\\n    \"label\": \"new_dataset\",\\n    \"explanation\": \"The paper introduces a dataset called NuInstruct with 91K multi-view video-QA pairs across 17 subtasks, each requiring holistic information for robust autonomous driving tasks. The authors detail its structure and methods for creation, confirming it as a newly developed dataset.\"\\n}\\n```',\n  {'label': 'new_dataset',\n   'explanation': 'The paper introduces a dataset called NuInstruct with 91K multi-view video-QA pairs across 17 subtasks, each requiring holistic information for robust autonomous driving tasks. The authors detail its structure and methods for creation, confirming it as a newly developed dataset.'}),\n ('&lt;think&gt;\\nOkay, so I need to figure out whether the paper \"BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis\" introduces a newly created dataset. Let me start by reading the title and abstract carefully.\\n\\nThe title mentions \"BRACE: The Breakdancing Competition Dataset.\" That suggests it\\'s introducing a dataset named BRACE. The abstract goes into more detail about what the dataset is. It says that generative models for audio-conditioned dance motion synthesis are trained, but these models rely on certain assumptions, like strong music-dance correlation and controlled motion data. The paper points out that existing datasets have limitations and introduces BRACE to challenge these assumptions by providing complex human poses, specifically focusing on breakdancing which includes acrobatic moves and tangled postures.\\n\\nThe authors mention that they used data from the Red Bull BC One competition videos. They faced challenges like estimating human keypoints due to the complexity of the dance and multiple cameras. To address this, they used a hybrid labeling pipeline combining deep estimation models and manual annotations to get high-quality keypoint sequences. The result is a dataset with over 3 hours and 30 minutes of densely annotated poses. They tested state-of-the-art methods and found their limitations with complex sequences, arguing that their dataset can advance dance motion synthesis by forcing models to reason more about body structure and movements.\\n\\nSo, putting this together, the paper is introducing a new dataset called BRACE. They created it by compiling dynamic dance sequences from Red Bull BC One videos, faced challenges in data annotation, and developed a hybrid approach to overcome these issues. They then used this dataset to evaluate existing methods and highlight its potential to push the boundaries of dance motion synthesis.\\n\\nI don\\'t see any indication that this dataset isn\\'t new. The abstract clearly states the creation of BRACE, describes the data sources and processing methods, and the purpose of the dataset. Therefore, it\\'s clear that the authors are introducing this dataset as part of their contribution.\\n&lt;/think&gt;\\n\\nThe paper introduces a new dataset called BRACE, which is specifically designed for dance motion synthesis. The authors detail the creation of this dataset, including the challenges faced and the methods used to compile and process the data. Therefore, the paper clearly introduces a newly created dataset.\\n\\n```json\\n{\\n    \"label\": \"new_dataset\",\\n    \"explanation\": \"The paper introduces a new dataset named BRACE, detailing its creation, challenges faced, and the purpose of providing complex dance sequences to advance motion synthesis.\"\\n}\\n```',\n  {'label': 'new_dataset',\n   'explanation': 'The paper introduces a new dataset named BRACE, detailing its creation, challenges faced, and the purpose of providing complex dance sequences to advance motion synthesis.'}),\n ('&lt;think&gt;\\nOkay, I need to determine whether the arXiv paper titled \"RET-CLIP: A Retinal Image Foundation Model Pre-trained with Clinical Diagnostic Reports\" introduces a newly created dataset. Let me start by reading through the title and abstract carefully to extract relevant information.\\n\\nThe title mentions that RET-CLIP is a model pre-trained using clinical diagnostic reports. The abstract elaborates that RET-CLIP was trained on a dataset of 193,865 patients. It highlights that the model focuses on extracting features from color fundus photographs (CFPs) and uses a tripartite optimization strategy covering the left eye, right eye, and patient level.\\n\\nNow, the key point is whether this dataset of 193,865 patients is newly created or if it\\'s referring to an existing dataset. The abstract doesn\\'t explicitly state that this dataset is new; it simply describes it as a dataset used for training. The focus is on the model\\'s architecture and training strategy rather than the dataset\\'s origin.\\n\\nTypically, when a paper mentions training a model on a dataset, they might reference an existing one unless they specify that it\\'s newly collected. Since the abstract doesn\\'t provide details about the dataset\\'s origin, such as whether it\\'s publicly available, if it\\'s proprietary, or if it\\'s a new collection, it\\'s safer to assume that the dataset might not be newly created. However, the exact nature of the dataset isn\\'t clarified, so without explicit information, it\\'s hard to confirm if it\\'s new.\\n\\nBut considering the context, the paper is about medical imaging and clinical reports, which are fields where large datasets are often publicly available or shared. The mention of 193,865 patients could imply a significant dataset, possibly derived from existing public resources. Therefore, it\\'s plausible that the dataset isn\\'t newly created but rather an aggregation or expansion of existing data.\\n\\nIn conclusion, there\\'s insufficient information to confirm that a new dataset was introduced, so the label should be \"no_new_dataset\".\\n&lt;/think&gt;\\n\\n```json\\n{\\n    \"label\": \"no_new_dataset\",\\n    \"explanation\": \"The paper describes a dataset of 193,865 patients used to train the RET-CLIP model but does not explicitly state that this dataset is newly created. It is possible that the dataset is derived from existing public resources or aggregated data in the medical field.\"\\n}\\n```',\n  {'label': 'no_new_dataset',\n   'explanation': 'The paper describes a dataset of 193,865 patients used to train the RET-CLIP model but does not explicitly state that this dataset is newly created. It is possible that the dataset is derived from existing public resources or aggregated data in the medical field.'})]\n\n\n\n\nCode\nlabels_and_explanations = [\n    (result[1].get(\"label\"), result[1].get(\"explanation\"))\n    if result[1] is not None and isinstance(result[1], dict)\n    else (None, None)\n    for result in parsed_results\n]\n\n# Unzip the list of tuples into separate lists\nlabels, explanations = zip(*labels_and_explanations)\nlables = list(labels)\nexplanations = list(explanations)\nsample_df = sample_df.with_columns(\n    pl.Series(lables).alias(\"labels\"),\n    pl.Series(explanations).alias(\"explanations\"),\n)\n\n\n\nsample_df.head(1)\n\n\nshape: (1, 16)\n\n\n\nid\nsubmitter\nauthors\ntitle\ncomments\njournal-ref\ndoi\nreport-no\ncategories\nlicense\nabstract\nversions\nupdate_date\nauthors_parsed\nlabels\nexplanations\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nlist[struct[2]]\ndatetime[ms]\nlist[list[str]]\nstr\nstr\n\n\n\n\n\"2401.00988\"\n\"Xinpeng Ding\"\n\"Xinpeng Ding and Jinahua Han a‚Ä¶\n\"Holistic Autonomous Driving Un‚Ä¶\nnull\nnull\nnull\nnull\n\"cs.CV\"\n\"http://arxiv.org/licenses/none‚Ä¶\n\"¬†¬†The rise of multimodal large‚Ä¶\n[{\"v1\",\"Tue, 2 Jan 2024 01:54:22 GMT\"}]\n2024-01-03 00:00:00\n[[\"Ding\", \"Xinpeng\", \"\"], [\"Han\", \"Jinahua\", \"\"], ‚Ä¶ [\"Li\", \"Xiaomeng\", \"\"]]\n\"new_dataset\"\n\"The paper introduces a dataset‚Ä¶\n\n\n\n\n\n\nLet‚Äôs take a look at the distribution of the labels.\n\nsample_df.select(pl.col(\"labels\").value_counts()).unnest(\"labels\")\n\n\nshape: (3, 2)\n\n\n\nlabels\ncount\n\n\nstr\nu32\n\n\n\n\n\"new_dataset\"\n648\n\n\n\"no_new_dataset\"\n2350\n\n\nnull\n2\n\n\n\n\n\n\nWe only get a few examples where the output doesn‚Äôt match the labels we want. We can filter these out.\n\nsample_df = sample_df.filter(pl.col(\"labels\").is_in([\"new_dataset\", \"no_new_dataset\"]))\n\nWe‚Äôll now convert the dataframe to a Hugging Face dataset and push it to the Hub.\n\n\nCode\nfrom datasets import Dataset, Features, Value, ClassLabel\nds = Dataset.from_polars(\n    sample_df.select([\"id\", \"title\", \"abstract\", \"labels\", \"explanations\"]),\n)\nlarge_string_columns = [\n    k\n    for k, v in ds.features.items()\n    if isinstance(v, Value) and v.dtype == \"large_string\"\n]\nfor column in large_string_columns:\n    ds = ds.cast_column(column, Value(\"string\"))\nds = ds.cast_column(\"labels\", ClassLabel(names=[\"new_dataset\", \"no_new_dataset\"]))\nds.push_to_hub(\"davanstrien/arxiv-new-datasets\", token=os.getenv(\"HF_TOKEN\"))\n\n\nHere is the resulting dataset."
  },
  {
    "objectID": "posts/2025/deepseek/distil-deepseek-modernbert.html#fine-tuning-modernbert",
    "href": "posts/2025/deepseek/distil-deepseek-modernbert.html#fine-tuning-modernbert",
    "title": "Distiling DeepSeek reasoning to ModernBERT classifiers",
    "section": "Fine tuning ModernBERT",
    "text": "Fine tuning ModernBERT\nSince the focus of this blog is on the data generation part I won‚Äôt go into too much detail here but you can see the code and the final results below.\n\n\nCode\n%pip install datasets setfit transformers accelerate --upgrade\n%pip install flash-attn --no-build-isolation\n\nfrom datasets import load_dataset\nfrom evaluate import load\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    EarlyStoppingCallback,\n)\nimport numpy as np\nfrom evaluate import load\n\n# Load data\nds = load_dataset(\"davanstrien/arxiv-new-datasets\", split=\"train\")\n\n# label info\nlabels = ds.features[\"labels\"].names\nlabel2id = {label: i for i, label in enumerate(labels)}\nid2label = {i: label for label, i in label2id.items()}\n\n# prep a text column combining title and abstract\nds = ds.map(lambda x: {\"text\": x[\"title\"] + \" \" + x[\"abstract\"]})\nds = ds.train_test_split(test_size=0.2, stratify_by_column=\"labels\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n\n\n# Tokenize function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n\n\n# Tokenize datasets\ntokenized_datasets = ds.map(tokenize_function, batched=True)\n\n# Load metrics\naccuracy = load(\"accuracy\")\nf1 = load(\"f1\")\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n\n    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n    f1_score = f1.compute(\n        predictions=predictions, references=labels, average=\"weighted\"\n    )\n\n    return {\n        \"accuracy\": accuracy_score[\"accuracy\"],\n        \"f1\": f1_score[\"f1\"],\n    }\n\n\n# Load model with increased dropout\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"answerdotai/ModernBERT-base\",\n    num_labels=2,\n    label2id=label2id,\n    id2label=id2label,\n)\n\n# Define improved training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=3e-5,  # Slightly higher initial learning rate\n    per_device_train_batch_size=8,  # Reduced batch size\n    per_device_eval_batch_size=64,\n    num_train_epochs=20,  # Reduced epochs\n    # Learning rate schedule\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    # Evaluation and saving\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    # Regularization\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    label_smoothing_factor=0.1,\n    # Logging\n    logging_dir=\"./logs\",\n    logging_strategy=\"epoch\",\n)\n\n# Create data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Initialize Trainer with early stopping\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[\n        EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)\n    ],\n)\n# Evaluate the model\neval_results = trainer.evaluate()\nprint(\"\\nFinal evaluation results:\", eval_results)\n\n# Save the best model\ntrainer.save_model(\"./best_model\")\n\n\nFinal evaluation results: {'eval_loss': 0.32631951570510864, 'eval_accuracy': 0.945, 'eval_f1': 0.9442747450661002, 'eval_runtime': 5.8106, 'eval_samples_per_second': 103.26, 'eval_steps_per_second': 1.721, 'epoch': 10.0}"
  },
  {
    "objectID": "posts/2024/12/23/fineweb-filter-polars.html",
    "href": "posts/2024/12/23/fineweb-filter-polars.html",
    "title": "Filtering FineWeb2 using Polars",
    "section": "",
    "text": "Recently FineWeb2 was released. FineWeb2 builds on the previous FineWeb dataset to add data for many languages. Building on this work we recently launched a community effort to build educational quality filters for many languages. See this blog post for more details."
  },
  {
    "objectID": "posts/2024/12/23/fineweb-filter-polars.html#loading-the-data-in-polars",
    "href": "posts/2024/12/23/fineweb-filter-polars.html#loading-the-data-in-polars",
    "title": "Filtering FineWeb2 using Polars",
    "section": "Loading the data in Polars",
    "text": "Loading the data in Polars\nWe can directly load the data from the Hugging Face Hub using the hf:// protocol. In this case we‚Äôll just load the train file for the scots language. We‚Äôll use read_parquet to load the data for now but we‚Äôll see below a better way to load the data if you are working with large datasets.\n\ndf = pl.read_parquet(f\"hf://datasets/HuggingFaceFW/fineweb-2/{scots[-1]}\")\n\nLet‚Äôs take a look at the data. We can see we have a number of columns including the actual text but also some other metadata fields that could be useful for filtering.\n\ndf.head(5)\n\n\nshape: (5, 11)\n\n\n\ntext\nid\ndump\nurl\ndate\nfile_path\nlanguage\nlanguage_score\nlanguage_script\nminhash_cluster_size\ntop_langs\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nf64\nstr\ni64\nstr\n\n\n\n\n\"2010 All Ford Mustangs Car Sho‚Ä¶\n\"&lt;urn:uuid:06f10aff-f1da-4d33-b‚Ä¶\n\"CC-MAIN-2013-20\"\n\"http://www.allfordmustangs.com‚Ä¶\n\"2013-05-23T16:34:05Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n0.764794\n\"Latn\"\n1258\n\"{\"sco_Latn_score\": 0.764793634‚Ä¶\n\n\n\"Interested in France? We'll se‚Ä¶\n\"&lt;urn:uuid:abc6bfe8-7af5-40b9-9‚Ä¶\n\"CC-MAIN-2013-20\"\n\"http://www.tripadvisor.com/All‚Ä¶\n\"2013-05-23T16:36:10Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n0.651096\n\"Latn\"\n12\n\"{\"sco_Latn_score\": 0.651095628‚Ä¶\n\n\n\"Sherlock Holmes Sherlock Holme‚Ä¶\n\"&lt;urn:uuid:11ceff04-f5f5-418c-8‚Ä¶\n\"CC-MAIN-2014-10\"\n\"http://sco.wikipedia.org/wiki/‚Ä¶\n\"2014-03-08T05:12:30Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n1.000008\n\"Latn\"\n58\n\"{\"sco_Latn_score\": 1.000008225‚Ä¶\n\n\n\"Munster History[eedit | eedit ‚Ä¶\n\"&lt;urn:uuid:5fd5fa85-72b1-43d3-b‚Ä¶\n\"CC-MAIN-2014-15\"\n\"http://sco.wikipedia.org/wiki/‚Ä¶\n\"2014-04-19T09:31:48Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n1.00001\n\"Latn\"\n79\n\"{\"sco_Latn_score\": 1.000009536‚Ä¶\n\n\n\"Snawbuirdin Frae Wikipedia Sna‚Ä¶\n\"&lt;urn:uuid:72c97fcb-4820-4a52-b‚Ä¶\n\"CC-MAIN-2014-15\"\n\"http://sco.wikipedia.org/wiki/‚Ä¶\n\"2014-04-19T09:31:00Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n1.00001\n\"Latn\"\n66\n\"{\"sco_Latn_score\": 1.000010013‚Ä¶\n\n\n\n\n\n\nWe can do some simple EDA style analysis if we want. For example, we can look at the distribution of the language scores.\n\ndf.select(pl.col(\"language_score\")).describe()\n\n\nshape: (9, 2)\n\n\n\nstatistic\nlanguage_score\n\n\nstr\nf64\n\n\n\n\n\"count\"\n75821.0\n\n\n\"null_count\"\n0.0\n\n\n\"mean\"\n0.537262\n\n\n\"std\"\n0.214123\n\n\n\"min\"\n0.300002\n\n\n\"25%\"\n0.371339\n\n\n\"50%\"\n0.465798\n\n\n\"75%\"\n0.634602\n\n\n\"max\"\n1.00001\n\n\n\n\n\n\nDo a groupby year of dump and language score and plot a bar chart to see if there is a trend.\n\ndf.with_columns(\n    pl.col(\"dump\").str.extract(r\"(\\d{4})\").cast(pl.Utf8).alias(\"year\")\n).group_by(\"year\").agg(pl.col(\"language_score\").mean()).sort(\n    \"year\", descending=True\n).plot.bar(x=\"year\", y=\"language_score\")"
  },
  {
    "objectID": "posts/2024/12/23/fineweb-filter-polars.html#heuristics-for-filtering-for-higher-educational-quality-in-fineweb2",
    "href": "posts/2024/12/23/fineweb-filter-polars.html#heuristics-for-filtering-for-higher-educational-quality-in-fineweb2",
    "title": "Filtering FineWeb2 using Polars",
    "section": "Heuristics for filtering for higher educational quality in FineWeb2",
    "text": "Heuristics for filtering for higher educational quality in FineWeb2\nWhilst the authors of FineWeb2 aimed to do general quality filtering, there are often additional heuristics that can be used to filter for higher educational quality. For example, we can use the tld to filter for higher quality websites. We can also use the url to filter for higher quality websites. Many of these heuristics will require some domain knowledge for a particular language and the web ecosystem for tha language.\nThe top level domain (tld) is a good heuristic for filtering for higher quality websites. The top level domain is the part of the url that is after the last dot. For example, the tld of https://www.wikipedia.org/ is org. This often corresponds to a country or organization. For example, ac.uk is the UK‚Äôs higher education domain. We can use this to filter for higher quality websites.\nWe can do this by mapping the url column to the tld and then filtering for the tlds we‚Äôre interested in. Let‚Äôs add a new column with the tld and then filter for the tlds we‚Äôre interested in.\n\ndf = df.with_columns(\n    pl.col(\"url\").map_elements(lambda x: get_tld(x), return_dtype=pl.Utf8).alias(\"tld\")\n)\n\n\nimport altair as alt\n\ndf.select(\"tld\").to_series().value_counts(sort=True).sort(\n    \"count\", descending=True\n).head(20).plot.bar(\n    x=alt.X(\"tld\", sort=\"-y\"),  # Sort x-axis based on y values in descending order\n    y=\"count\",\n)\n\n\n\n\n\n\n\nWe may already have some knowledge or intuitions about the tlds that are more likely to be higher quality. For example .us is relatively high, this is likely partially due this domain being more present in the Web generally. We may also see some personal blogs using this domain. Let‚Äôs take a look at a few examples.\n\ndf.filter(pl.col(\"tld\").str.contains(\"us\")).sort(\n    \"language_score\", descending=True\n).select(\"url\").to_series().to_list()[:30]\n\n['https://coremc.us/forno-microonde-incasso.html',\n 'https://www.awesomedownloadfilestoday.us/1141-haircut-places-near-my-location.html',\n 'https://www.awesomedownloadfilestoday.us/1376-haircut-near-my-location.html',\n 'https://www.awesomedownloadfilestoday.us/1857-short-haircuts-for-fine-straight-hair.html',\n 'https://www.awesomedownloadfilestoday.us/2081-twa-styles-4c-hair.html',\n 'http://winserver.us/mid-century-modern-front-door-colors/mid-century-modern-front-door-colors-mid-century-modern-front-doors-door-colors-handles-mi-mid-century-modern-front-door-colours/',\n 'https://www.awesomedownloadfilestoday.us/3450-hair-styles-for-thick-short-hair.html',\n 'https://notwttodaytes.us/casa-mezcal-mexican-grill-cantina.html',\n 'https://www.awesomedownloadfilestoday.us/1857-short-haircuts-for-fine-straight-hair.html',\n 'https://www.awesomedownloadfilestoday.us/1737-short-haircuts-for-curly-thick-hair.html',\n 'http://uggbootsclearanceoutlet.us/jaguar-xj-sport-2003-2003-jaguar-xj-car-for-sale-in.html',\n 'https://www.awesomedownloadfilestoday.us/5489-hair-colour-and-styles-for-short-hair.html',\n 'https://www.awesomedownloadfilestoday.us/6213-short-haircuts-for-thick-hair-pictures.html',\n 'https://www.awesomedownloadfilestoday.us/4993-short-haircuts-for-thick-curly-frizzy-hair.html',\n 'https://www.awesomedownloadfilestoday.us/1773-short-hair-styles-for-thick-wavy-hair.html',\n 'https://www.awesomedownloadfilestoday.us/6129-short-haircuts-for-very-fine-thin-hair.html',\n 'https://www.awesomedownloadfilestoday.us/1808-haircuts-for-straight-thin-hair.html',\n 'https://www.awesomedownloadfilestoday.us/4478-best-short-haircuts-for-thick-coarse-hair.html',\n 'https://www.awesomedownloadfilestoday.us/9513-short-haircuts-for-thin-hair.html',\n 'https://www.awesomedownloadfilestoday.us/3414-short-sassy-haircuts-for-thin-hair.html',\n 'https://www.awesomedownloadfilestoday.us/4630-very-short-haircuts-for-thin-hair.html',\n 'https://joesstuff.us/search.php?mfr=141',\n 'https://www.awesomedownloadfilestoday.us/3738-short-hair-styles-for-thick-curly-hair.html',\n 'https://layartancep.us/twa-hairstyles.html',\n 'https://www.awesomedownloadfilestoday.us/1101-best-haircuts-for-fine-thin-hair.html',\n 'https://www.awesomedownloadfilestoday.us/4963-short-haircut-for-fine-thin-hair.html',\n 'https://www.awesomedownloadfilestoday.us/4963-short-haircut-for-fine-thin-hair.html',\n 'https://www.awesomedownloadfilestoday.us/5010-haircuts-for-coarse-wavy-hair.html',\n 'https://www.awesomedownloadfilestoday.us/5201-best-short-haircuts-for-fine-thin-hair.html',\n 'https://www.awesomedownloadfilestoday.us/1911-haircut-for-round-face-and-thin-hair.html']\n\n\nThese don‚Äôt look super promising! Some domains where we might expect higher quality text for scots are the .sco domain which is a domain for websites relating to Scotland.\n\ndf.filter(pl.col(\"tld\").str.contains(\"sco\")).sort(\n    \"language_score\", descending=True\n).select(\"url\").to_series().to_list()[:30]\n\n['https://stormplay.scot/sco/aboot.html',\n 'https://www.makforrit.scot/2020/08/29/anent-the-scots-wikipedia-an-sundays-editathon/',\n 'https://www.makforrit.scot/2018/12/23/daein-it-yersel/',\n 'https://www.makforrit.scot/2019/09/22/uisin-oor-vyce-hou-we-can-gar-political-action-on-scots-inevitable/',\n 'https://www.makforrit.scot/2018/02/03/naewey-tae-bide/',\n 'https://www.makforrit.scot/',\n 'https://www.makforrit.scot/2018/01/27/than-an-nou-poverty-makkin-dae-an-leukin-out-for-ilk-ither/',\n 'https://www.makforrit.scot/',\n 'https://salvo.scot/the-scottis-constitutional-covin/',\n 'https://amylord.scot/gd/hello-welcome/',\n 'https://www.makforrit.scot/category/scotland/',\n 'https://projects.handsupfortrad.scot/scotslanguageawards/gies-a-scots-phrase-day-2021/',\n 'https://scoblog.stormplay.scot/t3ngist-is-gaunae-need-tae-be-delayed.html',\n 'https://www.makforrit.scot/2019/10/29/halloween/',\n 'https://www.makforrit.scot/category/history/',\n 'https://www.makforrit.scot/2018/11/19/three-days-in-october/',\n 'http://mindyerlanguage.scot/teachin',\n 'http://mindyerlanguage.scot/category/video',\n 'https://scoblog.stormplay.scot/rossies-3d-an-t3ngist-are-gaunae-be-delayed.html',\n 'https://www.gov.scot/publications/consultation-scots-government-commitments-tae-gaelic-scots-scots-langages-bill/pages/2/',\n 'https://projects.handsupfortrad.scot/scotslanguageawards/nominations-open-for-scots-language-awards-2020/',\n 'https://scoblog.stormplay.scot/were-gaunae-mak-big-chynges-tae-wir-main-wabsteid-an-a-cynge-o-hou-we-dae-things.html',\n 'https://newsnet.scot/archive/mono-or-stereo/',\n 'https://scoblog.stormplay.scot/happy-birthday-gamerstorm.html',\n 'http://mindyerlanguage.scot/aboot',\n 'https://www.makforrit.scot/category/opinion/',\n 'https://projects.handsupfortrad.scot/scotslanguageawards/gies-a-scots-phrase-day-2022/',\n 'https://newsnet.scot/archive/scots-railweys-scots-leids-an-scots-cairtes/',\n 'https://www.makforrit.scot/author/jamie/',\n 'https://stormplay.scot/games/pinkeye/sco/scotland.html']\n\n\nEven inside these URLs we can see some scots language so this is promising.\nOne of the issues with some of the Scots data in FineWeb2 is that it is in the wrong language. One way we can try and get a sense of where better language data might be in FineWeb2 is to look at the tlds that have the highest language scores. We can do this by grouping by tld and then taking the mean of the language scores. We can then filter for the tlds that have more than 50 row to make sure we‚Äôre considering the tlds that have a good amount of data.\n\n(\n    df.group_by(\"tld\")\n    .agg(\n        [\n            pl.col(\"language_score\").count().alias(\"count\"),\n            pl.col(\"language_score\").mean().alias(\"language_score\"),\n        ]\n    )\n    .filter(pl.col(\"count\") &gt; 50)  # Replace n with your desired minimum count\n    .sort(\"language_score\", descending=True)\n)\n\n\nshape: (41, 3)\n\n\n\ntld\ncount\nlanguage_score\n\n\nstr\nu32\nf64\n\n\n\n\n\"scot\"\n102\n0.998978\n\n\n\"ac.uk\"\n255\n0.95732\n\n\n\"org.uk\"\n267\n0.926128\n\n\n\"org\"\n8806\n0.814764\n\n\n\"co.uk\"\n659\n0.770529\n\n\n\"blogspot.com\"\n561\n0.65765\n\n\n\"top\"\n85\n0.581157\n\n\n\"eu\"\n275\n0.558302\n\n\n\"de\"\n362\n0.544635\n\n\n\"club\"\n807\n0.543638\n\n\n\"co\"\n2258\n0.530558\n\n\n\"nl\"\n1247\n0.52335\n\n\n\"ca\"\n75\n0.521135\n\n\n\"com.co\"\n54\n0.520628\n\n\n\"ie\"\n52\n0.514327\n\n\n\"info\"\n15820\n0.506197\n\n\n\"com.au\"\n88\n0.505842\n\n\n\"com\"\n25240\n0.500029\n\n\n\"me\"\n4114\n0.490317\n\n\n\"online\"\n182\n0.489941\n\n\n\"mobi\"\n183\n0.484193\n\n\n\"pl\"\n78\n0.475017\n\n\n\"net\"\n3416\n0.473797\n\n\n\"es\"\n56\n0.466831\n\n\n\"tk\"\n53\n0.461307\n\n\n\"fr\"\n141\n0.461128\n\n\n\"it\"\n79\n0.460331\n\n\n\"site\"\n832\n0.456072\n\n\n\"xyz\"\n122\n0.450356\n\n\n\"co.za\"\n66\n0.450212\n\n\n\"store\"\n119\n0.443785\n\n\n\"in\"\n135\n0.439045\n\n\n\"co.ke\"\n135\n0.433248\n\n\n\"us\"\n4655\n0.43162\n\n\n\"pro\"\n131\n0.431553\n\n\n\"pages.dev\"\n92\n0.422582\n\n\n\"ru\"\n2110\n0.416347\n\n\n\"live\"\n64\n0.404111\n\n\n\"edu\"\n102\n0.386884\n\n\n\"website\"\n54\n0.371444\n\n\n\"cn\"\n97\n0.366857\n\n\n\n\n\n\nWe can see some other potentially promising tlds. For example, ac.uk is the UK‚Äôs higher education domain. We can take a look at the urls that have this tld.\n\ndf.filter(pl.col(\"tld\").str.contains(\"ac.uk\")).sort(\n    \"language_score\", descending=True\n).select(\"url\").to_series().to_list()[:30]\n\n['https://www.scottishcorpus.ac.uk/document/?documentid=1699',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1759',\n 'https://www.abdn.ac.uk/elphinstone/kist/search/display.php?sblk65.dat',\n 'https://www.abdn.ac.uk/elphinstone/kist/display/folk-history/357/',\n 'https://scotslanguagepolicy.ac.uk/warkshoaps/',\n 'https://scotslanguagepolicy.ac.uk/survey-final-weekend/',\n 'http://www.abdn.ac.uk/elphinstone/kist/search/display.php?fhrg01.dat',\n 'https://www.abdn.ac.uk/elphinstone/kist/display/761/',\n 'https://scotslanguagepolicy.glasgow.ac.uk/hae-yer-say/',\n 'http://www.abdn.ac.uk/elphinstone/kist/search/display.php?lwee66.dat',\n 'https://scotslanguagepolicy.ac.uk/jist-fir-burns-nicht/',\n 'https://scotslanguagepolicy.ac.uk/aboot/',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=122',\n 'http://www.abdn.ac.uk/elphinstone/kist/search/display.php?bgre04.dat',\n 'http://www.abdn.ac.uk/elphinstone/kist/search/display.php?arob01.dat',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1713&highlight=athort',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1695&highlight=projeck',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1709&highlight=projeck',\n 'https://www.abdn.ac.uk/elphinstone/kist/display/work/938/',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1714',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1739',\n 'https://www.abdn.ac.uk/elphinstone/kist/display/work/341/',\n 'http://www.abdn.ac.uk/elphinstone/kist/search/display.php?acru01.dat',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1697&highlight=aroon',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1742&highlight=projeck',\n 'https://www.abdn.ac.uk/elphinstone/kist/search/display.php?kmac01.dat',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1704&highlight=projeck',\n 'https://scottishcorpus.ac.uk/document/?documentid=1710',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1725&highlight=direck',\n 'https://www.scottishcorpus.ac.uk/document/?documentid=1715&highlight=projeck']\n\n\nIn this case using some EDA and domain knowledge we can filter for the tlds which are likely to be:\n\nin the scots language\nhigher quality educational websites\n\nWe can reduce the FineWeb2 dataset to only include the rows that have these tlds.\n\ngood_tlds = [\"sco\", \"ac.uk\", \"org.uk\", \"org\"]\n\n\ndf.filter(pl.col(\"tld\").is_in(good_tlds)).sort(\"language_score\", descending=True).head(\n    5\n)\n\n\nshape: (5, 12)\n\n\n\ntext\nid\ndump\nurl\ndate\nfile_path\nlanguage\nlanguage_score\nlanguage_script\nminhash_cluster_size\ntop_langs\ntld\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nf64\nstr\ni64\nstr\nstr\n\n\n\n\n\"Snawbuirdin Frae Wikipedia Sna‚Ä¶\n\"&lt;urn:uuid:72c97fcb-4820-4a52-b‚Ä¶\n\"CC-MAIN-2014-15\"\n\"http://sco.wikipedia.org/wiki/‚Ä¶\n\"2014-04-19T09:31:00Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n1.00001\n\"Latn\"\n66\n\"{\"sco_Latn_score\": 1.000010013‚Ä¶\n\"org\"\n\n\n\"Banner o the Sahrawi Arab Demo‚Ä¶\n\"&lt;urn:uuid:67052692-6020-4870-9‚Ä¶\n\"CC-MAIN-2014-15\"\n\"http://sco.wikipedia.org/wiki/‚Ä¶\n\"2014-04-24T06:38:13Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n1.00001\n\"Latn\"\n27\n\"{\"sco_Latn_score\": 1.000010013‚Ä¶\n\"org\"\n\n\n\"Potos√≠ is a ceety an the caipi‚Ä¶\n\"&lt;urn:uuid:e49b07bb-d7c9-4905-b‚Ä¶\n\"CC-MAIN-2014-15\"\n\"http://sco.wikipedia.org/wiki/‚Ä¶\n\"2014-04-21T15:05:27Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n1.00001\n\"Latn\"\n34\n\"{\"sco_Latn_score\": 1.000010013‚Ä¶\n\"org\"\n\n\n\"Port Moresby Port Moresby (Ing‚Ä¶\n\"&lt;urn:uuid:bb6b995d-b3e8-4dcd-9‚Ä¶\n\"CC-MAIN-2014-35\"\n\"http://sco.wikipedia.org/wiki/‚Ä¶\n\"2014-08-30T16:16:49Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n1.00001\n\"Latn\"\n80\n\"{\"sco_Latn_score\": 1.000010013‚Ä¶\n\"org\"\n\n\n\"Seville Seville is a ceety in ‚Ä¶\n\"&lt;urn:uuid:cdcca31a-693e-463b-a‚Ä¶\n\"CC-MAIN-2014-42\"\n\"http://sco.wikipedia.org/wiki/‚Ä¶\n\"2014-10-22T21:45:17Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"sco\"\n1.00001\n\"Latn\"\n31\n\"{\"sco_Latn_score\": 1.000010013‚Ä¶\n\"org\"\n\n\n\n\n\n\n\nfiltered_df = df.filter(pl.col(\"tld\").is_in(good_tlds)).sort(\n    \"language_score\", descending=True\n)\n\nWe can now save the filtered data to a new file. We‚Äôll save the ids of the rows that are in the filtered dataset to a file. These ids can then be used to upload additional filtered data to the Argilla dataset for the language we‚Äôre working on.\n\nwith open(\"good_ids\", \"w\") as f:\n    for id in filtered_df.select(\"id\").to_series().to_list():\n        f.write(f\"{id}\\n\")"
  },
  {
    "objectID": "posts/2024/12/23/fineweb-filter-polars.html#filtering-other-languages",
    "href": "posts/2024/12/23/fineweb-filter-polars.html#filtering-other-languages",
    "title": "Filtering FineWeb2 using Polars",
    "section": "Filtering other languages",
    "text": "Filtering other languages\nWe can also use the same techniques to filter other languages. Some languages have a lot of data and so we can use the scan_parquet function to create a LazyFrame this will avoid loading all the data into memory. In addition, Polars will perform query optimizations on the LazyFrame. This will make the code we use for filtering more efficient without much work on our part.\n\ndef get_paths_for_language(language: str):\n    return [\n        path\n        for path in list_repo_files(\"HuggingFaceFW/fineweb-2\", repo_type=\"dataset\")\n        if path.endswith(\"parquet\")\n        and \"removed\" not in path\n        and \"train\" in path\n        and language in path\n    ]"
  },
  {
    "objectID": "posts/2024/12/23/fineweb-filter-polars.html#filtering-with-a-higher-language-score",
    "href": "posts/2024/12/23/fineweb-filter-polars.html#filtering-with-a-higher-language-score",
    "title": "Filtering FineWeb2 using Polars",
    "section": "Filtering with a higher language score",
    "text": "Filtering with a higher language score\nSome language in fineweb2 are not identified as the correct language. Language identification is still not a ‚Äúsolved‚Äù problem but we may be able to use a higher confidence filter to get a set of data that is more likely to be the correct language. We can then label this data for the educational quality of the text without having to remove as many examples as being in the incorrect language.\n\npaths = get_paths_for_language(\"asm\")\npaths\n\n['data/asm_Beng/train/000_00000.parquet',\n 'data/asm_Latn/train/000_00000.parquet']\n\n\nLet‚Äôs load the data for the Assamese language using only the train file.\n\ndf = pl.read_parquet(f\"hf://datasets/HuggingFaceFW/fineweb-2/{paths[-1]}\")\n\n\ndf.shape\n\n(1104, 11)\n\n\nWe can use the describe function to get a sense of the distribution of the language scores.\n\ndf.select(\"language_score\").describe()\n\n\nshape: (9, 2)\n\n\n\nstatistic\nlanguage_score\n\n\nstr\nf64\n\n\n\n\n\"count\"\n1104.0\n\n\n\"null_count\"\n0.0\n\n\n\"mean\"\n0.829071\n\n\n\"std\"\n0.231866\n\n\n\"min\"\n0.303687\n\n\n\"25%\"\n0.660899\n\n\n\"50%\"\n0.970777\n\n\n\"75%\"\n0.995925\n\n\n\"max\"\n0.999965\n\n\n\n\n\n\nYou can see that compared to some other languages the mean language score is quite low. We might be able to get a better subset of data by filtering for a higher language score. Let‚Äôs take a look at some examples of the text that have a high language score. This can help give us a sense of what threshold might have less false positives.\n\nfrom rich import print as rprint\n\nexamples_to_show = 3\n\nrprint(\n    df.filter(pl.col(\"language_score\") &gt; 0.9)\n    .head(examples_to_show)\n    .select(\"text\")\n    .to_series()\n    .to_list()\n)\n\n[\n    'eitu ajir pora 2 bosor agor kotha moi NIT r pora pass out hoisu just. Vaal job nopowa baabe moi keidinmaanor \nbabe temporary hisape eta national company t humai asilu. Tate moi taniya k log palu , tai tate as aadvise r hisape\nhumaise.Prothom dekhate taik kiba vaal lagi gol. Dekha t tai bor dhuniya, mihi gulopia gaal, dudu keita niomia \nakaror, khali olop sapor. jetiya tai r logot kotha pati thaku, tetiya moi issa koi pen tu tolot palai dio, aru tai \njetiya pen tu uthaboloi hauli diye moi tair dudu keitar dorshon koru aru gharat jai tair kotha vabi pura haat \nmaaru. Eitu mur kiba eta routine r dore hoi goisil .\\nEdin moi duporia duty t kaam koi vagor logat olop rest lobor \nbabe kahote thoka rest roomot goi bohilu, ami praye kaam koi koi boar hole rest roomot goi olop hui ahi fresh hoi \nkamot lagi jau. Restroom mane eta medium size r room aru ekhon soki aru bisona. Hadharonote ejon rest koi thoka \nhomoyot oin manuh restroomoloi nahe. Heidinau moi tenekoi ahi restroomor bisonat bagor dilu, hui thakute ketiya \ntuponi ahi gol gomei napalu. Hothate kiba eta gaat loga jen pai soku khul khale, dekhilu taniyai mur pantor uporote\ntika khon sui ase, tai gomei pua nasil moi har pua buli, lahe lahe tai haat khon mur jounangor uporot muharibo \ndhorile, tai pura habodhane moi jate haar napau tenekoi sui asil, mur bor moja lagil, vabilu aji hopun pura \nhobo.Moi suddenly huar pora uthi bohi golu, tai sok khai uthil aru lajot ronga poi gol, tai haat khon mur baridalor\nuporor pora putkoi atorai dile aru muk sorry sorry buli kobo dhorile. Moi kolu sorry kole ki hobo tumi mur tika aru\nlingo duita sula dekhun moiu tumar bostu bilaak soom.Tai sage bhobai nasil enekua situation ahi jabo pare buli , \nheye tai thor lagi mur pine saie thakil.Moi bisonat bohiye bhori duta bisonar pora nomai dilu aru dui hatere dhori \ntaik mur usorot loi ahilu, tai kole ki koribo khujisa rubul. Moi kolu moi just badla lom tumi mur ji ji suisa moiu \ntumaar soom buli koi haat duta tair pithir pora nomai thoi dui tika r uporot tholu aru jure jure tika khon tipibo \ndhorilu, ki mast gand asil tair mur bari dall pantor vitorot ei for forai asil.Lahe lahe moi hat dukhonere tair \ntika r major angso tu anguli etare subo dhorilu aru anguli tu aru honmukhor fale koi diyat moi tair boos khon feel \nkoibo dhoilu . Mor tair boos khon suar bohut mon hobo dhorile aru tair salwaror pant tu tolfale tanibo dhorilu \nkintu kokalot rosire gathi thua babe tolfale nahil. Moi eibaar tair kurta tu uporoloi uthai dilu aru tair salwaror \nrosi dal khuli dilu ,nije nije eibaar pant tu tololoi naami ahil. moi tair boga vorikhon sai aru thakibo nuarilu , \nbisonar pora nami tair thigh khonot pura suma khabo dhorilu, aru maje maje kamuribo dhorilu, laheke aru olop \nuporoloi juat tair heujiya rongor panty tu dekha palu , panty r uporote moi tair boos khonot suma khabo dhorilu, \nolop homoi teneke thokar pisot moi tair panty tu tolfale nomai dilu aru taniya r mukholoi salu , tai mur kando \nkarkhana bilakor pora moja pai ase buli gam palu. Tair booskhon gulopia boosor uporor sulikhini shave kora kintu \nkahe kahe thoka suli khini thaki goisil. Bor moja gundho eta ahisil booskhonor pora, moi tairboos khon dui hatere \nmeli dhori boosor uporor angso tu jivare subo dhorilu, tair gaat current loga r nisinake jopiai uthil , tair val \nloga buli gam pai moi aru jure jure boos khon supibo dhorilu, taik moi bisonar uporot uthai dilu, salwaror pant \nkorobat poi rol , aru tair kurta tu petoloike uthai akou moi tair boos khonot mukh maribo dhorilu, jivare boos khon\nseleki seleki , right handor anguli eta tair pokoror futat humai dilu, ki masti lagisil kintu tai bohut dukh pale \nkarone moi pokoror pora anguli tu ulai dibo loga hol.\\nEibaar moi mur pantu khuli bari dal ulai hatere sui sui \nboosot humuar babe ready koibo dhorilu. Moi tair kurta tu ekebare khuli dilu, aru boga bra tu tololoi tani dilu, \ntair dudu duta moi bhobatkoi olop horu, holeo moi dui hater pura tipibo dhorilu, aru nipple tu maje maje suhibo \ndhorilu. Hothate mur eta B.F t dekha pose monot poile aru moi tair bukut bohi lolu aru mur 7 inchi r bari daal tair\ndudu dutar majot rakhi dui hatere dudu keitare sepi dhorilu, dudu dutar majot ji olop thai baki rol heikonokei boos\nmorar nisinake kokal maribo dhorilu , eibar aru olop uporoloi uthi bari daal taniya r mukhor vitorot humuai dilu, \ntai ready hua nasil jodiyo jur jobordosti humuai dilu mukhot, olop apsot tai adjust hol aru mur lund daal bor \nmoromere ice cream khuar nisinake suhibo dhorile, tai mur guti duta eta hatere sui, eta hater bari dal muthi mari \npura jure jure suhi asil.\\nMur ulabo hua jen logat tair mukhor pora bari daal ulialu aru eibaar tair boosor kahot \ngoi boosot baari daal thoi thela marilu. kintu tair boos khon pura tight thoka babe moi bohut sesta koiu humaabo \nnuarilu, Tetiya mur monot poril uth fota babe maie muk vaseline r tema eta disil. ajihe vaseline tur asol upojug \nhobo buli moi poketor pora vaseline tema tu uliyai olop maan vaseline mur baari dalor agtut aru tair boosot hani \ndilu, eibar eke thelate fir firkoi baari daal humai gol, taar pasot aru kune pai pura tika dangi dangi taik sudibo \ndhorilu. Dui hater bike r handle dhorar dore tair dudu keita dhori taik sudilu. 45 min maan sudar pasot maalpani \nulai gol, boosor vitorote uliai dilu, vabilu ji hoi hobo sala imaan controll koibo nuari. Olop homoy teneke hui \nthaki dui jone dress thik koi rest room r pora ulai duty korat lagilu.\\nEtiya ami prai hodayei restroomot sex koru,\nami duijone rest room tur naam sex room thoisu. Taniya mur girlfriend nohol jodiyo, mur best friend hoi thakil, \netiya ami prayei kotha patu aru phonote bea bea kotha pati thaku.',\n    \"Front Page\\nHome\\nChat\\nEntertainment Page\\nAssamese Music\\nAssamese Lyrics\\nBest viewed @ 1024x768 \nresolution\\nXoixobote Dhemalite - Lyrics of Assamese Songs\\nSinger : Dr. Bhupen Hazarika\\nMusic Director : Dr. \nBhupen Hazarika\\nLyricist : Dr. Bhupen Hazarika\\nXoixobote(Shoishobot) Dhemalite tumaare umola monot aase..\\nBohag \nmaahor luit khonot duyu xaature monot aase\\nJowbonote duyure dehaar laajuki porox monot aase\\nMur abihone sip jori \nloba buli kuwa mur monot aase..\\nBohag maahor logote edin aahil bordoisila\\nXei dhumuhat kaarubar xote tumi dekhu \ngusi gola\\nMonor goraaki eri tumi Dhonor goraaki bolila\\nDhon Dhon buli dhonor premere swarup prakax korila\\nBohu \ndin gol hothaate xidina tumak dekh monot aase..\\naatoror pora dekhilu tumar swarna gohona jilikise\\nTumar abihone \nsip jori lom buli tumi bhaabisa\\nBhul korisa xunjoni mur\\nAleek xapun dekhisa\\nJiyaai thaakim..\\nJiyaai thaaki \nekhon xomaj gorhibor mur mon aase\\nJot xunot koiu manuhor damm olop holeu besi aase!!\\n*** 'X' stands for assamese \nphonetic letter whose pronounciation is in between 'H' and 'KH'.\\nYou can listen to this song in the\\nAssamese \nSongs Playlist\\nDiscuss, query and comments on Assamese songs and music in this\\nforum\\n.\\nCopyright ¬© 2007-2008 \nonlinesivasagar.com(Abhijit Borah)\",\n    '\\n\\n#1\\n\\n\\nAssamese Sex Stories\\nMOI ARU MAUSUMI (Assamese Sex Stories)\\nHi friends aitu mur first post, akha\nkoru apunalukor val lagibo... mur nam Momi moi aji apunalukok mur nijor first sex\\nexperienceor bikhoya jonabo \nbisarisu. Moi akhon grils collegeot TDC\\n2nd year scienceot porhi asu. Ghorot khub strict babe muk porhat \nbhal\\nholeo muk bahirot porhibo jabo nidile. Ghorot muk okole kotu jabo\\nnidea, korbat gole maa nohole papa nohole \ndada logot jai, kintu mur\\nchildhood friend Mausumi,k amar ghorot khub trust kore babe tair logot\\nkorbat gole \nbadha nidiya. Mausumi mur akai batchor kintu tai artsot\\nakai collegeot porha. ami duyu khub bhal friend, ami duyu \nhokolu kotha\\nshare koru, even tair boyfriendor logot kora romanceor bikhoye hokolu\\nkotha. Tair boyfriend notunke \nBankot join korise, tar nam Pallab.mur\\nhihotor romanceor kotha huni khub bhal lage. Katiyaba bhabu muru \njodi\\nkunuba boyfriend thakil haten moyu sage tair nisina romance koribo\\nparilu haten, kintu mur ja ulai juar \nchance nai. Mausumir kintu ghorot\\nkunu restriction nai tai bindas ghuribo pare. Tai bohudin amar\\nghororloi oha \nnasil haibabe adin collegot taik ghoroloi ahibo kolu\\n(actually tair romanceor kotha hunibole bor mon goisil). Tai \nata\\nSunday ahibo buli kole. Mur montu bhal lagi gol, moi bor akhare Sunday\\nloi wait koribo dhorilu..\\nSunday \nhunkale uthi moi hokolu kam hekh kori tair babe wait koribo\\ndhorilu tair dari hua babe abar taik fon kori hunkale \nmatilu, tai ahi\\nthoka buli kole aru thik 15 minitor pasot tai amar ghorot palehi, amar\\nghorot coffee khai \nhokolure logot kotha pati ami mur roomot humalu‚Ä¶\\nkisu homoi kotha potar pasot moi tair boyfriendor logot hua \nromanceor\\nkotha hudhilu.. tai koi jabo dhorile‚Ä¶ hihotor first sex lifeor kotha‚Ä¶.\\nLove huar pasot hihot priya \nouting jai jodeo hihotor majot sex hua\\nnasil‚Ä¶ last month first weekor kotha hihote outing plan korisil.\\nPallabe \ntaik collegor pora pickup kori lunchor babe resturentor\\ncabinot loi gol, waiterok lunch order kori pallabe bahiror\npora\\nnadekhakoi cabinor porda tani dile, tar pasot hi tair usorot bohi tair\\nhatkhon nijor hatot loi kotha patibo \ndhorile‚Ä¶ Pallabe tair hatkhon\\nmuhari taik koisil ‚Äì tumak aji bor dhunia lagise mousumi‚Ä¶.. tumak pai\\nmoi bohut \nlucky‚Ä¶. Moi tumak bohut bhal pao‚Ä¶. Aibuli pallabe tar akhon\\nhat tair galot moromera muharibo dhorile.. mausumie \nmathu tar kotha\\nhuni tak support kori asil‚Ä¶. Aibar hi mousumik kobo dhorile mousumi\\ntumar gal dukhon gulopia hoi \ngoise, voi lagise niki ? voi koribo\\nnalage moi tumak bhal pao‚Ä¶. Mousumie kole ‚Ä¶ nai voi loga nai tumak\\nkiyo voi \nkorim?... moiu tumak bahut val pao‚Ä¶. Pallabe kole‚Ä¶ mausumi\\ntumar gulopia galkhonot ata kiss korile beya paba niki?\nMousumia kole\\nnai kintu kunuba ahibo para‚Ä¶ pallabe kole‚Ä¶ kunu nahe, order ahibo dari\\nase‚Ä¶. Aibuli koi pallabe tar\nmukhon mousumir galot logai kiss koribo\\ndhorile.. mousumir tar hator touch pai aru tar moromor kiss pai\\nuttejona \nbarhibo dhorile‚Ä¶ taiu pallabok support kori tair duyukhon\\nhat tar chulit khamus mari dhori tair lips kait pallabor\nlipsor fale\\naag borhai dile‚Ä¶ pallabe chance api tair lipsot kiss kori tair lips\\nduta chupibo dhorile‚Ä¶ aidore kisu\nhomoi kiss kori thakute hothat\\ncabinor doorot knock poril. Hunkale duyu easy hoi waitarok vitoroloi\\nahibo kole, \nwaiter ahi lunch di gol.. waiter juar pasot aibar bipllobe\\ncabinor door khon bandh kori di tair usorot bohi loi \ntaik haboti loi\\npunor lipsot koribo dhrile aru aibar olop aag barhi goi akhon hatare\\ntair boobs ata laheka \nkapuror uporote laheka muharibo dhorile mousumir\\nprotest korar s≈•ance nasil.. tairu bhal lagi tak support \nkoribo\\ndhorile‚Ä¶ pallabe gom pale ja mousumie tar pora aru bisarise ‚Ä¶ aibar hi\\nakhon hat tair thaisot thoi \nmousumir laggingor uporedi laheka muharibo\\ndhorile‚Ä¶. Mousumi uttajonat aku kobo pora nasil mathu choku kaita \nmudi\\npleasure loi asil‚Ä¶. Pallabe chance pai tair dangor dangor thais kaita\\nmuhari muhari hat khon duyu thaisor \nmajor pussy area loi nibor babe\\nthais kaita fak koribo dhorile‚Ä¶ mousumie kunu protest nokori vori\\nkaita fak kori \ndile‚Ä¶ pallabe hubidha pai tar hatkhon tair pussy loi ni\\nkapuror uporote pussy muharibo dhorile‚Ä¶. Pussyt hat porar \nloge loge\\nmousumir mukhera aahhh‚Ä¶ aaahhhh‚Ä¶ ulai gol , tai uttajonat robo pora\\nnasil tai tak support kori tar \nhatkhon khamusi dhori aru jure pussy\\nhasi dile‚Ä¶. Pallabe tair condition buji tair kane kane kole mousumi\\nplease \ntumar legging tu khuli diyana‚Ä¶ mousumie kole nai pallab ai\\nresturent amar babe safe nohoi‚Ä¶ pallabe kole‚Ä¶.. aku \nnohoi cabinor\\ndorja lock ase‚Ä¶ kunu nahe .. aibuli koi hi mousumir legging or\\nelastic dalot anguli duta vorai \nlegging tu knee level loika tani nomai\\ndile‚Ä¶. Mousumieu badha nidi tak legging tu toloi nomuat hohai kori\\ndile‚Ä¶ \ntai ata off white color panty pindhi asil ‚Ä¶ tair panty tu dekhi\\nhi robo nuari tair pantyr uporote pussy khon \nkhumusi dhori muharibo\\ndhorile‚Ä¶ aidore kori abar pallabe tar hatkhon tair pantyr vitoroloi ni\\ntair pussy fakot \nata anguli thoi lahekoi pussyr vitoroloi vorai de\\nangulitu ulua humua koribo dhorile‚Ä¶ aidore korat mousumi robo \nnuari\\ntai pussyra pani uliai panty aru pallabor hat tiyai dile‚Ä¶ aibar\\npallabe robo nuari mausumik kole ‚Ä¶ mousumi \ntumar pantytu khuli\\ndibaniki? Mausumie kole ‚Ä¶ etiya nalage eaat ami safe nohoi‚Ä¶ balag adin\\nkorim.. pallab manti \nhoi kole‚Ä¶ thik ase mousumi tumak aru alop basi\\nmorom koribo mon goise, please tumi olop uthi diyana tumak valke \nmorom\\nkoru‚Ä¶. mausumia chairor pora uthi dile.. loge loge pallabe taik\\nhoabti dhori kiss kori kori akhon hatare \ntar pantor chain dal khuli\\ntar erect pains tu uliai mausumir hatot dhorai dile‚Ä¶. Aru mausumir\\nkanot kole‚Ä¶ please \nmausomi tumar pussyt logai diyana‚Ä¶. Mousumie kole\\nnalage pallab please aibur nokoriba‚Ä¶. Pallabe kole ‚Ä¶ nohoi \nmousumi\\nnakoru nohoi tumar pantyr uporot pussyt logoi diya‚Ä¶ Mausumie manti\\nhol‚Ä¶ tar panes tu tair pantyr uporedi \npussyt logai dile‚Ä¶. Pallabe tar\\npanes tu tair pussyt gohabo dhorile‚Ä¶. Aidore korat mausumi uttajonat\\npallabok \nhaboti dhori tak support koribo dhorile aru 2nd time pani\\nulieai dele‚Ä¶. Aidore kisu homoi korat pallaboru spurm \nulai gol aru\\nmousumir panty tar spurme tiay dile‚Ä¶.. tar pasot duyu lunch kori ghuri\\nahil‚Ä¶‚Ä¶..\\nAi story huni muru \nexcitement barhi goisil aru‚Ä¶‚Ä¶..\\nMausumie muk hudhile kua Momi storitu huni kanakua lagil‚Ä¶. Moi kolu ‚Ä¶\\nval lagil ‚Ä¶\ntai kole okol val lagil ne aru kiba hol ‚Ä¶. moi kolu.. mur\\nexperience nai babe moi tumar feelings pabo pora nai‚Ä¶. \nmur jodi kunuba\\nboyfriend thakil haten moiu tumar dore enjoy kkoribo parilu haten‚Ä¶.\\ntai kole thik ase tumar \nboyfriend nathkileo moi tumak real sex felling\\nkanekua dibo paru‚Ä¶.. moi hudhilu kintu atiya kanakoi muk sex \nfeelings\\ndiba‚Ä¶ tai kole first tumi rumor door khon valka lock kora jate kunu\\nahibo nuare‚Ä¶ moi kolu tumi chinta \nnokoriba door lock ase aru atiya\\nrumoloi kunu nahe‚Ä¶ tai kole thik ase tumi bisonat mur logot boha aru\\nchoku kaita\nmudi diya aru feel kora ja moi tumar boyfriend‚Ä¶ moi\\nbisonat tair usorot bohi choku kaita mudi dilu aru mind tu \nconcentrate\\nkori bhabibo lolu ja mur boyfriend mur kakhot bohi ase‚Ä¶ aibar tai muk\\nlahikoi kokalot hat thoi \nmuharibo dhorile aru mur lipsot kiss koribo\\ndhorile‚Ä¶ prothombar karubar kiss pai mur ata ojan feelings \nahibo\\ndhorile aru moyu taik lips kiss koribo dhorilu‚Ä¶ tai kiss kori kori\\ntair hatkhon lahekoi mur boobs kaita \nmuharibo dhorile‚Ä¶. Boobsot tair\\nhat porar loge loge moi pagolor dore hobo dhorilu aru taik haboti\\ndhori kiss \nkoribo dhorilu‚Ä¶ aibar tai kurtistur vitoriloi hat vorai\\nbratu uporoloi dangi mur boobs kaita muharibo dhorile‚Ä¶ mur\nexcitement\\naru barhi gol‚Ä¶ tai boobs kaita tapi tapi mur nipple kaita muhari diat\\nmoi uttajonat 1st time pussyre \npani uleai dilu‚Ä¶‚Ä¶. aibar tai toptu\\nboobsor uporoloika uthai tair mukhon boobsoloi ni nipple kaita suck\\nkoribo \ndhorile‚Ä¶ moi uttajonat tair murot hat thoi murtu jurke chapi\\ndhorilu‚Ä¶ aibar tai muk bisonat huai di boobs kaita \nsuck kori tair\\nhatkhon mur Thaisor uporot thoi kapuror uporote muharibo dhorile‚Ä¶\\naidore kori tair hatkhon lahekoi\nmur kapuror uporote mur pussy\\nmuharibo dhorile aru angulire mur pussy fakot press dibo dhorile‚Ä¶.\\nUttajonat moi \ndui vorire tair hatkhon chapa mari dhorilu‚Ä¶ aibar tair\\nhatkhon mur salwar pantor rosidalot gol‚Ä¶. Laheka tai pantor\nrosidal\\nkhuli pantu tololoi nomabo dhorile‚Ä¶. Moi taik pantu khulat help kori\\ndilu.. pantu khuli tai mur vori \nkaita fak kori panty tur uporote\\nhatere muhari angulire pussy hole tu press kori muharibo dhorile‚Ä¶\\nhosakoi‚Ä¶ \njibonot eman hukh moi katiyau pua nasilu‚Ä¶ aibar tai muk\\nbisonar pora uthibo kole ‚Ä¶ moi bisonar pora uthat tai mur \ntoptu khuli\\nbra aru panty khuli muk naked kori dile‚Ä¶. Mousumiru sex fellings\\nhoisil.. muk naked kori tai nijor \nt-shirt tu muror uporere khuli loi\\nbra tur huk khuli bratu khuli palala tar pasot tai jeans tu khulibo\\ndhorile.. \njeans tu khulat tai pindhi thoka light pink color panty tu\\nmur chokut poril.. tair panty tu titi asil.. aibar tai \npanty tu khuli\\npalai complete naked hoi gol‚Ä¶‚Ä¶ tar pasot tai muk bisonat huai di mur\\nvori kaita fak kori mur \npussyt tair mukhon ni mur pussy suck koribo\\ndhorile.. aibar uttajonat mur mukhere aahhhh‚Ä¶ aahhhh‚Ä¶moaning \nulabo\\ndhorile.. tai suck kori kori tair jivakhon mur pussy fakot vorai diat\\nmur uttajona barhi gol aru 2nd time \npani ulai tair mukh tiaye dile‚Ä¶\\ntai aidore kisu homoi korar pasot muk kole ‚Ä¶ momi moi aibar tumak\\ncomplete \nsatisfaction dim‚Ä¶ tumar eat candle ase niki..? moi kolu\\ntableor drawerot ase tai draweror pora adal candal ani mur\npussyt\\nlaheke logai press dibo dhorile‚Ä¶ aru vitoriloi press koribo dhorile‚Ä¶\\nhosai mur ane lagisil jan kunuba \nloraya muk fuck korise‚Ä¶ first tai\\nlahekoi candledal vitoroloi ni pasot jure ulua humua koribo dhorat moi\\nrobo \nnuari 3rd time‚Ä¶ pani uleai dilu‚Ä¶‚Ä¶ already tairu uttajona barhi\\ngoisil‚Ä¶ aru tai robo nuari candle dal mur pussyr \npora uleai tair nijor\\npussyt vorai in-out kori pani uleai dile‚Ä¶ ami duyu bahut tired hoi\\ngoisilu, ami duyu bohu \nhomoi naked hoi bisonat pori thokar pasot dress\\nkori lolu‚Ä¶..\\nAidore ji dhorone tai muk sex satisfaction dile‚Ä¶ moi\njibonot pahoribo\\nnuaru‚Ä¶..\\napunalukor hohari pale pasot aru potham..\\nLast edited by sourav002 : 8th June 2013 at \n02:15 PM.\\n|Thread Tools||Search this Thread|\\n\\n'\n]\n\n\n\nIf we find a better language score we can filter for this. For example, we can filter for the language score to be greater than 0.95.\n\ndf_filtered = df.filter(pl.col(\"language_score\") &gt; 0.95)\n\n(697, 11)\n\n\n\nwith open(\"good_ids\", \"w\") as f:\n    for id in df_filtered.select(\"id\").to_series().to_list():\n        f.write(f\"{id}\\n\")"
  },
  {
    "objectID": "posts/2024/12/23/fineweb-filter-polars.html#filtering-bigger-languages",
    "href": "posts/2024/12/23/fineweb-filter-polars.html#filtering-bigger-languages",
    "title": "Filtering FineWeb2 using Polars",
    "section": "Filtering bigger languages",
    "text": "Filtering bigger languages\nSome languages have a lot of data and so we can use the scan_parquet function to create a LazyFrame. Let‚Äôs see how we can do this for the Japanese language.\n\npaths = get_paths_for_language(\"jpn\")\nlen(paths)\n\n148\n\n\nYou can see here we have many more files. If you have a lot of memory, you could use the standard read_parquet function. However, if you don‚Äôt have a lot of memory, you could use the scan_parquet function. This will read the data in chunks and is more memory efficient. Even with this we might want to start with a subset of the data to experiment with and then work with the full dataset once we‚Äôre confident in our filtering.\n\nimport random\n\nrandom.seed(42)\n\nsample_paths = random.choices(paths, k=2)\n\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nPath(\"temp_data\").mkdir(exist_ok=True)\n\nfor path in tqdm(sample_paths):\n    hf_hub_download(\n        repo_id=\"HuggingFaceFW/fineweb-2\",\n        repo_type=\"dataset\",\n        filename=path,\n        local_dir=\"temp_data\",\n    )\n\n  0%|          | 0/2 [00:00&lt;?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00,  5.87it/s]\n\n\n\ndf = pl.scan_parquet(\"temp_data/**/*.parquet\")\ndf.head(5).collect()\n\n\nshape: (5, 11)\n\n\n\ntext\nid\ndump\nurl\ndate\nfile_path\nlanguage\nlanguage_score\nlanguage_script\nminhash_cluster_size\ntop_langs\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nf64\nstr\ni64\nstr\n\n\n\n\n\"Ê¨≤„Åó„Åã„Å£„ÅüËªä„ÇíÊé¢„Åõ„Çã„Çµ„Ç§„Éà Áã¨Ë∫´ÊôÇ‰ª£„ÅØ„ÄÅ„Åü„Å†‰πó„Çå„Çå„Å∞„ÅÑ„ÅÑ„Å®ÊÄù‚Ä¶\n\"&lt;urn:uuid:9221bbac-4ab3-4d7b-9‚Ä¶\n\"CC-MAIN-2013-20\"\n\"http://careerspaceezine.com/\"\n\"2013-05-20T01:19:20Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"jpn\"\n1.000009\n\"Jpan\"\n1\n\"{\"jpn_Jpan_score\": 1.000009059‚Ä¶\n\n\n\" „Åµ„Åè„ÇÄ„Åô„ÇÅ„Å©„ÅÜ„Çè„Åó„ÇÖ„ÅÜ(Hukumusume fairy‚Ä¶\n\"&lt;urn:uuid:d03fc65f-99bb-4095-b‚Ä¶\n\"CC-MAIN-2013-20\"\n\"http://hukumusume.com/douwa/En‚Ä¶\n\"2013-05-20T01:18:14Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"jpn\"\n0.992212\n\"Jpan\"\n2\n\"{\"jpn_Jpan_score\": 0.992212295‚Ä¶\n\n\n\"ÂÆ∂ÈõªÈÄö‰ø°„Çí„ÅäÂ±ä„Åë„Åó„Åæ„Åô ÂÆ∂Èõª„ÅØ‰∏ÄÂ∫¶Ë≥ºÂÖ•„Åó„Åü„Çâ„ÄÅ‰ΩïÂπ¥„ÇÇ‰Ωø„ÅÑÁ∂ö„Åë‚Ä¶\n\"&lt;urn:uuid:89b3dae5-8a49-4d51-a‚Ä¶\n\"CC-MAIN-2013-20\"\n\"http://wnclivehosting.com/inde‚Ä¶\n\"2013-05-20T01:57:39Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"jpn\"\n1.00001\n\"Jpan\"\n1\n\"{\"jpn_Jpan_score\": 1.000010013‚Ä¶\n\n\n\"Âá∫ÁâàÁ§æ„Åã„Çâ„ÅÆ„Ç≥„É°„É≥„Éà MovableType„ÅÆÁâπÂæ¥„ÅÆ„Å≤„Å®„Å§„Å®‚Ä¶\n\"&lt;urn:uuid:84019b07-0424-4d79-b‚Ä¶\n\"CC-MAIN-2013-20\"\n\"http://www.amazon.co.jp/MOVABL‚Ä¶\n\"2013-05-20T01:50:55Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"jpn\"\n1.000009\n\"Jpan\"\n2\n\"{\"jpn_Jpan_score\": 1.000008940‚Ä¶\n\n\n\"FrontPage ÁßÅ„ÇÇÁµêÂ©ö„Åô„Çã„Åì„Å®„Åß„ÄÅ‰ªä„ÅÆ‰øùÈô∫„Å´ÂÖ•„Çç„ÅÜ„ÅãËÄÉ‚Ä¶\n\"&lt;urn:uuid:3fc5c2a5-c3a7-409c-b‚Ä¶\n\"CC-MAIN-2013-20\"\n\"http://www.christian-louboutin‚Ä¶\n\"2013-05-20T01:59:19Z\"\n\"s3://commoncrawl/crawl-data/CC‚Ä¶\n\"jpn\"\n1.00001\n\"Jpan\"\n15\n\"{\"jpn_Jpan_score\": 1.000009894‚Ä¶\n\n\n\n\n\n\n\ndf.select(\"language_score\").describe()\n\n\nshape: (9, 2)\n\n\n\nstatistic\nlanguage_score\n\n\nstr\nf64\n\n\n\n\n\"count\"\n3.3735e7\n\n\n\"null_count\"\n0.0\n\n\n\"mean\"\n0.999791\n\n\n\"std\"\n0.002776\n\n\n\"min\"\n0.886358\n\n\n\"25%\"\n0.999996\n\n\n\"50%\"\n1.000007\n\n\n\"75%\"\n1.000009\n\n\n\"max\"\n1.00001\n\n\n\n\n\n\n\ndf.filter(pl.col(\"url\").str.contains(\"wikipedia\")).count().collect(streaming=True)\n\n\nshape: (1, 11)\n\n\n\ntext\nid\ndump\nurl\ndate\nfile_path\nlanguage\nlanguage_score\nlanguage_script\nminhash_cluster_size\ntop_langs\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n55053\n55053\n55053\n55053\n55053\n55053\n55053\n55053\n55053\n55053\n55053\n\n\n\n\n\n\n\njapanese_edu_domains = [\n    \"http://www.asagaku.com/\",\n    \"www3.nhk.or.jp/news/easy/\",\n    \"http://kids.yahoo.co.jp/\",\n]\n\n\ndf.filter(pl.col(\"url\").is_in(japanese_edu_domains)).count().collect(streaming=True)\n\n\nshape: (1, 11)\n\n\n\ntext\nid\ndump\nurl\ndate\nfile_path\nlanguage\nlanguage_score\nlanguage_script\nminhash_cluster_size\ntop_langs\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n\n\n\n\n\n\nWe‚Äôd obviously want to expand this list to include more domains but you can see how we can still use the same techniques to filter very large datasets without running out of memory."
  },
  {
    "objectID": "posts/plain-text/2023-11-27-model-card.html",
    "href": "posts/plain-text/2023-11-27-model-card.html",
    "title": "Extracting Insights from Model Cards Using Open Large Language Models",
    "section": "",
    "text": "Model Cards are a vital tool for documenting machine learning models. Model Cards are stored in README.md files on the Hugging Face Hub.\nThere are currently over 400,000 models openly shared on the Hugging Face Hub. How can we better understand what information is shared in these model cards?\nSome of the concepts we‚Äôll see emerge from Model Card READMEs"
  },
  {
    "objectID": "posts/plain-text/2023-11-27-model-card.html#what-do-people-talk-about-in-their-model-readme.md",
    "href": "posts/plain-text/2023-11-27-model-card.html#what-do-people-talk-about-in-their-model-readme.md",
    "title": "Extracting Insights from Model Cards Using Open Large Language Models",
    "section": "What do people talk about in their model README.md?",
    "text": "What do people talk about in their model README.md?\nVarious organisations, groups and individuals develop models on the Hugging Face Hub; they cover a broad range of tasks and have a wide variety of audiences in mind. In turn, READMEs for models are also diverse. Some READMEs will follow a Model Card template, whilst others will use a very different format and focus on describing very different attributes of a model. How can we better understand what people discuss in model cards?"
  },
  {
    "objectID": "posts/plain-text/2023-11-27-model-card.html#can-we-extract-metadata-from-model-readmes",
    "href": "posts/plain-text/2023-11-27-model-card.html#can-we-extract-metadata-from-model-readmes",
    "title": "Extracting Insights from Model Cards Using Open Large Language Models",
    "section": "Can we extract metadata from model READMEs?",
    "text": "Can we extract metadata from model READMEs?\nOne of the things I want to understand better is what information people are talking about in their READMEs. Are they mostly talking about the training? How often do they mention the dataset? Do they discuss evaluations in detail? Partly, I want to understand this purely out of curiosity, but I am also interested in knowing if there are features that regularly appear in model cards that could potentially be extracted into more structured metadata for a model.\nAs an example of this kind of work, recently, the Hub added a metadata field for base_model. This metadata makes it easier to know the model used as a starting point for fine-tuning a new model. You can, for example, find models fine-tuned from mistralai/Mistral-7B-v0.1 using this filter https://huggingface.co/models?other=base_model:mistralai/Mistral-7B-v0.1. However, for this to be possible, the base_model field has to be stored as metadata. In the run to adding this base_model filtering to the Hub via Librarian-Bots, I made a bunch of automated pull requests adding this metadata using the information available in the model README.md.\n\n An example of a pull request made to add metadata to a model\n\nPotentially, other data of this kind could also be drawn out of model cards and exposed in a more structured way, which makes filtering and searching for models on the Hub easier."
  },
  {
    "objectID": "posts/plain-text/2023-11-27-model-card.html#annotating-with-large-language-models",
    "href": "posts/plain-text/2023-11-27-model-card.html#annotating-with-large-language-models",
    "title": "Extracting Insights from Model Cards Using Open Large Language Models",
    "section": "Annotating with Large Language Models?",
    "text": "Annotating with Large Language Models?\nAs part of my work as Hugging Face‚Äôs Machine Learning Librarian, I have created a dataset of model cards from the Hugging Face Hub. This dataset is updated daily. This dataset currently has over 400,000 rows. This makes analysing this data by hand difficult.\nA recent blog post from NuMind discusses their approach to creating a foundation model for Named Entity Recognition. As part of this work, they created a large dataset using an LLM to annotate concepts ‚Äî the term they use for entities ‚Äî in a large dataset derived from the Pile. They do this by prompting the model to annotate in an open-ended way, i.e.¬†instead of prompting the model to label specific types of entities; they prompt the model to label ‚Äúas many entities, concepts, and ideas as possible in the input text.‚Äù\nWhilst we sometimes want to have an LLM help annotate a specific type of entity, this open approach allows us to use an LLM to help us explore a dataset.\nIn the NuMind work, they used GPT-4. I wanted to use an open LLM instead. After some exploring I landed on teknium/OpenHermes-2.5-Mistral-7B:\n\nOpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, which trained on additional code datasets.\n\nI found that the model responded well to an adapted version of the original prompt used by NuMind, and since the model is a 7 Billion parameter model, it‚Äôs a little expensive to run both financially and in terms of environmental impact compared to other larger models which could also be used for this task.\nI hosted the model on Inference Endpoints and ran inference using the huggingface_hub Python library. The code for getting an annotation looked roughly like this:\ndef get_annotations(input):\n    message = f\"\"\"\n    The goal is to create a dataset for entity recognition.\n    Label as many entities, concepts, and ideas as possible in the input text.\n    Invent new entity types that may not exist in traditional NER Tasks such as more abstract concepts and ideas.\n    Make sure the entity concept is not part of speech but something more meaningful.\n    Avoid finding meaningless entities.\n    Output format (separate entities with new lines, everything, including description, and entity concept is written in English): entity from the text -|- entity concept -|- description of entity group/concept.\n\n    Example:\n\n    Input: \"Fine-tuned XLSR-53 large model for speech recognition in English\"\n\n    Output:\n\n    XLSR-53 -|- model -|- a large pre-trained language model specifically designed for speech recognition in English.\n\n    English -|- language -|- the language of the text and the model's target language.\n\n    Fine-tuned -|- model modification -|- the process of adapting the pre-trained model to a specific task, in this case, speech recognition.\n\n    Input: \"{input}\"\n\n    Output:\"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are Hermes 2. A system designed to annotate textual data\",\n        },\n        {\"role\": \"user\", \"content\": message},\n    ]\n    gen_input = tokenizer.apply_chat_template(messages, tokenize=False)\n\n    return client.text_generation(\n        gen_input,\n        max_new_tokens=450,\n        do_sample=True,\n        temperature=0.7,\n        top_k=50,\n        top_p=0.95,\n    )\nSome examples of output I got from this:\nInput: Fine-tuned XLSR-53 large model for speech recognition in English\n\nOutput:\n\nXLSR-53 -|- model -|- a large pre-trained language model specifically designed for speech recognition in English.\n\nEnglish -|- language -|- the language of the text and the model's target language.\n\nFine-tuned -|- model modification -|- the process of adapting the pre-trained model to a specific task, in this case, speech recognition.\nAs you can see, the model did a pretty good job of labelling concepts. Let‚Äôs take a deeper dive into the results."
  },
  {
    "objectID": "posts/plain-text/2023-11-27-model-card.html#what-concepts-did-we-find-in-model-cards",
    "href": "posts/plain-text/2023-11-27-model-card.html#what-concepts-did-we-find-in-model-cards",
    "title": "Extracting Insights from Model Cards Using Open Large Language Models",
    "section": "What concepts did we find in Model Cards?",
    "text": "What concepts did we find in Model Cards?\nThe dataset we‚Äôve produced via this approach contains annotations for the original entity/concept, i.e.¬†the word that the model annotated, a ‚Äúcategory‚Äù, which is the type of that concept (as labelled by the model), as well as a description produced by the LLM about that category.\nTo start, here is some high-level information about our dataset:\n\n146,800 total annotations, i.e.¬†concepts\n46,240 unique subjects\n16,581 unique categories\n\nWe can see the number of unique subjects and even unique subjects and categories. Whilst this wouldn‚Äôt be desirable if we had a fixed set of labels we wanted to annotate, for this more open-ended exploration, this is less of an issue and more of a challenge for us in how best to understand this data!\n\nThe most frequently appearing subjects\nTo start with let‚Äôs take a look at the top 20 most frequently appearing subjects in our model cards:\n\n\n\nsubject\nproportion (%)\n\n\n\n\nTraining\n1.00272\n\n\nEntry\n0.807221\n\n\nMore\n0.651226\n\n\nModel\n0.612398\n\n\nmodel\n0.54564\n\n\ninformation\n0.504087\n\n\nneeded\n0.501362\n\n\nLimitations\n0.472071\n\n\nMore Information Needed\n0.433243\n\n\nlearning_rate\n0.398501\n\n\nFine-tuned\n0.387602\n\n\nTransformers\n0.378747\n\n\nTokenizers\n0.376703\n\n\nIntended uses\n0.370572\n\n\nhyperparameters\n0.361717\n\n\nEvaluation\n0.360354\n\n\nTraining procedure\n0.358992\n\n\nVersions\n0.352861\n\n\nAdam\n0.34673\n\n\nHyperparameters\n0.343324\n\n\n\nWe may see some of these terms like ‚ÄúMore‚Äù, ‚Äúinformation‚Äù, ‚Äúneeded‚Äù are artefacts from placeholder text put into the model card templates. It‚Äôs reassuring to see ‚ÄúEvaluation‚Äù and ‚ÄúIntended uses‚Äù appearing this frequently. Since the ‚Äúsubjects‚Äù are quite diverse, let‚Äôs also take a look at the 20 most common categories:\n\n\n\ncategory\nproportion\n\n\n\n\nmodel\n3.97752\n\n\nmodel modification\n2.29837\n\n\nnumerical value\n2.01226\n\n\naction\n1.68869\n\n\ndataset\n1.53951\n\n\nmetric\n1.25409\n\n\nprocess\n1.23229\n\n\nsoftware\n1.22684\n\n\nentity\n1.15736\n\n\nsoftware version\n1.14237\n\n\nconcept\n1.09741\n\n\ndata\n0.936649\n\n\ndata type\n0.867166\n\n\nperson\n0.787466\n\n\nquantity\n0.773842\n\n\norganization\n0.730926\n\n\nlanguage\n0.729564\n\n\nlibrary\n0.647139\n\n\nnumeric value\n0.626022\n\n\nversion\n0.613079\n\n\n\nWe would expect to see many of these categories, i.e.¬†‚Äòmodel‚Äô and ‚Äòmodel modification‚Äô, ‚Äònumerical value‚Äô. Some of these categories are a little more abstract, i.e.¬†‚Äòaction‚Äô. Let‚Äôs look at the description field for some of these:\n['the process of adding new software to a system.',\n 'the action of preserving or retaining something.',\n 'an invitation to interact with the content, usually by clicking on a link or button.',\n 'the action of visiting or viewing the webpage.',\n 'the interaction between the user and the software.']\nand at the actual ‚Äúsubjects‚Äù where this has been applied\n['install', 'Kept', 'click', 'accessed', 'experience']\nWe can also view these categories as a wordcloud (the subject wordcloud is at the start of this post)"
  },
  {
    "objectID": "posts/plain-text/2023-11-27-model-card.html#what-can-we-extract",
    "href": "posts/plain-text/2023-11-27-model-card.html#what-can-we-extract",
    "title": "Extracting Insights from Model Cards Using Open Large Language Models",
    "section": "What can we extract?",
    "text": "What can we extract?\nComing back to one of the motivations of this work, trying to find ‚Äòconcepts‚Äô in model cards that could be extracted as metadata, what might we consider interesting to extract? From the categories, we can see datasets appear frequently. While I have already done some work to extract those, there is more to be done on extracting all dataset mentions from model cards.\nWhilst likely more challenging, we can see that the ‚Äòmetric‚Äô category appears pretty often. Let‚Äôs filter the dataset to examples where the category has been labelled ‚Äòmetric‚Äô and take the top 30 most frequent examples.\n\n\n\nsubject\nproportion\n\n\n\n\nValidation Loss\n11.6212\n\n\nTraining Loss\n7.63271\n\n\nAccuracy\n6.97274\n\n\nLoss\n6.74319\n\n\nf1\n5.39455\n\n\naccuracy\n3.18508\n\n\nresults\n2.38164\n\n\nrecall\n2.066\n\n\nRecall\n2.066\n\n\nprecision\n1.95122\n\n\nValidation Accuracy\n1.66428\n\n\nResults\n1.5495\n\n\n‚Äòf1‚Äô\n1.46341\n\n\n‚Äòprecision‚Äô\n1.43472\n\n\n‚Äòrecall‚Äô\n1.43472\n\n\nTrain Loss\n1.34864\n\n\ntraining_precision\n1.34864\n\n\nPrecision\n1.29125\n\n\nRouge1\n0.774749\n\n\n‚Äúaccuracy‚Äù\n0.774749\n\n\nValidation\n0.659971\n\n\nPerformance\n0.631277\n\n\nRouge2\n0.573888\n\n\nTrain Accuracy\n0.573888\n\n\nF1\n0.516499\n\n\nBleu\n0.487805\n\n\nIou\n0.45911\n\n\nnum_epochs\n0.45911\n\n\nMicro F1 score\n0.373027\n\n\nMatthews Correlation\n0.344333\n\n\n\nWhile the results here are a little noisy, with a bit of work, we can potentially begin to think about how to extract mentions of metrics from model cards and merge duplicated metrics that have been expressed differently. This sort of data could start to give us very interesting ‚Äòon-the-ground‚Äô insights into how people are evaluating their models."
  },
  {
    "objectID": "posts/plain-text/2023-11-27-model-card.html#conclusion",
    "href": "posts/plain-text/2023-11-27-model-card.html#conclusion",
    "title": "Extracting Insights from Model Cards Using Open Large Language Models",
    "section": "Conclusion",
    "text": "Conclusion\nIf you want to play with the results yourself, you can find the full dataset here: librarian-bots/model-card-sentences-annotated.\nYou may also want to check out the Model Card GuideBook\nIf you have other ideas about working with this kind of data, I‚Äôd love to hear from you! You can follow me on the Hub (you should also follow Librarian bot!)."
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "",
    "text": "There are many potential uses of computer vision in GLAM (Galleries, Libraries, Archives and Museums). These uses include:\n\nimage similarity search, i.e., given an image, find similar images\ntext search of images, i.e., given a text string ‚Äúa picture of a dog eating an ice cream,‚Äù return relevant images\npage layout recognition, i.e., pull out semantically important parts of a document (articles, photos, titles, etc.)\nOptical Character Recognition (OCR)\n\nAll of these use cases require some technical work to implement or use. Usually, they need some programming knowledge too. However, there are many tasks in GLAM where computer vision could be helpful to, requiring less technical work to implement. In particular, many uses of computer vision can be framed as an image classification task (putting images into categories).\nLast year, Kaspar Beelen, Melvin Wevers, Thomas Smits, Katherine McDonough, and I shared a two-part Programming Historian lesson, Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification.\nThis lesson aimed to provide an introduction to how computer vision can be leveraged to work with images ‚Äòat scale‚Äô ‚Äì in particular for research applications. While we tried hard to make the lesson (particularly part one) accessible, there are still barriers to getting started:\n\nYou need some Python knowledge: while we tried to keep the Python code simple (helped massively by the fastai library we use in the lesson), knowing how to code is still required. I couldn‚Äôt find a good citation for this, but most estimates for the number of people who know how to program are around 0.5-1% of the global population. Of this percentage, fewer will know Python.\nNeed to have access to a GPU: whilst you can train deep learning models (the type of machine learning model introduced in our Programming Historian tutorial), it is a lot slower without them. However, setting up access to a GPU can be annoying. While ‚Äòfree‚Äô access is possible, this can also come with constraints.\nThe costs involved in training deep learning models can be hard to predict. You can usually get started for free, but often at some point, you need to invest some money in cloud computing. However, it can be difficult to know before you start training a model(s) how much it will cost.\n\nBeyond this, there is also a bigger question of how much energy you might want to invest in all of the above stuff involved in getting machine learning set up. This is especially true if you don‚Äôt want to become a machine learning engineer and want to do something practical with machine learning.\n\n\nMany machine learning engineers will grimace at the title of this section. However, many use cases of machine learning exist where an existing machine learning architecture will work well. Training a model is not what would benefit most from human intervention.\nFor novel applications of machine learning or situations where you want to ensure a model is well suited to your domain, you may need to spend time creating training data. After training your model, there is also a step where you need to decide how to integrate machine learning into existing or new workflows. This is partially a technical question but often involves considerations beyond how I set up an API to serve my model.\nHand-training models can eat up a lot of time. Sometimes this time might be warranted but other times you might wish you could make some of this process less hands-on."
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html#introduction",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html#introduction",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "",
    "text": "There are many potential uses of computer vision in GLAM (Galleries, Libraries, Archives and Museums). These uses include:\n\nimage similarity search, i.e., given an image, find similar images\ntext search of images, i.e., given a text string ‚Äúa picture of a dog eating an ice cream,‚Äù return relevant images\npage layout recognition, i.e., pull out semantically important parts of a document (articles, photos, titles, etc.)\nOptical Character Recognition (OCR)\n\nAll of these use cases require some technical work to implement or use. Usually, they need some programming knowledge too. However, there are many tasks in GLAM where computer vision could be helpful to, requiring less technical work to implement. In particular, many uses of computer vision can be framed as an image classification task (putting images into categories).\nLast year, Kaspar Beelen, Melvin Wevers, Thomas Smits, Katherine McDonough, and I shared a two-part Programming Historian lesson, Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification.\nThis lesson aimed to provide an introduction to how computer vision can be leveraged to work with images ‚Äòat scale‚Äô ‚Äì in particular for research applications. While we tried hard to make the lesson (particularly part one) accessible, there are still barriers to getting started:\n\nYou need some Python knowledge: while we tried to keep the Python code simple (helped massively by the fastai library we use in the lesson), knowing how to code is still required. I couldn‚Äôt find a good citation for this, but most estimates for the number of people who know how to program are around 0.5-1% of the global population. Of this percentage, fewer will know Python.\nNeed to have access to a GPU: whilst you can train deep learning models (the type of machine learning model introduced in our Programming Historian tutorial), it is a lot slower without them. However, setting up access to a GPU can be annoying. While ‚Äòfree‚Äô access is possible, this can also come with constraints.\nThe costs involved in training deep learning models can be hard to predict. You can usually get started for free, but often at some point, you need to invest some money in cloud computing. However, it can be difficult to know before you start training a model(s) how much it will cost.\n\nBeyond this, there is also a bigger question of how much energy you might want to invest in all of the above stuff involved in getting machine learning set up. This is especially true if you don‚Äôt want to become a machine learning engineer and want to do something practical with machine learning.\n\n\nMany machine learning engineers will grimace at the title of this section. However, many use cases of machine learning exist where an existing machine learning architecture will work well. Training a model is not what would benefit most from human intervention.\nFor novel applications of machine learning or situations where you want to ensure a model is well suited to your domain, you may need to spend time creating training data. After training your model, there is also a step where you need to decide how to integrate machine learning into existing or new workflows. This is partially a technical question but often involves considerations beyond how I set up an API to serve my model.\nHand-training models can eat up a lot of time. Sometimes this time might be warranted but other times you might wish you could make some of this process less hands-on."
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html#can-we-approach-this-in-another-way",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html#can-we-approach-this-in-another-way",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "Can we approach this in another way?",
    "text": "Can we approach this in another way?\nAutoTrain is a tool that allow us to train machine learning models without needing to use Python, setup compute infrastructure or deal with unpredictable costs for training our models. In the rest of this blog post we‚Äôll go through the steps to using AutoTrain for a semi-realistic computer vision problem.\n\nThe dataset\nFor this project we‚Äôll use a dataset created by the Internet Archive as part of a request for help to judge a book by its cover. The blog post presents a use case for wanting to know if an image of a book cover is ‚Äòuseful‚Äô or ‚Äònot useful‚Äô. They provide some examples\nUseful image example:\n Not useful image example:\n\n\n\nA picture of a the front cover of a book which is blank\n\n\nEssentially the task is to decide whether an image of a digitized book cover is ‚Äòuseful‚Äô or ‚Äònot useful,‚Äô i.e.¬†whether showing this cover to Internet Archive users would give them useful information or not. The Internet Archive shared a dataset along with this blog post which contains examples for each category.\n\nWhat type of machine learning task is this?\nIf we look at the dataset shared by the Internet Archive, we have a directory structure that looks like this:\n.\n‚îú‚îÄ‚îÄ year-1923-not-very-useful-covers\n‚îî‚îÄ‚îÄ year-1923-useful-covers\nWe have two folders containing images. Each folder contains examples of image belonging to the name of each folder. Essentially, we want a model that learns which image belongs in each folder (based on the examples) and can put new images into the correct folder/category. This is known as an image classification task (as was mentioned in the introduction). The Hugging Face tasks page for this gives an excellent overview: https://huggingface.co/tasks/image-classification\n\n\n\nWhat are the steps involved?\nHow do we go from the dataset we started with to a trained model that we can begin to explore? For this particular example, the steps are as follows:\n\nDownload our data\nPrepare our data\nchoose our autotrain task\nUpload our data to autotrain\nTrain our models\nEvaluate our models\n\n\n\nDownload our data\nThis step will depend on where your data is and how it‚Äôs arranged, but in this example, we can download the dataset from the Internet Archive. Three folders are provided in this case covering useful/not-useful for 1923 and for the year 2000 useful. Since the types of cover will have changed a fair bit in this time period we‚Äôll just download the folders for 1923.\n\n\n\nScreenshot of IA downloads\n\n\n\n\nPreparing our data\nThere isn‚Äôt much prep we need to do for our data; however, we can provide data to AutoTrain in a few different ways for our image classification task. In this case we‚Äôll use the imagefolder format. This is essentially what we have already (folders containing examples of the labels we‚Äôre interested in). We‚Äôll create a top-level directory for our image data cover, which contains two subfolders with our example images.\n\n\n\nFolder screenshot\n\n\n\nResize our images (optional)\nThis step isn‚Äôt strictly necessary, but it‚Äôll save time when uploading our dataset to AutoTrain. Most machine learning models expect training images to be relatively small (often 224x224 or 512x512 pixels). You can do this from the command line, but most operating systems have inbuilt tools for bulk resizing images, e.g., https://www.makeuseof.com/tag/batch-convert-resize-images-mac/\n\n\n\nSetup AutoTrain\nFrom the projects page, we can create a new project.\n\nHere we give our project a name and choose a task (image classification). We can also specify for AutoTrain to use a particular model. If you don‚Äôt have a solid reason to select a model you can leave this decision to AutoTrain ü§ó.\nOnce you‚Äôve created your project, you‚Äôll need to upload your data. There are different ways of doing this depending on the task. For image classification, we can use pre-arranged folders with a CSV/JSONL file with the labels or upload a dataset hosted on the Hugging Face hub.\n\nWe already have an organized folder so we can upload data.\n\nOnce we‚Äôve uploaded our images, we‚Äôll need to wait for the data to be uploaded. How long this takes depends on your internet speed. We can now click on Go to trainings.\n\nHere you will see that AutoTrain is formatting your uploaded data.\n\nOnce your data has been prepared, you can decide how many models you want AutoTrain to train for you. This decision depends on how much you want to spend on training your models and where you are in your project. If you are getting started and want to know how well a model may do, you may choose a lower number. If you want the best possible chance of getting the best-performing model, you could choose to train a more significant number of models.\n\nOnce you are ready, you can smash the start model training button!üî• The nice thing is that AutoTrain will ask you to confirm how much model training will cost. Once your models start training, a screen pops up with some randomly named models. Depending on the size of your dataset, it might take a bit longer to start seeing metrics for your model, but after a little while, you will begin to see scores (in this case, accuracy).\n\nAs the models train, you will see some models overtake others in performance. If you are easily amused like me, you will treat this like a fun spectator sport.\n\nYou also have a metrics overview tab for all the models you have trained. This makes it easy to sort by different metrics.\n\nEach of these models created by AutoTrain is a ‚Äòreal‚Äô model hosted in a model repository on the Hugging Face hub. Some AutoTrain solutions hide away the actual artifacts and only allow you to interact with the models through their API. These models are available in the same way as any other model on the hub. By default, the models are made private, but you can decide to make the models openly available for others to use ü§ó.\nYou‚Äôll also see in the screenshot that the models come with the outlines of a model card."
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html#why-does-our-model-suck",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html#why-does-our-model-suck",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "Why does our model suck?",
    "text": "Why does our model suck?\nFor this particular dataset, our models don‚Äôt do super well (around 92% accuracy). Why is this?\n\nThe importance of training data\nStart to dig into the training data examples provided. You‚Äôll see that quite a few images might be reasonably classified as belonging to the other category. In particular, quite a few images of the not-useful folder are similar to those in the useful folder. This is going to make it hard for our model to learn what we‚Äôre after.\nThis also shows the importance of focusing on data and not over-focusing on model training. In this case, fixing our data will likely yield much better results than messing around with how we train the models. Using a tool like AutoTrain can quickly help you spot these issues early on so you can iterate on your training data.\n\n\nHow can we fix this??\nMove images between folders!!\nThere are better ways, but spending 30 mins removing examples you don‚Äôt think the fit will make a big difference to the model performance. At some point, you are likely to want to use a proper annotation tool but to start with; you might be able to get quite far by using your operating systems file browser to re-arrange your training data.\nBelow is an example from another similar dataset where we get models with 99% accuracy. All of this without writing a line of code!"
  },
  {
    "objectID": "posts/plain-text/autotrain/2023-02-22-autotrain.html#what-can-i-do-with-this-model",
    "href": "posts/plain-text/autotrain/2023-02-22-autotrain.html#what-can-i-do-with-this-model",
    "title": "Using Hugging Face AutoTrain to train an image classifier without writing any code.",
    "section": "What can I do with this model?",
    "text": "What can I do with this model?\nThere are various ways in which you can use the model you‚Äôve created. How you want to use it depends largely on your use case. In a follow-up blog post I‚Äôll suggest a few options for how you can continue on the no/low-code journey to creating and using ML tools customised to your needs and data ü§ó.\n\nShow me the models!\nYou can find the best models shown above here:\n\nhttps://huggingface.co/davanstrien/autotrain-ia-useful-covers-3665397856\nhttps://huggingface.co/davanstrien/autotrain-encyclopaedia-illustrations-blog-post-3327992158"
  },
  {
    "objectID": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html",
    "href": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html",
    "title": "Combining Hugging Face datasets with dask",
    "section": "",
    "text": "Hugging Face datasets is a super useful library for loading, processing and sharing datasets with other people.\nFor many pre-processing steps it works beautifully. The one area where it can be a bit trickier to use is for EDA style analysis. This column-wise EDA is often important as an early step in working with some data or for preparing a data card.\nFortunately combining datasets and another data library, dask works pretty smoothly. This isn‚Äôt intended to be a full intro to either datasets or dask but hopefully gives you a sense of how both libaries work and how they can complement each other.\nFirst, make sure we have the required libraries. Rich is there for a little added visual flair ‚ú®\n%%capture\n!pip install datasets toolz rich[jupyter] dask\n%load_ext rich"
  },
  {
    "objectID": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#load-some-data",
    "href": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#load-some-data",
    "title": "Combining Hugging Face datasets with dask",
    "section": "Load some data",
    "text": "Load some data\nFor this example we will use a the blbooksgenre dataset that contains metadata about some digitised books from the British Library. This collection also includes some annotations for the genre of the book which we could use to train a machine learning model.\nWe can load a dataset hosted on the Hugging Face hub by using the load_dataset function.\n\nfrom datasets import load_dataset\n\n\nds = load_dataset(\"blbooksgenre\", \"annotated_raw\", split=\"train\")\n\nReusing dataset bl_books_genre (/Users/dvanstrien/.cache/huggingface/datasets/bl_books_genre/annotated_raw/1.1.0/1e01f82403b3d9344121c3b81e5ad7c130338b250bf95dad4c6ab342c642dbe8)\n\n\nSince we requested only the train split we get back a Dataset\n\nds\n\nDataset({\n    features: ['BL record ID', 'Name', 'Dates associated with name', 'Type of name', 'Role', 'All names', 'Title', 'Variant titles', 'Series title', 'Number within series', 'Country of publication', 'Place of publication', 'Publisher', 'Date of publication', 'Edition', 'Physical description', 'Dewey classification', 'BL shelfmark', 'Topics', 'Genre', 'Languages', 'Notes', 'BL record ID for physical resource', 'classification_id', 'user_id', 'subject_ids', 'annotator_date_pub', 'annotator_normalised_date_pub', 'annotator_edition_statement', 'annotator_FAST_genre_terms', 'annotator_FAST_subject_terms', 'annotator_comments', 'annotator_main_language', 'annotator_other_languages_summaries', 'annotator_summaries_language', 'annotator_translation', 'annotator_original_language', 'annotator_publisher', 'annotator_place_pub', 'annotator_country', 'annotator_title', 'Link to digitised book', 'annotated', 'Type of resource', 'created_at', 'annotator_genre'],\n    num_rows: 4398\n})\n\n\n\nWe can see this has a bunch of columns. One that is of interest is the Data of publication column. Since we could use this dataset to train some type of classifier we may want to check whether we have enough examples across different time periods in the dataset.\n\nds[0][\"Date of publication\"]\n\n'1879'\n\n\n\n\nUsing toolz to calculate frequencies for a column\nOne quick way we can get the frequency count for a column is using the wonderful toolz library\nIf our data fits in memory, we can simply pass in a column containing a categorical value to a frequency function to get a frequency count.\n\nfrom toolz import frequencies, topk\n\n\ndates = ds[\"Date of publication\"]\n\n\n# collapse_hide\n\nfrequencies(dates)\n\n{\n    '1879': 99,\n    '1774': 5,\n    '1765': 5,\n    '1877': 69,\n    '1893': 222,\n    '1891': 148,\n    '1827': 29,\n    '1868': 42,\n    '1878': 72,\n    '1895': 189,\n    '1897': 120,\n    '1899': 104,\n    '1896': 174,\n    '1876': 48,\n    '1812': 13,\n    '1799': 8,\n    '1830': 32,\n    '1870': 42,\n    '1894': 155,\n    '1864': 28,\n    '1855': 42,\n    '1871': 42,\n    '1836': 37,\n    '1883': 51,\n    '1880': 111,\n    '1884': 69,\n    '1822': 16,\n    '1856': 38,\n    '1872': 42,\n    '1875': 57,\n    '1844': 35,\n    '1890': 134,\n    '1886': 43,\n    '1840': 15,\n    '1888': 109,\n    '1858': 43,\n    '1867': 53,\n    '1826': 24,\n    '1800': 3,\n    '1851': 43,\n    '1838': 14,\n    '1824': 20,\n    '1887': 58,\n    '1874': 42,\n    '1857': 44,\n    '1873': 34,\n    '1837': 16,\n    '1846': 32,\n    '1881': 55,\n    '1898': 104,\n    '1906': 4,\n    '1892': 134,\n    '1869': 25,\n    '1885': 69,\n    '1882': 71,\n    '1863': 55,\n    '1865': 53,\n    '1635': 3,\n    '1859': 39,\n    '1818': 17,\n    '1845': 28,\n    '1852': 43,\n    '1841': 23,\n    '1842': 29,\n    '1848': 28,\n    '1828': 23,\n    '1850': 38,\n    '1860': 45,\n    '1889': 140,\n    '1815': 5,\n    '1861': 28,\n    '1814': 13,\n    '1843': 28,\n    '1817': 12,\n    '1819': 16,\n    '1853': 34,\n    '1833': 5,\n    '1854': 36,\n    '1839': 33,\n    '1803': 7,\n    '1835': 14,\n    '1813': 8,\n    '1695': 4,\n    '1809-1811': 5,\n    '1832': 9,\n    '1823': 17,\n    '1847': 28,\n    '1816': 8,\n    '1806': 5,\n    '1866': 26,\n    '1829': 13,\n    '1791': 5,\n    '1637': 5,\n    '1821': 4,\n    '1807': 14,\n    '1862': 22,\n    '1795': 5,\n    '1834': 12,\n    '1831': 10,\n    '1849': 13,\n    '1811': 1,\n    '1825': 1,\n    '1809': 3,\n    '1905': 1,\n    '1808': 1,\n    '1900': 5,\n    '1892-1912': 1,\n    '1804': 4,\n    '1769': 5,\n    '1910': 1,\n    '1805': 5,\n    '1802': 3,\n    '1871-': 1,\n    '1901': 5,\n    '1884-1909': 1,\n    '1873-1887': 1,\n    '1979': 1,\n    '1852-1941': 1,\n    '1903': 1,\n    '1871-1873': 1,\n    '1810': 3,\n    '1907': 1,\n    '1820': 5,\n    '1789': 5\n}"
  },
  {
    "objectID": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#make-it-parallel",
    "href": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#make-it-parallel",
    "title": "Combining Hugging Face datasets with dask",
    "section": "Make it parallel!",
    "text": "Make it parallel!\nIf our data doesn‚Äôt fit in memory or we want to do things in parallel we might want to use a slightly different approach. This is where dask can play a role.\nDask offers a number of different collection abstractions that make it easier to do things in parallel. This includes dask bag.\nFirst we‚Äôll create a dask client here, I won‚Äôt dig into the details of this here but you can get a good overview in the getting started pages.\n\nfrom distributed import Client\n\n\nclient = Client()\n\nSince we don‚Äôt want to load all of our data into memory we can great a generator that will yield one row at a time. In this case we‚Äôll start by exploring the Title column\n\ndef yield_titles():\n    for row in ds:\n        yield row[\"Title\"]\n\nWe can see that this returns a generator\n\nyield_titles()\n\n&lt;generator object yield_titles at 0x7ffc28fdc040&gt;\n\n\n\n\nnext(iter(yield_titles()))\n\n'The Canadian farmer. A missionary incident [Signed: W. J. H. Y, i.e. William J. H. Yates.]'\n\n\n\nWe can store this in a titles variable.\n\ntitles = yield_titles()\n\nWe‚Äôll now import dask bag.\nimport dask.bag as db\nWe can create a dask bag object using the from_sequence method.\n\nbag = db.from_sequence(titles)\n\n\nbag\n\ndask.bag&lt;from_sequence, npartitions=1&gt;\n\n\n\nWe can look at an example using the take method\n\nbag.take(1)\n\n(\n    [\n        'The',\n        'Canadian',\n        'farmer.',\n        'A',\n        'missionary',\n        'incident',\n        '[Signed:',\n        'W.',\n        'J.',\n        'H.',\n        'Y,',\n        'i.e.',\n        'William',\n        'J.',\n        'H.',\n        'Yates.]'\n    ],\n)\n\n\n\ndask bag has a bunch of handy methods for processing data (some of these we could also do in ü§ó datasets but others are not available as specific methods in datasets).\nFor example we can make sure we only have unique titles using the distinct method.\n\nunique_titles = bag.distinct()\n\n\nunique_titles.take(4)\n\n(\n    'The Canadian farmer. A missionary incident [Signed: W. J. H. Y, i.e. William J. H. Yates.]',\n    'A new musical Interlude, called the Election [By M. P. Andrews.]',\n    'An Elegy written among the ruins of an Abbey. By the author of the Nun [E. Jerningham]',\n    \"The Baron's Daughter. A ballad by the author of Poetical Recreations [i.e. William C. Hazlitt] . F.P\"\n)\n\n\n\nSimilar to ü§ó datasets we have a map method that we can use to apply a function to all of our examples. In this case we split the title text into individual words.\n\ntitle_words_split = unique_titles.map(lambda x: x.split(\" \"))\n\n\ntitle_words_split.take(2)\n\n(\n    [\n        'The',\n        'Canadian',\n        'farmer.',\n        'A',\n        'missionary',\n        'incident',\n        '[Signed:',\n        'W.',\n        'J.',\n        'H.',\n        'Y,',\n        'i.e.',\n        'William',\n        'J.',\n        'H.',\n        'Yates.]'\n    ],\n    [\n        'A',\n        'new',\n        'musical',\n        'Interlude,',\n        'called',\n        'the',\n        'Election',\n        '[By',\n        'M.',\n        'P.',\n        'Andrews.]'\n    ]\n)\n\n\n\nWe can see we now have all our words in a list. Helpfully dask bag has a flatten method. This will consume our lists and put all the words in a single sequence.\n\nflattend_title_words = title_words_split.flatten()\n\n\nflattend_title_words.take(2)\n\n('The', 'Canadian')\n\n\n\nWe could now use the frequencies method to get the top words.\n\nfreqs = flattend_title_words.frequencies(sort=True)\n\n\nfreqs\n\ndask.bag&lt;sorted, npartitions=1&gt;\n\n\n\nSince dask bag methods are lazy by default nothing has actually been calculated yet. We could just grab the top 10 words.\n\ntop_10_words = freqs.topk(10, key=1)\n\nIf we want the results of something we call compute which will call all of the chained methods on our bag.\n\ntop_10_words.compute()\n\n[\n    ('of', 808),\n    ('the', 674),\n    ('and', 550),\n    ('...', 518),\n    ('in', 402),\n    ('van', 306),\n    ('etc', 301),\n    ('de', 258),\n    ('en', 258),\n    ('a', 231)\n]\n\n\n\nWe could also do the same with lowered version\n\nlowered_title_words = flattend_title_words.map(lambda x: x.lower())\n\n\nfreqs = lowered_title_words.frequencies(sort=True)\n\nThe visualize method gives you some insights into how the computation is managed by dask.\n\nfreqs.visualize(engine=\"cytoscape\", optimize_graph=True)"
  },
  {
    "objectID": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#moving-from-datasets-to-a-dask-dataframe",
    "href": "posts/post-with-code/dask/2022-06-20-dask-and-datasets.html#moving-from-datasets-to-a-dask-dataframe",
    "title": "Combining Hugging Face datasets with dask",
    "section": "Moving from datasets to a dask dataframe",
    "text": "Moving from datasets to a dask dataframe\nFor some operations, dask bag is super easy to use. Sometimes though you will hurt your brain trying to crow bar your problem into the dask bag API üòµ‚Äçüí´ This is where dask dataframes come in! Using parquet, we can easily save our ü§ó dataset as a parquet file.\n\nds.to_parquet(\"genre.parquet\")\n\n3583138\n\n\n\nimport dask.dataframe as dd\n\nand load from this file\n\nddf = dd.read_parquet(\"genre.parquet\")\n\nAs dask dataframe works quite similar to a pandas dataframe. It is lazy by default so if we just print it out\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nBL record ID\nName\nDates associated with name\nType of name\nRole\nAll names\nTitle\nVariant titles\nSeries title\nNumber within series\nCountry of publication\nPlace of publication\nPublisher\nDate of publication\nEdition\nPhysical description\nDewey classification\nBL shelfmark\nTopics\nGenre\nLanguages\nNotes\nBL record ID for physical resource\nclassification_id\nuser_id\nsubject_ids\nannotator_date_pub\nannotator_normalised_date_pub\nannotator_edition_statement\nannotator_FAST_genre_terms\nannotator_FAST_subject_terms\nannotator_comments\nannotator_main_language\nannotator_other_languages_summaries\nannotator_summaries_language\nannotator_translation\nannotator_original_language\nannotator_publisher\nannotator_place_pub\nannotator_country\nannotator_title\nLink to digitised book\nannotated\nType of resource\ncreated_at\nannotator_genre\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nobject\nbool\nint64\ndatetime64[ns]\nint64\n\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: read-parquet, 1 tasks\n\n\nYou‚Äôll see we don‚Äôt actually get back any data. If we use head we get the number of examples we ask for.\n\nddf.head(3)\n\n\n\n\n\n\n\n\nBL record ID\nName\nDates associated with name\nType of name\nRole\nAll names\nTitle\nVariant titles\nSeries title\nNumber within series\n...\nannotator_original_language\nannotator_publisher\nannotator_place_pub\nannotator_country\nannotator_title\nLink to digitised book\nannotated\nType of resource\ncreated_at\nannotator_genre\n\n\n\n\n0\n014603046\nYates, William Joseph H.\n\nperson\n\n[Yates, William Joseph H. [person] , Y, W. J....\nThe Canadian farmer. A missionary incident [Si...\n\n\n\n...\n\nNONE\nLondon\nenk\nThe Canadian farmer. A missionary incident [Si...\nhttp://access.bl.uk/item/viewer/ark:/81055/vdc...\nTrue\n0\n2020-08-11 14:30:33\n0\n\n\n1\n014603046\nYates, William Joseph H.\n\nperson\n\n[Yates, William Joseph H. [person] , Y, W. J....\nThe Canadian farmer. A missionary incident [Si...\n\n\n\n...\n\nNONE\nLondon\nenk\nThe Canadian farmer. A missionary incident [Si...\nhttp://access.bl.uk/item/viewer/ark:/81055/vdc...\nTrue\n0\n2021-04-15 09:53:23\n0\n\n\n2\n014603046\nYates, William Joseph H.\n\nperson\n\n[Yates, William Joseph H. [person] , Y, W. J....\nThe Canadian farmer. A missionary incident [Si...\n\n\n\n...\n\nNONE\nLondon\nenk\nThe Canadian farmer. A missionary incident [Si...\nhttp://access.bl.uk/item/viewer/ark:/81055/vdc...\nTrue\n0\n2020-09-24 14:27:54\n0\n\n\n\n\n3 rows √ó 46 columns\n\n\n\nWe have some familiar methods from pandas available to us\n\nddf = ddf.drop_duplicates(subset=\"Title\")\n\nAs an example of something that would be a bit tricky in datasets, we can see how to groupby the mean title length by year of publication. First we create a new column for title length\n\nddf[\"title_len\"] = ddf[\"Title\"].map(lambda x: len(x))\n\nWe can then groupby the date of publication\n\ngrouped = ddf.groupby(\"Date of publication\")\n\nand then calculate the mean title_len\n\nmean_title_len = grouped[\"title_len\"].mean()\n\nTo actually compute this value we call the compute method\n\nmean_title_len.compute()\n\nDate of publication\n1635    248.0\n1637     67.0\n1695     63.0\n1765     86.0\n1769     20.0\n        ...  \n1905    141.0\n1906    225.0\n1907    142.0\n1910     65.0\n1979     43.0\nName: title_len, Length: 124, dtype: float64\n\n\n\nWe can also create a plot in the usual way\n\nmean_title_len.compute().plot()\n\n&lt;AxesSubplot:xlabel='Date of publication'&gt;\n\n\n\n&lt;Figure size 432x288 with 1 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nThis was a very quick overview. The dask docs go into much more detail as do the Hugging Face datasets docs."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html",
    "title": "Using the ü§ó Hub for model storage",
    "section": "",
    "text": "Part of what the flyswot should take care of is handle machine learning models. The flyswot tool is essentially just a command-line wrapper for the machine learning model. However, these two things the command-line tool, and the model, are kept separate for a number of reasons:\n\nthe flyswot tool might have changes separate from the model changing i.e.¬†some new functionality is added or some bug fixed\nwe might want to update the model based on new training data or a change in the labels used\n\nWe want to be able to release a new model without having to create a new release of the flyswot tool and vice-versa. As a result of this, both of these things are versioned separately.\nWe might want to keep our model separate because flyswot is made available as a Python Package. Since a computer vision model can be pretty large, we probably don‚Äôt want this to be included as a part of the Python package."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#intro",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#intro",
    "title": "Using the ü§ó Hub for model storage",
    "section": "",
    "text": "Part of what the flyswot should take care of is handle machine learning models. The flyswot tool is essentially just a command-line wrapper for the machine learning model. However, these two things the command-line tool, and the model, are kept separate for a number of reasons:\n\nthe flyswot tool might have changes separate from the model changing i.e.¬†some new functionality is added or some bug fixed\nwe might want to update the model based on new training data or a change in the labels used\n\nWe want to be able to release a new model without having to create a new release of the flyswot tool and vice-versa. As a result of this, both of these things are versioned separately.\nWe might want to keep our model separate because flyswot is made available as a Python Package. Since a computer vision model can be pretty large, we probably don‚Äôt want this to be included as a part of the Python package."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#how-is-this-currently-being-done",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#how-is-this-currently-being-done",
    "title": "Using the ü§ó Hub for model storage",
    "section": "How is this currently being done",
    "text": "How is this currently being done\nCurrently models are stored in a separate GitHub repository separate from the repository used to store the flyswot code. flyswot has some functionality for checking against this GitHub repository to see if a more recent remote model has superseded a local model. If there is a more recent model available (and a CLI flag indicates that the latest model should be used), then flyswot downloads the new model and stores it in a new directory."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#what-is-wrong-with-this",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#what-is-wrong-with-this",
    "title": "Using the ü§ó Hub for model storage",
    "section": "What is wrong with this",
    "text": "What is wrong with this\nWhilst this approach does work okay there is quite a surprising amount of code that is needed to take care of some of this. Currently the option to pass a specific snapshot of a model doesn‚Äôt exist.\nOn the storage side although GitHub is great for storing code there are some limitations to it for storing large files. I‚Äôve created a GitHub action to create a release when new pull requests to update the model are made. This then creates a new release with date information in the filename. Again, this works okay, but there might be a better way‚Ä¶"
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#hub-to-the-rescue",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#hub-to-the-rescue",
    "title": "Using the ü§ó Hub for model storage",
    "section": "ü§ó hub to the rescue?",
    "text": "ü§ó hub to the rescue?\nI have already been using the huggingface hub when using other peoples models and uploading fine-tuned transformer models. However, digging around the docs, it seemed like there are a few things in this ecosystem that could be useful for flyswot.\n\nWhat is the ü§ó hub?\nIf you haven‚Äôt come across the ü§ó hub before, it is essentially a place where models can be uploaded and explored by others. So, for example, if I want a language model trained on Arabic text, I might find it in the hub. The goal is to help avoid duplication of effort and allow other people to use or adapt existing work.\nThis video gives a helpful overview of navigating the hub and finding models you might be interested in using.\n\n\nPart of the aim of sharing the flyswot models on GitHub (or the ü§ó hub) is to make them available to other people to use. The ü§ó hub well supports this use case. We can easily share models (including large ones) because of the underpinning use of git-lfs. However, our interest is not only in sharing a model for others to use but also in grabbing the correct model for the flyswot CLI tool easier. Some other components might help here."
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#the-hub-vs-huggingface_hub-library",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#the-hub-vs-huggingface_hub-library",
    "title": "Using the ü§ó Hub for model storage",
    "section": "The hub vs huggingface_hub library",
    "text": "The hub vs huggingface_hub library\nThe ü§ó hub already provides a place to store the model. You can interact with this model using the web interface only but what we want is also to download models using our CLI from the hub. We already have a way to do this with GitHub, so ideally, we want something that works better than our current approach.\nThis is where the huggingface_hub Python Library might come in. This Python library provides us with various ways of interacting with the hub. This could give us enough ways of interacting with the hub that we can delete some of the code that currently does this with GitHub (and there is nothing nicer than deleting code üòÉ)\nI‚Äôll use the remainder of this blog to see if we can use the ü§ó hub and the [huggingface_hub](https://pypi.org/project/huggingface-hub/) library for this purpose as a replacement for the current approach.\nWe‚Äôll start by installing the huggingface_hub library\n\n# hide_output\n!pip install huggingface_hub\n\nRequirement already satisfied: huggingface_hub in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (0.2.1)\nRequirement already satisfied: requests in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (2.26.0)\nRequirement already satisfied: pyyaml in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (4.0.1)\nRequirement already satisfied: tqdm in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (4.62.3)\nRequirement already satisfied: filelock in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (3.4.0)\nRequirement already satisfied: packaging&gt;=20.9 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from packaging&gt;=20.9-&gt;huggingface_hub) (3.0.6)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from requests-&gt;huggingface_hub) (1.26.7)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from requests-&gt;huggingface_hub) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from requests-&gt;huggingface_hub) (2.0.8)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/dvanstrien/miniconda3/envs/blog/lib/python3.8/site-packages (from requests-&gt;huggingface_hub) (2021.10.8)"
  },
  {
    "objectID": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#getting-information-about-a-model",
    "href": "posts/post-with-code/2021-12-30-hf-hub-model-storage.html#getting-information-about-a-model",
    "title": "Using the ü§ó Hub for model storage",
    "section": "Getting information about a model",
    "text": "Getting information about a model\nOne of the things we need to be able to do is get the latest version of the model. One way we could try and do this is by grabbing metadata about the model. This is the current approach taken by flyswot. We can import model_info to do this:\n\nfrom huggingface_hub import model_info\n\n\ninfo = model_info(\"distilbert-base-cased\")\ninfo\n\nModelInfo: {\n    modelId: distilbert-base-cased\n    sha: 935ac13b473164bb9d578640e33d9f21144c365e\n    lastModified: 2020-12-11T21:23:53.000Z\n    tags: ['pytorch', 'tf', 'distilbert', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1910.01108', 'transformers', 'license:apache-2.0', 'infinity_compatible']\n    pipeline_tag: None\n    siblings: [ModelFile(rfilename='.gitattributes'), ModelFile(rfilename='README.md'), ModelFile(rfilename='config.json'), ModelFile(rfilename='pytorch_model.bin'), ModelFile(rfilename='tf_model.h5'), ModelFile(rfilename='tokenizer.json'), ModelFile(rfilename='tokenizer_config.json'), ModelFile(rfilename='vocab.txt')]\n    config: {'model_type': 'distilbert'}\n    id: distilbert-base-cased\n    private: False\n    downloads: 3556770\n    library_name: transformers\n    mask_token: [MASK]\n    likes: 4\n    model-index: None\n    cardData: {'language': 'en', 'license': 'apache-2.0', 'datasets': ['bookcorpus', 'wikipedia']}\n}\n\n\n\ntype(info)\n\nhuggingface_hub.hf_api.ModelInfo\n\n\nYou can see this gives us back a bunch of information about the model. We could for example grab the date the model was changed:\n\ninfo.lastModified\n\n'2020-12-11T21:23:53.000Z'\n\n\nThis already gives us what we need for checking if a model is updated in comparison to a local model already downloaded by the flyswot CLI. However we might be able to cut out some of this checking work.\nLets see if there are other ways we can do this in the library. Since huggingface_hub requires git-lfs lets start by installing this.\n\n!apt install git-lfs\n\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.3.4-1).\n0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n\n\nWe also need to make sure we have git-lfs setup\n\n!git init && git lfs install\n\n\nhint: Using 'master' as the name for the initial branch. This default branch name\n\nhint: is subject to change. To configure the initial branch name to use in all\n\nhint: of your new repositories, which will suppress this warning, call:\n\nhint: \n\nhint:  git config --global init.defaultBranch &lt;name&gt;\n\nhint: \n\nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n\nhint: 'development'. The just-created branch can be renamed via this command:\n\nhint: \n\nhint:  git branch -m &lt;name&gt;\n\nInitialized empty Git repository in /Users/dvanstrien/Documents/daniel/blog/_notebooks/.git/\n\nUpdated git hooks.\n\nGit LFS initialized.\n\n\n\n\n\nDownloading files from the hub\nWe can use hf_hub_url to get the url for a specific file from a repository\n\nfrom huggingface_hub import hf_hub_url\n\n\nonnx_model_url = hf_hub_url(\"davanstrien/flyswot-test\", \"2021-09-22.onnx\")\nonnx_model_url\n\n'https://huggingface.co/davanstrien/flyswot-test/resolve/main/2021-09-22.onnx'\n\n\nWe can pass this url to cached_download, this will download the file for us if we don‚Äôt have the latest version, we can also specify a place to download the file. This is important so we can make sure we put the file somewhere flyswot can find.\n\nfrom huggingface_hub import cached_download\n\n\ncached_download(onnx_model_url, cache_dir=\".\")\n\n\n\n\n'./de9d2ce054e3e410e3fc61b5c2ad55da7861d30e3b90aa018615b7d902e6e51e.1300a5792e44de2c59f4d25c4f7efd447ef91d69971a121e6b4df8b95047ad7c'\n\n\nIf we try and download this again it won‚Äôt download, and will instead return the path to the model\n\npath = cached_download(onnx_model_url, cache_dir=\".\")\npath\n\n'./de9d2ce054e3e410e3fc61b5c2ad55da7861d30e3b90aa018615b7d902e6e51e.1300a5792e44de2c59f4d25c4f7efd447ef91d69971a121e6b4df8b95047ad7c'\n\n\n\n\nDownloading all files from the hub\nThis is quite close to what we want our current approach requires us to get a bunch of different files in a folder. To replicate this we can instead use snapshot_download\n\nfrom huggingface_hub import snapshot_download\n\nLet‚Äôs see what this does\n\n?snapshot_download\n\n\nSignature:\nsnapshot_download(\n    repo_id: str,\n    revision: Union[str, NoneType] = None,\n    cache_dir: Union[str, pathlib.Path, NoneType] = None,\n    library_name: Union[str, NoneType] = None,\n    library_version: Union[str, NoneType] = None,\n    user_agent: Union[Dict, str, NoneType] = None,\n    proxies=None,\n    etag_timeout=10,\n    resume_download=False,\n    use_auth_token: Union[bool, str, NoneType] = None,\n    local_files_only=False,\n) -&gt; str\nDocstring:\nDownloads a whole snapshot of a repo's files at the specified revision.\nThis is useful when you want all files from a repo, because you don't know\nwhich ones you will need a priori.\nAll files are nested inside a folder in order to keep their actual filename\nrelative to that folder.\nAn alternative would be to just clone a repo but this would require that\nthe user always has git and git-lfs installed, and properly configured.\nNote: at some point maybe this format of storage should actually replace\nthe flat storage structure we've used so far (initially from allennlp\nif I remember correctly).\nReturn:\n    Local folder path (string) of repo snapshot\nFile:      ~/miniconda3/envs/blog/lib/python3.8/site-packages/huggingface_hub/snapshot_download.py\nType:      function\n\n\n\n\nThis will do something similar to cached_download but will instead do it for a whole model repository. If we pass our repository it will download the directory if we don‚Äôt have the latest version of the files, if for example, the model has been updated.\n\nmodel = snapshot_download(\"davanstrien/flyswot-test\", cache_dir=\".\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\n\n'./davanstrien__flyswot-test.main.e54a7421f5e5eb240783452ab734288f252bb402'\n\n\nIf we look inside this directory we can see we have the files from the repository.\n\n!ls {model}\n\n2021-09-22.onnx README.md       modelcard.md    vocab.txt\n\n\nIf we try and download it again we just get back the directory path without having to download the files again.\n\nmodel = snapshot_download(\"davanstrien/flyswot-test\", cache_dir=\".\")\nmodel\n\n'./davanstrien__flyswot-test.main.e54a7421f5e5eb240783452ab734288f252bb402'\n\n\nThis gives a replication of what we currently have setup for flyswot in terms of downloading models. There are a few extra things we might want though to be able to make flyswot more flexible. First though let‚Äôs look at how we can upload to the model hub.\n\n\nUploading to the hub\nAt the moment flyswot models are uploaded to a GitHub repository which then creates a release. It would be nice to simplify this and upload directly at the end of model training. For this we can use the Repository class.\n\nfrom huggingface_hub import Repository\n\n\n?Repository\n\n\nInit signature:\nRepository(\n    local_dir: str,\n    clone_from: Union[str, NoneType] = None,\n    repo_type: Union[str, NoneType] = None,\n    use_auth_token: Union[bool, str] = True,\n    git_user: Union[str, NoneType] = None,\n    git_email: Union[str, NoneType] = None,\n    revision: Union[str, NoneType] = None,\n    private: bool = False,\n    skip_lfs_files: bool = False,\n)\nDocstring:     \nHelper class to wrap the git and git-lfs commands.\nThe aim is to facilitate interacting with huggingface.co hosted model or dataset repos,\nthough not a lot here (if any) is actually specific to huggingface.co.\nInit docstring:\nInstantiate a local clone of a git repo.\nIf specifying a `clone_from`:\nwill clone an existing remote repository, for instance one\nthat was previously created using ``HfApi().create_repo(name=repo_name)``.\n``Repository`` uses the local git credentials by default, but if required, the ``huggingface_token``\nas well as the git ``user`` and the ``email`` can be explicitly specified.\nIf `clone_from` is used, and the repository is being instantiated into a non-empty directory,\ne.g. a directory with your trained model files, it will automatically merge them.\nArgs:\n    local_dir (``str``):\n        path (e.g. ``'my_trained_model/'``) to the local directory, where the ``Repository`` will be initalized.\n    clone_from (``str``, `optional`):\n        repository url (e.g. ``'https://huggingface.co/philschmid/playground-tests'``).\n    repo_type (``str``, `optional`):\n        To set when creating a repo: et to \"dataset\" or \"space\" if creating a dataset or space, default is model.\n    use_auth_token (``str`` or ``bool``, `optional`, defaults to ``True``):\n        huggingface_token can be extract from ``HfApi().login(username, password)`` and is used to authenticate against the hub\n        (useful from Google Colab for instance).\n    git_user (``str``, `optional`):\n        will override the ``git config user.name`` for committing and pushing files to the hub.\n    git_email (``str``, `optional`):\n        will override the ``git config user.email`` for committing and pushing files to the hub.\n    revision (``str``, `optional`):\n        Revision to checkout after initializing the repository. If the revision doesn't exist, a\n        branch will be created with that revision name from the default branch's current HEAD.\n    private (``bool``, `optional`, defaults to ``False``):\n        whether the repository is private or not.\n    skip_lfs_files (``bool``, `optional`, defaults to ``False``):\n        whether to skip git-LFS files or not.\nFile:           ~/miniconda3/envs/blog/lib/python3.8/site-packages/huggingface_hub/repository.py\nType:           type\nSubclasses:     \n\n\n\n\nI‚Äôll use flyswot-test as a way of playing around with this. To start with we can use Repository to clone the current version of the model.\n\nrepo = Repository(local_dir=\"flyswot-models\", clone_from=\"davanstrien/flyswot-test\")\n\nCloning https://huggingface.co/davanstrien/flyswot-test into local empty directory.\n\n\n\nrepo\n\n&lt;huggingface_hub.repository.Repository at 0x7fb10cccbc10&gt;\n\n\nWe‚Äôll need to be logged in to push changes\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n\n\n\nTo start with let‚Äôs mock making a change to some of the repo files and seeing how we can upload these changes. We can use the Repository class as a context manager to make changes and have them committed to our model repository. Here we update the vocab file to add a new label.\n\nwith Repository(\n    local_dir=\"flyswot-models\",\n    clone_from=\"davanstrien/flyswot-test\",\n    git_user=\"Daniel van Strien\",\n).commit(\"update model\"):\n    with open(\"vocab.txt\", \"a\") as f:\n        f.write(\"new label\")\n\n/Users/dvanstrien/Documents/daniel/blog/_notebooks/flyswot-models is already a clone of https://huggingface.co/davanstrien/flyswot-test. Make sure you pull the latest changes with `repo.git_pull()`.\nPulling changes ...\nTo https://huggingface.co/davanstrien/flyswot-test\n   e54a742..18d149e  main -&gt; main\n\n\n\nThis could already be used at the end of our training script. Currently I have some util files that package up the model vocab, convert Pytorch to ONNX etc. This could easily be adapted to also push the updated model to the hub. There is only one thing we might still want to add.\n\n\nAdding more metadata: creating revision branches\nCurrently flyswot uses filenames to capture metadata about the model version. The models are versioned using calendar versioning. This works okay but we might be able to manage this in a slightly better way. One of the nice features that hf_hub (the Python library) offers that flyswot currently doesn‚Äôt support well is being able to pass in a specific revision when using snapshot_download. This would then allow someone to run a specific older version of the model. This might be useful for various different scenarios. To do this we‚Äôll create a revision branch for the date the model was created. All that we‚Äôll do now is pass in a formatted date as the revision.\n\nfrom datetime import datetime\n\n\ndate_now = datetime.now()\nnow = date_now.strftime(\"%Y-%m-%d\")\nnow\n\n'2021-12-30'\n\n\n\nwith Repository(\n    local_dir=\"flyswot-models\",\n    clone_from=\"davanstrien/flyswot-test\",\n    git_user=\"Daniel van Strien\",\n    revision=now,\n).commit(f\"update model {now}\"):\n    for model in Path(\".\").glob(\".onnx\"):\n        model.rename(f\"{now}-model.onnx\")\n\n/Users/dvanstrien/Documents/daniel/blog/_notebooks/flyswot-models is already a clone of https://huggingface.co/davanstrien/flyswot-test. Make sure you pull the latest changes with `repo.git_pull()`.\nChecked out 2021-12-30 from 2021-12-30.\nYour branch is up to date with 'origin/2021-12-30'.\n\nPulling changes ...\nSeveral commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\nEverything up-to-date\n\n\n\nThis creates a new revision branch for the current date. Since I also want to have the default branch be the current model we would also push the same model to the default branch. This would then mean that we end up with a bunch of different branches with model snapshots that could be passed in but for the default behavior we can easily grab the latest model by not specifying a revision."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html",
    "title": "Training an object detection model using Hugging Face",
    "section": "",
    "text": "The Hugging Face transformers library has increasingly expanded from its original focus on Natural Language Processing tasks to include more models covering a range of computer vision tasks. This blog post will look at how we can train an object detection model using the Hugging Face transformers and datasets libraries."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#training-a-detr-object-detection-model-using-hugging-face-transformers-and-datasets",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#training-a-detr-object-detection-model-using-hugging-face-transformers-and-datasets",
    "title": "Training an object detection model using Hugging Face",
    "section": "",
    "text": "The Hugging Face transformers library has increasingly expanded from its original focus on Natural Language Processing tasks to include more models covering a range of computer vision tasks. This blog post will look at how we can train an object detection model using the Hugging Face transformers and datasets libraries."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#what-is-object-detection",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#what-is-object-detection",
    "title": "Training an object detection model using Hugging Face",
    "section": "What is object detection?",
    "text": "What is object detection?\nObject detection is the task of predicting objects contained within an image.\n\nObject detection can be helpful in several applications where you want to know not only whether a thing is in an image but where (and how many) of that thing there are. Various approaches have been developed over the years for this task, often relying on various complex hand-crafted features.\nAs with other areas of computer vision, there has been an increasing adoption of transformer-based solutions to this task. One model using transformers is the Detr architecture."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#what-is-detr",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#what-is-detr",
    "title": "Training an object detection model using Hugging Face",
    "section": "What is Detr?",
    "text": "What is Detr?\n\n\n\nDiagram of the DETR architecture\n\n\nDetr (DEtection TRansformer) is a model architecture introduced in the paper End-to-End Object Detection with Transformers. We won‚Äôt dig into the architecture in massive detail in this blog since we‚Äôre focused on the practical use of this model architecture in this post. One thing that is important to note here is that DETR still uses a CNN backbone. More recently, other models such as YOLOS use a transformer backbone too. Currently, however, these fully transformer-based approaches show some performance gap over more traditional techniques (because this is deep learning, ‚Äòtraditional‚Äô refers to stuff from last year, of course)."
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#using-hugging-face-for-object-detection",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#using-hugging-face-for-object-detection",
    "title": "Training an object detection model using Hugging Face",
    "section": "Using Hugging Face for object detection",
    "text": "Using Hugging Face for object detection\nThere are existing examples for using the Hugging Face transformers library and datasets with the Trainer class to do image classification. There are also example notebooks showing how to fine-tune a Detr model on custom data. However, I didn‚Äôt find examples that use the datasets library and the Trainer class to manage training. Training an object detection model using datasets and the transformers library is what this blog post covers.\n\nWhy the datasets library?\nYou may ask why it is helpful to provide an example of using the datasets library for training an object detection model, i.e.¬†why not use PyTorch for the data loading, which already has many examples for training object detection models?\nThere are a few reasons why trying to use datasets for this can be helpful. A significant one for me is the close integration between the datasets library and the Hugging Face datasets hub. Loading a dataset from the hugging face hub often involves two lines of code (including the imports).\nQuickly loading a dataset and then using the same library to prepare the dataset for training an object detection model removes some friction. This becomes especially helpful when you are iterating on the process of creating training data, training a model, and creating more training data. In this iterative process, the hub can be used for storing models and datasets at each stage. Having a clear provenance of these changes (without relying on additional tools) is also a benefit of this workflow. This is the kind of pipeline hugit is intended to support (in this case, for image classification models).\n\n\nScope of this blog post\nAt the moment, this is mainly intended to give a quick overview of the steps involved. It isn‚Äôt intended to be a proper tutorial. If I have time later, I may flesh this out (particularly if other projects I‚Äôm working on that use object detection progress further).\nEnough talk, let‚Äôs get started. First we install required libraries.\n\n%%capture\n!pip install datasets transformers timm wandb rich[jupyter]\n\nI‚Äôm a big fan of the rich library so almost always have this extension loaded.\n\n%load_ext rich\n\nThe next couple of lines gets us authenticated with the Hugging Face hub.\n\n!git config --global credential.helper store\n\n\nfrom huggingface_hub import notebook_login\n\n\nnotebook_login()\n\nLogin successful\nYour token has been saved to /root/.huggingface/token\n\n\nWe‚Äôll use Weights and Biases for tracking our model training.\n\nimport wandb\n\n\nwandb.login()\n\n\n%env WANDB_PROJECT=chapbooks\n%env WANDB_ENTITY=davanstrien"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#loading-the-dataset",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#loading-the-dataset",
    "title": "Training an object detection model using Hugging Face",
    "section": "Loading the dataset",
    "text": "Loading the dataset\nIn this blog post will use a dataset being added to the Hugging Face datasets hub as part of the BigLAM hackathon. This dataset has a configuration for object detection and image classification, so we‚Äôll need to specify which one we want. Since the dataset doesn‚Äôt define train/test/valid splits for us, we‚Äôll grab the training split. I won‚Äôt provide a full description of the dataset in this blog post since the dataset is still in the process of being documented. The tl;dr summary is that the dataset includes images of digitized books with bounding boxes for illustrations.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\n    \"biglam/nls_chapbook_illustrations\", \"illustration-detection\", split=\"train\"\n)\n\nReusing dataset nls_chapbook_illustrations (/Users/dvanstrien/.cache/huggingface/datasets/biglam___nls_chapbook_illustrations/illustration-detection/1.0.0/75f355eb0ba564ef120939a78730eb187a4d3eb682e987ed1f682a5bea5466eb)\n\n\nLet‚Äôs take a look at one example from this dataset to get a sense of how the data looks\n\ndataset[0]\n\n{\n    'image_id': 4,\n    'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x1080 at 0x7FDD6504FAD0&gt;,\n    'width': 600,\n    'height': 1080,\n    'url': None,\n    'date_captured': '',\n    'objects': [\n        {\n            'category_id': 0,\n            'image_id': '4',\n            'id': 1,\n            'area': 110901,\n            'bbox': [34.529998779296875, 556.8300170898438, 401.44000244140625, 276.260009765625],\n            'segmentation': [\n                [\n                    34.529998779296875,\n                    556.8300170898438,\n                    435.9700012207031,\n                    556.8300170898438,\n                    435.9700012207031,\n                    833.0900268554688,\n                    34.529998779296875,\n                    833.0900268554688\n                ]\n            ],\n            'iscrowd': False\n        }\n    ]\n}\n\n\n\nYou will see we hav some metadata for the image, the image itself and the field objects contains the annotations themselves. Looking just at an example of the annotations:\n\n# hide_output\n{\n    \"category_id\": 0,\n    \"image_id\": \"4\",\n    \"id\": 1,\n    \"area\": 110901,\n    \"bbox\": [\n        34.529998779296875,\n        556.8300170898438,\n        401.44000244140625,\n        276.260009765625,\n    ],\n    \"segmentation\": [\n        [\n            34.529998779296875,\n            556.8300170898438,\n            435.9700012207031,\n            556.8300170898438,\n            435.9700012207031,\n            833.0900268554688,\n            34.529998779296875,\n            833.0900268554688,\n        ]\n    ],\n    \"iscrowd\": False,\n}\n\n{'category_id': 0,\n 'image_id': '4',\n 'id': 1,\n 'area': 110901,\n 'bbox': [34.529998779296875,\n  556.8300170898438,\n  401.44000244140625,\n  276.260009765625],\n 'segmentation': [[34.529998779296875,\n   556.8300170898438,\n   435.9700012207031,\n   556.8300170898438,\n   435.9700012207031,\n   833.0900268554688,\n   34.529998779296875,\n   833.0900268554688]],\n 'iscrowd': False}\n\n\nWe see here, that we again have some metadata for each image. We also have a category_id and a bbox. Some of these fields should look familiar to you if you are familiar with the coco format. This will become relevant later, so don‚Äôt worry if these aren‚Äôt familiar to you.\nOne issue we can run into when training object detection models is stray bounding boxes (i.e.¬†ones where the bounding boxes stretch beyond the edge of the image). We can check and remove these quite easily. This is some ugly code/there is probably a better way, but this is a quick check, so I‚Äôll forgive myself.\n\nfrom tqdm.auto import tqdm\n\n\nremove_idx = []\nfor idx, row in tqdm(enumerate(dataset)):\n    objects_ = row[\"objects\"]\n    for ob in objects_:\n        bbox = ob[\"bbox\"]\n        negative = [box for box in bbox if box &lt; 0]\n        if negative:\n            remove_idx.append(idx)\n\n\n\n\n\nlen(remove_idx)\n\n1\n\n\n\n\nkeep = [i for i in range(len(dataset)) if i not in remove_idx]\nlen(keep)\n\n7257\n\n\n\nThe above code has given us a list of indexes to keep so we use the select method to grab those.\n\ndataset = dataset.select(keep)\n\nWe also create a test split. If we were properly doing this we‚Äôd likely want to be a bit more thoughfull about how to do this split.\n\ndataset = dataset.train_test_split(0.1)"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#preparing-the-data",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#preparing-the-data",
    "title": "Training an object detection model using Hugging Face",
    "section": "Preparing the data",
    "text": "Preparing the data\nThis section of the blog post is the bit which focuses on getting data ready for an object detection model such as detr via the datasets library. This is, therefore, also the section which will differ most from the other examples showing how to train models using PyTorch data loaders.\n\nThe Feature Extractor\nIf you are familiar with Hugging Face for natural language tasks, you are probably familiar with using Tokenizer_for_blah_model when pre-processing text. Often if you are using a pre-trained model, you will use AutoTokenizer.from_pretrained, passing in the ID to the model you want to fine-tune. This tokenizer then ensures that the tokenization matches the approach used for the pre-trained model.\nThe Feature Extractor performs a similar task. Let‚Äôs look at this more closely. We‚Äôll use a pre-trained model for this example and fine-tune it. I also include commented-out code, which shows how you could use the same process with any CNN backbone. This may be useful if you have particular requirements about what backbone to use or if you have a CNN backbone that is already fine-tuned on your domain.\n\nmodel_checkpoint = \"facebook/detr-resnet-50\"\n\n\nfrom transformers import DetrFeatureExtractor\n\nfeature_extractor = DetrFeatureExtractor.from_pretrained(model_checkpoint)\n\n\n\n\nIf you wanted to use a different CNN backbone as your starting point you would instead define a config.\n# from transformers import DetrConfig\n# from transformers import DetrFeatureExtractor\n# feature_extractor = DetrFeatureExtractor()\n\nWhat does the feature extractor do?\nTo check what feature extractor does we can make use of the handy inspect function\n\nfrom rich import inspect\n\n\n# collapse_show\ninspect(feature_extractor, methods=True, dunder=True)\n\n‚ï≠‚îÄ DetrFeatureExtractor {   \"do_normalize\": true,   \"do_resize\": true,   \"feature_extractor_type\": \"DetrFeatureEx‚îÄ‚ïÆ\n‚îÇ def (images: Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), List[PIL.Image.Image],           ‚îÇ\n‚îÇ List[numpy.ndarray], List[ForwardRef('torch.Tensor')]], annotations: Union[List[Dict], List[List[Dict]]] =      ‚îÇ\n‚îÇ None, return_segmentation_masks: Union[bool, NoneType] = False, masks_path: Union[pathlib.Path, NoneType] =     ‚îÇ\n‚îÇ None, pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors: Union[str,                       ‚îÇ\n‚îÇ transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt;                                           ‚îÇ\n‚îÇ transformers.feature_extraction_utils.BatchFeature:                                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Constructs a DETR feature extractor.                                                                            ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ                _auto_class = None                                                                               ‚îÇ\n‚îÇ                   __dict__ = {                                                                                  ‚îÇ\n‚îÇ                                  '_processor_class': None,                                                      ‚îÇ\n‚îÇ                                  'feature_extractor_type': 'DetrFeatureExtractor',                              ‚îÇ\n‚îÇ                                  'format': 'coco_detection',                                                    ‚îÇ\n‚îÇ                                  'do_resize': True,                                                             ‚îÇ\n‚îÇ                                  'size': 800,                                                                   ‚îÇ\n‚îÇ                                  'max_size': 1333,                                                              ‚îÇ\n‚îÇ                                  'do_normalize': True,                                                          ‚îÇ\n‚îÇ                                  'image_mean': [0.485, 0.456, 0.406],                                           ‚îÇ\n‚îÇ                                  'image_std': [0.229, 0.224, 0.225]                                             ‚îÇ\n‚îÇ                              }                                                                                  ‚îÇ\n‚îÇ               do_normalize = True                                                                               ‚îÇ\n‚îÇ                  do_resize = True                                                                               ‚îÇ\n‚îÇ                    __doc__ = '\\n    Constructs a DETR feature extractor.\\n\\n    This feature extractor inherits ‚îÇ\n‚îÇ                              from [`FeatureExtractionMixin`] which contains most of the main methods. Users\\n   ‚îÇ\n‚îÇ                              should refer to this superclass for more information regarding those               ‚îÇ\n‚îÇ                              methods.\\n\\n\\n    Args:\\n        format (`str`, *optional*, defaults to            ‚îÇ\n‚îÇ                              `\"coco_detection\"`):\\n            Data format of the annotations. One of           ‚îÇ\n‚îÇ                              \"coco_detection\" or \"coco_panoptic\".\\n        do_resize (`bool`, *optional*,       ‚îÇ\n‚îÇ                              defaults to `True`):\\n            Whether to resize the input to a certain         ‚îÇ\n‚îÇ                              `size`.\\n        size (`int`, *optional*, defaults to 800):\\n            Resize    ‚îÇ\n‚îÇ                              the input to the given size. Only has an effect if `do_resize` is set to `True`.   ‚îÇ\n‚îÇ                              If size is a\\n            sequence like `(width, height)`, output size will be     ‚îÇ\n‚îÇ                              matched to this. If size is an int, smaller edge of\\n            the image will be ‚îÇ\n‚îÇ                              matched to this number. i.e, if `height &gt; width`, then image will be rescaled to   ‚îÇ\n‚îÇ                              `(size *\\n            height / width, size)`.\\n        max_size (`int`,            ‚îÇ\n‚îÇ                              *optional*, defaults to `1333`):\\n            The largest size an image dimension  ‚îÇ\n‚îÇ                              can have (otherwise it\\'s capped). Only has an effect if `do_resize` is\\n          ‚îÇ\n‚îÇ                              set to `True`.\\n        do_normalize (`bool`, *optional*, defaults to `True`):\\n   ‚îÇ\n‚îÇ                              Whether or not to normalize the input with mean and standard deviation.\\n          ‚îÇ\n‚îÇ                              image_mean (`int`, *optional*, defaults to `[0.485, 0.456, 0.406]`):\\n             ‚îÇ\n‚îÇ                              The sequence of means for each channel, to be used when normalizing images.        ‚îÇ\n‚îÇ                              Defaults to the ImageNet mean.\\n        image_std (`int`, *optional*, defaults to  ‚îÇ\n‚îÇ                              `[0.229, 0.224, 0.225]`):\\n            The sequence of standard deviations for     ‚îÇ\n‚îÇ                              each channel, to be used when normalizing images. Defaults to the\\n                ‚îÇ\n‚îÇ                              ImageNet std.\\n    '                                                               ‚îÇ\n‚îÇ     feature_extractor_type = 'DetrFeatureExtractor'                                                             ‚îÇ\n‚îÇ                     format = 'coco_detection'                                                                   ‚îÇ\n‚îÇ                 image_mean = [0.485, 0.456, 0.406]                                                              ‚îÇ\n‚îÇ                  image_std = [0.229, 0.224, 0.225]                                                              ‚îÇ\n‚îÇ                   max_size = 1333                                                                               ‚îÇ\n‚îÇ          model_input_names = ['pixel_values', 'pixel_mask']                                                     ‚îÇ\n‚îÇ                 __module__ = 'transformers.models.detr.feature_extraction_detr'                                 ‚îÇ\n‚îÇ           _processor_class = None                                                                               ‚îÇ\n‚îÇ                       size = 800                                                                                ‚îÇ\n‚îÇ                __weakref__ = None                                                                               ‚îÇ\n‚îÇ                   __call__ = def __call__(images: Union[PIL.Image.Image, numpy.ndarray,                         ‚îÇ\n‚îÇ                              ForwardRef('torch.Tensor'), List[PIL.Image.Image], List[numpy.ndarray],            ‚îÇ\n‚îÇ                              List[ForwardRef('torch.Tensor')]], annotations: Union[List[Dict],                  ‚îÇ\n‚îÇ                              List[List[Dict]]] = None, return_segmentation_masks: Union[bool, NoneType] =       ‚îÇ\n‚îÇ                              False, masks_path: Union[pathlib.Path, NoneType] = None,                           ‚îÇ\n‚îÇ                              pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors:           ‚îÇ\n‚îÇ                              Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt;   ‚îÇ\n‚îÇ                              transformers.feature_extraction_utils.BatchFeature:                                ‚îÇ\n‚îÇ                              Main method to prepare for the model one or several image(s) and optional          ‚îÇ\n‚îÇ                              annotations. Images are by default                                                 ‚îÇ\n‚îÇ                              padded up to the largest image in a batch, and a pixel mask is created that        ‚îÇ\n‚îÇ                              indicates which pixels are                                                         ‚îÇ\n‚îÇ                              real/which are padding.                                                            ‚îÇ\n‚îÇ                center_crop = def center_crop(image, size):                                                      ‚îÇ\n‚îÇ                              Crops `image` to the given size using a center crop. Note that if the image is too ‚îÇ\n‚îÇ                              small to be cropped to the                                                         ‚îÇ\n‚îÇ                              size given, it will be padded (so the returned result has the size asked).         ‚îÇ\n‚îÇ                  __class__ = class __class__(format='coco_detection', do_resize=True, size=800, max_size=1333,  ‚îÇ\n‚îÇ                              do_normalize=True, image_mean=None, image_std=None, **kwargs): Constructs a DETR   ‚îÇ\n‚îÇ                              feature extractor.                                                                 ‚îÇ\n‚îÇ  convert_coco_poly_to_mask = def convert_coco_poly_to_mask(segmentations, height, width):                       ‚îÇ\n‚îÇ                convert_rgb = def convert_rgb(image): Converts `PIL.Image.Image` to RGB format.                  ‚îÇ\n‚îÇ        _create_or_get_repo = def _create_or_get_repo(repo_path_or_name: Union[str, NoneType] = None, repo_url:  ‚îÇ\n‚îÇ                              Union[str, NoneType] = None, organization: Union[str, NoneType] = None, private:   ‚îÇ\n‚îÇ                              bool = None, use_auth_token: Union[bool, str, NoneType] = None) -&gt;                 ‚îÇ\n‚îÇ                              huggingface_hub.repository.Repository:                                             ‚îÇ\n‚îÇ                __delattr__ = def __delattr__(name, /): Implement delattr(self, name).                           ‚îÇ\n‚îÇ                    __dir__ = def __dir__(): Default dir() implementation.                                       ‚îÇ\n‚îÇ   _ensure_format_supported = def _ensure_format_supported(image):                                               ‚îÇ\n‚îÇ                     __eq__ = def __eq__(value, /): Return self==value.                                          ‚îÇ\n‚îÇ                expand_dims = def expand_dims(image): Expands 2-dimensional `image` to 3 dimensions.             ‚îÇ\n‚îÇ                 __format__ = def __format__(format_spec, /): Default object formatter.                          ‚îÇ\n‚îÇ                  from_dict = def from_dict(feature_extractor_dict: Dict[str, Any], **kwargs) -&gt;                 ‚îÇ\n‚îÇ                              ForwardRef('SequenceFeatureExtractor'):                                            ‚îÇ\n‚îÇ                              Instantiates a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a ‚îÇ\n‚îÇ                              Python dictionary of                                                               ‚îÇ\n‚îÇ                              parameters.                                                                        ‚îÇ\n‚îÇ             from_json_file = def from_json_file(json_file: Union[str, os.PathLike]) -&gt;                          ‚îÇ\n‚îÇ                              ForwardRef('SequenceFeatureExtractor'):                                            ‚îÇ\n‚îÇ                              Instantiates a feature extractor of type                                           ‚îÇ\n‚îÇ                              [`~feature_extraction_utils.FeatureExtractionMixin`] from the path to              ‚îÇ\n‚îÇ                              a JSON file of parameters.                                                         ‚îÇ\n‚îÇ            from_pretrained = def from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike],        ‚îÇ\n‚îÇ                              **kwargs) -&gt; ForwardRef('SequenceFeatureExtractor'):                               ‚îÇ\n‚îÇ                              Instantiate a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a  ‚îÇ\n‚îÇ                              feature extractor, *e.g.* a                                                        ‚îÇ\n‚îÇ                              derived class of [`SequenceFeatureExtractor`].                                     ‚îÇ\n‚îÇ                     __ge__ = def __ge__(value, /): Return self&gt;=value.                                          ‚îÇ\n‚îÇ get_feature_extractor_dict = def get_feature_extractor_dict(pretrained_model_name_or_path: Union[str,           ‚îÇ\n‚îÇ                              os.PathLike], **kwargs) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:                  ‚îÇ\n‚îÇ                              From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to  ‚îÇ\n‚îÇ                              be used for instantiating a                                                        ‚îÇ\n‚îÇ                              feature extractor of type [`~feature_extraction_utils.FeatureExtractionMixin`]     ‚îÇ\n‚îÇ                              using `from_dict`.                                                                 ‚îÇ\n‚îÇ    _get_repo_url_from_name = def _get_repo_url_from_name(repo_name: str, organization: Union[str, NoneType] =   ‚îÇ\n‚îÇ                              None, private: bool = None, use_auth_token: Union[bool, str, NoneType] = None) -&gt;  ‚îÇ\n‚îÇ                              str:                                                                               ‚îÇ\n‚îÇ           __getattribute__ = def __getattribute__(name, /): Return getattr(self, name).                         ‚îÇ\n‚îÇ                     __gt__ = def __gt__(value, /): Return self&gt;value.                                           ‚îÇ\n‚îÇ                   __hash__ = def __hash__(): Return hash(self).                                                 ‚îÇ\n‚îÇ                   __init__ = def __init__(format='coco_detection', do_resize=True, size=800, max_size=1333,     ‚îÇ\n‚îÇ                              do_normalize=True, image_mean=None, image_std=None, **kwargs): Set elements of     ‚îÇ\n‚îÇ                              `kwargs` as attributes.                                                            ‚îÇ\n‚îÇ          __init_subclass__ = def __init_subclass__(...) This method is called when a class is subclassed.       ‚îÇ\n‚îÇ           _is_valid_format = def _is_valid_format(format):                                                      ‚îÇ\n‚îÇ                     __le__ = def __le__(value, /): Return self&lt;=value.                                          ‚îÇ\n‚îÇ                     __lt__ = def __lt__(value, /): Return self&lt;value.                                           ‚îÇ\n‚îÇ               _max_by_axis = def _max_by_axis(the_list):                                                        ‚îÇ\n‚îÇ                     __ne__ = def __ne__(value, /): Return self!=value.                                          ‚îÇ\n‚îÇ                    __new__ = def __new__(*args, **kwargs): Create and return a new object.  See help(type) for  ‚îÇ\n‚îÇ                              accurate signature.                                                                ‚îÇ\n‚îÇ                 _normalize = def _normalize(image, mean, std, target=None): Normalize the image with a certain  ‚îÇ\n‚îÇ                              mean and std.                                                                      ‚îÇ\n‚îÇ                  normalize = def normalize(image, mean, std):                                                   ‚îÇ\n‚îÇ                              Normalizes `image` with `mean` and `std`. Note that this will trigger a conversion ‚îÇ\n‚îÇ                              of `image` to a NumPy array                                                        ‚îÇ\n‚îÇ                              if it's a PIL Image.                                                               ‚îÇ\n‚îÇ  pad_and_create_pixel_mask = def pad_and_create_pixel_mask(pixel_values_list: List[ForwardRef('torch.Tensor')], ‚îÇ\n‚îÇ                              return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] =      ‚îÇ\n‚îÇ                              None): Pad images up to the largest image in a batch and create a corresponding    ‚îÇ\n‚îÇ                              `pixel_mask`.                                                                      ‚îÇ\n‚îÇ               post_process = def post_process(outputs, target_sizes):                                           ‚îÇ\n‚îÇ                              Converts the output of [`DetrForObjectDetection`] into the format expected by the  ‚îÇ\n‚îÇ                              COCO api. Only supports                                                            ‚îÇ\n‚îÇ                              PyTorch.                                                                           ‚îÇ\n‚îÇ      post_process_instance = def post_process_instance(results, outputs, orig_target_sizes, max_target_sizes,   ‚îÇ\n‚îÇ                              threshold=0.5):                                                                    ‚îÇ\n‚îÇ                              Converts the output of [`DetrForSegmentation`] into actual instance segmentation   ‚îÇ\n‚îÇ                              predictions. Only supports                                                         ‚îÇ\n‚îÇ                              PyTorch.                                                                           ‚îÇ\n‚îÇ      post_process_panoptic = def post_process_panoptic(outputs, processed_sizes, target_sizes=None,             ‚îÇ\n‚îÇ                              is_thing_map=None, threshold=0.85): Converts the output of [`DetrForSegmentation`] ‚îÇ\n‚îÇ                              into actual panoptic predictions. Only supports PyTorch.                           ‚îÇ\n‚îÇ  post_process_segmentation = def post_process_segmentation(outputs, target_sizes, threshold=0.9,                ‚îÇ\n‚îÇ                              mask_threshold=0.5): Converts the output of [`DetrForSegmentation`] into image     ‚îÇ\n‚îÇ                              segmentation predictions. Only supports PyTorch.                                   ‚îÇ\n‚îÇ                    prepare = def prepare(image, target, return_segmentation_masks=False, masks_path=None):      ‚îÇ\n‚îÇ     prepare_coco_detection = def prepare_coco_detection(image, target, return_segmentation_masks=False):        ‚îÇ\n‚îÇ                              Convert the target in COCO format into the format expected by DETR.                ‚îÇ\n‚îÇ      prepare_coco_panoptic = def prepare_coco_panoptic(image, target, masks_path, return_masks=True):           ‚îÇ\n‚îÇ               _push_to_hub = def _push_to_hub(repo: huggingface_hub.repository.Repository, commit_message:      ‚îÇ\n‚îÇ                              Union[str, NoneType] = None) -&gt; str:                                               ‚îÇ\n‚îÇ                push_to_hub = def push_to_hub(repo_path_or_name: Union[str, NoneType] = None, repo_url:          ‚îÇ\n‚îÇ                              Union[str, NoneType] = None, use_temp_dir: bool = False, commit_message:           ‚îÇ\n‚îÇ                              Union[str, NoneType] = None, organization: Union[str, NoneType] = None, private:   ‚îÇ\n‚îÇ                              Union[bool, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None,   ‚îÇ\n‚îÇ                              **model_card_kwargs) -&gt; str:                                                       ‚îÇ\n‚îÇ                              Upload the feature extractor file to the ü§ó Model Hub while synchronizing a local  ‚îÇ\n‚îÇ                              clone of the repo in                                                               ‚îÇ\n‚îÇ                              `repo_path_or_name`.                                                               ‚îÇ\n‚îÇ                 __reduce__ = def __reduce__(): Helper for pickle.                                               ‚îÇ\n‚îÇ              __reduce_ex__ = def __reduce_ex__(protocol, /): Helper for pickle.                                 ‚îÇ\n‚îÇ    register_for_auto_class = def register_for_auto_class(auto_class='AutoFeatureExtractor'):                    ‚îÇ\n‚îÇ                              Register this class with a given auto class. This should only be used for custom   ‚îÇ\n‚îÇ                              feature extractors as the ones                                                     ‚îÇ\n‚îÇ                              in the library are already mapped with `AutoFeatureExtractor`.                     ‚îÇ\n‚îÇ                   __repr__ = def __repr__(): Return repr(self).                                                 ‚îÇ\n‚îÇ                    _resize = def _resize(image, size, target=None, max_size=None):                              ‚îÇ\n‚îÇ                              Resize the image to the given size. Size can be min_size (scalar) or (w, h) tuple. ‚îÇ\n‚îÇ                              If size is an int, smaller                                                         ‚îÇ\n‚îÇ                              edge of the image will be matched to this number.                                  ‚îÇ\n‚îÇ                     resize = def resize(image, size, resample=2, default_to_square=True, max_size=None):        ‚îÇ\n‚îÇ                              Resizes `image`. Enforces conversion of input to PIL.Image.                        ‚îÇ\n‚îÇ            save_pretrained = def save_pretrained(save_directory: Union[str, os.PathLike], push_to_hub: bool =   ‚îÇ\n‚îÇ                              False, **kwargs):                                                                  ‚îÇ\n‚îÇ                              Save a feature_extractor object to the directory `save_directory`, so that it can  ‚îÇ\n‚îÇ                              be re-loaded using the                                                             ‚îÇ\n‚îÇ                              [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`] class method. ‚îÇ\n‚îÇ       _set_processor_class = def _set_processor_class(processor_class: str): Sets processor class as an         ‚îÇ\n‚îÇ                              attribute.                                                                         ‚îÇ\n‚îÇ                __setattr__ = def __setattr__(name, value, /): Implement setattr(self, name, value).             ‚îÇ\n‚îÇ                 __sizeof__ = def __sizeof__(): Size of object in memory, in bytes.                              ‚îÇ\n‚îÇ                    __str__ = def __str__(): Return str(self).                                                   ‚îÇ\n‚îÇ           __subclasshook__ = def __subclasshook__(...) Abstract classes can override this to customize          ‚îÇ\n‚îÇ                              issubclass().                                                                      ‚îÇ\n‚îÇ                    to_dict = def to_dict() -&gt; Dict[str, Any]: Serializes this instance to a Python dictionary.  ‚îÇ\n‚îÇ               to_json_file = def to_json_file(json_file_path: Union[str, os.PathLike]): Save this instance to a ‚îÇ\n‚îÇ                              JSON file.                                                                         ‚îÇ\n‚îÇ             to_json_string = def to_json_string() -&gt; str: Serializes this instance to a JSON string.            ‚îÇ\n‚îÇ             to_numpy_array = def to_numpy_array(image, rescale=None, channel_first=True):                       ‚îÇ\n‚îÇ                              Converts `image` to a numpy array. Optionally rescales it and puts the channel     ‚îÇ\n‚îÇ                              dimension as the first                                                             ‚îÇ\n‚îÇ                              dimension.                                                                         ‚îÇ\n‚îÇ               to_pil_image = def to_pil_image(image, rescale=None):                                             ‚îÇ\n‚îÇ                              Converts `image` to a PIL Image. Optionally rescales it and puts the channel       ‚îÇ\n‚îÇ                              dimension back as the last axis if                                                 ‚îÇ\n‚îÇ                              needed.                                                                            ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nThe output of inspect can be pretty verbose, but I often find this a handy tool for quickly trying to work out a new library of API.\nWe‚Äôll look at the most critical parts in more detail, but I‚Äôll point out a few things; you‚Äôll see some attributes that will probably sound familiar.\nimage_mean = [0.485, 0.456, 0.406]                                                              \nimage_std = [0.229, 0.224, 0.225]          \nThese are the mean and standard deviation used during the model training. It‚Äôs essential when we‚Äôre doing inference or fine-tuning to replicate these, and having these all stored inside a feature_extractor means we don‚Äôt have to go poking around in papers to try and work out what these values should be.\nAnother thing to point out is the push_to_hub method. We can store feature_extractors in the hub just as we can store models and tokenizers. Having to track the appropriate pre-processing steps for an image manually is super annoying to do manually. Storing this as we do other model components is much simpler and helps avoid errors resulting from tracing these things by hand.\nThe __call__ method for the DetrFeatureExtractor is what we‚Äôll use to prepare our images before we pass them into the model, let‚Äôs dig more closely into this.\n\ninspect(\n    feature_extractor.__call__,\n)\n\n‚ï≠‚îÄ &lt;bound method DetrFeatureExtractor.__call__ of DetrFeatureExtractor {   \"do_normalize\": true,   \"do_resize\": t‚îÄ‚ïÆ\n‚îÇ def DetrFeatureExtractor.__call__(images: Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'),     ‚îÇ\n‚îÇ List[PIL.Image.Image], List[numpy.ndarray], List[ForwardRef('torch.Tensor')]], annotations: Union[List[Dict],   ‚îÇ\n‚îÇ List[List[Dict]]] = None, return_segmentation_masks: Union[bool, NoneType] = False, masks_path:                 ‚îÇ\n‚îÇ Union[pathlib.Path, NoneType] = None, pad_and_return_pixel_mask: Union[bool, NoneType] = True, return_tensors:  ‚îÇ\n‚îÇ Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -&gt;                                ‚îÇ\n‚îÇ transformers.feature_extraction_utils.BatchFeature:                                                             ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ Main method to prepare for the model one or several image(s) and optional annotations. Images are by default    ‚îÇ\n‚îÇ padded up to the largest image in a batch, and a pixel mask is created that indicates which pixels are          ‚îÇ\n‚îÇ real/which are padding.                                                                                         ‚îÇ\n‚îÇ                                                                                                                 ‚îÇ\n‚îÇ 27 attribute(s) not shown. Run inspect(inspect) for options.                                                    ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\nUnderstanding what the __call__ method expected, and how to make sure that is what‚Äôs delivered by the datasets library is the key thing I needed to work out. What does it expect:\n\nimages: this can be a list or a single image (and stored in different formats)\nannotations this should be of type Union[List[Dict],‚îÇ‚îÇ List[List[Dict]]].\n\nThe images part is not too tricky to understand. We can either pass in a single image, a NumPy array representing an image or a list of images or NumPy arrays.\nThe annotations part is where Python type annotations don‚Äôt always do us many favours since we only know we‚Äôre expecting a list of dictionaries, but we can safely assume those dictionaries probably need to have a particular format. We can try and see what happens if we pass in an image and a list of a random dictionary.\n\nimport io\n\nimport requests\nfrom PIL import Image\n\n\nim = Image.open(\n    io.BytesIO(\n        requests.get(\n            \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/cute-cat-photos-1593441022.jpg?crop=1.00xw:0.749xh;0,0.154xh&resize=980:*\"\n        ).content\n    )\n)\nim\n\n\n\n\n\n\n\n\n\nlabels = [\n    {\n        \"bbox\": [\n            0.0,\n            3,\n            3,\n            4,\n        ]\n    }\n]\n\n\nfeature_extractor(im, labels)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [23], in &lt;cell line: 1&gt;()\n----&gt; 1 feature_extractor(im, labels)\n\nFile /usr/local/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/models/detr/feature_extraction_detr.py:524, in DetrFeatureExtractor.__call__(self, images, annotations, return_segmentation_masks, masks_path, pad_and_return_pixel_mask, return_tensors, **kwargs)\n    521                         valid_annotations = True\n    523     if not valid_annotations:\n--&gt; 524         raise ValueError(\n    525             \"\"\"\n    526             Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object\n    527             detection, each dictionary should contain the keys 'image_id' and 'annotations', with the latter\n    528             being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\n    529             should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a list\n    530             of annotations in COCO format.\n    531             \"\"\"\n    532         )\n    534 # Check that masks_path has a valid type\n    535 if masks_path is not None:\n\nValueError: \n                    Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of object\n                    detection, each dictionary should contain the keys 'image_id' and 'annotations', with the latter\n                    being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\n                    should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a list\n                    of annotations in COCO format.\n                    \n\n\n\nWe can see that this raises a ValueError. We also get some more information here that gives us a clue where we went wrong. Specifically we can see that the annotations for a single image should be a Dict or List[Dict] if we‚Äôre using a batch of images. We also see that we should pass in this data in the COCO format. Since our data is already in this format we should be able to pass in an example.\n\nimage = dataset[\"train\"][0][\"image\"]\nimage\n\n\n\n\n\n\n\n\n\nannotations = dataset[\"train\"][0][\"objects\"]\nannotations\n\n[\n    {\n        'category_id': 0,\n        'image_id': '8081',\n        'id': 646,\n        'area': 114552,\n        'bbox': [81.0, 408.0, 387.0, 296.0],\n        'segmentation': [[81.0, 408.0, 468.0, 408.0, 468.0, 704.0, 81.0, 704.0]],\n        'iscrowd': False\n    }\n]\n\n\n\n\nfeature_extractor(images=image, annotations=annotations, return_tensors=\"pt\")\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Traceback (most recent call last) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ /var/folders/tj/54sfzlyj6_573fn82y996grc0000gr/T/ipykernel_47936/1569761185.py:1 in &lt;cell line:  ‚îÇ\n‚îÇ 1&gt;                                                                                               ‚îÇ\n‚îÇ                                                                                                  ‚îÇ\n‚îÇ [Errno 2] No such file or directory:                                                             ‚îÇ\n‚îÇ '/var/folders/tj/54sfzlyj6_573fn82y996grc0000gr/T/ipykernel_47936/1569761185.py'                 ‚îÇ\n‚îÇ                                                                                                  ‚îÇ\n‚îÇ /usr/local/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/transformers/models/det ‚îÇ\n‚îÇ r/feature_extraction_detr.py:524 in __call__                                                     ‚îÇ\n‚îÇ                                                                                                  ‚îÇ\n‚îÇ   521 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   valid_annotations = True                                   ‚îÇ\n‚îÇ   522 ‚îÇ   ‚îÇ   ‚îÇ                                                                                  ‚îÇ\n‚îÇ   523 ‚îÇ   ‚îÇ   ‚îÇ   if not valid_annotations:                                                      ‚îÇ\n‚îÇ ‚ù± 524 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   raise ValueError(                                                          ‚îÇ\n‚îÇ   525 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \"\"\"                                                                    ‚îÇ\n‚îÇ   526 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   Annotations must of type `Dict` (single image) or `List[Dict]` (batc   ‚îÇ\n‚îÇ   527 ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   detection, each dictionary should contain the keys 'image_id' and 'a   ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nValueError: \n                    Annotations must of type `Dict` (single image) or `List[Dict]` (batch of images). In case of \nobject\n                    detection, each dictionary should contain the keys 'image_id' and 'annotations', with the \nlatter\n                    being a list of annotations in COCO format. In case of panoptic segmentation, each dictionary\n                    should contain the keys 'file_name', 'image_id' and 'segments_info', with the latter being a \nlist\n                    of annotations in COCO format.\n                    \n\n\n\nOh no! It still doesn‚Äôt work. At this point, it‚Äôs we probably either want to dig into the source code to work out what we should be passing to the feature_extractor. The relevant function is def prepare_coco_detection.\nWe also have another tutorial to consult. In this tutorial we see that the annotations are stored in a dictionary target with the keys image_id and annotations.\ntarget = {'image_id': image_id, 'annotations': target}\nencoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\nWith a bit more wrangling let‚Äôs see if this works.\n\ntarget = {\"image_id\": 4, \"annotations\": annotations}\n\n\nfeature_extractor(images=image, annotations=target, return_tensors=\"pt\")\n\n{\n    'pixel_values': tensor([[[[-0.7650, -0.9705, -0.9705,  ..., -1.4158, -1.3815, -1.3815],\n          [-0.7822, -0.9020, -0.9020,  ..., -1.3815, -1.3644, -1.4500],\n          [-0.8164, -0.9020, -0.9192,  ..., -1.3815, -1.3815, -1.4500],\n          ...,\n          [ 1.5297,  1.5639,  1.5810,  ...,  1.4612,  1.4612,  1.4783],\n          [ 1.5125,  1.5297,  1.5468,  ...,  1.4783,  1.4783,  1.4954],\n          [ 1.4954,  1.5125,  1.5125,  ...,  1.5125,  1.5125,  1.5297]],\n\n         [[-0.6527, -0.8627, -0.8627,  ..., -1.3179, -1.2829, -1.2829],\n          [-0.7052, -0.8102, -0.8277,  ..., -1.3004, -1.2829, -1.3704],\n          [-0.7402, -0.8102, -0.8277,  ..., -1.3529, -1.3529, -1.4405],\n          ...,\n          [ 1.5357,  1.5707,  1.5882,  ...,  1.3957,  1.3957,  1.4132],\n          [ 1.5182,  1.5357,  1.5532,  ...,  1.4132,  1.4132,  1.4307],\n          [ 1.5007,  1.5182,  1.5182,  ...,  1.4482,  1.4482,  1.4657]],\n\n         [[-0.4275, -0.6367, -0.6367,  ..., -1.1073, -1.0898, -1.0898],\n          [-0.4624, -0.5670, -0.5844,  ..., -1.1247, -1.1247, -1.2119],\n          [-0.5147, -0.6018, -0.6193,  ..., -1.2293, -1.2467, -1.3339],\n          ...,\n          [ 1.4548,  1.4897,  1.5071,  ...,  1.3154,  1.3154,  1.3328],\n          [ 1.4374,  1.4548,  1.4722,  ...,  1.3328,  1.3328,  1.3502],\n          [ 1.4200,  1.4374,  1.4374,  ...,  1.3677,  1.3677,  1.3851]]]]),\n    'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]]),\n    'labels': [\n        {\n            'boxes': tensor([[0.4575, 0.5120, 0.6450, 0.2726]]),\n            'class_labels': tensor([0]),\n            'image_id': tensor([4]),\n            'area': tensor([172346.9688]),\n            'iscrowd': tensor([0]),\n            'orig_size': tensor([1086,  600]),\n            'size': tensor([1332,  736])\n        }\n    ]\n}\n\n\n\nThis is looking more like it! Now we have one example working we can translate this to a function that can prepare a batch into the same format.\nSince we get a batch at a time we might need to refactor things slightly. In this example I‚Äôve just grabbed the relevant lists for the images, image_id and annotations. We then use a list compression to store these in the dictionary format expected by the feature_extractor.\n\ndef transform(example_batch):\n    images = example_batch[\"image\"]\n    ids_ = example_batch[\"image_id\"]\n    objects = example_batch[\"objects\"]\n    targets = [\n        {\"image_id\": id_, \"annotations\": object_} for id_, object_ in zip(ids_, objects)\n    ]\n    return feature_extractor(images=images, annotations=targets, return_tensors=\"pt\")\n\nWe could apply this to our data using map but it often makes more sense to applay these on the fly using the with_transform method.\n\ndataset[\"train\"] = dataset[\"train\"].with_transform(transform)\n\nLet‚Äôs take a look at an example\n\ndataset[\"train\"][0]\n\n{\n    'pixel_values': tensor([[[-0.7650, -0.9705, -0.9705,  ..., -1.4158, -1.3815, -1.3815],\n         [-0.7822, -0.9020, -0.9020,  ..., -1.3815, -1.3644, -1.4500],\n         [-0.8164, -0.9020, -0.9192,  ..., -1.3815, -1.3815, -1.4500],\n         ...,\n         [ 1.5297,  1.5639,  1.5810,  ...,  1.4612,  1.4612,  1.4783],\n         [ 1.5125,  1.5297,  1.5468,  ...,  1.4783,  1.4783,  1.4954],\n         [ 1.4954,  1.5125,  1.5125,  ...,  1.5125,  1.5125,  1.5297]],\n\n        [[-0.6527, -0.8627, -0.8627,  ..., -1.3179, -1.2829, -1.2829],\n         [-0.7052, -0.8102, -0.8277,  ..., -1.3004, -1.2829, -1.3704],\n         [-0.7402, -0.8102, -0.8277,  ..., -1.3529, -1.3529, -1.4405],\n         ...,\n         [ 1.5357,  1.5707,  1.5882,  ...,  1.3957,  1.3957,  1.4132],\n         [ 1.5182,  1.5357,  1.5532,  ...,  1.4132,  1.4132,  1.4307],\n         [ 1.5007,  1.5182,  1.5182,  ...,  1.4482,  1.4482,  1.4657]],\n\n        [[-0.4275, -0.6367, -0.6367,  ..., -1.1073, -1.0898, -1.0898],\n         [-0.4624, -0.5670, -0.5844,  ..., -1.1247, -1.1247, -1.2119],\n         [-0.5147, -0.6018, -0.6193,  ..., -1.2293, -1.2467, -1.3339],\n         ...,\n         [ 1.4548,  1.4897,  1.5071,  ...,  1.3154,  1.3154,  1.3328],\n         [ 1.4374,  1.4548,  1.4722,  ...,  1.3328,  1.3328,  1.3502],\n         [ 1.4200,  1.4374,  1.4374,  ...,  1.3677,  1.3677,  1.3851]]]),\n    'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]]),\n    'labels': {\n        'boxes': tensor([[0.4575, 0.5120, 0.6450, 0.2726]]),\n        'class_labels': tensor([0]),\n        'image_id': tensor([8081]),\n        'area': tensor([172346.9688]),\n        'iscrowd': tensor([0]),\n        'orig_size': tensor([1086,  600]),\n        'size': tensor([1332,  736])\n    }\n}\n\n\n\nThe next thing we need to take care of is a collate function. ‚ÄòData collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.‚Äô source.\n\ndef collate_fn(batch):\n    pixel_values = [item[\"pixel_values\"] for item in batch]\n    encoding = feature_extractor.pad_and_create_pixel_mask(\n        pixel_values, return_tensors=\"pt\"\n    )\n    labels = [item[\"labels\"] for item in batch]\n    batch = {} # collated batch  \n    batch['pixel_values'] = encoding['pixel_values']\n    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n    batch[\"labels\"] = labels\n    return batch"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#creating-a-detr-model",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#creating-a-detr-model",
    "title": "Training an object detection model using Hugging Face",
    "section": "Creating a detr model",
    "text": "Creating a detr model\n\nAvoiding ambiguous labels\nWe‚Äôre almost at the point where we can start training the model. We just do a little bit of housekeeping to make sure our model knows what our encoded labels are. It‚Äôs super annoying when you are trying a model out on the Hugging Face Hub and you get back labels, 0 or 3 with no clue what these labels refer to. We can avoid this by telling our model what labels we have. This mapping will then be bundled with the model when we push it to the hub.\n\nid2label = dict(enumerate(dataset[\"train\"].features[\"objects\"][0][\"category_id\"].names))\nlabel2id = {v: k for k, v in id2label.items()}\nlabel2id\n\n{'early_printed_illustration': 0}\n\n\n\nNow we can create the DetrForObjectDetection model. This should all look familiar if you‚Äôve used transformers for other tasks.\n\nfrom transformers import DetrForObjectDetection\n\nmodel = DetrForObjectDetection.from_pretrained(\n    model_checkpoint,\n    num_labels=1,\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True,\n)\n\n\n\n\n\n\n\nDownloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_a1_0-14fe96d1.pth\nSome weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nIf you wanted to use another backbone you could do something like:\n# from transformers import DetrForObjectDetection\n\n# config = DetrConfig(backbone='regnetz_e8',id2label=id2label, label2id=label2id)\n# model = DetrForObjectDetection(config)"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#training-the-detr-model",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#training-the-detr-model",
    "title": "Training an object detection model using Hugging Face",
    "section": "Training the detr model",
    "text": "Training the detr model\nWe now specify our TrainingArguments\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"detr-resnet-50_fine_tuned_nls_chapbooks\",\n    per_device_train_batch_size=8,\n    num_train_epochs=10,\n    fp16=False,\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=1e-4,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=True,\n    hub_model_id=\"davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks\",\n)\n\nand create our Trainer\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=dataset[\"train\"],\n    tokenizer=feature_extractor,\n)\n\nCloning https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks into local empty directory.\n\n\n\n#collapse\ntrainer.train()\n\n/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 6531\n  Num Epochs = 10\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 8170\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\nChanges to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to the W&B docs.\n\n\n\nwandb: Currently logged in as: davanstrien (flyswot). Use `wandb login --relogin` to force relogin\n\n\n\n\nTracking run with wandb version 0.12.21\n\n\nRun data is saved locally in /content/wandb/run-20220724_144321-sgzdksxm\n\n\nSyncing run detr-resnet-50_fine_tuned_nls_chapbooks to Weights & Biases (docs)\n\n\n\n      \n      \n      [8170/8170 5:15:38, Epoch 10/10]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n50\n1.975700\n\n\n100\n3.254000\n\n\n150\n1.563300\n\n\n200\n1.103100\n\n\n250\n1.468000\n\n\n300\n1.169700\n\n\n350\n1.326600\n\n\n400\n1.413800\n\n\n450\n1.101600\n\n\n500\n1.054500\n\n\n550\n0.946000\n\n\n600\n0.871600\n\n\n650\n0.723600\n\n\n700\n0.866800\n\n\n750\n0.740400\n\n\n800\n0.753300\n\n\n850\n0.748900\n\n\n900\n0.919600\n\n\n950\n0.805800\n\n\n1000\n0.902200\n\n\n1050\n0.788800\n\n\n1100\n0.734400\n\n\n1150\n0.635700\n\n\n1200\n0.769000\n\n\n1250\n0.673000\n\n\n1300\n0.766200\n\n\n1350\n0.664800\n\n\n1400\n0.653700\n\n\n1450\n0.589500\n\n\n1500\n0.580900\n\n\n1550\n0.583200\n\n\n1600\n0.736000\n\n\n1650\n0.594900\n\n\n1700\n0.701400\n\n\n1750\n0.600300\n\n\n1800\n0.470900\n\n\n1850\n0.522800\n\n\n1900\n0.590300\n\n\n1950\n0.566300\n\n\n2000\n0.586800\n\n\n2050\n0.623800\n\n\n2100\n0.523400\n\n\n2150\n0.562500\n\n\n2200\n0.604100\n\n\n2250\n0.518000\n\n\n2300\n0.525100\n\n\n2350\n0.499100\n\n\n2400\n0.564900\n\n\n2450\n0.455100\n\n\n2500\n0.465000\n\n\n2550\n0.533200\n\n\n2600\n0.512500\n\n\n2650\n0.465100\n\n\n2700\n0.521800\n\n\n2750\n0.519500\n\n\n2800\n0.456800\n\n\n2850\n0.444400\n\n\n2900\n0.429600\n\n\n2950\n0.445400\n\n\n3000\n0.425600\n\n\n3050\n0.439600\n\n\n3100\n0.468000\n\n\n3150\n0.426500\n\n\n3200\n0.433500\n\n\n3250\n0.479400\n\n\n3300\n0.421800\n\n\n3350\n0.449500\n\n\n3400\n0.399300\n\n\n3450\n0.424500\n\n\n3500\n0.447700\n\n\n3550\n0.428900\n\n\n3600\n0.403800\n\n\n3650\n0.448300\n\n\n3700\n0.424300\n\n\n3750\n0.396600\n\n\n3800\n0.405900\n\n\n3850\n0.436300\n\n\n3900\n0.371500\n\n\n3950\n0.412300\n\n\n4000\n0.389200\n\n\n4050\n0.391900\n\n\n4100\n0.403200\n\n\n4150\n0.386800\n\n\n4200\n0.382500\n\n\n4250\n0.402000\n\n\n4300\n0.374400\n\n\n4350\n0.355900\n\n\n4400\n0.390800\n\n\n4450\n0.402600\n\n\n4500\n0.397100\n\n\n4550\n0.399700\n\n\n4600\n0.363900\n\n\n4650\n0.373600\n\n\n4700\n0.391600\n\n\n4750\n0.339200\n\n\n4800\n0.351900\n\n\n4850\n0.381800\n\n\n4900\n0.381800\n\n\n4950\n0.326000\n\n\n5000\n0.388300\n\n\n5050\n0.359100\n\n\n5100\n0.380500\n\n\n5150\n0.357100\n\n\n5200\n0.389500\n\n\n5250\n0.386200\n\n\n5300\n0.373000\n\n\n5350\n0.340000\n\n\n5400\n0.337100\n\n\n5450\n0.357500\n\n\n5500\n0.338900\n\n\n5550\n0.334500\n\n\n5600\n0.362000\n\n\n5650\n0.426100\n\n\n5700\n0.329500\n\n\n5750\n0.321500\n\n\n5800\n0.328800\n\n\n5850\n0.322400\n\n\n5900\n0.385900\n\n\n5950\n0.373800\n\n\n6000\n0.326000\n\n\n6050\n0.335200\n\n\n6100\n0.341600\n\n\n6150\n0.309600\n\n\n6200\n0.295700\n\n\n6250\n0.338600\n\n\n6300\n0.326800\n\n\n6350\n0.305600\n\n\n6400\n0.287200\n\n\n6450\n0.307700\n\n\n6500\n0.297000\n\n\n6550\n0.296700\n\n\n6600\n0.292700\n\n\n6650\n0.305300\n\n\n6700\n0.289300\n\n\n6750\n0.290600\n\n\n6800\n0.277100\n\n\n6850\n0.296500\n\n\n6900\n0.291800\n\n\n6950\n0.285800\n\n\n7000\n0.291400\n\n\n7050\n0.282500\n\n\n7100\n0.271500\n\n\n7150\n0.278300\n\n\n7200\n0.272100\n\n\n7250\n0.273800\n\n\n7300\n0.313500\n\n\n7350\n0.288600\n\n\n7400\n0.258700\n\n\n7450\n0.275700\n\n\n7500\n0.248200\n\n\n7550\n0.280800\n\n\n7600\n0.268500\n\n\n7650\n0.258700\n\n\n7700\n0.302800\n\n\n7750\n0.288700\n\n\n7800\n0.278400\n\n\n7850\n0.260700\n\n\n7900\n0.271200\n\n\n7950\n0.247900\n\n\n8000\n0.234700\n\n\n8050\n0.263900\n\n\n8100\n0.251900\n\n\n8150\n0.246900\n\n\n\n\n\n\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200/preprocessor_config.json\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/preprocessor_config.json\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400/preprocessor_config.json\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-1800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-2800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-3800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-4800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-5800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6600] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-6800] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7000] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7200] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7800/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7400] due to args.save_total_limit\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-8000/preprocessor_config.json\nDeleting older checkpoint [detr-resnet-50_fine_tuned_nls_chapbooks/checkpoint-7600] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(\n    global_step=8170,\n    training_loss=0.5031144771902768,\n    metrics={\n        'train_runtime': 18954.8014,\n        'train_samples_per_second': 3.446,\n        'train_steps_per_second': 0.431,\n        'total_flos': 5.190134584879058e+19,\n        'train_loss': 0.5031144771902768,\n        'epoch': 10.0\n    }\n)\n\n\n\nOnce we‚Äôve finished training we can use push_to_hub to share our model on the hub.\n\ntrainer.push_to_hub(\"finished training\")\n\nSaving model checkpoint to detr-resnet-50_fine_tuned_nls_chapbooks\nConfiguration saved in detr-resnet-50_fine_tuned_nls_chapbooks/config.json\nModel weights saved in detr-resnet-50_fine_tuned_nls_chapbooks/pytorch_model.bin\nFeature extractor saved in detr-resnet-50_fine_tuned_nls_chapbooks/preprocessor_config.json\nSeveral commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\n\n\n\n\n\n\n\n\nTo https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks\n   c94bb78..5c7b9d8  main -&gt; main\n\nDropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Object Detection', 'type': 'object-detection'}, 'dataset': {'name': 'nls_chapbook_illustrations', 'type': 'nls_chapbook_illustrations', 'args': 'illustration_detection'}}\nTo https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks\n   5c7b9d8..2ece586  main -&gt; main\n\n\n\n'https://huggingface.co/davanstrien/detr-resnet-50_fine_tuned_nls_chapbooks/commit/5c7b9d8907981c7ee0005334a3d96d5a8d623957'"
  },
  {
    "objectID": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#conclusion",
    "href": "posts/post-with-code/detr/2022-08-16-detr-object-detection.html#conclusion",
    "title": "Training an object detection model using Hugging Face",
    "section": "Conclusion",
    "text": "Conclusion\nWe‚Äôve seen how we can use the datasets library to perform object detection. The main thing we need to work out when we‚Äôre trying to use datasets for a computer vision task in the transfortmer libraryy is how to ensure we can create a transform that gets the images and annotations into a format understood by the relevant feature_extractor. Once we‚Äôve done this for one example we need to replicate the same thing with a batch of examples so we can use it as a transform.\nOne this is in place many of the same approaches to defining the model and training arguments should look very familiar. I didn‚Äôt spend much time on the training process in this post. I‚Äôll dig into tha in a fguure post as well as covering the process of using/evaluating the model."
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html",
    "title": "Searching for machine learning models using semantic search",
    "section": "",
    "text": "The Hugging Face model hub has (at the time of the last checking) 60,509 models publicly available. Some of these models are useful as base models for further fine-tuning; these include your classics like bert-base-uncased.\nThe hub also has more obscure indie hits that might already do a good job on your desired downstream task or be a closer start. For example, if one wanted to classify the genre of 18th Century books, it might make sense to start with a model for classifying 19th Century books."
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#finding-candidate-models",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#finding-candidate-models",
    "title": "Searching for machine learning models using semantic search",
    "section": "Finding candidate models",
    "text": "Finding candidate models\nIdeally, we‚Äôd like a quick way to identify if a model might already do close to what we want. From there, we would likely want to review a bunch of other info about the model before deciding if it might be helpful for us or not.\nUnfortunately, finding suitable models on the hub isn‚Äôt always that easy. Even knowing that models for genre classification exist on the hub, we don‚Äôt find any results.\n\n\n\nSearch results\n\n\nIt‚Äôs not documented exactly how the search on the hub works, but it seems to be based mainly on the model‚Äôs name rather than the README or other information. In this blog post, I will continue some previous experiments with embeddings to see if there might be different ways in which we could identify potential models.\nThis will be a very rough experiment and is more about establishing whether this is an avenue worth exploring rather than a fully fleshed-out approach.\nFirst install some libraries we‚Äôll use:\n\nimport torch\n\n\ndeps = [\"datasets\" ,\"sentence-transformers\", \"rich['jupyter']\", \"requests\"]\nif torch.cuda.is_available():\n    deps.append(\"faiss-gpu\")\nelse:\n    deps.append(\"faise-cpu\")\n\n\n%%capture\n!pip install {\" \".join(deps)} --upgrade\n\n\n!git config --global credential.helper store\n\nThese days I almost always have the rich extension loaded!\n\n%load_ext rich"
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#using-the-huggingface_hub-api-to-download-some-model-metadata",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#using-the-huggingface_hub-api-to-download-some-model-metadata",
    "title": "Searching for machine learning models using semantic search",
    "section": "Using the huggingface_hub API to download some model metadata",
    "text": "Using the huggingface_hub API to download some model metadata\nOur goal is to see if we might be able to find suitable models more efficiently using some form of semantic search (i.e.¬†using embeddings). To do this, we should grab some model data from the hub. The easiest way to do this is using the hub API.\n\nfrom huggingface_hub import hf_api\nimport re\nfrom rich import print\n\n\napi = hf_api.HfApi()\n\n\napi\n\n&lt;huggingface_hub.hf_api.HfApi object at 0x7f63832ff810&gt;\n\n\n\nWe can take a look at some example models\n\nall_models = api.list_models()\nall_models[:3]\n\n[\n    ModelInfo: {\n        modelId: hfl/chinese-macbert-base\n        sha: None\n        lastModified: None\n        tags: []\n        pipeline_tag: fill-mask\n        siblings: None\n        private: False\n        author: None\n        config: None\n        id: hfl/chinese-macbert-base\n},\n    ModelInfo: {\n        modelId: bert-base-uncased\n        sha: None\n        lastModified: None\n        tags: []\n        pipeline_tag: fill-mask\n        siblings: None\n        private: False\n        author: None\n        config: None\n        id: bert-base-uncased\n},\n    ModelInfo: {\n        modelId: microsoft/deberta-base\n        sha: None\n        lastModified: None\n        tags: []\n        pipeline_tag: None\n        siblings: None\n        private: False\n        author: None\n        config: None\n        id: microsoft/deberta-base\n}\n]\n\n\n\nFor a particular model we can also see what files there are.\n\nfiles = api.list_repo_files(all_models[0].modelId)\n\n\nfiles\n\n[\n    '.gitattributes',\n    'README.md',\n    'added_tokens.json',\n    'config.json',\n    'flax_model.msgpack',\n    'pytorch_model.bin',\n    'special_tokens_map.json',\n    'tf_model.h5',\n    'tokenizer.json',\n    'tokenizer_config.json',\n    'vocab.txt'\n]\n\n\n\n\nFiltering\nTo limit the scope of this blog post, we‚Äôll focus only on Pytorch models and ‚Äòtext classification‚Äô models. The metadata about the model type is likely usually pretty reliable. The model task metadata, on the other hand, is not always reliable in my experience. This means we probably have some models that aren‚Äôt text-classification models and don‚Äôt include some actual text classification models in our dataset. For now, we won‚Äôt worry too much about this.\n\nfrom huggingface_hub import ModelSearchArguments\n\n\nmodel_args = ModelSearchArguments()\n\n\nfrom huggingface_hub import ModelFilter\n\nmodel_filter = ModelFilter(\n    task=model_args.pipeline_tag.TextClassification, \n    library=model_args.library.PyTorch\n)\napi.list_models(filter=model_filter)[0]\n\nModelInfo: {\n        modelId: distilbert-base-uncased-finetuned-sst-2-english\n        sha: 00c3f1ef306e837efb641eaca05d24d161d9513c\n        lastModified: 2022-07-22T08:00:55.000Z\n        tags: ['pytorch', 'tf', 'rust', 'distilbert', 'text-classification', 'en', 'dataset:sst2', 'dataset:glue', 'transformers', 'license:apache-2.0', 'model-index']\n        pipeline_tag: text-classification\n        siblings: [RepoFile(rfilename='.gitattributes'), RepoFile(rfilename='README.md'), RepoFile(rfilename='config.json'), RepoFile(rfilename='map.jpeg'), RepoFile(rfilename='pytorch_model.bin'), RepoFile(rfilename='rust_model.ot'), RepoFile(rfilename='tf_model.h5'), RepoFile(rfilename='tokenizer_config.json'), RepoFile(rfilename='vocab.txt')]\n        private: False\n        author: None\n        config: None\n        id: distilbert-base-uncased-finetuned-sst-2-english\n        downloads: 5185721\n        likes: 76\n        library_name: transformers\n}\n\n\n\nNow we have a filter we‚Äôll use that to grab all the models that match this filter.\n\nall_models = api.list_models(filter=model_filter)\n\n\nall_models[0]\n\nModelInfo: {\n        modelId: distilbert-base-uncased-finetuned-sst-2-english\n        sha: 00c3f1ef306e837efb641eaca05d24d161d9513c\n        lastModified: 2022-07-22T08:00:55.000Z\n        tags: ['pytorch', 'tf', 'rust', 'distilbert', 'text-classification', 'en', 'dataset:sst2', 'dataset:glue', 'transformers', 'license:apache-2.0', 'model-index']\n        pipeline_tag: text-classification\n        siblings: [RepoFile(rfilename='.gitattributes'), RepoFile(rfilename='README.md'), RepoFile(rfilename='config.json'), RepoFile(rfilename='map.jpeg'), RepoFile(rfilename='pytorch_model.bin'), RepoFile(rfilename='rust_model.ot'), RepoFile(rfilename='tf_model.h5'), RepoFile(rfilename='tokenizer_config.json'), RepoFile(rfilename='vocab.txt')]\n        private: False\n        author: None\n        config: None\n        id: distilbert-base-uncased-finetuned-sst-2-english\n        downloads: 5185721\n        likes: 76\n        library_name: transformers\n}\n\n\n\nLet‚Äôs see how many models that gives us.\n\nlen(all_models)\n\n6860\n\n\n\nLater on, in this blog, we‚Äôll want to work with the config.json files (we‚Äôll get back to why later!), so we‚Äôll quickly check that all our models have this.\n\ndef has_config(model):\n    has_config = False\n    files = model.siblings\n    for file in files:\n        if \"config.json\" in file.rfilename:\n            has_config = True\n            return has_config\n        else:\n            continue\n\n\nhas_config(all_models[0])\n\nTrue\n\n\n\n\nhas_config = [model for model in all_models if has_config(model)]\n\nLet‚Äôs check how many we have now\n\nlen(has_config)\n\n6858\n\n\n\nWe can also download a particular file from the hub\n\nfrom huggingface_hub import hf_hub_download\nfile = hf_hub_download(repo_id=all_models[0].modelId, filename=\"config.json\")\n\n\n\n\n\nfile\n\n'/root/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/00c3f1ef306e837efb641eaca05d24d161d9513c/config.json'\n\n\n\nUnable to display output for mime type(s): application/vnd.google.colaboratory.intrinsic+json\n\n\n\nimport json\nwith open(file) as f:\n    data = json.load(f)\n\n\ndata\n\n{\n    'activation': 'gelu',\n    'architectures': ['DistilBertForSequenceClassification'],\n    'attention_dropout': 0.1,\n    'dim': 768,\n    'dropout': 0.1,\n    'finetuning_task': 'sst-2',\n    'hidden_dim': 3072,\n    'id2label': {'0': 'NEGATIVE', '1': 'POSITIVE'},\n    'initializer_range': 0.02,\n    'label2id': {'NEGATIVE': 0, 'POSITIVE': 1},\n    'max_position_embeddings': 512,\n    'model_type': 'distilbert',\n    'n_heads': 12,\n    'n_layers': 6,\n    'output_past': True,\n    'pad_token_id': 0,\n    'qa_dropout': 0.1,\n    'seq_classif_dropout': 0.2,\n    'sinusoidal_pos_embds': False,\n    'tie_weights_': True,\n    'vocab_size': 30522\n}\n\n\n\nWe can also check if the model has a README.md\n\ndef has_file_in_repo(model,file_name):\n    has_file = False\n    files = model.siblings\n    for file in files:\n        if file_name in file.rfilename:\n            has_file = True\n            return has_file\n        else:\n            continue    \n\n\nhas_file_in_repo(has_config[0],'README.md')\n\nTrue\n\n\n\n\nhas_readme = [model for model in has_config if has_file_in_repo(model,\"README.md\")]\n\nWe can see that there are more configs than READMEs\n\nlen(has_readme)\n\n3482\n\n\n\n\nlen(has_config)\n\n6858\n\n\n\nWe now write some functions to grab both the README.md and config.json files from the hub.\n\nfrom requests.exceptions import JSONDecodeError\nimport concurrent.futures\n\n\n@lru_cache(maxsize=None)\ndef get_model_labels(model):\n    try:\n        url = hf_hub_url(repo_id=model.modelId, filename=\"config.json\")\n        return model.modelId, list(requests.get(url).json()['label2id'].keys())\n    except (KeyError, JSONDecodeError, AttributeError):\n        return model.modelId, None\n    \n\n\nget_model_labels(has_config[0])\n\n('distilbert-base-uncased-finetuned-sst-2-english', ['NEGATIVE', 'POSITIVE'])\n\n\n\n\ndef get_model_readme(model):\n    url = hf_hub_url(repo_id=model.modelId, filename=\"README.md\")\n    return requests.get(url).text\n\n\ndef get_data(model):\n    readme = get_model_readme(model)\n    _, labels = get_model_labels(model)\n    return model.modelId, labels, readme\n\nSince this takes a little while we make a progress bar and do this using multiple threads\n\nfrom tqdm.auto import tqdm\n\n\nwith tqdm(total=len(has_config)) as progress:\n    with concurrent.futures.ThreadPoolExecutor() as e:\n        tasks = []\n        for model in has_config:\n            future = e.submit(get_data, model)\n            future.add_done_callback(lambda p: progress.update())\n            tasks.append(future)\nresults = [task.result() for task in tasks]\n\n\n\n\nLoad our data using Pandas.\n\nimport pandas as pd\n\n\ndf = pd.DataFrame(results,columns=['modelId','label','readme'])\n\n\ndf\n\n\n    \n      \n\n\n\n\n\n\nmodelId\nlabel\nreadme\n\n\n\n\n0\ndistilbert-base-uncased-finetuned-sst-2-english\n[NEGATIVE, POSITIVE]\n---\\nlanguage: en\\nlicense: apache-2.0\\ndatase...\n\n\n1\ncross-encoder/ms-marco-MiniLM-L-12-v2\n[LABEL_0]\n---\\nlicense: apache-2.0\\n---\\n# Cross-Encoder...\n\n\n2\ncardiffnlp/twitter-xlm-roberta-base-sentiment\n[Negative, Neutral, Positive]\n---\\nlanguage: multilingual\\nwidget:\\n- text: ...\n\n\n3\nfacebook/bart-large-mnli\n[contradiction, entailment, neutral]\n---\\nlicense: mit\\nthumbnail: https://huggingf...\n\n\n4\nProsusAI/finbert\n[positive, negative, neutral]\n---\\nlanguage: \"en\"\\ntags:\\n- financial-sentim...\n\n\n...\n...\n...\n...\n\n\n6845\njinwooChoi/SKKU_AP_SA_KBT6\n[LABEL_0, LABEL_1, LABEL_2]\nEntry not found\n\n\n6846\njinwooChoi/SKKU_AP_SA_KBT7\n[LABEL_0, LABEL_1, LABEL_2]\nEntry not found\n\n\n6847\nnaem1023/electra-phrase-clause-classification-...\nNone\nEntry not found\n\n\n6848\nnaem1023/electra-phrase-clause-classification-...\nNone\n---\\nlicense: apache-2.0\\n---\\n\n\n\n6849\nYYAH/Bert_Roman_Urdu\n[LABEL_0, LABEL_1, LABEL_2, LABEL_3]\n---\\nlicense: unknown\\n---\\n\n\n\n\n\n6850 rows √ó 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\nYou can see we now have a DataFrame containing the modelID, the model labels and the README.md for each model (where it exists).\nSince the README.md (the model card) is the obvious source of information about a model we‚Äôll start here. One question we may have is how long our the README.md is. Some models have very detailed model cards whilst others have very little information in the model card. We can get a bit of a sense of this by looking at the range of README.md lenghts:\n\ndf['readme'].apply(len).describe()\n\ncount     6850.000000\nmean      1009.164818\nstd       1750.509155\nmin          0.000000\n25%         15.000000\n50%         20.500000\n75%       1736.000000\nmax      56172.000000\nName: readme, dtype: float64\n\n\n\nWe might want to filter on the length of the README so we‚Äôll store that info in a new column.\n\ndf['readme_len'] = df['readme'].apply(len)\n\nSince we might want to work with this data again, let‚Äôs load it into a datasets Dataset and use push_to_hub to store a copy.\n\nfrom datasets import Dataset\n\n\nds = Dataset.from_pandas(df)\nds\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len'],\n    num_rows: 6850\n})\n\n\n\n\nfrom huggingface_hub import notebook_login\n\n\nnotebook_login()\n\nLogin successful\nYour token has been saved to /root/.huggingface/token\n\n\n\nds.push_to_hub('davanstrien/hf_model_metadata')\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:1951: FutureWarning: `identical_ok` has no effect and is deprecated. It will be removed in 0.11.0.\n  FutureWarning,\n\n\nWe can now load it again using load_dataset.\n\nfrom datasets import load_dataset\n\n\nds = load_dataset('davanstrien/hf_model_metadata', split='train')\n\n\n\n\nUsing custom data configuration davanstrien--hf_model_metadata-019f1ad4bdf705b5\n\n\nDownloading and preparing dataset None/None (download: 3.71 MiB, generated: 10.64 MiB, post-processed: Unknown size, total: 14.35 MiB) to /root/.cache/huggingface/datasets/davanstrien___parquet/davanstrien--hf_model_metadata-019f1ad4bdf705b5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/davanstrien___parquet/davanstrien--hf_model_metadata-019f1ad4bdf705b5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n\n\nClean up some memory‚Ä¶\n\ndel df"
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#semantic-search-of-model-cards",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#semantic-search-of-model-cards",
    "title": "Searching for machine learning models using semantic search",
    "section": "Semantic search of model cards",
    "text": "Semantic search of model cards\nWe now get to the main point of all of this. Can we use semantic search to try and find models of interest? For this, we‚Äôll use the sentence-transformers library. This blog won‚Äôt cover all the background of this library. The docs give a helpful overview and some tutorials.\nTo start, we‚Äôll see if we can search using the information in the README.md. This should, in theory, contain data that might be similar to the kinds of things we want to search for when finding candidate models. We might prefer to use semantic search over an exact match because the terms we use might be different, or there is a related concept/model that might be close enough to make it worthwhile for fine-tuning.\nFirst, we import the SentenceTransformer class and some util functions.\n\nfrom sentence_transformers import SentenceTransformer, util\n\nWe‚Äôll now download an embedding model. There are many we could choose from but since we‚Äôre just trying things out at the moment we won‚Äôt stress about the particular model we use here.\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs start on longer README‚Äôs, here i mean a long readme that is just not super short‚Ä¶\n\nds_longer_readmes = ds.filter(lambda x: x['readme_len']&gt;100)\n\n\n\n\nWe now create embeddings for the readme column and store this in a new embedding column\n\ndef encode_readme(readme):\n    return model.encode(readme,device='cuda')\n\n\nds_with_embeddings = ds_longer_readmes.map(lambda example: \n                                           {\"embedding\":encode_readme(example['readme'])},batched=True, batch_size=16)\n                                                                      \n\n\n\n\n\nds_with_embeddings\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'embedding'],\n    num_rows: 3284\n})\n\n\n\nWe can now use the add_fais_index to create an index which allows us to efficiently query these embeddings\n\nds_with_embeddings.add_faiss_index(column='embedding')\n\n\n\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'embedding'],\n    num_rows: 3284\n})\n\n\n\n\nSimilar models\nTo start, we‚Äôll take a readme for a model and see how well the model performs on finding similar models.\n\nquery_readme = ds_with_embeddings[35]['readme']\n\n\nprint(query_readme)\n\n# Twitter-roBERTa-base for Irony Detection\n\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark.\n\n- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = [\n    ]\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) &gt; 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='irony'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) &gt; 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Great, it broke the first day...\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Great, it broke the first day...\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking]\n    s = scores[ranking]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) irony 0.914\n2) non_irony 0.086\n```\n\n\n\n\nWe pass this README into the model we used to create our embedding. This creates a query embedding for this README.\n\nq = model.encode(query_readme)\n\nWe can use get_nearest_examples to look for the most similar results to this query.\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embedding', q, k=10)\n\nLet‚Äôs take a look at the first result\n\n print(retrieved_examples['modelId'][0])\n\ncardiffnlp/twitter-roberta-base-irony\n\n\n\n\nprint(retrieved_examples[\"readme\"][0])\n\n# Twitter-roBERTa-base for Irony Detection\n\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark.\n\n- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = [\n    ]\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) &gt; 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='irony'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) &gt; 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Great, it broke the first day...\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Great, it broke the first day...\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking]\n    s = scores[ranking]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) irony 0.914\n2) non_irony 0.086\n```\n\n\n\n\nand a lower similarity result\n\nprint(retrieved_examples[\"readme\"][9])\n\n---\nlanguage: \"en\"\ntags:\n- roberta\n- sentiment\n- twitter\n\nwidget:\n- text: \"Oh no. This is bad..\"\n- text: \"To be or not to be.\"\n- text: \"Oh Happy Day\"\n\n---\n\nThis RoBERTa-based model can classify the sentiment of English language text in 3 classes:\n\n- positive üòÄ\n- neutral üòê\n- negative üôÅ\n\nThe model was fine-tuned on 5,304 manually annotated social media posts. \nThe hold-out accuracy is 86.1%. \nFor details on the training approach see Web Appendix F in Hartmann et al. (2021). \n\n# Application\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\", \nreturn_all_scores=True)\nclassifier(\"This is so nice!\")\n```\n\n```python\nOutput:\n[[{'label': 'negative', 'score': 0.00016451838018838316},\n  {'label': 'neutral', 'score': 0.000174045650055632},\n  {'label': 'positive', 'score': 0.9996614456176758}]]\n```\n\n# Reference\nPlease cite (https://journals.sagepub.com/doi/full/10.1177/00222437211037258) when you use our model. Feel free to \nreach out to (mailto:j.p.hartmann@rug.nl) with any questions or feedback you may have.\n```\n@article{hartmann2021,\n  title={The Power of Brand Selfies},\n  author={Hartmann, Jochen and Heitmann, Mark and Schamp, Christina and Netzer, Oded},\n  journal={Journal of Marketing Research}\n  year={2021}\n}\n```\n\n\n\nThe results seem pretty reasonable; the first result appears to be a duplicate. The lower result is for a slightly different task using social media data.\n\n\nSearching\nBeing able to find similar model cards is a start but we actually wanted to be able to search directly. Let‚Äôs see how our results do if we instead search for some terms we might use to try and find suitable models.\n\nq = model.encode(\"fake news\")\n\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embedding', q, k=10)\n\n\nprint(retrieved_examples[\"readme\"][0])\n\nThis model is fined tuned for the Fake news classifier: Train a text classification model to detect fake news \narticles. Base on the Kaggle dataset(https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset).\n\n\n\n\n\nprint(retrieved_examples[\"readme\"][1])\n\nFake news classifier\nThis model trains a text classification model to detect fake news articles, \n\nit uses distilbert-base-uncased-finetuned-sst-2-english pretrained model to work on \n\nfake and real news dataset from kaggle (https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)\n\n\n\n\nprint(retrieved_examples[\"readme\"][2])\n\n---\nlicense: mit\n---\n# Fake and real news classification task \n\nModel  : [DistilRoBERTa base model](https://huggingface.co/distilroberta-base)\n\nDataset : [Fake and real news dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)\n\n\n\n\n\n\n\n\n\nNot a bad start. Let‚Äôs try another one\n\nq = model.encode(\"financial sentiment\")\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embedding', q, k=10)\nprint(retrieved_examples[\"readme\"][0])\n\n---\nlanguage: en\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\ndatasets:\n- financial_phrasebank\nwidget:\n- text: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 %\nof net sales.\n- text: Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR \n4,000.\n- text: Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in the \ncorresponding period of 2008.\n---\n### FinancialBERT for Sentiment Analysis\n\n[*FinancialBERT*](https://huggingface.co/ahmedrachid/FinancialBERT) is a BERT model pre-trained on a large corpora \nof financial texts. The purpose is to enhance financial NLP research and practice in financial domain, hoping that \nfinancial practitioners and researchers can benefit from this model without the necessity of the significant \ncomputational resources required to train the model. \n\nThe model was fine-tuned for Sentiment Analysis task on _Financial PhraseBank_ dataset. Experiments show that this \nmodel outperforms the general BERT and other financial domain-specific models.\n \nMore details on `FinancialBERT`'s pre-training process can be found at: \nhttps://www.researchgate.net/publication/358284785_FinancialBERT_-_A_Pretrained_Language_Model_for_Financial_Text_M\nining\n\n### Training data\nFinancialBERT model was fine-tuned on [Financial \nPhraseBank](https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10), a dataset consisting of \n4840 Financial News categorised by sentiment (negative, neutral, positive).\n\n### Fine-tuning hyper-parameters\n- learning_rate = 2e-5\n- batch_size = 32\n- max_seq_length = 512\n- num_train_epochs = 5\n\n### Evaluation metrics\nThe evaluation metrics used are: Precision, Recall and F1-score. The following is the classification report on the \ntest set.\n\n| sentiment  | precision        | recall           | f1-score  | support  |\n| ------------- |:-------------:|:-------------:|:-------------:| -----:|\n| negative | 0.96      | 0.97 | 0.97 | 58 |\n| neutral | 0.98      | 0.99 | 0.98 | 279 |\n| positive | 0.98     | 0.97 | 0.97 | 148 |\n| macro avg | 0.97     | 0.98 | 0.98 | 485 |\n| weighted avg | 0.98     | 0.98 | 0.98 | 485 |\n\n ### How to use \nThe model can be used thanks to Transformers pipeline for sentiment analysis.\n```python\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nmodel = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\ntokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n\nnlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\nsentences = [\"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing\n7.7 % of net sales.\",  \n             \"Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least \nEUR 4,000.\", \n             \"Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in \nthe corresponding period of 2008.\", \n             ]\nresults = nlp(sentences)\nprint(results)\n\n[{'label': 'positive', 'score': 0.9998133778572083},\n {'label': 'neutral', 'score': 0.9997822642326355},\n {'label': 'negative', 'score': 0.9877365231513977}]\n```\n\n&gt; Created by [Ahmed Rachid Hazourli](https://www.linkedin.com/in/ahmed-rachid/)\n\n\n\n\n\nprint(retrieved_examples[\"readme\"][1])\n\n---\nlanguage: \"en\"\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: \"Stocks rallied and the British pound gained.\"\n---\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT\nlanguage model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial \nsentiment classification. [Financial \nPhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientation\ns_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper \n[FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our \nrelated (https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on \nMedium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and \ninvesting globally in markets with long-term growth potential, Prosus builds leading consumer internet companies \nthat empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araciprosuscom and Zulkuf Genc zulkuf.gencprosuscom about any FinBERT related issues\nand questions.\n\n\n\n\nprint(retrieved_examples[\"readme\"][9])\n\n---\nlicense: apache-2.0\ntags:\n- Finance-sentiment-analysis\n- generated_from_trainer\nmetrics:\n- f1\n- accuracy\n- precision\n- recall\nmodel-index:\n- name: bert-base-finance-sentiment-noisy-search\n  results: []\nwidget:\n - text: \"Third quarter reported revenues were $10.9 billion, up 5 percent compared to prior year and up 8 percent \non a currency-neutral basis\"\n   example_title: \"Positive\"\n - text: \"The London-listed website for businesses reported a pretax loss of $26.6 million compared with a loss of \n$12.9 million the previous year\"\n   example_title: \"Negative\"\n - text:  \"Microsoft updates Outlook, Teams, and PowerPoint to be hybrid work ready\"\n   example_title: \"Neutral\"\n---\n\n&lt;!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. --&gt;\n\n# bert-base-finance-sentiment-noisy-search\n\nThis model is a fine-tuned version of (https://huggingface.co/bert-base-uncased) on Kaggle finance news sentiment \nanalysis with data enhancement using noisy search. The process is explained below:\n\n1. First \"bert-base-uncased\" was fine-tuned on Kaggle's finance news sentiment analysis \nhttps://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news dataset achieving accuracy of about 88%\n2. We then used a logistic-regression classifier on the same data. Here we looked at coefficients that contributed \nthe most to the \"Positive\" and \"Negative\" classes by inspecting only bi-grams. \n3. Using the top 25 bi-grams per class (i.e. \"Positive\" / \"Negative\") we invoked Bing news search with those \nbi-grams and retrieved up to 50 news items per bi-gram phrase.\n4. We called it \"noisy-search\" because it is assumed the positive bi-grams (e.g. \"profit rose\" , \"growth net\") give\nrise to positive examples whereas negative bi-grams (e.g. \"loss increase\", \"share loss\") result in negative \nexamples but note that we didn't test for the validity of this assumption (hence: noisy-search)\n5. For each article we kept the title + excerpt and labeled it according to pre-assumptions on class associations.\n6. We then trained the same model on the noisy data and apply it to an held-out test set from the original data set\nsplit.\n7. Training with couple of thousands noisy \"positives\" and \"negatives\" examples yielded a test set accuracy of \nabout 95%. \n8. It shows that by automatically collecting noisy examples using search we can boost accuracy performance from \nabout 88% to more than 95%.\n\nAccuracy results for Logistic Regression (LR) and BERT (base-cased) are shown in the attached pdf:\n\nhttps://drive.google.com/file/d/1MI9gRdppactVZ_XvhCwvoaOV1aRfprrd/view?usp=sharing \n\n\n## Model description\n\nBERT model trained on noisy data from search results. See PDF for more details.\n\n## Intended uses & limitations\n\nIntended for use on finance news sentiment analysis with 3 options: \"Positive\", \"Neutral\" and \"Negative\"\nTo get the best results feed the classifier with the title and either the 1st paragraph or a short news \nsummarization e.g. of up to 64 tokens. \n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n\n\n\n\nThese seem like a good starting point. However, we have a few issues relying on model cards alone. Firstly a lot of models don‚Äôt include them and the quality of them can be mixed. It‚Äôs maybe a question if we want to use a model that has no model card at all but it is possible that despite a good model card we don‚Äôt capture everything we‚Äôd need for searching in the README."
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#can-we-search-using-model-labels",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#can-we-search-using-model-labels",
    "title": "Searching for machine learning models using semantic search",
    "section": "Can we search using model labels?",
    "text": "Can we search using model labels?\nWe‚Äôre only working with classification models in this case. For most Pytorch models on the hub, we have a config file. This config usually contains the model‚Äôs labels. For example, ‚Äòpositive‚Äô, ‚Äònegative‚Äô.\nMaybe instead of relying only on the metadata, we can search ‚Äòinside‚Äô the model. The labels will often be a helpful reflection of what we‚Äôre looking for. For example, we want to find a sentiment classification model that roughly puts text into positive or negative sentiment. Again, relying on exact label matches may not work well, but maybe embeddings get around this problem. Let‚Äôs try it out!\nLet‚Äôs look at an example label.\n\nds[0]['label']\n\n['NEGATIVE', 'POSITIVE']\n\n\n\nSince we‚Äôre expecting labels to match this format lets filter out any that don‚Äôt fit this structure.\n\nds = ds.filter(lambda example: isinstance(example['label'],list))\n\n\n\n\n\nHow to create embeddings for our labels?\nHow should we encode our labels? At the moment, we have a list of labels. One option would be to create an embedding for every single label, which will require us to query multiple embeddings to check for a match. We may also prefer intuatively to have an embedding for the combination of labels. This is because we probably know more about the model type from all its labels rather than looking at one label at a time. We‚Äôll deal with the labels very crudely by joining them on , and creating a single string out of all the labels. I‚Äôm sure this isn‚Äôt the best possible approach, but it might be a good place to start testing this idea.\n\nds = ds.map(lambda example: {\"string_label\": \",\".join(example['label'])})\n\n\n\n\n\nds\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'string_label'],\n    num_rows: 4175\n})\n\n\n\n\nds_with_embeddings = ds.map(lambda example: \n                                           {\"label_embedding\":encode_readme(example['string_label'])},batched=True, batch_size=16)\n                                                                      \n\n\n\n\n\nds_with_embeddings\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'string_label', 'label_embedding'],\n    num_rows: 4175\n})\n\n\n\n\n\nSearching with labels\nNow we have some embeddings for the labels, let‚Äôs try searching. Let‚Äôs start with an existing set of labels to see how well we can match those.\n\nds_with_embeddings[0]['string_label']\n\n'NEGATIVE,POSITIVE'\n\n\n\nUnable to display output for mime type(s): application/vnd.google.colaboratory.intrinsic+json\n\n\n\nq = model.encode(\"negative\")\n\n\nds_with_embeddings.add_faiss_index(column='label_embedding')\n\n\n\n\nDataset({\n    features: ['modelId', 'label', 'readme', 'readme_len', 'string_label', 'label_embedding'],\n    num_rows: 4175\n})\n\n\n\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('label_embedding', q, k=10)\n\n\nretrieved_examples['label'][:10]\n\n[\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive'],\n    ['negative', 'positive']\n]\n\n\n\nSo far, these results look pretty good, although we haven‚Äôt done anything we couldn‚Äôt do with simple string matching. Let‚Äôs see what happens if we use a slightly more abstract search.\n\nq = model.encode(\"music\")\n\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('label_embedding', q, k=10)\n\n\nretrieved_examples['label'][:10]\n\n[\n    ['Dance', 'Heavy Metal', 'Hip Hop', 'Indie', 'Pop', 'Rock'],\n    ['Dance', 'Heavy Metal', 'Hip Hop', 'Indie', 'Pop', 'Rock'],\n    ['Dance', 'Heavy Metal', 'Hip Hop', 'Indie', 'Pop', 'Rock'],\n    [\n        'Alternative',\n        'Country',\n        'Eletronic Music',\n        'Gospel and Worship Songs',\n        'Hip-Hop',\n        'Jazz/Blues',\n        'Pop',\n        'R&B/Soul',\n        'Reggae',\n        'Rock'\n    ],\n    ['business', 'entertainment', 'sports'],\n    ['_silence_', '_unknown_', 'down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes'],\n    ['angry', 'happy', 'others', 'sad'],\n    ['Feeling', 'Thinking'],\n    [\n        'am_thuc',\n        'bong_da',\n        'cho_thue',\n        'doi_song',\n        'dong_vat',\n        'mua_ban',\n        'nhac',\n        'phim',\n        'phu_kien',\n        'sach',\n        'showbiz',\n        'the_thao',\n        'thoi_trang_nam',\n        'thoi_trang_nu',\n        'thuc_vat',\n        'tin_bds',\n        'tin_tuc',\n        'tri_thuc'\n    ],\n    ['intimacy']\n]\n\n\n\nWe can see that we get back labels related to music genre: ['Dance', 'Heavy Metal', 'Hip Hop', 'Indie', 'Pop', 'Rock'], for our first four results. After that, we get back ['business', 'entertainment', 'sports'], which might not be too far off what we want if we searched for music.\nHow about another search term\n\nq = model.encode(\"hateful\")\n\n\nscores, retrieved_examples = ds_with_embeddings.get_nearest_examples('label_embedding', q, k=10)\n\n\nretrieved_examples['label'][:10]\n\n[\n    ['Hateful', 'Not hateful'],\n    ['Hateful', 'Not hateful'],\n    ['hateful', 'non-hateful'],\n    ['hateful', 'non-hateful'],\n    ['hateful', 'non-hateful'],\n    ['HATE', 'NOT_HATE'],\n    ['NON_HATE', 'HATE'],\n    ['NON_HATE', 'HATE'],\n    ['NON_HATE', 'HATE'],\n    ['NON_HATE', 'HATE']\n]\n\n\n\nAgain here we have something quite close to what we‚Äôd get with string matching, but we have a bit more flexibility in how we spell/define our labels which might help surface more possible results.\nWe‚Äôll try a bunch more things‚Ä¶\n\ndef query_labels(query:str):\n    q = model.encode(query)\n    scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('label_embedding', q, k=10)\n    print(f\"results for: {query}\")\n    print(list(zip(retrieved_examples['label'][:10],retrieved_examples['modelId'][:10])))\n\n\nquery_labels(\"politics\")\n\nresults for: politics\n\n\n\n[\n    (['Democrat', 'Republican'], 'm-newhauser/distilbert-political-tweets'),\n    (['Geopolitical', 'Personal', 'Political', 'Religious'], 'dee4hf/autotrain-deephate2-1093539673'),\n    (['None', 'Environmental', 'Social', 'Governance'], 'yiyanghkust/finbert-esg'),\n    (['business', 'entertainment', 'sports'], 'bipin/malayalam-news-classifier'),\n    (\n        ['CRIME', 'ENTERTAINMENT', 'Finance', 'POLITICS', 'SPORTS', 'Terrorism'],\n        'Yarn007/autotrain-Napkin-872827783'\n    ),\n    (['business', 'entertainment', 'politics', 'sport', 'tech'], 'abhishek/autonlp-bbc-roberta-37249301'),\n    (\n        ['business', 'entertainment', 'politics', 'sport', 'tech'],\n        'abhishek/autonlp-bbc-news-classification-37229289'\n    ),\n    (['business', 'entertainment', 'politics', 'sport', 'tech'], 'Yarn/autotrain-Traimn-853827191'),\n    (['Neutral', 'Propaganda'], 'Real29/my-model-proppy'),\n    (['Neutral', 'Propaganda'], 'Real29/my-model-ptc')\n]\n\n\n\n\nquery_labels(\"fiction, non_fiction\")\n\nresults for: fiction, non_fiction\n\n\n\n[\n    (\n        ['action', 'drama', 'horror', 'sci_fi', 'superhero', 'thriller'],\n        'Tejas3/distillbert_110_uncased_movie_genre'\n    ),\n    (['action', 'drama', 'horror', 'sci_fi', 'superhero', 'thriller'], 'Tejas3/distillbert_110_uncased_v1'),\n    (\n        ['action', 'animation', 'comedy', 'drama', 'romance', 'thriller'],\n        'langfab/distilbert-base-uncased-finetuned-movie-genre'\n    ),\n    (['HATE', 'NON_HATE'], 'anthonny/dehatebert-mono-spanish-finetuned-sentiments_reviews_politicos'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-english'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-german'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-italian'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-spanish'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-portugese'),\n    (['NON_HATE', 'HATE'], 'Hate-speech-CNERG/dehatebert-mono-polish')\n]\n\n\n\nLet‚Äôs try the set of emotions one should feel everyday.\n\nquery_labels(\"worry, disgust, anxiety, fear\")\n\nresults for: worry, disgust, anxiety, fear\n\n\n\n[\n    (['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'], 'crcb/isear_bert'),\n    (\n        ['anger', 'disgust', 'fear', 'joy', 'others', 'sadness', 'surprise'],\n        'pysentimiento/robertuito-emotion-analysis'\n    ),\n    (\n        ['anger', 'disgust', 'fear', 'joy', 'others', 'sadness', 'surprise'],\n        'daveni/twitter-xlm-roberta-emotion-es'\n    ),\n    (\n        ['anger', 'disgust', 'fear', 'joy', 'others', 'sadness', 'surprise'],\n        'finiteautomata/beto-emotion-analysis'\n    ),\n    (\n        ['anger', 'disgust', 'fear', 'joy', 'others', 'sadness', 'surprise'],\n        'finiteautomata/bertweet-base-emotion-analysis'\n    ),\n    (['ANGER', 'DISGUST', 'FEAR', 'HAPPINESS', 'NEUTRALITY', 'SADNESS', 'SURPRISED'], 'Gunulhona/tbecmodel'),\n    (\n        ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust'],\n        'Yuetian/bert-base-uncased-finetuned-plutchik-emotion'\n    ),\n    (['anger', 'fear', 'happy', 'love', 'sadness'], 'jasonpratamas7/Thesis-Model-1'),\n    (['anger', 'fear', 'happy', 'love', 'sadness'], 'jasonpratamas7/Thesis-Model1'),\n    (['anger', 'fear', 'happy', 'love', 'sadness'], 'StevenLimcorn/indonesian-roberta-base-emotion-classifier')\n]\n\n\n\nThis example of searching for a set of labels might be a better approach in general since the query will better match the format of the intitial search."
  },
  {
    "objectID": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#conclusion",
    "href": "posts/post-with-code/semantic-model-search/2022-07-26-semantic-search-ml-models.html#conclusion",
    "title": "Searching for machine learning models using semantic search",
    "section": "Conclusion",
    "text": "Conclusion\nIt seems like there is some merit in exploring some of these ideas further. There are a lot of improvements that could be made: - how the embeddings are created - removing some ‚Äònoise‚Äô from the README, for example, by first parsing the Markdown - improving how the embeddings are created for the labels - combining the embeddings in some way either upfront or when queryig - a bunch of other things‚Ä¶\nIf I find some spare time, I plan to dig into these topics a bit further. This is also a nice excuse to play with one of the new open source embedding databases that have popped up in the last couple of years."
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html",
    "href": "posts/post-with-code/2020-05-03-multi-model.html",
    "title": "Multi-model metadata generation",
    "section": "",
    "text": "Deep learning models usually take one type of input (image, text etc.) to predict output labels (category, entities etc). This usually makes sense if the data you are using to make predictions contains a lot of information. i.e.¬†a chunk of text from a movie review or an image.\nRecently I have been playing around with a Website Classification Dataset from the UK web archive. The dataset is derived from a manually curated web archive which contains a primary and secondary category for each web page. The UK web archive has made a dataset available based on this archive which contains the manually classified subject categories alongside the page URL and the page title.\nAs part of playing around with this dataset I was keen to see if a multi-input model would work well. In this case exploring a model that takes both text and tabular data as input. A preview of the data:\n\n#hide_input\nimport pandas as pd\ntsv ='https://gist.githubusercontent.com/davanstrien/5e22b725046eddc2f1ee06b108f27e48/raw/71426e6b92c7fa98140a95728a5ea55171b948cd/classification.tsv'\ndf = pd.read_csv(tsv, error_bad_lines=False, index_col=0)\ndf.head()\n\n\n\n\n\n\n\n\nPrimary Category\nSecondary Category\nTitle\nURL\n\n\n\n\n0\nArts & Humanities\nArchitecture\n68 Dean Street\nhttp://www.sixty8.com/\n\n\n1\nArts & Humanities\nArchitecture\nAbandoned Communities\nhttp://www.abandonedcommunities.co.uk/\n\n\n2\nArts & Humanities\nArchitecture\nAlexander Thomson Society\nhttp://www.greekthomson.com/\n\n\n3\nArts & Humanities\nArchitecture\nArab British Centre, The\nhttp://www.arabbritishcentre.org.uk/\n\n\n4\nArts & Humanities\nArchitecture\nArchitectural Association School of Architecture\nhttp://www.aaschool.ac.uk/\n\n\n\n\n\n\n\nBased on this data the UK web archive are interested: &gt;‚Äúin understanding whether high-level metadata like this can be used to train an appropriate automatic classification system so that we might use this manually generated dataset to partially automate the categorisation of our larger archives.‚Äù\nThis is going to be fairly tricky but offers a nice excuse to try to use models with multiple inputs to predict our categories.\n\n\nTaking a closer look at the data:\n\n#hide_input \ntsv = 'https://gist.githubusercontent.com/davanstrien/5e22b725046eddc2f1ee06b108f27e48/raw/71426e6b92c7fa98140a95728a5ea55171b948cd/classification.tsv'\ndf = pd.read_csv(tsv, error_bad_lines=False,)\n\n\n\n\nlen(df['Primary Category'].unique())\n\n24\n\n\n\n\n\n\nlen(df['Secondary Category'].unique())\n\n104\n\n\nPredicting a 104 different labels is going to be pretty difficult so I‚Äôve only used ‚ÄòPrimary Category‚Äô as the the y target. What is the distribution of these categories like?\n\n#hide_input\ndf['Primary Category'].value_counts()\n\nArts & Humanities                                              5299\nGovernment, Law & Politics                                     4832\nBusiness, Economy & Industry                                   2988\nSociety & Culture                                              2984\nScience & Technology                                           2420\nMedicine & Health                                              2164\nEducation & Research                                           2118\nCompany Web Sites                                               843\nDigital Society                                                 737\nSports and Recreation                                           710\nReligion                                                        417\nTravel & Tourism                                                374\nSocial Problems and Welfare                                     270\nPolitics, Political Theory and Political Systems                123\nCrime, Criminology, Police and Prisons                          101\nLiterature                                                       87\nLaw and Legal System                                             81\nComputer Science, Information Technology and Web Technology      54\nLibraries, Archives and Museums                                  52\nEnvironment                                                      38\nHistory                                                          34\nPublishing, Printing and Bookselling                             26\nPopular Science                                                  23\nLife Sciences                                                    23\nName: Primary Category, dtype: int64\n\n\nüò¨ We also have a fairly skewed datasets. I could drop some of rows which don‚Äôt occur often but since the main objective here is to see if we can use a multi-input model we‚Äôll leave the data as it is for now."
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#looking-at-the-data",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#looking-at-the-data",
    "title": "Multi-model metadata generation",
    "section": "",
    "text": "Taking a closer look at the data:\n\n#hide_input \ntsv = 'https://gist.githubusercontent.com/davanstrien/5e22b725046eddc2f1ee06b108f27e48/raw/71426e6b92c7fa98140a95728a5ea55171b948cd/classification.tsv'\ndf = pd.read_csv(tsv, error_bad_lines=False,)\n\n\n\n\nlen(df['Primary Category'].unique())\n\n24\n\n\n\n\n\n\nlen(df['Secondary Category'].unique())\n\n104\n\n\nPredicting a 104 different labels is going to be pretty difficult so I‚Äôve only used ‚ÄòPrimary Category‚Äô as the the y target. What is the distribution of these categories like?\n\n#hide_input\ndf['Primary Category'].value_counts()\n\nArts & Humanities                                              5299\nGovernment, Law & Politics                                     4832\nBusiness, Economy & Industry                                   2988\nSociety & Culture                                              2984\nScience & Technology                                           2420\nMedicine & Health                                              2164\nEducation & Research                                           2118\nCompany Web Sites                                               843\nDigital Society                                                 737\nSports and Recreation                                           710\nReligion                                                        417\nTravel & Tourism                                                374\nSocial Problems and Welfare                                     270\nPolitics, Political Theory and Political Systems                123\nCrime, Criminology, Police and Prisons                          101\nLiterature                                                       87\nLaw and Legal System                                             81\nComputer Science, Information Technology and Web Technology      54\nLibraries, Archives and Museums                                  52\nEnvironment                                                      38\nHistory                                                          34\nPublishing, Printing and Bookselling                             26\nPopular Science                                                  23\nLife Sciences                                                    23\nName: Primary Category, dtype: int64\n\n\nüò¨ We also have a fairly skewed datasets. I could drop some of rows which don‚Äôt occur often but since the main objective here is to see if we can use a multi-input model we‚Äôll leave the data as it is for now."
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#tabular-model",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#tabular-model",
    "title": "Multi-model metadata generation",
    "section": "Tabular model",
    "text": "Tabular model\nIn the dataset above we start of with two columns of data which can be used as inputs for the model. The title is fairly obviously something which we can treat like other text inputs. The URL is a little less obvious. It could be treated as a text input but an alternative is to treat a URL as parts which each contain some information which could be useful for our model.\n\n#hide_input\nprint(df.URL.sample(10).to_list()[3])\nprint(df.URL.sample(10).to_list()[4])\nprint(df.URL.sample(10).to_list()[3])\n\nhttp://www.specialschool.org/\nhttp://www.bbc.co.uk/news/health-12668398\nhttp://www.monarchit.co.uk/\n\n\nEach part of the URL could be split into smaller parts\n\n#hide_input\nprint(df.URL.sample(10).to_list()[3].split('.'))\n\n['http://www', 'darwincountry', 'org/']\n\n\nWhether a url has ‚Äò.org‚Äô or ‚Äò.uk‚Äô or ‚Äò.com‚Äô could be meaningful for predicting our categories (it might also not be meaningful). It also offers us a way of taking the URLs and composing it into a format which looks more tabular.\n\n#hide_input\ncsv ='https://gist.githubusercontent.com/davanstrien/5e22b725046eddc2f1ee06b108f27e48/raw/4c2a27772bf4d959bf3e58cfa8de9e0b9be69ca7/03_classification_valid_train.csv'\ndf = pd.read_csv(csv, index_col=0)\ndf[['scheme','url1','url3','url4','url5']].sample(5)\n\n\n\n\n\n\n\n\nscheme\nurl1\nurl3\nurl4\nurl5\n\n\n\n\n20011\nhttp\nwww\norg\nNaN\nNaN\n\n\n15825\nhttp\nwww\ncom\nNaN\nNaN\n\n\n6068\nhttp\nwww\nco\nuk\nNaN\n\n\n16507\nhttp\nwww\nco\nuk\nNaN\n\n\n9723\nhttp\nwww\nco\nuk\nNaN\n\n\n\n\n\n\n\nSo far I‚Äôve only done this very crudely. I suspect tidying up this part of the data will help improve things. At this point though we have something which is a little more tabular looking we can pass to fastai.tabular learner. Now we have some ‚Äòcategories‚Äô rather than unique urls.\n\nprint(len(df.url3.unique()))\nprint(len(df.url4.unique()))\n\n279\n56"
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#how-does-this-tabular-model-do",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#how-does-this-tabular-model-do",
    "title": "Multi-model metadata generation",
    "section": "How does this tabular model do?",
    "text": "How does this tabular model do?\nOnce some preprocessing of the url has been done we train a model using the tabular learner. I didn‚Äôt do much to try to optimize this model. Tracking best f2 score we end up with:\nBetter model found at epoch 36 with f_beta value: 0.17531482875347137 and an accuracy of 0.334121"
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#how-well-does-a-text-model-do",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#how-well-does-a-text-model-do",
    "title": "Multi-model metadata generation",
    "section": "How well does a text model do?",
    "text": "How well does a text model do?\nNext I tried training using the title field in a NLP model. I tried a few things here.\n\nSentencePiece tokenization\nBy default fastai uses SpaCy to do tokenization with a few additional special tokens added by fastai. I wanted to see if using sentencePiece would work better for processing title fields. SentencePiece allows for various sub-word tokeinzation. This can be useful for agglutinative languages but could also be useful when you have a lot of out of vocabulary words in your corpus. I wanted to see if this also was useful for processing titles since these may contain domain specific terms. I only tried using SentencePiece with ‚Äòunigram‚Äô tokenization. The best score I got for this was:\nBetter model found at epoch 1 with f_beta value: 0.21195338666439056. ### Default SpaCy tokenization\nI compared the above to using the default fastai tokenizer which uses SpaCy. In this case the default approach worked better. This is probably because we didn‚Äôt have a large pre-trained model using the SentencePiece tokenization to use as a starting point. The best score I got for this model was:\nBetter model found at epoch 27 with f_beta value: 0.33327043056488037.\n\n\nUsing the URL as text input\nI wanted to do a quick comparison to the tabular model and use the URL as a text input instead. In this case I used SentencePiece with byte-pair-encoding (BPE). The best score in this case was:\nBetter model found at epoch 3 with f_beta value: 0.2568161189556122.\nThis might end up being a better approach compared to the tabular approach described above."
  },
  {
    "objectID": "posts/post-with-code/2020-05-03-multi-model.html#how-does-this-combined-model-do",
    "href": "posts/post-with-code/2020-05-03-multi-model.html#how-does-this-combined-model-do",
    "title": "Multi-model metadata generation",
    "section": "How does this combined model do? ü§∑‚Äç‚ôÇÔ∏è",
    "text": "How does this combined model do? ü§∑‚Äç‚ôÇÔ∏è\nThe best result I got wasf_beta value: 0.39341238141059875 with an accuracy of 0.595348. A summary of the scores for each models:\n\n\n\nModel\nF2 score\n\n\n\n\nSentencePiece text\n0.211\n\n\nSpacy text\n0.333\n\n\nTabular\n0.175\n\n\nConcat\n0.393\n\n\n\nThis provides some improvement on the tabular or nlp models on their own. I found the combined model was fairly tricky to train and suspect that there could be some improvements in how the model is set up that might improve it‚Äôs performance. I am keen to try a similar approach with a dataset where there is more abundant information available to train with."
  },
  {
    "objectID": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html",
    "href": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html",
    "title": "Exploring language metadata for datasets on the Hugging Face Hub",
    "section": "",
    "text": "%pip install huggingface_hub backoff wordcloud tabulate toolz matplotlib\n\nRequirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.15.1)\nRequirement already satisfied: backoff in ./.venv/lib/python3.11/site-packages (2.2.1)\nRequirement already satisfied: wordcloud in ./.venv/lib/python3.11/site-packages (1.9.2)\nRequirement already satisfied: tabulate in ./.venv/lib/python3.11/site-packages (0.9.0)\nRequirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (3.12.2)\nRequirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2023.6.0)\nRequirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.65.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.7.1)\nRequirement already satisfied: packaging&gt;=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (23.1)\nRequirement already satisfied: numpy&gt;=1.6.1 in ./.venv/lib/python3.11/site-packages (from wordcloud) (1.25.0)\nRequirement already satisfied: pillow in ./.venv/lib/python3.11/site-packages (from wordcloud) (10.0.0)\nRequirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (from wordcloud) (3.7.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (4.40.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (1.4.4)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (3.1.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (2.8.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in ./.venv/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (3.1.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in ./.venv/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (2.0.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (2023.5.7)\nRequirement already satisfied: six&gt;=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;wordcloud) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nfrom huggingface_hub import list_datasets\nfrom toolz import valmap, countby, groupby, topk, valmap\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#load-datasets",
    "href": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#load-datasets",
    "title": "Exploring language metadata for datasets on the Hugging Face Hub",
    "section": "Load datasets",
    "text": "Load datasets\n\ndatasets = list(iter(list_datasets(full=True, sort=\"downloads\", direction=-1)))\n\n\ndef get_lang(dataset):\n    card_data = dataset.cardData\n    if card_data:\n        lang = card_data.get(\"language\")\n        if lang is None:\n            return False\n        if len(lang) &gt;= 1:\n            return True\n    if not card_data:\n        return \"No card data\"\n\n\nhas_lang = groupby(get_lang, datasets)\n\n\nhas_lang.keys()\n\ndict_keys([True, 'No card data', False, None])\n\n\n\nhas_language_freqs = countby(get_lang, datasets)\nhas_language_percents = valmap(\n    lambda x: round(x / sum(has_language_freqs.values()) * 100, ndigits=2),\n    has_language_freqs,\n)\n\n\n\nplt.style.use(\"ggplot\")\ndata = {True: 13.31, \"No card data\": 42.03, False: 44.5}\n# Convert the keys to strings\nkeys = [str(key) for key in data]\n\n# Separate the values from the dictionary\nvalues = list(data.values())\n\n# Create a bar chart\nplt.bar(keys, values)\n\n# Set the labels for x and y axes\nplt.ylabel(\"Percent\")\n\n# Set the title of the chart\nplt.title(\"Has language information?\")\n\n# Display the chart\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#filter-to-datasets-with-language-information",
    "href": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#filter-to-datasets-with-language-information",
    "title": "Exploring language metadata for datasets on the Hugging Face Hub",
    "section": "Filter to datasets with language information",
    "text": "Filter to datasets with language information\n\nwith_lang = has_lang[True]\n\n\ndef count_langs(dataset):\n    langs = dataset.cardData.get(\"language\")\n    return len(langs)"
  },
  {
    "objectID": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#get-languages",
    "href": "posts/post-with-code/2023-06-07-hub-dataset-danguage_detection.html#get-languages",
    "title": "Exploring language metadata for datasets on the Hugging Face Hub",
    "section": "Get languages",
    "text": "Get languages\n\ndef get_langs(dataset):\n    return dataset.cardData.get(\"language\")\n\n\nfrom toolz import concat, frequencies\n\n\nlang_freqs = frequencies(concat(get_langs(d) for d in with_lang))\n\nNumber of unique languages specified on the hub\n\nlen(lang_freqs.keys())\n\n1719\n\n\n\nimport pandas as pd\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 import pandas as pd\n\nModuleNotFoundError: No module named 'pandas'\n\n\n\n\ndf = pd.DataFrame({\"Language\": lang_freqs.keys(), \"Frequency\": lang_freqs.values()})\ndf\n\n\n    \n      \n\n\n\n\n\n\nLanguage\nFrequency\n\n\n\n\n0\nen\n3949\n\n\n1\nja\n221\n\n\n2\nace\n19\n\n\n3\nacm\n6\n\n\n4\nacq\n5\n\n\n...\n...\n...\n\n\n1714\nmr-\n1\n\n\n1715\nxx\n1\n\n\n1716\nnbl\n2\n\n\n1717\nsep\n1\n\n\n1718\nssw\n2\n\n\n\n\n1719 rows √ó 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\n\ndf.sort_values(\"Frequency\", ascending=False).iloc[:20].set_index(\"Language\").plot.bar()\n\n\n\n\n\n\n\n\n\ndf[\"Percent\"] = (df.Frequency / df.Frequency.sum() * 100).round(3)\n\n\nprint(\n    df.sort_values(\"Frequency\", ascending=False)\n    .iloc[:20]\n    .set_index(\"Language\")\n    .to_markdown()\n)\n\n| Language   |   Frequency |   Percent |\n|:-----------|------------:|----------:|\n| en         |        3949 |    19.04  |\n| fr         |         394 |     1.9   |\n| zh         |         390 |     1.88  |\n| es         |         358 |     1.726 |\n| de         |         350 |     1.687 |\n| ru         |         333 |     1.606 |\n| pt         |         238 |     1.147 |\n| it         |         229 |     1.104 |\n| ja         |         221 |     1.066 |\n| pl         |         207 |     0.998 |\n| ar         |         195 |     0.94  |\n| ko         |         184 |     0.887 |\n| nl         |         182 |     0.877 |\n| tr         |         156 |     0.752 |\n| vi         |         155 |     0.747 |\n| sv         |         153 |     0.738 |\n| id         |         149 |     0.718 |\n| hi         |         148 |     0.714 |\n| th         |         145 |     0.699 |\n| fi         |         144 |     0.694 |\n\n\n\ndf = df.drop(columns=\"Percent\")\n\n\ndf.sort_values(\"Frequency\", ascending=False).iloc[:20].set_index(\"Language\").plot.barh()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ndf[df.Language != \"en\"].sort_values(\"Frequency\", ascending=False).iloc[:50].set_index(\n    \"Language\"\n).plot.bar(ax=ax)\n\nax.set_xlabel(\"Language\")\nax.set_ylabel(\"Frequency\")\nax.set_title(\"Top 50 Languages (excluding English)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Define Hugging Face brand colors\nhugging_face_colors = [\"#FFD21E\", \"#FF9D00\", \"#6B7280\"]\n# Create custom colormap\ncolor_map = LinearSegmentedColormap.from_list(\"hugging_face\", hugging_face_colors)\n\n\nwordcloud = WordCloud(width=800, height=400, colormap=color_map)\n\nwordcloud.generate_from_frequencies(lang_freqs)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "",
    "text": "#hide\n!pip -q install fastai2 optuna swifter toolz \n\n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 194kB 3.5MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 204kB 8.2MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 10.9MB/s \n\n  Installing build dependencies ... done\n\n  Getting requirements to build wheel ... done\n\n    Preparing wheel metadata ... done\n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 7.6MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.6MB 16.8MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 450kB 45.4MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 7.4MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112kB 53.0MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 6.5MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 8.0MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645kB 54.2MB/s \n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102kB 11.6MB/s \n\n  Building wheel for alembic (PEP 517) ... done\n\n  Building wheel for optuna (setup.py) ... done\n\n  Building wheel for psutil (setup.py) ... done\n\n  Building wheel for pyperclip (setup.py) ... done\n\n  Building wheel for locket (setup.py) ... done\n\n  Building wheel for contextvars (setup.py) ... done\n\nERROR: distributed 2.18.0 has requirement tornado&gt;=5; python_version &lt; \"3.8\", but you'll have tornado 4.5.3 which is incompatible.\n#hide\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n\nEnter your authorization code:\n¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\nMounted at /content/drive\n#hide\nfrom pathlib import Path\nfrom fastai2.text.all import *\nfrom fastai2.vision.all import *\nfrom fastai2.vision.all import *\nimport pandas as pd\nimport optuna\nimport pprint\n#hide\nPath('data').mkdir(exist_ok=True)\nout_path = Path('/content/drive/My Drive/Models/')\npath = Path('data/')"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#tldr",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#tldr",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "tl;dr",
    "text": "tl;dr\nThis post covers:\n\nthe motivations for ‚Äòpragmatic hyperparameters optimization‚Äô\nhow to do this using Optuna (with an example applied to the fastai2 library)"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#optimizing-hyperparameters",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#optimizing-hyperparameters",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Optimizing hyperparameters?",
    "text": "Optimizing hyperparameters?\nDeep learning models have a range of Hyperparameters. These include the basic building blocks of a model like the number of layers used or the size of embedding layers, and the parameters for the training of models such as learning rate. Changing some of these parameters will improve the performance of a model. There is therefore a potential win from finding the right values for these parameters.\n\nAuto ML vs pragmatic hyperparameters optimization\nAs a way of framing ‚Äòpragmatic search‚Äô, it is useful to contrast it to Auto ML. If you haven‚Äôt come across it before:\n\nThe term AutoML has traditionally been used to describe automated methods for model selection and/or hyperparameter optimization. - {% fn 1 %}.\n\nIn particular what is termed Auto ML often includes a search across model and Hyperparameters but can also refer to ‚ÄòNeural Architecture Search‚Äô in which the objective is to piece together a new model type for a specific problem or dataset. An underlying assumption of some of this Auto ML approach is that each problem or dataset requires a unique model architecture.\nIn contrast a more ‚Äòpragmatic‚Äô approach uses an existing model architectures which have been shown to work across a range of datasets and tasks, and utilise transfer learning and other ‚Äòtricks‚Äô like cyclical learning rates and data augmentation. In a heritage context, it is likely that there are going to be bigger issues with imbalanced classes, noisy labels etc, and focusing on designing a custom architecture is probably going to lead to modest improvements in the performance of the model.\n\n\nSo what remains to be optimized?\nIn contrast to Auto ML which can involve looking at huge range of potential architectures and parameters we could instead limit our focus to smaller set of things which may have a large impact on the performance of your model.\nAs an example use case for hyperparameters optimization I‚Äôll use two datasets which contain transcripts of trials from the Old Bailey online and which are classified into various categories (theft, deception, etc). One of the datasets is drawn the decade 1830 the other one 1730.\nThe approach taken to classifying these trials will be to follow the ‚ÄúUniversal Language Model Fine-tuning for Text Classification‚Äù approach. {% fn 2 %}.\nI won‚Äôt give an in depth summary of the approach here but idea is that:\n\nA language model - in this case a LSTM based model - is trained on a Wikipedia text. This provides a ‚Äúgeneral‚Äù language model that learns to ‚Äúunderstand‚Äù general features of a language, in this case English\nthis language model is then fine-tuned on a target dataset, in the orginal paper this is IMDB movie reviews.\none this language model has been fine-tuned on the target dataset this fine-tuned language model is used as input for a classifier\n\nThe intuition here is that by utilising a pre-trained language model the Wikipedia part, and the fine-tuning part we get the benefits of a massive training set (Wikipedia) whilst also being able to ‚Äòfocus‚Äô the language model on a target corpus which will use language differently. This makes a lot of intuitive sense, but a question in this use case is how much to fine-tune the language model on our target datasets. A reasonable assumption might be that since language will be more different in 1730 compared to 1830 we may want to fine tune the language model trained on Wikipedia more on the 1730 dataset.\nWe could of course test through some trial and error experiments, but this is a question which may benefit from some more systematic searching for appropriate hyperparameters. Before we get into this example in more depth I‚Äôll discuss the library I‚Äôm working with for doing this hyperparameter searching."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#optuna-a-hyperparameter-optimization-framework",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#optuna-a-hyperparameter-optimization-framework",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Optuna: A hyperparameter optimization framework",
    "text": "Optuna: A hyperparameter optimization framework\nIn this post I will be using Optuna ‚Äúan automatic hyperparameter optimization software framework, particularly designed for machine learning‚Äù. {% fn 3 %}.\nThere are some really nice features in Optuna which I‚Äôll cover in this post as I explore the question of language model fine-tuning, so hopefully even if you don‚Äôt care about the specific use case it might still provide a useful overview of Optuna.\nIn this blog post my examples will use version two of the fastai library but there really isn‚Äôt anything that won‚Äôt translate to other frameworks. Optuna has integrations for a number of libraries (including version 1 of fastai) but for this blog I won‚Äôt use this integration."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#a-simple-optimization-example",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#a-simple-optimization-example",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "A simple optimization example",
    "text": "A simple optimization example\nTo show the approach used in Optuna I‚Äôll use a simple image classification example. In this case using a toy example of classifying people vs cats in images taken from 19th Century books.\nOptuna has two main concepts to understand: study and trial. A study is the overarching process of optimization based on some objective function. A trial is a single test/execution of the objective function. We‚Äôll return to this in more detail. For now lets look at a simple example.\nFor our first example we‚Äôll just use Optuna to test whether to use a pre-trained model or not. If the option is True then the ResNet18 model we use will use weights from pre-training on ImageNet, if False the model will start with random weights.\nLooking at the high level steps of using Optuna (I‚Äôll go into more detail later). We create an objective function:\n\n#collapse-hide\n!wget -q https://zenodo.org/record/3689444/files/humancats.zip?download=1\n!unzip -q *humancats.zip* -d data/\n\n\ndef objective(trial):\n    is_pretrained = trial.suggest_categorical('pre_trained', [True, False])\n    dls = ImageDataLoaders.from_folder('data/human_vs_cats/', valid_pct=0.4, item_tfms=Resize(64))\n    learn = cnn_learner(dls, resnet18, pretrained=is_pretrained, metrics=[accuracy])\n    learn.fit(1)\n    acc = learn.recorder.values[-1][-1]\n    return acc\n\nMost of this will look familiar if you are have used fastai before. Once we have this we create a study:\n\nstudy = optuna.create_study(direction='maximize')\n\nand then optimize this study:\n\nstudy.optimize(objective, n_trials=2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.503035\n0.710954\n0.555556\n00:06\n\n\n\n\n\n\n[I 2020-06-04 16:58:49,862] Finished trial#0 with value: 0.5555555820465088 with parameters: {'pre_trained': False}. Best is trial#0 with value: 0.5555555820465088.\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.691165\n1.218440\n0.555556\n00:05\n\n\n\n\n\n\n[I 2020-06-04 16:58:56,272] Finished trial#1 with value: 0.5555555820465088 with parameters: {'pre_trained': False}. Best is trial#0 with value: 0.5555555820465088.\n\n\n\n\nOnce we‚Äôve run some trials we can inspect the study object for the best value we‚Äôre optimizing for. In this case this is the accuracy but it will be whatever is returned by our function. We can also see the parameters which led to this value.\n\nstudy.best_value, study.best_params\n\n(0.5555555820465088, {'pre_trained': False})\n\n\nThis toy example wasn‚Äôt particularly useful (it just confirmed we probably want to use a pre-trained model) but going through the steps provides an overview of the main things required by Optuna. Starting with defining a function objective\ndef objective(trial):\nthis is the function we want to optimize. We could call it something else but following the convention in the Optuna docs the function we‚Äôll call it objective. This function takes ‚Äòtrial‚Äô as an argument.\nis_pretrained = trial.suggest_categorical('pre_trained', [True, False])\nhere we use trial to ‚Äúsuggest‚Äù a categorical in this case one of two options (whether pre trained is set to true or false). We do this using trial.suggest_categorical and pass it the potential options (in this case True or False).\ntrial.suggest_blah defines the paramater ‚Äúsearch space‚Äù for Optuna. We‚Äôll look at all of the options for this later on. The final step in defining our objective function i.e.¬†the thing we want to optimize:\nreturn acc\nThis return value is objective value that Optuna will optimize. Because this is just the return value of a function there is a lot of flexibility in what this can be. In this example it is accuracy but it could be training or validation loss, or another training metrics. Later on we‚Äôll look at this in more detail.\nNow let‚Äôs look at the study part:\n\nstudy = optuna.create_study(direction='maximize')\n\nThis is the most simple way of creating a study. This creates a study object, again, we‚Äôll look at more options as we go along. The one option we pass here is the direction. This refers to to whether Optuna should try to increase the return value of our optimization function or decrease it. This depends on what you a tracking i.e.¬†you‚Äôd want to minimize error or validation loss but increase accuracy or F1 score.\nLooking at the overview provided in the Optuna docs we have three main building blocks:\n\nTrial: A single call of the objective function\nStudy: An optimization session, which is a set of trials\nParameter: A variable whose value is to be optimized"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#parameter-search-space",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#parameter-search-space",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Parameter search space",
    "text": "Parameter search space\nBorrowing once more from the docs:\n\nThe difficulty of optimization increases roughly exponentially with regard to the number of parameters. That is, the number of necessary trials increases exponentially when you increase the number of parameters, so it is recommended to not add unimportant parameters\n\nThis is a crucial point. Particularly if we want to use optimization in a pragmatic way. When we have existing knowledge or evidence about what works well for a particular problem, we should use that rather than asking Optuna to find this out for us. There are some extra tricks to make our search for the best parameters more efficient which will be explored below but for now let‚Äôs get back to the example use case."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#fine-tuning-a-language-model",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#fine-tuning-a-language-model",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Fine-tuning a language model",
    "text": "Fine-tuning a language model\n\n#collapse-hide\ndf_1830 = pd.read_csv('https://gist.githubusercontent.com/davanstrien/4bc85d8a4127a2791732280ffaa43293/raw/cd1a3cc53674b64c8f130edbcb34e835afa665fb/1830trial.csv')\ndf_1730 = pd.read_csv('https://gist.githubusercontent.com/davanstrien/4bc85d8a4127a2791732280ffaa43293/raw/cd1a3cc53674b64c8f130edbcb34e835afa665fb/1730trial.csv')\n\nFor the sake of brevity I won‚Äôt cover the steps to generate this dataset the instructions for doing so for the 1830s trials can be found here (and can be easily adapted for the 1730s trial).\n\n#hide_input \ndf_1830.head(2)\n\n\n\n\n\n\n\n\nUnnamed: 0\nUnnamed: 0.1\n0\nfile\nbroad\nnarrow\ntext\n\n\n\n\n0\n14463.0\nt18361128-57a\ntheft-housebreaking\nt18361128-57a.txt\ntheft\nhousebreaking\n\\n\\n\\n\\n\\n57. \\n\\n\\n\\n\\nJOHN BYE\\n the younger and \\n\\n\\n\\n\\nFREDERICK BYE\\n were indicted for\\n\\n feloniously breaking and entering the dwelling-house of \\n\\n\\n\\nJohn Bye, on the \\n21st of November, at \\nSt. Giles-in-the-Fields, and stealing therein 12 apples, value 9d.; 1 box, value 1d.; 24 pence, and 1 twopenny-piece; the goods and monies of \\n\\n\\n\\nMary Byrne.\\n\\n\\n\\n\\n\\n\\nMARY BYRNE\\n. I sell fruit; I live in Titchbourne-court, Holborn. On the 21st of November I went out at one o'clock, and locked my door?I left 2s. worth of penny-pieces in my drawer, and two dozen large apples?I came...\n\n\n1\n19021.0\nt18380917-2214\ntheft-pocketpicking\nt18380917-2214.txt\ntheft\npocketpicking\n\\n\\n\\n\\n2214. \\n\\n\\n\\n\\nMARY SMITH\\n was indicted\\n\\n for stealing, on the \\n16th of September, 1 purse, value 2d.; 3 half-crowns, and twopence; the goods and monies of \\n\\n\\n\\nGeorge Sainsbury, from his person.\\n\\n\\n\\n\\n\\n\\nGEORGE SAINSBURY\\n. Between twelve and one o'clock, on the 16th of September, I went to sleep in the fields, at Barnsbury-park, Islington, I had three half-crowns, and twopence, in my pocket?I was awoke, and missed my money?I went to the prisoner, and charged her with it?she said she had not got it?I followed her, and saw her drop ray purse down, it had two penny piece...\n\n\n\n\n\n\n\nWe load the data using fastai2 TextDataLoaders\n\n# collapse-show\ndef load_lm_data(df):\n    data_lm = TextDataLoaders.from_df(\n        df.sample(frac=0.5), text_col=\"text\", is_lm=True, bs=128\n    )\n    return data_lm\n\n\n# Classification data\ndef load_class_data(df, data_lm):\n    data_class = TextDataLoaders.from_df(\n        df.sample(frac=0.5),\n        text_col=\"text\",\n        label_col=\"broad\",\n        valid_pct=0.3,\n        bs=128,\n        text_vocab=data_lm.vocab,\n    )\n    return data_class\n\n\ndata_lm = load_lm_data(df_1830)\ndata_class = load_class_data(df_1830, data_lm)\n\n\n\n\n\n\n\nCreate the language model learner and classifier learner:\n\n# collapse-show\ndef create_lm():\n    return language_model_learner(data_lm, AWD_LSTM, pretrained=True).to_fp16()\n\n\ndef create_class_learn():\n    return text_classifier_learner(\n        data_class, AWD_LSTM, metrics=[accuracy, F1Score(average=\"weighted\")]\n    ).to_fp16()"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#optuna-trial-suggest",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#optuna-trial-suggest",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Optuna trial suggest",
    "text": "Optuna trial suggest\nIn the example above trial.suggest_categorical was used to define the potential parameter. Optuna has five kinds of parameters which can be optimized. These all work through the trial.suggest method.\n\nCategorical\nThis can be used for models, optimizers, and for True/False flags.\noptimizer = trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam'])\n\n\nInteger\nn_epochs = trial.suggest_int('num_epochs', 1, 3)\n\n\nUniform\nmax_zoom = trial.suggest_uniform('max_zoom', 0.0, 1.0)\n\n\nLoguniform\nlearning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n\n\nDiscrete-uniform\ndrop_path_rate = trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0)\nThe string value provides a key for the parameters which is used to access these parameters later, it‚Äôs therefore important to give them a sensible name.\n\n\nLimiting parameters?\nAdding additional trial.suggest to your optimization function increases the search space for Optuna to optimize over so you should avoid adding additional parameters if they are not necessary.\nThe other way in which the search space can be constrained is to limit the range of the search i.e.¬†for learning rate\nlearning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\nis preferable over\nlearning_rate = trial.suggest_loguniform('learning_rate', 1e-10, 1e-1)\nif it‚Äôs not likely the optimal learning rate will sit outside of this range.\nHow many parameters you include will also depend on the type of model you are trying to train. In the use case of fine-tuning a language model we will want to limit the options more since language models are generally quite slow to train. If, on the other hand, we were trying to improve an image classification model which only takes minutes to train then searching through a larger parameter space would become more feasible."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#objective-function-for-fine-tuning-a-language-model",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#objective-function-for-fine-tuning-a-language-model",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Objective function for fine-tuning a language model",
    "text": "Objective function for fine-tuning a language model\nThe objective function below has two stages; train a language model, use the encoder from this language model for a classifier.\nThe parameters we‚Äôre trying to optimize in this case are:\n\nlearning rate for the frozen language model\nnumber of epochs to train only the final layers of the language model\nlearning rate for the unfrozen language model\nnumber of epochs for training the whole language model\n\nWe use lm_learn.no_bar() as a context manager to reduce the amount of logging.\n\ndef objective(trial):\n    lm_learn = create_lm()\n    lr_frozen = trial.suggest_loguniform(\"learning_rate_frozen\", 1e-4, 1e-1)\n    head_epochs = trial.suggest_int(\"head_epochs\", 1, 5)\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen)\n    # Unfrozen Language model\n    lr_unfreeze = trial.suggest_loguniform(\"learning_rate_unfrozen\", 1e-7, 1e-1)\n    body_epochs = trial.suggest_int(\"lm_body_epochs\", 1, 5)\n    lm_learn.unfreeze()\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(body_epochs, lr_unfreeze)\n    lm_learn.save_encoder(\"finetuned\")\n    # Classification\n    cl_learn = create_class_learn()\n    cl_learn.load_encoder(\"finetuned\")\n    cl_learn.fit_one_cycle(3)\n    f1 = cl_learn.recorder.values[-1][-1]\n    return f1\n\nWe can give our study a name and also store it in a database. This allows for resuming previous trials later and accessing the history of previous trials. There are various options for database backends outlined in the documentation.\n\nCreating the study\n\nstudy_name = \"tunelm1830\"\nstudy = optuna.create_study(\n    study_name=study_name,\n    direction=\"maximize\",\n    storage=f\"sqlite:///{out_path}/optuma/example.db\",\n)\n\n\n[I 2020-06-05 15:09:05,470] A new study created with name: tunelm1830\n\n\n\n\n\n\nOptimize\nNow we‚Äôll run 3 trials and use show_progress_bar=True to give an ETA on when the trials will finish.\n\nstudy.optimize(objective, n_trials=3, show_progress_bar=True)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nProgress bar is experimental (supported from v1.2.0). The interface can change in the future.\n\n\n\n\n\n\n\n\n\n(#4) [0,5.382655620574951,4.875850200653076,'00:24']\n(#4) [1,5.292355537414551,4.737764835357666,'00:24']\n(#4) [2,5.183778285980225,4.647550106048584,'00:24']\n(#4) [3,5.11093282699585,4.608272552490234,'00:24']\n(#4) [4,5.072442054748535,4.601930618286133,'00:24']\n(#4) [0,4.7495622634887695,4.241390228271484,'00:27']\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n2.326032\n2.070412\n0.020000\n0.017034\n00:10\n\n\n1\n2.302230\n2.136864\n0.023333\n0.003590\n00:10\n\n\n2\n2.269061\n2.180663\n0.016667\n0.004408\n00:10\n\n\n\n\n\n\n[I 2020-06-05 15:12:20,128] Finished trial#0 with value: 0.00440805109922757 with parameters: {'learning_rate_frozen': 0.00014124685078723662, 'head_epochs': 5, 'learning_rate_unfrozen': 0.00010276862511970148, 'lm_body_epochs': 1}. Best is trial#0 with value: 0.00440805109922757.\n\n(#4) [0,4.713407516479492,3.7350399494171143,'00:24']\n\n(#4) [1,3.998744249343872,3.3055806159973145,'00:24']\n\n(#4) [2,3.6486754417419434,3.192685842514038,'00:24']\n\n(#4) [3,3.4996860027313232,3.1756556034088135,'00:24']\n\n(#4) [0,3.4227023124694824,3.163315534591675,'00:27']\n\n(#4) [1,3.3954737186431885,3.140226364135742,'00:27']\n\n(#4) [2,3.3778774738311768,3.125929117202759,'00:27']\n\n(#4) [3,3.357388973236084,3.119621753692627,'00:27']\n\n(#4) [4,3.3542206287384033,3.1186859607696533,'00:27']\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n2.368984\n2.121307\n0.013333\n0.000759\n00:11\n\n\n1\n2.335033\n2.022853\n0.250000\n0.368652\n00:10\n\n\n2\n2.296630\n1.948786\n0.313333\n0.452365\n00:10\n\n\n\n\n\n\n[I 2020-06-05 15:16:49,562] Finished trial#1 with value: 0.45236502121696065 with parameters: {'learning_rate_frozen': 0.0060643425219262335, 'head_epochs': 4, 'learning_rate_unfrozen': 2.734844423029637e-05, 'lm_body_epochs': 5}. Best is trial#1 with value: 0.45236502121696065.\n\n(#4) [0,5.3748459815979,4.851675987243652,'00:24']\n\n(#4) [1,5.247058868408203,4.672318935394287,'00:24']\n\n(#4) [2,5.111597061157227,4.559732437133789,'00:24']\n\n(#4) [3,5.026832103729248,4.512131690979004,'00:24']\n\n(#4) [4,4.982809066772461,4.5044732093811035,'00:24']\n\n(#4) [0,4.915407657623291,4.423311233520508,'00:27']\n\n(#4) [1,4.857243061065674,4.394893646240234,'00:27']\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n2.368439\n2.036706\n0.240000\n0.360355\n00:10\n\n\n1\n2.359790\n2.093103\n0.033333\n0.045878\n00:09\n\n\n2\n2.331945\n2.140194\n0.016667\n0.013589\n00:10\n\n\n\n\n\n\n[I 2020-06-05 15:20:20,119] Finished trial#2 with value: 0.013588651008106425 with parameters: {'learning_rate_frozen': 0.0001971120155925954, 'head_epochs': 5, 'learning_rate_unfrozen': 1.0649951798153689e-05, 'lm_body_epochs': 2}. Best is trial#1 with value: 0.45236502121696065."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#results",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#results",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Results",
    "text": "Results\nYou can see how trials are peforming in the logs with the last part of the log reporting the best trial so far. We can now access the best value and best_params.\n\nstudy.best_value, study.best_params\n\n(0.45236502121696065,\n {'head_epochs': 4,\n  'learning_rate_frozen': 0.0060643425219262335,\n  'learning_rate_unfrozen': 2.734844423029637e-05,\n  'lm_body_epochs': 5})"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#suggesting-a-learning-rate",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#suggesting-a-learning-rate",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Suggesting a learning rate",
    "text": "Suggesting a learning rate\nOne of the yummiest features in fastai which has also made it into other deep-learning libraries is the learning rate finer lr_find(). As a reminder:\n\nthe LR Finder trains the model with exp onentially growing learning rates from start_lr to end_lr for num_it and stops in case of divergence (unless stop_div=False) then plots the losses vs the learning rates with a log scale.\n\nSince the Learning rate finder often gives a good learning rate we should see if we can use this as a starting point for our trials.\n\nEnqueue trial\nUsing enqueue_trial you can queue up trials with specied paramters. This can be for all of the parameters or just a subset. We can use lr_find to suggest a learning rate for the language model and then que a trial with this learning rate.\n\nlm_learn = create_lm()\nlm_learn.unfreeze()\n\n\nlr_min,lr_steep = lm_learn.lr_find(suggestions=True)\n\n\n\n\n\n\n\n\n\n\n\n\nlr_min, lr_steep\n\n(0.014454397559165954, 0.033113110810518265)\n\n\n\nstudy.enqueue_trial({'learning_rate_unfrozen': lr_steep})\nstudy.optimize(objective, n_trials=1)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nenqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n\n\n\n(#4) [0,5.322241306304932,4.736147403717041,'00:24']\n(#4) [1,5.095097541809082,4.474568843841553,'00:24']\n(#4) [2,4.91882848739624,4.365659713745117,'00:24']\n(#4) [3,4.820737838745117,4.348053455352783,'00:24']\n(#4) [0,3.5270116329193115,3.0885186195373535,'00:27']\n(#4) [1,3.1028788089752197,2.8053553104400635,'00:27']\n(#4) [2,2.7882776260375977,2.611638069152832,'00:27']\n(#4) [3,2.49800705909729,2.539992094039917,'00:27']\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n2.325723\n2.102552\n0.010000\n0.006709\n00:10\n\n\n1\n2.293266\n2.006258\n0.216667\n0.332841\n00:10\n\n\n2\n2.258634\n1.928858\n0.566667\n0.686662\n00:10\n\n\n\n\n\n\n[I 2020-06-05 15:28:58,707] Finished trial#3 with value: 0.6866621960133127 with parameters: {'head_epochs': 4, 'learning_rate_frozen': 0.0003841551576945897, 'learning_rate_unfrozen': 0.033113110810518265, 'lm_body_epochs': 4}. Best is trial#3 with value: 0.6866621960133127.\n\n\n\n\nUsing the learning rate from the LR_finder gives us our best trial so far. This is likely to be because learning rate is a particularly important hyper-parameter. The suggested learning rate from lr_find may not always be the best but using either the suggested one or picking one based on the plot as a starting point for the trial may help Optuna to start from sensible starting point while still giving the freedom for optuna to diverge away from this in later trials if helps the objective function."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#pruning-trials",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#pruning-trials",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Pruning trials",
    "text": "Pruning trials\nThe next feature of Optuna which helps make parameter searching more efficient is pruning. Pruning is a process for stopping bad trials early.\nFor example if we have the following three trials: - Trial 1 - epoch 1: 87% accuracy - Trial 2 - epoch 1: 85% accuracy - Trial 3 - epoch 1: 60% accuracy\nprobably it‚Äôs not worth continuing with trial 3. Pruning trials helps focus computational resources on trials which are likely to improve on previous trials. The likely here is important. It is possible that some trials may be pruned early which actually would have done better in the end. Optuna offers a number of different pruning algorithms, I won‚Äôt cover these here but the documentation gives a good overview and includes links to the papers which propose the implemented pruning algorithms.\n\nHow to do pruning in Optuna?\nOptuna has intergrations with various machine learning libraries. These intergrations can help with the pruning but setting up pruning manually is also pretty straight forward to do.\nThe two things we need to do is report the value and the stage in the training porcess:\ntrial.report(metric, step)\nthen we call:\nif trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\nDepending on your objective function this will be put in different places. In the example of fine-tuning the language model, because we‚Äôre trying to optimize the classification part it, it means the pruning step can only be called quite late in the traing loop. Ideally it would be called earlier but we still save a little bit of time on unpromising trials.\nThe new objective function with pruning:\n\ndef objective(trial):\n    lm_learn = create_lm()\n    lr_frozen = trial.suggest_loguniform(\"learning_rate_frozen\", 1e-4, 1e-1)\n    head_epochs = trial.suggest_int(\"head_epochs\", 1, 5)\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen)\n    # Unfrozen Language model\n    lr_unfreeze = trial.suggest_loguniform(\"learning_rate_unfrozen\", 1e-7, 1e-1)\n    body_epochs = trial.suggest_int(\"lm_body_epochs\", 1, 5)\n    lm_learn.unfreeze()\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(body_epochs, lr_unfreeze)\n    lm_learn.save_encoder(\"finetuned\")\n    # Classification\n    cl_learn = create_class_learn()\n    cl_learn.load_encoder(\"finetuned\")\n    for step in range(3):\n        cl_learn.fit(1)\n        # Pruning\n        intermediate_f1 = cl_learn.recorder.values[-1][\n            -1\n        ]  # get f1 score for current step\n        trial.report(intermediate_f1, step)  # report f1\n        if trial.should_prune():  # let optuna decide whether to prune\n            raise optuna.exceptions.TrialPruned()\n    f1 = cl_learn.recorder.values[-1][-1]\n    return f1\n\nWe can load the same study as before using the python load_if_exists flag.\n\nstudy_name = \"tunelm1830\"\nstudy = optuna.create_study(\n    study_name=study_name,\n    direction=\"maximize\",\n    storage=f\"sqlite:///{out_path}/optuma/example.db\",\n    load_if_exists=True,\n    pruner=optuna.pruners.SuccessiveHalvingPruner(),\n)\n\n\n[I 2020-06-06 14:30:47,724] Using an existing study with name 'tunelm1830' instead of creating a new one.\n\n\n\n\nWe can now run some more trials. Instead of specifying the number of trials we can also specify how long optuma should search for.\n\nstudy.enqueue_trial({'learning_rate_unfrozen': lr_steep})\nstudy.optimize(objective, timeout=60*60*0.5)\n\nand get the best trial:\n\nstudy.best_trial\n\nFrozenTrial(number=13, value=0.8657462002717475, datetime_start=datetime.datetime(2020, 6, 5, 15, 59, 26, 230967), datetime_complete=datetime.datetime(2020, 6, 5, 16, 3, 26, 392390), params={'head_epochs': 4, 'learning_rate_frozen': 0.0012866609022148768, 'learning_rate_unfrozen': 1.3302852136460371e-06, 'lm_body_epochs': 4}, distributions={'head_epochs': IntUniformDistribution(high=5, low=1, step=1), 'learning_rate_frozen': LogUniformDistribution(high=0.1, low=0.0001), 'learning_rate_unfrozen': LogUniformDistribution(high=0.1, low=1e-07), 'lm_body_epochs': IntUniformDistribution(high=5, low=1, step=1)}, user_attrs={}, system_attrs={'completed_rung_0': 0.8156506309537317}, intermediate_values={0: 0.251088767516275, 1: 0.8156506309537317, 2: 0.8657462002717475}, trial_id=14, state=TrialState.COMPLETE)\n\n\nand best value and pararms:\n\nstudy.best_value, study.best_params\n\n(0.8657462002717475,\n {'head_epochs': 4,\n  'learning_rate_frozen': 0.0012866609022148768,\n  'learning_rate_unfrozen': 1.3302852136460371e-06,\n  'lm_body_epochs': 4})"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#paramters-for-the-1730s-trials-data",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#paramters-for-the-1730s-trials-data",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Paramters for the 1730s trials data",
    "text": "Paramters for the 1730s trials data\nWe can do the same process with the 1730s trials, starting with a suggested learning rate.\n\n#collapse-show\ndata_lm = load_lm_data(df_1730)\ndata_class = load_class_data(df_1730, data_lm)\nlm_learn = create_lm()\nlm_learn.unfreeze()\nlr_min,lr_steep = lm_learn.lr_find(suggestions=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# collapse-hide\ndef objective(trial):\n    lm_learn = create_lm()\n    lr_frozen = trial.suggest_loguniform(\"learning_rate_frozen\", 1e-4, 1e-1)\n    head_epochs = trial.suggest_int(\"head_epochs\", 1, 5)\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(head_epochs, lr_max=lr_frozen)\n    # Unfrozen Language model\n    lr_unfreeze = trial.suggest_loguniform(\"learning_rate_unfrozen\", 1e-7, 1e-1)\n    body_epochs = trial.suggest_int(\"lm_body_epochs\", 1, 5)\n    lm_learn.unfreeze()\n    with lm_learn.no_bar():\n        lm_learn.fit_one_cycle(body_epochs, lr_unfreeze)\n    lm_learn.save_encoder(\"finetuned\")\n    # Classification\n    cl_learn = create_class_learn()\n    cl_learn.load_encoder(\"finetuned\")\n    for step in range(3):\n        cl_learn.fit(1)\n        intermediate_f1 = cl_learn.recorder.values[-1][-1]\n        trial.report(intermediate_f1, step)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n    f1 = cl_learn.recorder.values[-1][-1]\n    return f1\n\n\n# collapse-hide\nstudy_name = \"tunelm1730\"\nstudy = optuna.create_study(\n    study_name=study_name,\n    direction=\"maximize\",\n    storage=f\"sqlite:///{out_path}/optuma/example.db\",\n    load_if_exists=True,\n    pruner=optuna.pruners.SuccessiveHalvingPruner(),\n)\n\n\n[I 2020-06-08 15:06:54,474] Using an existing study with name 'tunelm1730' instead of creating a new one.\n\n\n\n\n\nstudy.enqueue_trial({'learning_rate_unfrozen': lr_steep})\nstudy.optimize(objective, timeout=60*60*0.5)\n\nTrials can be accssed as part of the study object. Running trials for 30 mins with early pruning results in 20 trials\n\nlen(study.trials)\n\n20\n\n\nWe can also see which was the best trial.\n\nstudy.best_trial.number\n\n2\n\n\nThe number of trials run depends mainly on how long your model takes to train, the size of the paramter search space and your patience. If trials are failing to improve better scores for a long time it‚Äôs probably better to actively think about how to improve your approach to the problem (better data, more data, chaning model design etc.) rather than hoping hyperaparmet tuning will fix the problem."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#comparing-language-model-parameters",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#comparing-language-model-parameters",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Comparing language model parameters",
    "text": "Comparing language model parameters\nPrevious trials can be loaded using load_study\n\nstudy1830 = optuna.load_study('tunelm1830', storage=f'sqlite:///{out_path}/optuma/example.db')\nstudy1730 = optuna.load_study('tunelm1730', storage=f'sqlite:///{out_path}/optuma/example.db')\n\nFirst comparing the best f1 values for both datasets:\n\nprint(f'Best 1830 value was: {study1830.best_value:.3}')\nprint(f'Best 1730 value was: {study1730.best_value:.3}')\n\nBest 1830 value was: 0.866\nBest 1730 value was: 0.781\n\n\nThe paramters used to get the best value:\n\n1830 parameters\n\n#hide_input\nstudy1830.best_params\n\n{'head_epochs': 4,\n 'learning_rate_frozen': 0.0012866609022148768,\n 'learning_rate_unfrozen': 1.3302852136460371e-06,\n 'lm_body_epochs': 4}\n\n\n\n\n1730 parameters\n\n#hide_input\nstudy1730.best_params\n\n{'head_epochs': 3,\n 'learning_rate_frozen': 0.002145480897071231,\n 'learning_rate_unfrozen': 9.889236991663078e-06,\n 'lm_body_epochs': 1}\n\n\nSpecific parameters can also be accessed\n\nstudy1830.best_params['learning_rate_unfrozen'], study1730.best_params['learning_rate_unfrozen']\n\n(1.3302852136460371e-06, 9.889236991663078e-06)"
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#visualizations",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#visualizations",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Visualizations",
    "text": "Visualizations\nOptuna has a variety of visulizations, I will only briefly show a few of these here.\nplot_intermediate_values shows the intermediate values. This can be useful for getting a sense of how trials progress and also help give a sense of whether some trials are being pruned prematurely\n\noptuna.visualization.plot_intermediate_values(study1830)\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nplot_parallel_coordinate plots parameters choices in relation to values. It can be hard to read these plots but they can also be helpful for giving a sense of which choices for parameters work best.\n\noptuna.visualization.plot_parallel_coordinate(study1830)\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\nParameter importance\nOptuna has experimental support for getting parameter importance.\n\noptuna.importance.get_param_importances(study1730)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nget_param_importances is experimental (supported from v1.3.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning:\n\nMeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future.\n\n\n\nOrderedDict([('learning_rate_frozen', 0.43423246892923717),\n             ('learning_rate_unfrozen', 0.2904735896601219),\n             ('head_epochs', 0.2433021650269149),\n             ('lm_body_epochs', 0.031991776383726155)])\n\n\n\noptuna.importance.get_param_importances(study1830)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nget_param_importances is experimental (supported from v1.3.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning:\n\nMeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future.\n\n\n\nOrderedDict([('learning_rate_unfrozen', 0.35548906967729954),\n             ('learning_rate_frozen', 0.33998779901100146),\n             ('head_epochs', 0.21196438930810765),\n             ('lm_body_epochs', 0.09255874200359132)])\n\n\nThese are broadly similar although learning rate frozen/unfrozen are in different places for the 1730 and 1830 trials."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#the-data",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#the-data",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "The data",
    "text": "The data\nThe data is images of maps and other things from historic newspapers. The aim is to classify whether the image is a map or something else.\n\ndls = ImageDataLoaders.from_folder(\n    \"data/1905_maps/\", valid_pct=0.3, item_tfms=Resize(256)\n)\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n#hide\nlearn = cnn_learner(dls, resnet50, metrics=[F1Score(average='weighted')])\n\n\nlearn.unfreeze()\nlr_min,unfrozen_lr_steep = learn.lr_find(suggestions=True)\n\n\n\n\n\n\n\n\n\n\n\n\nExcessive model parameter search\nSince the time to train the model is more reasonable we can add a more parameters to the search space. In practice this is pretty overkill but is useful as an example of working with the outputs of trials with many parameters.\n\ndef objective(trial):\n    apply_tfms = trial.suggest_categorical(\"apply_tfms\", [True, False])\n    if apply_tfms:\n        aug_tfms = aug_transforms(\n            mult=trial.suggest_uniform(\"mult\", 0.0, 1.0),\n            do_flip=trial.suggest_categorical(\"do_flip\", [True, False]),\n            flip_vert=trial.suggest_categorical(\"flip_vert\", [True, False]),\n            max_rotate=trial.suggest_uniform(\"max_rotate\", 0, 180),\n            max_zoom=trial.suggest_uniform(\"max_zoom\", 0, 3.0),\n            max_lighting=trial.suggest_uniform(\"max_lighting\", 0.0, 1.0),\n        )\n    else:\n        aug_tfms = None\n    dls = ImageDataLoaders.from_folder(\n        \"data/1905_maps/\", valid_pct=0.3, item_tfms=Resize(256), aug_transforms=aug_tfms\n    )\n    model = trial.suggest_categorical(\n        \"model\", [\"resnet18\", \"resnet50\", \"xresnet50\", \"squeezenet1_0\", \"densenet121\"]\n    )\n    learn = cnn_learner(\n        dls, arch=eval(model), pretrained=True, metrics=[F1Score(average=\"weighted\")]\n    ).to_fp16()\n    epochs = trial.suggest_int(\"epochs\", 1, 10)\n    for step in range(epochs):\n        with learn.no_bar():\n            learn.fit_one_cycle(\n                1, base_lr=trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1)\n            )\n    unfrozen_epochs = trial.suggest_int(\"unfrozen_epochs\", 1, 10)\n    unfrozen_lr = trial.suggest_loguniform(\"unfrozen_learning_rate\", 1e-10, 1e-1)\n    learn.unfreeze()\n    for step in range(unfrozen_epochs):\n        with learn.no_bar():\n            learn.fit_one_cycle(1, lr_max=unfrozen_lr)\n            int_f1 = learn.recorder.values[-1][-1]\n            trial.report(int_f1, step)\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n    t0 = time.time()\n    learn.validate()\n    t1 = time.time()\n    execute_time = t1 - t0\n    trial.set_user_attr(\"execute_time\", execute_time)\n    f1 = learn.recorder.values[-1][-1]\n    return f1\n\nCreate the study\n\nstudy_name = \"mapsmegastudyXL\"  # Unique identifier of the study.\nstudy = optuna.create_study(\n    direction=\"maximize\",\n    load_if_exists=True,\n    study_name=study_name,\n    storage=f\"sqlite:///{out_path}/optuma/blog.db\",\n    pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=2),\n)\n\n\n[I 2020-06-07 15:03:24,138] Using an existing study with name 'mapsmegastudyXL' instead of creating a new one.\n\n\n\n\nQueue up with some parameters\n\nstudy.enqueue_trial(\n    {\n        \"pre_trained\": True,\n        \"apply_tfms\": True,\n        \"epochs\": 5,\n        \"learning_rate\": lr_steep,\n        \"model\": \"resnet50\",\n        \"unfrozen_learning_rate\": unfrozen_lr_steep,\n    }\n)\nstudy.optimize(objective, n_trials=1, show_progress_bar=True)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nenqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nProgress bar is experimental (supported from v1.2.0). The interface can change in the future.\n\n\n\n\n\n\n(#5) [0,1.162647008895874,1.0643662214279175,0.38594077225581136,'00:04']\n(#5) [0,1.1730060577392578,0.8583190441131592,0.45458674870439575,'00:02']\n(#5) [0,0.7940309047698975,0.40110471844673157,0.8101934029975151,'00:02']\n(#5) [0,0.3774714767932892,0.3251221776008606,0.8738329238329239,'00:02']\n(#5) [0,0.20592834055423737,0.304998517036438,0.8914149443561209,'00:02']\n(#5) [0,0.14400754868984222,0.3399428725242615,0.9003332765709003,'00:03']\n(#5) [0,0.11649172753095627,0.3571062982082367,0.8729641116526362,'00:03']\n\n\n\n\n\n\n[I 2020-06-07 15:03:57,405] Finished trial#0 with value: 0.8729641116526362 with parameters: {'apply_tfms': True, 'do_flip': False, 'epochs': 5, 'flip_vert': True, 'learning_rate': 0.00015848931798245758, 'max_lighting': 0.5155363265412508, 'max_rotate': 93.50185801538605, 'max_zoom': 2.5014402368129147, 'model': 'resnet50', 'mult': 0.7973732804273224, 'unfrozen_epochs': 2, 'unfrozen_learning_rate': 1.4454397387453355e-05}. Best is trial#0 with value: 0.8729641116526362.\n\n\n\n\n\n\nqueue up with some less sensible defaults\n\nstudy.enqueue_trial(\n    {\"pre_trained\": False, \"apply_tfms\": False, \"epochs\": 1, \"unfrozen_epochs\": 1}\n)\nstudy.optimize(objective, n_trials=1, show_progress_bar=True)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nenqueue_trial is experimental (supported from v1.2.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nProgress bar is experimental (supported from v1.2.0). The interface can change in the future.\n\n\n\n\n\n\n(#5) [0,1.1455873250961304,1.7333940267562866,0.3123010228273386,'00:01']\n(#5) [0,1.0485259294509888,1.4432364702224731,0.4545249081834448,'00:01']\n\n\n\n\n\n\n[I 2020-06-07 15:04:18,823] Finished trial#1 with value: 0.4545249081834448 with parameters: {'apply_tfms': False, 'epochs': 1, 'learning_rate': 1.4039901997074766e-05, 'model': 'resnet18', 'unfrozen_epochs': 1, 'unfrozen_learning_rate': 4.041607859100835e-07}. Best is trial#0 with value: 0.8729641116526362.\n\n\n\n\n\n\nNow optimize for 500 trials\n\nstudy.optimize(objective, n_trials=500,show_progress_bar=True)\n\n\nstudy = optuna.load_study('mapsmegastudyXL', storage=f'sqlite:///{out_path}/optuma/blog.db')\n\nThe best finishing values and parameters:\n\nstudy.best_value, study.best_params\n\n(0.963975663975664,\n {'apply_tfms': True,\n  'do_flip': True,\n  'epochs': 10,\n  'flip_vert': False,\n  'learning_rate': 0.0785689562916925,\n  'max_lighting': 0.5064203068969654,\n  'max_rotate': 168.972217754609,\n  'max_zoom': 1.6141746329756919,\n  'model': 'densenet121',\n  'mult': 0.6087267126078458,\n  'unfrozen_epochs': 4,\n  'unfrozen_learning_rate': 7.6080876225791396e-06})\n\n\n\nVisualization\nTaking a look at parallel_coordinate in this case gives some sense of which options work best.\n\noptuna.visualization.plot_parallel_coordinate(study)\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\nImportance\n\noptuna.importance.get_param_importances(study)\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:61: ExperimentalWarning:\n\nget_param_importances is experimental (supported from v1.3.0). The interface can change in the future.\n\n/usr/local/lib/python3.6/dist-packages/optuna/_experimental.py:83: ExperimentalWarning:\n\nMeanDecreaseImpurityImportanceEvaluator is experimental (supported from v1.5.0). The interface can change in the future.\n\n\n\nOrderedDict([('unfrozen_learning_rate', 0.6945560978629778),\n             ('epochs', 0.13207296757949719),\n             ('model', 0.07996254760084977),\n             ('unfrozen_epochs', 0.04455237119259635),\n             ('learning_rate', 0.04014544684326522),\n             ('apply_tfms', 0.008710568920813712)])\n\n\nLearning rate is by far the most important learning rate, again this suggests that using learning rate finder makes a lot of sense as a starting point."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#working-with-optuna-trial-data",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#working-with-optuna-trial-data",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Working with Optuna trial data",
    "text": "Working with Optuna trial data\nThere are now ~500 trials which are stored in the study. Each of these trials contains the parameters used, metadata about the trial, the value of the thing being optimized, and importantly for this example the user attribute which stores the validation time. Optuna makes it very easy to export this information to a dataframe.\n\ndf = study.trials_dataframe()\ndf.head(3)\n\n\n\n\n\n\n\n\nnumber\nvalue\ndatetime_start\ndatetime_complete\nduration\nparams_apply_tfms\nparams_do_flip\nparams_epochs\nparams_flip_vert\nparams_learning_rate\nparams_max_lighting\nparams_max_rotate\nparams_max_zoom\nparams_model\nparams_mult\nparams_unfrozen_epochs\nparams_unfrozen_learning_rate\nuser_attrs_execute_time\nsystem_attrs_completed_rung_0\nsystem_attrs_completed_rung_1\nsystem_attrs_fixed_params\nstate\n\n\n\n\n0\n0\n0.872964\n2020-06-07 15:03:29.911841\n2020-06-07 15:03:57.151460\n00:00:27.239619\nTrue\nFalse\n5.0\nTrue\n0.000158\n0.515536\n93.501858\n2.50144\nresnet50\n0.797373\n2.0\n1.445440e-05\n0.82319\nNaN\nNaN\n{'pre_trained': True, 'apply_tfms': True, 'epochs': 5, 'learning_rate': 0.00015848931798245758, 'model': 'resnet50', 'unfrozen_learning_rate': 1.4454397387453355e-05}\nCOMPLETE\n\n\n1\n1\n0.454525\n2020-06-07 15:04:11.520248\n2020-06-07 15:04:18.419082\n00:00:06.898834\nFalse\nNaN\n1.0\nNaN\n0.000014\nNaN\nNaN\nNaN\nresnet18\nNaN\n1.0\n4.041608e-07\n0.67698\nNaN\nNaN\n{'pre_trained': False, 'apply_tfms': False, 'epochs': 1, 'unfrozen_epochs': 1}\nCOMPLETE\n\n\n2\n2\nNaN\n2020-06-07 15:04:32.047588\nNaT\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nRUNNING\n\n\n\n\n\n\n\n\n#hide\ndf.to_csv('optuna_study.csv')\n\n\n#hide\ndf = pd.read_csv('https://gist.githubusercontent.com/davanstrien/0c9670d02cdf8a9a866b8a467664b690/raw/cb3222f1baf8ae894923e2b8898beaa22ebeadd8/optuna_trials.csv')\n\nWe can now easily work with the trial data using pandas. Lets start by getting the best two values\n\ndf.sort_values(['value'], ascending=False).head(2)\n\n\n\n\n\n\n\n\nUnnamed: 0\nnumber\nvalue\ndatetime_start\ndatetime_complete\nduration\nparams_apply_tfms\nparams_do_flip\nparams_epochs\nparams_flip_vert\n...\nparams_max_zoom\nparams_model\nparams_mult\nparams_unfrozen_epochs\nparams_unfrozen_learning_rate\nuser_attrs_execute_time\nsystem_attrs_completed_rung_0\nsystem_attrs_completed_rung_1\nsystem_attrs_fixed_params\nstate\n\n\n\n\n177\n177\n177\n0.963976\n2020-06-07 16:48:36.232551\n2020-06-07 16:49:21.393454\n0 days 00:00:45.160903000\nTrue\nTrue\n10.0\nFalse\n...\n1.614175\ndensenet121\n0.608727\n4.0\n7.608088e-06\n0.880459\n0.954955\nNaN\nNaN\nCOMPLETE\n\n\n302\n302\n302\n0.955064\n2020-06-07 18:11:00.667449\n2020-06-07 18:11:45.658241\n0 days 00:00:44.990792000\nTrue\nTrue\n10.0\nFalse\n...\n0.921689\ndensenet121\n0.115708\n4.0\n6.210737e-10\n0.878865\n0.945946\nNaN\nNaN\nCOMPLETE\n\n\n\n\n2 rows √ó 23 columns\n\n\n\nWe can see how often transforms were applied in the trials\n\ndf['params_apply_tfms'].value_counts()\n\nTrue     360\nFalse    142\nName: params_apply_tfms, dtype: int64\n\n\nViewing the number of trials for each model which had a value over 90\n\ndf['params_model'][df['value'] &gt;= 0.90].value_counts()\n\ndensenet121      181\nresnet50           9\nsqueezenet1_0      2\nName: params_model, dtype: int64\n\n\nFiltering a bit more aggressively (value above 94)\n\ndf94 = df[df['value'] &gt;= 0.94]\n\n\nlen(df94)\n\n13\n\n\nHow often were transforms applied for these trials\n\ndf94['params_apply_tfms'].value_counts()\n\nTrue     11\nFalse     2\nName: params_apply_tfms, dtype: int64\n\n\nThe number of unfrozen epochs\n\ndf94['params_unfrozen_epochs'].value_counts()\n\n4.0    6\n2.0    3\n3.0    2\n6.0    1\n5.0    1\nName: params_unfrozen_epochs, dtype: int64\n\n\nGetting back to the validation time we can get the max, min and mean values\n\ndf['user_attrs_execute_time'].max(), df['user_attrs_execute_time'].min(), df['user_attrs_execute_time'].mean()\n\n(0.9760787487030028, 0.6313643455505371, 0.8461264789613903)\n\n\nIf we did care about reducing the execution time we could use these values to find the trial with the shortest execution time:\n\ndf94['user_attrs_execute_time'].sort_values()\n\n96     0.837618\n426    0.848863\n394    0.849243\n395    0.851704\n438    0.852672\n500    0.863168\n344    0.875520\n432    0.877422\n302    0.878865\n177    0.880459\n473    0.884703\n372    0.906770\n294    0.907221\nName: user_attrs_execute_time, dtype: float64\n\n\nIf we were happy with slightly lower performance we could pick the study with the shortest execution time which is still achieves a f1 above 94%\n\ndf94.loc[96]\n\nnumber                                                   96\nvalue                                              0.945755\ndatetime_start                   2020-06-07 15:57:20.382634\ndatetime_complete                2020-06-07 15:57:54.848296\nduration                             0 days 00:00:34.465662\nparams_apply_tfms                                     False\nparams_do_flip                                          NaN\nparams_epochs                                             9\nparams_flip_vert                                        NaN\nparams_learning_rate                            8.47479e-05\nparams_max_lighting                                     NaN\nparams_max_rotate                                       NaN\nparams_max_zoom                                         NaN\nparams_model                                    densenet121\nparams_mult                                             NaN\nparams_unfrozen_epochs                                    2\nparams_unfrozen_learning_rate                   4.31178e-07\nuser_attrs_execute_time                            0.837618\nsystem_attrs_completed_rung_0                           NaN\nsystem_attrs_completed_rung_1                           NaN\nsystem_attrs_fixed_params                               NaN\nstate                                              COMPLETE\nName: 96, dtype: object\n\n\nThis is a slightly artificial example but hopefully shows the possibility of logging user attributes which can then be accessed easily later without prematurely optimizing for something which may not be important."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#further-reading",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#further-reading",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "Further reading",
    "text": "Further reading\nHopefully this post has been a helpful overview of Optuna with a somewhat realistic use case. I would recommend reading the Optuna docs which covers things in much more detail."
  },
  {
    "objectID": "posts/post-with-code/optuna/2020-07-01-optuna.html#references",
    "href": "posts/post-with-code/optuna/2020-07-01-optuna.html#references",
    "title": "Hyperparameter Optimization for Transfer Learning",
    "section": "References",
    "text": "References\n{{‚ÄòAuto ml [auto-ml, fastai blog(https://www.fast.ai/2018/07/16/auto-ml2/#auto-ml‚Äô | fndetail: 1 }}\n{{‚Äòintroducting ulmfit nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html‚Äô | fndetail: 2}}\n{{‚ÄòTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta,and Masanori Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In KDD.‚Äô | fndetail: 3 }}"
  },
  {
    "objectID": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html",
    "href": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html",
    "title": "Image labeling vs classification models",
    "section": "",
    "text": "The ‚Äòhello world‚Äô example for introducing deep learning based computer vision often involves classifying images as üê∂ or üê±. An alternative approach to classifying images is to instead apply labels. This is usually introduced in the context of multi-label classification i.e.¬†where an image can have more than one label. In this blog post I discuss some of the differences between these two approaches, specifically the difference in loss functions, and how these two approaches might work better depending on the application. The post starts with a conceptual overview of the differences between these two approaches, before showing the different loss functions and then moving to a practical example of training these two different types of model."
  },
  {
    "objectID": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html#image-classification-vs-image-labeling",
    "href": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html#image-classification-vs-image-labeling",
    "title": "Image labeling vs classification models",
    "section": "Image Classification vs Image Labeling",
    "text": "Image Classification vs Image Labeling\nIn a classification model, an input can have only one label. This could be one of a few or one of a hundred, regardless of the number of potential classes, it is assumed that the input only belongs to one of these. With a model that applies labels this is not true an input can have one, multiple or no labels.\n\nSorting through family photos\nWe can use an analogy to illustrate the difference between these two approaches. Let‚Äôs say you were sorting through some old family photographs. You might ‚Äúclassify‚Äù the photos into one (and only one) of two photo albums, depending on whether they are black-and-white or colour. This would be comparable to using a classification model since each photo will go into exactly one of these two albums - a photo cannot be both simultaneously colour and black-and-white, and it cannot be neither colour nor black-and-white.\nYou may at the same time also want to make it easier to find photos of particular people in your family. You could do this by assigning labels to each photo, indicating or ‚Äútagging‚Äù the family members who appear in the photo. In this case, a photo may have one label (a photo of your sister), more than one label (a photo of your sister and aunt), or it may have no labels (a photograph of a landscape taken on a holiday). This would be analogous to a multi-label classification model.\nThe choice between using a model which performs classification or a model which assigns labels should be considered in relation to the role your model has. It is also useful to look a little bit more closely as how these different types of models work under the hood."
  },
  {
    "objectID": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html#crossentropyloss-vs-bcewithlogitsloss",
    "href": "posts/post-with-code/2020-10-12-labelling_vs_classification_models.html#crossentropyloss-vs-bcewithlogitsloss",
    "title": "Image labeling vs classification models",
    "section": "CrossEntropyLoss vs BCEWithLogitsLoss",
    "text": "CrossEntropyLoss vs BCEWithLogitsLoss\nWhen we create a model which does classifications or applies labels, the distinction, if using the same data is that they use different loss functions.\nA classification model will use a variant of Cross Entropy Loss whilst the label model will use a BCE with Logits Loss. We‚Äôll see how this is inferred by fastai below but fore now take my word for it‚Ä¶\nLet‚Äôs take a look at a snippet of the Pytorch docs for each of these loss functions\n\nCrossEntropyLoss\n\nThis criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. Read more\n\n\n\nBCEWithLogitsLoss\n\nThis loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. Read more\n\nLet‚Äôs see what these do to some activations. First we‚Äôll import required packages\n\nimport torch.nn as nn\nimport numpy as np\nimport torch"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "",
    "text": "I‚Äôm currently going through the Full Stack Deep Learning course. As part of this we‚Äôve been going through tools for different parts of the machine learning pipeline. This post talks about data annotation, and how we can combine Label Studio and the Hugging Face Datasets hub. I‚Äôll use the example of annotating image data for an image classification task. The details of why I‚Äôm annotating this data will wait for a future post!\nnote this post assumes you already know roughly what the Hugging Face Hub is. If you don‚Äôt this is a nice intro."
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#full-stack-deep-learning-annotating-data",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#full-stack-deep-learning-annotating-data",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "",
    "text": "I‚Äôm currently going through the Full Stack Deep Learning course. As part of this we‚Äôve been going through tools for different parts of the machine learning pipeline. This post talks about data annotation, and how we can combine Label Studio and the Hugging Face Datasets hub. I‚Äôll use the example of annotating image data for an image classification task. The details of why I‚Äôm annotating this data will wait for a future post!\nnote this post assumes you already know roughly what the Hugging Face Hub is. If you don‚Äôt this is a nice intro."
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#what-is-the-goal",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#what-is-the-goal",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "What is the goal?",
    "text": "What is the goal?\nWe want to have a way of easily moving from different stages of our machine learning project pipeline. For many projects, especially the weird stuff I‚Äôm likely to do, you will need to do some of your own annotating. It almost always makes sense to move quickly between annotating a first batch of data, trying to train a model and iterating. This can help:\n\nflag issues with your data\nidentify if you have ambiguous labels\nhelp you get some sense of how a model might perform on the task you are working on\nallow you to deploy a model early so you can begin iterating on the whole pipeline\n‚Ä¶\n\n\n\nfrom Imgflip Meme Generator\n\nThis approach can cause some challenges; how do you keep updating your annotations, how can you version the changes?"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#a-more-mundane-challenge",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#a-more-mundane-challenge",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "A more mundane challenge",
    "text": "A more mundane challenge\nIn the full stack deep learning course, one of the labs covered using Label Studio to annotate data. Label studio is a great open source tool for annotating data across a range of domains and for a variety of tasks.\nLabel studio has great support for annotating image data. One challenge we can face, however, is how to load images into label studio. This can be particularly tricky if you only have the images locally since label studio prefers images to be available via a URL. There are various ways around this but we may also be able to tackle this challenge using the datasets hub.\nWe‚Äôll start by downloading a dataset we want annotate warning this dataset is pretty big ~44GB uncompressed.\n\n%%bash\nwget https://nlsfoundry.s3.amazonaws.com/data/nls-data-encyclopaediaBritannica.zip\nunzip *.zip\n\nWe‚Äôll import some standard libraries\n\nimport pandas as pd\nfrom pathlib import Path"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#create-a-new-dataset-on-the-hub",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#create-a-new-dataset-on-the-hub",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Create a new dataset on the Hub",
    "text": "Create a new dataset on the Hub\nSince we want to upload our data to the Hugging Face hub we‚Äôll create a new dataset on the Hugging Face Hub via the CLI.\nhuggingface-cli repo create encyclopaedia_britannica --type dataset \nUnder the hood, Hugging Face hub datasets (and models) are Git repositories. We‚Äôll clone this repo and move the downloaded dataset into this new Git repository.\ngit clone https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica\nmv nls-data-encyclopaediaBritannica encyclopaedia_britannica/\nSince the number of examples in this dataset is beyond what we‚Äôre likely to annotate we do a bit of deleting of the dataset. You could also take a sample of the original but in this case I‚Äôm happy to reclaim some space on my hardrive!\n\nimport shutil\nfrom tqdm.auto import tqdm\n\nfirst we get rid of some alto folders that we don‚Äôt need for the dataset we‚Äôre aiming to create\n\nfor directory in tqdm(\n    list(\n        (\n            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n                \"*alto\"\n            )\n        )\n    )\n):\n    shutil.rmtree(directory)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 195/195 [00:34&lt;00:00,  5.62it/s]\n\n\nthere are a few other *.xml files in this dataset we also remove\n\nfor xml_file in tqdm(\n    list(\n        (\n            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n                \"*xml\"\n            )\n        )\n    )\n):\n    xml_file.unlink()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 195/195 [00:00&lt;00:00, 1464.47it/s]\n\n\nLet‚Äôs take a look at how many images we have now\n\nimage_files = list(\n    (Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\"*jpg\"))\n)\n\n\nlen(image_files)\n\n155388\n\n\nWe‚Äôre not likely to annotate this many images, let‚Äôs aim to have a max of 10,000 images. This is also likely to be more than we‚Äôll annotate but we may use a smaller sample for unsupervised pre-training.\n\nnum_to_remove = len(image_files) - 10_000\n\nWe‚Äôll now randomly remove the extra images we don‚Äôt need beyond our sample\n\nimport random\n\n\nto_remove = random.sample(image_files, num_to_remove)\nfor file in tqdm(to_remove):\n    file.unlink()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90000/90000 [00:33&lt;00:00, 2659.02it/s]\n\n\n\nlen(\n    list(\n        (\n            Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\n                \"*jpg\"\n            )\n        )\n    )\n)\n\n10000\n\n\n\nUploading our raw data to the hub\nWe can now upload this data to the Hugging Face Hub. Under the hood the Hub uses Git so everything you love (and hate) about Git should be familiar. The main difference between using the hub and GitHub or another Git hosting platform is that the Hugging Face hub has support for large files. This means we can more easily work with large files (like our images).\ncd encyclopaedia_britannica\ngit lfs track \"*.jpg\"\ngit add .gitattributes\ngit add nls-data-encyclopaediaBritannica\ngit commit -m \"add image files\"\ngit push"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#loading-local-files-and-metadata",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#loading-local-files-and-metadata",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Loading local files and metadata",
    "text": "Loading local files and metadata\nThe particular dataset we‚Äôre working with also has a metadata file associated with it. We can grab all of the images so we can put them in a DataFrame and merge this with metadata about these images. We may not use this extra metadata but it‚Äôs nice to have this additional metadata about our items alongside our annotations. This can help us debug where our model is performing badly later on.\n\nimage_files = list(\n    Path(\"encyclopaedia_britannica/nls-data-encyclopaediaBritannica\").rglob(\"*.jpg\")\n)\n\n\ndf = pd.DataFrame(image_files, columns=[\"filename\"])\n\nThis dataset also comes with some metadata. We‚Äôll load that in to another DataFrame\n\nmetadata_df = pd.read_csv(\n    \"encyclopaedia_britannica/nls-data-encyclopaediaBritannica/encyclopaediaBritannica-inventory.csv\",\n    header=None,\n    names=[\"id\", \"meta\"],\n    dtype={\"id\": \"int64\"},\n)\n\n\ndf[\"id\"] = df.filename.apply(lambda x: x.parts[-3]).astype(\"int64\")\n\n\ndf = df.merge(metadata_df, on=\"id\")\ndf\n\n\n\n\n\n\n\n\nfilename\nid\nmeta\n\n\n\n\n0\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n1\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n2\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n3\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n4\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291\nEncyclopaedia Britannica - Third edition, Volu...\n\n\n...\n...\n...\n...\n\n\n9995\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n9996\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n9997\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n9998\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n9999\nencyclopaedia_britannica/nls-data-encyclopaedi...\n193696083\nEncyclopaedia Britannica - Seventh edition, Vo...\n\n\n\n\n10000 rows √ó 3 columns"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#annotating-using-label-studio",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#annotating-using-label-studio",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Annotating using label studio",
    "text": "Annotating using label studio\nNow we have our images uploaded to the Hugging Face hub, how we go about annotating? As was mentioned already the Hugging Face hub is essentially a Git repo. Since we uploaded our image files individually i.e.¬†not in a compressed folder, we can access each file from that repo. We mentioned before that label studio can load images from URLs. The hub has an API that we can use to interact with our repository. Let‚Äôs see how we can use this to get our data ready for label studio.\n\nfrom huggingface_hub import list_repo_files, hf_hub_url\n\n\nfiles = list_repo_files(\"davanstrien/encyclopaedia_britannica\", repo_type=\"dataset\")\nfiles[:2]\n\n['.gitattributes',\n 'nls-data-encyclopaediaBritannica/144133901/image/188082865.3.jpg']\n\n\nWe‚Äôll filter out some data we are not interested in annotating\n\nfiles = [file for file in files if not file.startswith(\".\")]\nlen(files)\n\n10002\n\n\nhf_hub_url can be used to generate the URL for a particular file\n\nhf_hub_url(\n    \"davanstrien/encyclopaedia_britannica\",\n    \"192866824.3.jpg\",\n    subfolder=\"sample/nls-data-encyclopaediaBritannica/192547788/image\",\n    repo_type=\"dataset\",\n)\n\n'https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/sample/nls-data-encyclopaediaBritannica/192547788/image/192866824.3.jpg'\n\n\nWe can use this to grab all of the URLs we‚Äôre interested in\n\nurls = []\nfor file in files:\n    file = Path(file)\n    urls.append(\n        hf_hub_url(\n            \"davanstrien/encyclopedia_britannica\",\n            file.name,\n            subfolder=file.parents[0],\n            repo_type=\"dataset\",\n        )\n    )\n\nWe can now load these into a DataFrame, and save this to a CSV file.\n\npd.DataFrame(urls, columns=[\"image\"]).to_csv(\"data.csv\", index=False)\n\n\npd.read_csv(\"data.csv\")\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n0\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n1\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n2\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n3\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n4\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n...\n...\n\n\n9997\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n9998\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n9999\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n10000\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n10001\nhttps://huggingface.co/datasets/davanstrien/en...\n\n\n\n\n10002 rows √ó 1 columns\n\n\n\n\nLoading annotations into label studio\nWe can use this file to load our data into label studio \nFrom here, we need to define our annotation task. We can then begin annotating data.\n\n\nExport annotations\nYou can either wait until you‚Äôve finished doing all the labels, however, we may have a lot of data to annotate so it‚Äôs likely instead that we will want to export once we‚Äôve either hit a reasonable number of labels or get too bored of annotating. There are various different export formats available in this case we‚Äôll use JSON-Min\n\n\n\nLoad annotations\nNow we have export our annotations lets load them into a new DatafFame. We‚Äôll only select the columns we‚Äôre interested in\n\nannotation_dataframe = pd.read_json(\"project-3-at-2022-09-08-15-16-4279e901.json\")[\n    [\"image\", \"choice\"]\n]\n\n\nannotation_dataframe\n\n\n\n\n\n\n\n\nimage\nchoice\n\n\n\n\n0\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n2\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n3\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n4\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n...\n...\n...\n\n\n1516\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1517\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1518\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1519\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n1520\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n\n\n1521 rows √ó 2 columns\n\n\n\nIf we take a look at the URL for one of the annotations, you‚Äôll see that we still have a nice path that mirrors the folder structure of the original data. This also means we can merge this annotations DataFrame with our previous metadata DataFrame.\n\nannotation_dataframe.loc[0, \"image\"]\n\n'https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/nls-data-encyclopaediaBritannica/192693396/image/192979378.3.jpg'\n\n\n\nannotation_dataframe.loc[0, \"image\"].split(\"/\")[-4:]\n\n['nls-data-encyclopaediaBritannica', '192693396', 'image', '192979378.3.jpg']\n\n\n\nannotation_dataframe[\"filename\"] = annotation_dataframe[\"image\"].apply(\n    lambda x: \"/\".join(x.split(\"/\")[-4:])\n)\n\n\nannotation_dataframe[\"filename\"] = annotation_dataframe[\"filename\"].astype(str)\n\n\ndf = df.merge(annotation_dataframe, on=\"filename\", how=\"outer\")\n\n\ndf\n\n\n\n\n\n\n\n\nfilename\nid\nmeta\nimage\nchoice\n\n\n\n\n0\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n1\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n2\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n3\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n4\nencyclopaedia_britannica/nls-data-encyclopaedi...\n190273291.0\nEncyclopaedia Britannica - Third edition, Volu...\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n11516\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n11517\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n11518\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n11519\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n11520\nnls-data-encyclopaediaBritannica/144133901/ima...\nNaN\nNaN\nhttps://huggingface.co/datasets/davanstrien/en...\ntext-only\n\n\n\n\n11521 rows √ó 5 columns\n\n\n\nThis means we can keep our nice orignal metadata intact but also add our additional metadata where it exists. Let‚Äôs check how many annotations we have\n\ndf.choice.value_counts()\n\ntext-only      1436\nillustrated      70\nName: choice, dtype: int64\n\n\nWe can also see how much of our dataset we have coverage for\n\nlen(df[df.choice.notna()]) / len(df)\n\n0.13071781963371235\n\n\n\n\nHow to use our annotations?\nWe now have some annoations inside a DataFrame. What should we do we these? We can also use the Hub for storing this. This comes with a few benefits: - we keep our data and annotations in the same place. - since the Hub uses Git under the hood we also get versioning for our dataset. We can use this version information to track for example how different models perform during training as we add more labels.\nAnother nice thing about the Hub is that we can create dataset loading scripts to load our data. This script can use this CSV we‚Äôve just created and only load the data we have examples for.\nFirst we‚Äôll save to a CSV file:\n\ndf.to_csv(\"annotations.csv\", index=None)\n\nWe can then copy these into the same repository used to host our dataset.\ncp annotations.csv encyclopedia_britannica/\nOnce we‚Äôve done this we can commit these and push our annotations to the hub:\ncd encyclopedia_britannica/\ngit add annotations.csv\ngit commit -m \"update annotations\"\ngit push"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#what-next",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#what-next",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "What next?",
    "text": "What next?\nWe now have a repository which contains a bunch of images, and a CSV file which contains annotations for some of these images. How do we use this for model training? From this point we can create a dataset loading script inside the same repository.\nThis dataset loading script will allow us to load the data from the hub using the datasets library. Additionally we can write this script so that it only loads data we have annotations for.\nWhat does this mean: - we have a dataset we can use to train our model - the dataset is hosted on the Hugging Face hub which means it‚Äôs easy to share with other people - we can keep adding new annotations to this dataset and pushing our changes to the hub - Since the datasets library has nice caching support it will only download the dataset if there are changes. This change will be triggered by changes to our annotations.csv file."
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#loading-the-dataset",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#loading-the-dataset",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Loading the dataset",
    "text": "Loading the dataset\nOnce we have our loading script we can load our annotations using the datasets library:\n\nfrom datasets import load_dataset\nimport datasets\n\n\nds = load_dataset('davanstrien/encyclopedia_britannica')\n\nUsing custom data configuration default\nReusing dataset encyclopedia_britannica (/Users/dvanstrien/.cache/huggingface/datasets/davanstrien___encyclopedia_britannica/default/1.1.0/8dd4d7982f31fd11ed71020b79b4b11a0068c8243080066e43b9fe3980934467)\n\n\n\n\n\n\nds['train'][0]\n\n{'metadata': 'nan',\n 'image': 'https://huggingface.co/datasets/davanstrien/encyclopaedia_britannica/resolve/main/nls-data-encyclopaediaBritannica/192693396/image/192979378.3.jpg',\n 'label': 0}\n\n\n\nfrom datasets import load_dataset\nfrom datasets.utils.file_utils import get_datasets_user_agent\nfrom functools import partial\nfrom concurrent.futures import ThreadPoolExecutor\nimport urllib\nimport io\nimport PIL\n\nUSER_AGENT = get_datasets_user_agent()\n\n\ndef fetch_single_image(image_url, timeout=None, retries=0):\n    for _ in range(retries + 1):\n        try:\n            request = urllib.request.Request(\n                image_url,\n                data=None,\n                headers={\"user-agent\": USER_AGENT},\n            )\n            with urllib.request.urlopen(request, timeout=timeout) as req:\n                image = PIL.Image.open(io.BytesIO(req.read()))\n            break\n        except Exception:\n            image = None\n    return image\n\n\ndef fetch_images(batch, num_threads, timeout=1, retries=0):\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        batch[\"image\"] = list(executor.map(fetch_single_image_with_args, batch[\"image\"]))\n    return batch\n\n\nnum_threads = 16\nds = ds.map(fetch_images, batched=True, batch_size=64, fn_kwargs={\"num_threads\": num_threads}, writer_batch_size=64)\n\nLoading cached processed dataset at /Users/dvanstrien/.cache/huggingface/datasets/encyclopaedia_britannica/default/1.1.0/f7fb8d1f26daa72fbaf883bb1707e13d304414c1af16f02c00782c985971f87c/cache-fda9502ac5b20332.arrow\n\n\n\nds = ds.cast_column('image', datasets.Image())\n\n\nds['train'][0]['image']"
  },
  {
    "objectID": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#where-wont-this-work",
    "href": "posts/post-with-code/label-studio-hub/2022-09-07-label-studio-annotations-hub.html#where-wont-this-work",
    "title": "Label Studio x Hugging Face datasets hub",
    "section": "Where won‚Äôt this work?",
    "text": "Where won‚Äôt this work?\nThis workflow is based on the assumption that the dataset you are annotating is public from the start. This is usually possible for the domain I work in (libraries) but could be a major blocker for other people. This workflow might also break if you have lots of people annotating. There are probably ways around this but things could start becoming a bit hacky‚Ä¶\nThe loading script for loading this dataset does some slightly strange things to avoid loading images that don‚Äôt yet have annotations. I think it would make sense to rework this script if you get to a point you are unlikely to do any more annotations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Fine-tuning Vision-Language Models for Art History: Iconclass Classification with TRL and HF Jobs\n\n\n\nhuggingface\n\nuv-scripts\n\nvlm\n\nhf Jobs\n\nart-history\n\niconclass\n\n\n\nLearn how to fine-tune open-source VLMs like Qwen2.5-VL for specialized art history tasks using Iconclass metadata. This tutorial shows how to use TRL‚Äôs new VLM support with Hugging Face Jobs for cloud-based training - no local GPU required!\n\n\n\n\n\nSep 4, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs\n\n\n\nhuggingface\n\nuv-scripts\n\nvllm\n\nhf Jobs\n\n\n\nGenerate responses for thousands of dataset prompts using Qwen/Qwen3-30B-A3B-Instruct-2507 across 4 GPUs with automatic prompt filtering and tensor parallelism\n\n\n\n\n\nJul 30, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nWho Benefits? Rethinking Library Data in the Age of AI\n\n\n\nAI\n\nlibraries\n\ndatasets\n\nGLAM\n\ncommunity\n\n\n\nFrom data providers to AI partners: How libraries can collaborate with developers to build tools that improve collections, enhance discovery, and serve communities while contributing to responsible AI development.\n\n\n\n\n\nJun 9, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Inference for ModernBERT Classifiers Using vLLM\n\n\n\nhuggingface\n\ndatasets\n\narxiv\n\nvllm\n\nmodernbert\n\n\n\nModern Inference for modern models. Using vLLM to scale ModernBERT based classifier models to clean and curate datasets\n\n\n\n\n\nApr 24, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nUsing QwQ to generate a reasoning dataset for structured data extraction\n\n\n\nhuggingface\n\ndatasets\n\nsynthetic-data\n\nqwq\n\nreasoning\n\n\n\nLearn how to use QwQ-32B to generate synthetic reasoning datasets for training smaller models on structured data extraction tasks\n\n\n\n\n\nMar 11, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nLog GRPO Completions to ü§ó Datasets\n\n\n\nhuggingface\n\ntrl\n\ndatasets\n\n\n\nLog GRPO training completions from trl to a ü§ó Dataset repo for easy analysis\n\n\n\n\n\nFeb 20, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nDistiling DeepSeek reasoning to ModernBERT classifiers\n\n\n\nhuggingface\n\ndatasets\n\narxiv\n\nsynthetic-data\n\ndeepseek\n\n\n\nHow can we use the reasoning ability of DeepSeek to generate synthetic labels for fine tuning a ModernBERT model?\n\n\n\n\n\nJan 29, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nHygge Data - Cozy Content Filtering for a finer Scandinavian FineWeb\n\n\n\npolars\n\nhuggingface\n\nfineweb\n\ndatasets\n\n\n\nTraining lightweight, disposable web scale data curation models for Scandinavian language texts using the FineWeb-c dataset\n\n\n\n\n\nJan 9, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nHow Fine is FineWeb2?\n\n\n\npolars\n\nhuggingface\n\nfineweb\n\n\n\nAnalysis of the FineWeb-C dataset\n\n\n\n\n\nJan 7, 2025\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering FineWeb2 using Polars\n\n\n\npolars\n\nhuggingface\n\n\n\nUsing Polars to filter the FineWeb2 dataset and other large Hugging Face datasets\n\n\n\n\n\nDec 30, 2024\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nRunning a Local Vision Language Model with LM Studio to sort out my screenshot mess\n\n\nHow I used a VLM to sort out my screenshot mess\n\n\n\n\n\nOct 23, 2024\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ColPali with Qdrant to index and search a UFO document dataset\n\n\nUsing Qdrant multivectors to index and search a UFO document dataset\n\n\n\n\n\nOct 2, 2024\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating a dataset of queries for training and fine-tuning ColPali models on a UFO dataset\n\n\nLearn how to generate custom ColPali dataset using an open VLM for multimodal retrieval model training and fine-tuning.\n\n\n\n\n\nSep 23, 2024\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic dataset generation techniques: generating custom sentence similarity data\n\n\n\ndata\n\nsynthetic-data\n\n\n\nThis post is part of a series on synthetic data generation techniques. You may also want to check out Awesome Synthetic (text) datasets, where I will be collecting these posts.\n\n\n\n\n\nMay 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic dataset generation techniques: Self-Instruct\n\n\n\ndata\n\nsynthetic-data\n\n\n\nThis post is part of a series on synthetic data generation techniques. You may also want to check out Awesome Synthetic (text) datasets, where I will be collecting these posts.\n\n\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTracing Text Generation Inference calls\n\n\nHow to trace text generation inference calls with Langfuse.\n\n\n\n\n\nApr 5, 2024\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Insights from Model Cards Using Open Large Language Models\n\n\n\nHugging Face\n\n\n\nWhat do people talk about in their model cards?\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to load a Hugging Face dataset into Qdrant?\n\n\nLoading a Hugging Face dataset into Qdrant is easy. This post shows how to do it.\n\n\n\n\n\nNov 8, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do groupby for Hugging Face datasets\n\n\nHow can you groupby in Hugging Face datasets?\n\n\n\n\n\nSep 18, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nExploring language metadata for datasets on the Hugging Face Hub\n\n\nUsing the huggingface_hub library to asses metadata on the hub\n\n\n\n\n\nJun 7, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nDynamically updating a Hugging Face hub organization README\n\n\nUsing the huggingface_hub library and Jinja to update a README dynamically\n\n\n\n\n\nMar 7, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Hugging Face AutoTrain to train an image classifier without writing any code.\n\n\n\nautotrain\n\n\n\nHow can we train useful machine learning models without writing code?\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA (very brief) intro to exploring metadata on the Hugging Face Hub\n\n\nHow we can use the huggingface_hub library to explore metadata on the Hugging Face Hub.\n\n\n\n\n\nJan 16, 2023\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nLabel Studio x Hugging Face datasets hub\n\n\nUsing label studio and the Hugging Face datasets hub to iteratively annotate a dataset\n\n\n\n\n\nSep 7, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nTraining an object detection model using Hugging Face\n\n\nTraining a Detr object detection model using Hugging Face transformers and datasets\n\n\n\n\n\nAug 16, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nSearching for machine learning models using semantic search\n\n\nFinding models on the Hugging Face hub using semantic search\n\n\n\n\n\nJul 26, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nCombining Hugging Face datasets with dask\n\n\nUsing ü§ó datasets in combination with dask\n\n\n\n\n\nJun 20, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ü§ó datasets for image search\n\n\nUsing the ü§ó datasets to make an image search engine for British Library Book Illustrations\n\n\n\n\n\nJan 13, 2022\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the ü§ó Hub for model storage\n\n\nHow I‚Äôm planning to use the huggingface hub for storing flyswot models\n\n\n\n\n\nDec 30, 2021\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nflyswot\n\n\n\nglam\n\nflyswot\n\n\n\nAttempting to deploy machine learning in an existing workflow\n\n\n\n\n\nDec 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nImage labeling vs classification models\n\n\nComparing the loss functions of label and classification models\n\n\n\n\n\nOct 12, 2020\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Optimization for Transfer Learning\n\n\nOptimising Hyperparameters using optuna and fastai2\n\n\n\n\n\nJul 1, 2020\n\n\nDaniel van Strien\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-model metadata generation\n\n\nExperiment in combining text and tabular models to generate web archive metadata\n\n\n\n\n\nMay 3, 2020\n\n\nDaniel van Strien\n\n\n\n\n\nNo matching items"
  }
]