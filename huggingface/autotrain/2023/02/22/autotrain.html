<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Using AutoTrain to detect illustrated ads | Daniel van Strien</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Using AutoTrain to detect illustrated ads" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Using Hugging Face AutoTrain to train an image classifer without writing any code." />
<meta property="og:description" content="Using Hugging Face AutoTrain to train an image classifer without writing any code." />
<link rel="canonical" href="https://danielvanstrien.xyz/huggingface/autotrain/2023/02/22/autotrain.html" />
<meta property="og:url" content="https://danielvanstrien.xyz/huggingface/autotrain/2023/02/22/autotrain.html" />
<meta property="og:site_name" content="Daniel van Strien" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-02-22T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://danielvanstrien.xyz/huggingface/autotrain/2023/02/22/autotrain.html","headline":"Using AutoTrain to detect illustrated ads","dateModified":"2023-02-22T00:00:00-06:00","datePublished":"2023-02-22T00:00:00-06:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://danielvanstrien.xyz/huggingface/autotrain/2023/02/22/autotrain.html"},"description":"Using Hugging Face AutoTrain to train an image classifer without writing any code.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://danielvanstrien.xyz/feed.xml" title="Daniel van Strien" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-59247814-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-59247814-2');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Daniel van Strien</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/projects/">Selected projects</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Using AutoTrain to detect illustrated ads</h1><p class="page-description">Using Hugging Face AutoTrain to train an image classifer without writing any code.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-02-22T00:00:00-06:00" itemprop="datePublished">
        Feb 22, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#huggingface">huggingface</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#autotrain">autotrain</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="introduction">Introduction</h2>

<p>There are many potential uses of computer vision in GLAM (Galleries, Libraries, Archives and Museums). These uses include:</p>

<ul>
  <li>image similarity search, i.e., given an image, find similar images</li>
  <li>text search of images, i.e., given a text string “a picture of a dog eating an ice cream,” return relevant images</li>
  <li>page layout recognition, i.e., pull out semantically important parts of a document (articles, photos, titles, etc.)</li>
  <li>Optical Character Recognition (OCR)</li>
</ul>

<p>All of these use cases require some technical work to implement or use. Usually, they need some programming knowledge too. However, there are many tasks in GLAM where computer vision could be helpful to, requiring less technical work to implement. In particular, many uses of computer vision can be framed as an image classification task (putting images into categories).</p>

<p>Last year, Kaspar Beelen, Melvin Wevers, Thomas Smits, Katherine McDonough, and I shared a two-part Programming Historian lesson, <a href="https://programminghistorian.org/en/lessons/computer-vision-deep-learning-pt1">‘Computer</a> Vision for the Humanities: An Introduction to Deep Learning for Image Classification.](https://programminghistorian.org/en/lessons/computer-vision-deep-learning-pt1). This lesson aimed to provide an introduction to how computer vision can be leveraged to work with images ‘at scale’ – in particular for research applications. While we tried hard to make the lesson (particularly part one) accessible, there are still barriers to getting started:</p>

<ul>
  <li>You need some Python knowledge: while we tried to keep the Python code simple (helped massively by the <a href="https://docs.fast.ai">fastai</a> library we use in the lesson), knowing how to code is still required. I couldn’t find a good citation for this, but most estimates for the number of people who know how to program are around 0.5-1% of the global population. Of this percentage, fewer will know Python.</li>
  <li>Need to have access to a GPU: whilst you can train deep learning models (the type of machine learning model introduced in our Programming Historian tutorial), it is a lot slower without them. However, setting up access to a GPU can be annoying. While ‘free’ access is possible, this can also come with constraints.</li>
  <li>The costs involved in training deep learning models can be hard to predict. You can usually get started for free, but often at some point, you need to invest some money in cloud computing. However, it can be difficult to know <em>before</em> you start training a model(s) how much it will cost.</li>
</ul>

<p>Beyond this, there is also a bigger question of how much energy you might want to invest in all of the above stuff involved in getting machine learning set up. This is especially true if you don’t want to become a machine learning engineer and want to do something practical with machine learning.</p>

<h3 id="training-a-machine-learning-model-is-the-boring-part">Training a machine learning model is the boring part</h3>

<p>Many machine learning engineers will grimace at the title of this section. However, many use cases of machine learning exist where an existing machine learning architecture will work well. Training a model is not what would benefit most from human intervention.</p>

<p>For novel applications of machine learning or situations where you want to ensure a model is well suited to your domain, you may need to spend time creating training data. After training your model, there is also a step where you need to decide how to integrate machine learning into existing or new workflows. This is partially a technical question but often involves considerations beyond how I set up an API to serve my model.</p>

<p>Hand-training models can eat up a lot of time. Sometimes this time might be warranted but other times you might wish you could make some of this process less hands-on.</p>

<h2 id="can-we-approach-this-in-another-way">Can we approach this in another way?</h2>

<p><a href="https://huggingface.co/autotrain">AutoTrain</a> is a tool that allow us to train machine learning models without needing to use Python, setup compute infrastructure or deal with unpredictable costs for training our models. In the rest of this blog post we’ll go through the steps to using AutoTrain for a semi-realistic computer vision problem.</p>

<h3 id="the-dataset">The dataset</h3>

<p>For this project we’ll use a dataset created by the <a href="https://archive.org/">Internet Archive</a> as part of a request for help to judge a book by its cover. The <a href="http://blog.archive.org/2019/01/05/helping-us-judge-a-book-by-its-cover-software-help-request/">blog post</a> presents a use case for wanting to know if an image of a book cover is ‘useful’ or ‘not useful’. They provide some examples</p>

<p>Useful image example:</p>

<p><img src="http://blog.archive.org/wp-content/uploads/2019/01/bigbookofknowled0000farn-802x1024.jpg" alt="Useful image" /></p>

<p>Not useful image example:</p>

<p><img src="http://blog.archive.org/wp-content/uploads/2019/01/10sermonspreache00donnuoft-626x1024.jpg" alt="Not useful" /></p>

<p>Essentially the task is to decide whether an image of a digitized book cover is ‘useful’ or ‘not useful,’ i.e. whether showing this cover to Internet Archive users would give them useful information or not. The Internet Archive shared a <a href="https://archive.org/details/year-1923-not-very-useful-covers">dataset</a> along with this blog post which contains examples for each category.</p>

<h4 id="what-type-of-machine-learning-task-is-this">What type of machine learning task is this?</h4>

<p>If we look at the dataset shared by the Internet Archive, we have a directory structure that looks like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">.</span>
├── year-1923-not-very-useful-covers
└── year-1923-useful-covers
</code></pre></div></div>

<p>We have two folders containing images. Each folder contains examples of image belonging to the name of each folder. Essentially, we want a model that learns which image belongs in each folder (based on the examples) and can put new images into the correct folder/category. This is known as an image classification task (as was mentioned in the introduction). The Hugging Face tasks page for this gives an excellent overview: <a href="https://huggingface.co/tasks/image-classification">https://huggingface.co/tasks/image-classification</a></p>

<h3 id="what-are-the-steps-involved">What are the steps involved?</h3>

<p>How do we go from the dataset we started with to a trained model that we can begin to explore? For this particular example, the steps are as follows:</p>

<ul>
  <li>Download our data</li>
  <li>Prepare our data</li>
  <li>choose our autotrain task</li>
  <li>Upload our data to autotrain</li>
  <li>Train our models</li>
  <li>Evaluate our models</li>
</ul>

<h3 id="download-our-data">Download our data</h3>
<p>This step will depend on where your data is and how it’s arranged, but in this example, we can download the dataset from the Internet Archive. Three folders are provided in this case covering useful/not-useful for 1923 and for the year 2000 useful. Since the types of cover will have changed a fair bit in this time period we’ll just download the folders for 1923.</p>

<p><img src="https://raw.githubusercontent.com/davanstrien/blog/master/images/_autotrain/ia_download_files.webp" alt="Screenshot of IA downloads" /></p>

<h3 id="preparing-our-data">Preparing our data</h3>
<p>There isn’t much prep we need to do for our data; however, we can provide data to AutoTrain in a few different ways for our image classification task. In this case we’ll use the imagefolder format. This is essentially what we have already (folders containing examples of the labels we’re interested in). We’ll create a top-level directory for our image data <code class="language-plaintext highlighter-rouge">cover</code>, which contains two subfolders with our example images.</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/folders-screenshot.webp?raw=true" alt="Folder screenshot" /></p>

<h4 id="resize-our-images-optional">Resize our images (optional)</h4>

<p>This step isn’t strictly necessary, but it’ll save time when uploading our dataset to AutoTrain. Most machine learning models expect training images to be relatively small (often 224x224 or 512x512 pixels). You can do this from the command line, but most operating systems have inbuilt tools for bulk resizing images, e.g., <a href="https://www.makeuseof.com/tag/batch-convert-resize-images-mac/">https://www.makeuseof.com/tag/batch-convert-resize-images-mac/</a></p>

<h3 id="setup-autotrain">Setup AutoTrain</h3>

<p>From the <a href="https://ui.autotrain.huggingface.co/projects">projects page</a>, we can create a new project.</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/project-creation.webp?raw=true" alt="" /></p>

<p>Here we give our project a name and choose a task (image classification). We can also specify for AutoTrain to use a particular model. If you don’t have a solid reason to select a model you can leave this decision to AutoTrain 🤗.</p>

<p>Once you’ve created your project, you’ll need to upload your data. There are different ways of doing this depending on the task. For image classification, we can use pre-arranged folders with a CSV/JSONL file with the labels or upload a dataset hosted on the Hugging Face hub.</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/data-upload.webp?raw=true" alt="" /></p>

<p>We already have an organized folder so we can upload data.</p>

<p><img src="https://raw.githubusercontent.com/davanstrien/blog/master/images/_autotrain/data-upload-finder.webp" alt="" /></p>

<p>Once we’ve uploaded our images, we’ll need to wait for the data to be uploaded. How long this takes depends on your internet speed. We can now click on <code class="language-plaintext highlighter-rouge">Go to trainings</code>.</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/congratulations.webp?raw=true" alt="" /></p>

<p>Here you will see that AutoTrain is formatting your uploaded data.</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/data-prep.webp?raw=true" alt="" /></p>

<p>Once your data has been prepared, you can decide how many models you want AutoTrain to train for you. This decision depends on how much you want to spend on training your models and where you are in your project. If you are getting started and want to know how well a model may do, you may choose a lower number. If you want the best possible chance of getting the best-performing model, you could choose to train a more significant number of models.</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/model-training-selection.webp?raw=true" alt="" /></p>

<p>Once you are ready, you can smash the <code class="language-plaintext highlighter-rouge">start model training</code> button!🔥
The nice thing is that AutoTrain will ask you to confirm how much model training will cost. Once your models start training, a screen pops up with some randomly named models. Depending on the size of your dataset, it might take a bit longer to start seeing metrics for your model, but after a little while, you will begin to see scores (in this case, accuracy).</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/model-training-progress.webp?raw=true" alt="" /></p>

<p>As the models train, you will see some models overtake others in performance. If you are easily amused like me, you will treat this like a fun spectator sport.</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/model-training-progress-race.webp?raw=true" alt="" /></p>

<p>You also have a metrics overview tab for all the models you have trained. This makes it easy to sort by different metrics.</p>

<p><img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/metrics-overview.webp?raw=true" alt="" /></p>

<p>Each of these models created by AutoTrain is a ‘real’ model hosted in a model repository on the Hugging Face hub. Some AutoTrain solutions hide away the actual artifacts and only allow you to interact with the models through their API. These models are available in the same way as any other model on the hub. By default, the models are made private, but you can decide to make the models openly available for others to use 🤗.</p>

<p>You’ll also see in the screenshot that the models come with the outlines of a model card.</p>

<p><img src="https://raw.githubusercontent.com/davanstrien/blog/master/images/_autotrain/metrics-overview.webp" alt="" /></p>

<h2 id="why-does-our-model-suck">Why does our model suck?</h2>

<p>For this particular dataset, our models don’t do super well (around 92% accuracy). Why is this?</p>

<h3 id="the-importance-of-training-data">The importance of training data</h3>
<p>Start to dig into the training data examples provided. You’ll see that quite a few images might be reasonably classified as belonging to the other category. In particular, quite a few images of the not-useful folder are similar to those in the useful folder. This is going to make it hard for our model to learn what we’re after.</p>

<p>This also shows the importance of focusing on data and not over-focusing on model training. In this case, fixing our data will likely yield much better results than messing around with how we train the models. Using a tool like AutoTrain can quickly help you spot these issues early on so you can iterate on your training data.</p>

<h3 id="how-can-we-fix-this">How can we fix this??</h3>

<p>Move images between folders!!</p>

<p>There are better ways, but spending 30 mins removing examples you don’t think the fit will make a big difference to the model performance. At some point, you are likely to want to use a proper annotation tool but to start with; you might be able to get quite far by using your operating systems file browser to re-arrange your training data.</p>

<p>Below is an example from another similar dataset where we get models with 99% accuracy. All of this without writing a line of code! 
<img src="https://github.com/davanstrien/blog/blob/master/images/_autotrain/illustrations-model-overview.webp?raw=true" alt="" /></p>


  </div><a class="u-url" href="/huggingface/autotrain/2023/02/22/autotrain.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>digital collections + computational methods</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/davanstrien" target="_blank" title="davanstrien"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/vanstriendaniel" target="_blank" title="vanstriendaniel"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
