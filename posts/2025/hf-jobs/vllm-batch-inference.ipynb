{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95d2743",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Efficient batch inference for LLMs with vLLM + UV Scripts on HF Jobs\"\n",
    "description: \"Generate responses for thousands of dataset prompts using Qwen/Qwen3-30B-A3B-Instruct-2507 across 4 GPUs with automatic prompt filtering and tensor parallelism\"\n",
    "author: \"Daniel van Strien\"\n",
    "categories: [\"huggingface\", \"uv-scripts\", \"vllm\", \"hf Jobs\"]\n",
    "image: https://github.com/davanstrien/blog/blob/a2ff89fbd48d22e0f4a984b03a81a92c07080eea/posts/2025/hf-jobs/assets/is-this-uv-script.jpg?raw=true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4b235",
   "metadata": {},
   "source": [
    "Recently, we launched HF Jobs, a new way to run jobs on the Hugging Face platform. This post will show you how to use it to run large language model inference jobs with vLLM and uv Scripts, processing thousands of prompts with models that don't fit on a single GPU.\n",
    "\n",
    "HF Jobs can be a very powerful tool for running a variety of compute jobs but I think it's particularly useful for running LLMS for batched infernece workloads where it can make a lot of sense to try and bring the data close to the model (to remove the latency of transferring data via an API) and to use trhe powerful auto batching features of vLLM to get the most out of your GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94b4e9",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Large language models like Qwen3-30B-A3B-Instruct (30 billion parameters) exceed single GPU memory limits. Running batch inference on datasets requires:\n",
    "- Multi-GPU setup with tensor parallelism\n",
    "- Handling prompts that exceed context limits\n",
    "- Managing dependencies and environment setup\n",
    "\n",
    "Traditional approaches involve complex Docker setups, manual dependency management, and custom scripts for GPU coordination. HF Jobs changes this.\n",
    "\n",
    "## What is vLLM?\n",
    "\n",
    "vLLM is a very well known and heavily used inference engine. It is known for its ability to scale inference for LLMs. While we can use vLLM via an OpenAI compatible API, it also has a powerful batch inference mode that allows us to process large datasets of prompts efficiently. This \"offline inference\" is particularly useful when we want to generate responses for a large number of prompts without the overhead of API calls.\n",
    "\n",
    "\n",
    "## What are uv Scripts?\n",
    "\n",
    "UV scripts are Python scripts with inline dependency metadata that automatically install and manage their requirements. Instead of separate `requirements.txt` files or complex setup instructions, everything needed to run the script is declared in the script itself:\n",
    "\n",
    "```python\n",
    "  # /// script\n",
    "  # requires-python = \">=3.10\"\n",
    "  # dependencies = [\n",
    "  #     \"vllm\",\n",
    "  #     \"transformers\",\n",
    "  #     \"datasets\",\n",
    "  # ]\n",
    "  # ///\n",
    "\n",
    "  # Your Python code here\n",
    "```\n",
    "When you run uv run script.py, UV automatically creates an isolated environment, installs dependencies, and executes your code. No virtual env setup, no pip install commands, no version conflicts.\n",
    "\n",
    "## The Solution: UV Scripts + vLLM + HF Jobs\n",
    "\n",
    "HF Jobs provides managed GPU infrastructure. This is already very useful but combined with uv Scripts we can more easily distribute scripts for a variety of ML/AI tasks in a (more) reproducible way.\n",
    "\n",
    "![](https://raw.githubusercontent.com/davanstrien/blog/a2ff89fbd48d22e0f4a984b03a81a92c07080eea/posts/2025/hf-jobs/assets/is-this-uv-script.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe352b",
   "metadata": {},
   "source": [
    "## The `uv scripts` Hugging Face org\n",
    "\n",
    "Since I'm so excited about uv Scripts, I created a Hugging Face org to host them: [uv scripts](https://huggingface.co/uv-scripts). This org will contain a variety of uv Scripts that you can use to run jobs on HF Jobs. For this example we'll use a script that allows us to run inference for a model using vLLM. This script exposes a bunch of parameters that allow you to control how the inference is run, including the model to use, the number of GPUs to use, and the batch size etc. \n",
    "\n",
    "In this case, the script expects as input a dataset with a column containing the input prompts (as messages). It will then run inference on the model using vLLM and return the generated responses in a new dataset. \n",
    "\n",
    "I'm personally quite excited to see people sharing more uv scripts for things that are not complex enough to justify a full repository but that are still useful to share and run on the Hugging Face platform!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a2b04",
   "metadata": {},
   "source": [
    "If you are curious, you can check out the script [here](https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py) or below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc2955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# /// script\n",
      "# requires-python = \">=3.10\"\n",
      "# dependencies = [\n",
      "#     \"datasets\",\n",
      "#     \"flashinfer-python\",\n",
      "#     \"huggingface-hub[hf_transfer]\",\n",
      "#     \"torch\",\n",
      "#     \"transformers\",\n",
      "#     \"vllm>=0.8.5\",\n",
      "# ]\n",
      "#\n",
      "# ///\n",
      "\"\"\"\n",
      "Generate responses for prompts in a dataset using vLLM for efficient GPU inference.\n",
      "\n",
      "This script loads a dataset from Hugging Face Hub containing chat-formatted messages,\n",
      "applies the model's chat template, generates responses using vLLM, and saves the\n",
      "results back to the Hub with a comprehensive dataset card.\n",
      "\n",
      "Example usage:\n",
      "    # Local execution with auto GPU detection\n",
      "    uv run generate-responses.py \\\\\n",
      "        username/input-dataset \\\\\n",
      "        username/output-dataset \\\\\n",
      "        --messages-column messages\n",
      "\n",
      "    # With custom model and sampling parameters\n",
      "    uv run generate-responses.py \\\\\n",
      "        username/input-dataset \\\\\n",
      "        username/output-dataset \\\\\n",
      "        --model-id meta-llama/Llama-3.1-8B-Instruct \\\\\n",
      "        --temperature 0.9 \\\\\n",
      "        --top-p 0.95 \\\\\n",
      "        --max-tokens 2048\n",
      "\n",
      "    # HF Jobs execution (see script output for full command)\n",
      "    hf jobs uv run --flavor a100x4 ...\n",
      "\"\"\"\n",
      "\n",
      "import argparse\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "from datetime import datetime\n",
      "from typing import Optional\n",
      "\n",
      "from datasets import load_dataset\n",
      "from huggingface_hub import DatasetCard, get_token, login\n",
      "from torch import cuda\n",
      "from tqdm.auto import tqdm\n",
      "from transformers import AutoTokenizer\n",
      "from vllm import LLM, SamplingParams\n",
      "\n",
      "# Enable HF Transfer for faster downloads\n",
      "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
      "\n",
      "logging.basicConfig(\n",
      "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
      ")\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "def check_gpu_availability() -> int:\n",
      "    \"\"\"Check if CUDA is available and return the number of GPUs.\"\"\"\n",
      "    if not cuda.is_available():\n",
      "        logger.error(\"CUDA is not available. This script requires a GPU.\")\n",
      "        logger.error(\n",
      "            \"Please run on a machine with NVIDIA GPU or use HF Jobs with GPU flavor.\"\n",
      "        )\n",
      "        sys.exit(1)\n",
      "\n",
      "    num_gpus = cuda.device_count()\n",
      "    for i in range(num_gpus):\n",
      "        gpu_name = cuda.get_device_name(i)\n",
      "        gpu_memory = cuda.get_device_properties(i).total_memory / 1024**3\n",
      "        logger.info(f\"GPU {i}: {gpu_name} with {gpu_memory:.1f} GB memory\")\n",
      "\n",
      "    return num_gpus\n",
      "\n",
      "\n",
      "def create_dataset_card(\n",
      "    source_dataset: str,\n",
      "    model_id: str,\n",
      "    messages_column: str,\n",
      "    sampling_params: SamplingParams,\n",
      "    tensor_parallel_size: int,\n",
      "    num_examples: int,\n",
      "    generation_time: str,\n",
      "    num_skipped: int = 0,\n",
      "    max_model_len_used: Optional[int] = None,\n",
      ") -> str:\n",
      "    \"\"\"Create a comprehensive dataset card documenting the generation process.\"\"\"\n",
      "    filtering_section = \"\"\n",
      "    if num_skipped > 0:\n",
      "        skip_percentage = (num_skipped / num_examples) * 100\n",
      "        processed = num_examples - num_skipped\n",
      "        filtering_section = f\"\"\"\n",
      "\n",
      "### Filtering Statistics\n",
      "\n",
      "- **Total Examples**: {num_examples:,}\n",
      "- **Processed**: {processed:,} ({100 - skip_percentage:.1f}%)\n",
      "- **Skipped (too long)**: {num_skipped:,} ({skip_percentage:.1f}%)\n",
      "- **Max Model Length Used**: {max_model_len_used:,} tokens\n",
      "\n",
      "Note: Prompts exceeding the maximum model length were skipped and have empty responses.\"\"\"\n",
      "\n",
      "    return f\"\"\"---\n",
      "tags:\n",
      "- generated\n",
      "- vllm\n",
      "- uv-script\n",
      "---\n",
      "\n",
      "# Generated Responses Dataset\n",
      "\n",
      "This dataset contains generated responses for prompts from [{source_dataset}](https://huggingface.co/datasets/{source_dataset}).\n",
      "\n",
      "## Generation Details\n",
      "\n",
      "- **Source Dataset**: [{source_dataset}](https://huggingface.co/datasets/{source_dataset})\n",
      "- **Messages Column**: `{messages_column}`\n",
      "- **Model**: [{model_id}](https://huggingface.co/{model_id})\n",
      "- **Number of Examples**: {num_examples:,}\n",
      "- **Generation Date**: {generation_time}{filtering_section}\n",
      "\n",
      "### Sampling Parameters\n",
      "\n",
      "- **Temperature**: {sampling_params.temperature}\n",
      "- **Top P**: {sampling_params.top_p}\n",
      "- **Top K**: {sampling_params.top_k}\n",
      "- **Min P**: {sampling_params.min_p}\n",
      "- **Max Tokens**: {sampling_params.max_tokens}\n",
      "- **Repetition Penalty**: {sampling_params.repetition_penalty}\n",
      "\n",
      "### Hardware Configuration\n",
      "\n",
      "- **Tensor Parallel Size**: {tensor_parallel_size}\n",
      "- **GPU Configuration**: {tensor_parallel_size} GPU(s)\n",
      "\n",
      "## Dataset Structure\n",
      "\n",
      "The dataset contains all columns from the source dataset plus:\n",
      "- `response`: The generated response from the model\n",
      "\n",
      "## Generation Script\n",
      "\n",
      "Generated using the vLLM inference script from [uv-scripts/vllm](https://huggingface.co/datasets/uv-scripts/vllm).\n",
      "\n",
      "To reproduce this generation:\n",
      "\n",
      "```bash\n",
      "uv run https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py \\\\\n",
      "    {source_dataset} \\\\\n",
      "    <output-dataset> \\\\\n",
      "    --model-id {model_id} \\\\\n",
      "    --messages-column {messages_column} \\\\\n",
      "    --temperature {sampling_params.temperature} \\\\\n",
      "    --top-p {sampling_params.top_p} \\\\\n",
      "    --top-k {sampling_params.top_k} \\\\\n",
      "    --max-tokens {sampling_params.max_tokens}{f\" \\\\\\\\\\\\n    --max-model-len {max_model_len_used}\" if max_model_len_used else \"\"}\n",
      "```\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def main(\n",
      "    src_dataset_hub_id: str,\n",
      "    output_dataset_hub_id: str,\n",
      "    model_id: str = \"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
      "    messages_column: str = \"messages\",\n",
      "    output_column: str = \"response\",\n",
      "    temperature: float = 0.7,\n",
      "    top_p: float = 0.8,\n",
      "    top_k: int = 20,\n",
      "    min_p: float = 0.0,\n",
      "    max_tokens: int = 16384,\n",
      "    repetition_penalty: float = 1.0,\n",
      "    gpu_memory_utilization: float = 0.90,\n",
      "    max_model_len: Optional[int] = None,\n",
      "    tensor_parallel_size: Optional[int] = None,\n",
      "    skip_long_prompts: bool = True,\n",
      "    hf_token: Optional[str] = None,\n",
      "):\n",
      "    \"\"\"\n",
      "    Main generation pipeline.\n",
      "\n",
      "    Args:\n",
      "        src_dataset_hub_id: Input dataset on Hugging Face Hub\n",
      "        output_dataset_hub_id: Where to save results on Hugging Face Hub\n",
      "        model_id: Hugging Face model ID for generation\n",
      "        messages_column: Column name containing chat messages\n",
      "        output_column: Column name for generated responses\n",
      "        temperature: Sampling temperature\n",
      "        top_p: Top-p sampling parameter\n",
      "        top_k: Top-k sampling parameter\n",
      "        min_p: Minimum probability threshold\n",
      "        max_tokens: Maximum tokens to generate\n",
      "        repetition_penalty: Repetition penalty parameter\n",
      "        gpu_memory_utilization: GPU memory utilization factor\n",
      "        max_model_len: Maximum model context length (None uses model default)\n",
      "        tensor_parallel_size: Number of GPUs to use (auto-detect if None)\n",
      "        skip_long_prompts: Skip prompts exceeding max_model_len instead of failing\n",
      "        hf_token: Hugging Face authentication token\n",
      "    \"\"\"\n",
      "    generation_start_time = datetime.now().isoformat()\n",
      "\n",
      "    # GPU check and configuration\n",
      "    num_gpus = check_gpu_availability()\n",
      "    if tensor_parallel_size is None:\n",
      "        tensor_parallel_size = num_gpus\n",
      "        logger.info(\n",
      "            f\"Auto-detected {num_gpus} GPU(s), using tensor_parallel_size={tensor_parallel_size}\"\n",
      "        )\n",
      "    else:\n",
      "        logger.info(f\"Using specified tensor_parallel_size={tensor_parallel_size}\")\n",
      "        if tensor_parallel_size > num_gpus:\n",
      "            logger.warning(\n",
      "                f\"Requested {tensor_parallel_size} GPUs but only {num_gpus} available\"\n",
      "            )\n",
      "\n",
      "    # Authentication - try multiple methods\n",
      "    HF_TOKEN = hf_token or os.environ.get(\"HF_TOKEN\") or get_token()\n",
      "\n",
      "    if not HF_TOKEN:\n",
      "        logger.error(\"No HuggingFace token found. Please provide token via:\")\n",
      "        logger.error(\"  1. --hf-token argument\")\n",
      "        logger.error(\"  2. HF_TOKEN environment variable\")\n",
      "        logger.error(\"  3. Run 'huggingface-cli login' or use login() in Python\")\n",
      "        sys.exit(1)\n",
      "\n",
      "    logger.info(\"HuggingFace token found, authenticating...\")\n",
      "    login(token=HF_TOKEN)\n",
      "\n",
      "    # Initialize vLLM\n",
      "    logger.info(f\"Loading model: {model_id}\")\n",
      "    vllm_kwargs = {\n",
      "        \"model\": model_id,\n",
      "        \"tensor_parallel_size\": tensor_parallel_size,\n",
      "        \"gpu_memory_utilization\": gpu_memory_utilization,\n",
      "    }\n",
      "    if max_model_len is not None:\n",
      "        vllm_kwargs[\"max_model_len\"] = max_model_len\n",
      "        logger.info(f\"Using max_model_len={max_model_len}\")\n",
      "\n",
      "    llm = LLM(**vllm_kwargs)\n",
      "\n",
      "    # Load tokenizer for chat template\n",
      "    logger.info(\"Loading tokenizer...\")\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "\n",
      "    # Create sampling parameters\n",
      "    sampling_params = SamplingParams(\n",
      "        temperature=temperature,\n",
      "        top_p=top_p,\n",
      "        top_k=top_k,\n",
      "        min_p=min_p,\n",
      "        max_tokens=max_tokens,\n",
      "        repetition_penalty=repetition_penalty,\n",
      "    )\n",
      "\n",
      "    # Load dataset\n",
      "    logger.info(f\"Loading dataset: {src_dataset_hub_id}\")\n",
      "    dataset = load_dataset(src_dataset_hub_id, split=\"train\")\n",
      "    total_examples = len(dataset)\n",
      "    logger.info(f\"Dataset loaded with {total_examples:,} examples\")\n",
      "\n",
      "    # Validate messages column\n",
      "    if messages_column not in dataset.column_names:\n",
      "        logger.error(\n",
      "            f\"Column '{messages_column}' not found. Available columns: {dataset.column_names}\"\n",
      "        )\n",
      "        sys.exit(1)\n",
      "\n",
      "    # Get effective max length for filtering\n",
      "    if max_model_len is not None:\n",
      "        effective_max_len = max_model_len\n",
      "    else:\n",
      "        # Get model's default max length\n",
      "        effective_max_len = llm.llm_engine.model_config.max_model_len\n",
      "    logger.info(f\"Using effective max model length: {effective_max_len}\")\n",
      "\n",
      "    # Process messages and apply chat template\n",
      "    logger.info(\"Applying chat template to messages...\")\n",
      "    all_prompts = []\n",
      "    valid_prompts = []\n",
      "    valid_indices = []\n",
      "    skipped_info = []\n",
      "\n",
      "    for i, example in enumerate(tqdm(dataset, desc=\"Processing messages\")):\n",
      "        messages = example[messages_column]\n",
      "        # Apply chat template\n",
      "        prompt = tokenizer.apply_chat_template(\n",
      "            messages, tokenize=False, add_generation_prompt=True\n",
      "        )\n",
      "        all_prompts.append(prompt)\n",
      "\n",
      "        # Count tokens if filtering is enabled\n",
      "        if skip_long_prompts:\n",
      "            tokens = tokenizer.encode(prompt)\n",
      "            if len(tokens) <= effective_max_len:\n",
      "                valid_prompts.append(prompt)\n",
      "                valid_indices.append(i)\n",
      "            else:\n",
      "                skipped_info.append((i, len(tokens)))\n",
      "        else:\n",
      "            valid_prompts.append(prompt)\n",
      "            valid_indices.append(i)\n",
      "\n",
      "    # Log filtering results\n",
      "    if skip_long_prompts and skipped_info:\n",
      "        logger.warning(\n",
      "            f\"Skipped {len(skipped_info)} prompts that exceed max_model_len ({effective_max_len} tokens)\"\n",
      "        )\n",
      "        logger.info(\"Skipped prompt details (first 10):\")\n",
      "        for idx, (prompt_idx, token_count) in enumerate(skipped_info[:10]):\n",
      "            logger.info(\n",
      "                f\"  - Example {prompt_idx}: {token_count} tokens (exceeds by {token_count - effective_max_len})\"\n",
      "            )\n",
      "        if len(skipped_info) > 10:\n",
      "            logger.info(f\"  ... and {len(skipped_info) - 10} more\")\n",
      "\n",
      "        skip_percentage = (len(skipped_info) / total_examples) * 100\n",
      "        if skip_percentage > 10:\n",
      "            logger.warning(f\"WARNING: {skip_percentage:.1f}% of prompts were skipped!\")\n",
      "\n",
      "    if not valid_prompts:\n",
      "        logger.error(\"No valid prompts to process after filtering!\")\n",
      "        sys.exit(1)\n",
      "\n",
      "    # Generate responses - vLLM handles batching internally\n",
      "    logger.info(f\"Starting generation for {len(valid_prompts):,} valid prompts...\")\n",
      "    logger.info(\"vLLM will handle batching and scheduling automatically\")\n",
      "\n",
      "    outputs = llm.generate(valid_prompts, sampling_params)\n",
      "\n",
      "    # Extract generated text and create full response list\n",
      "    logger.info(\"Extracting generated responses...\")\n",
      "    responses = [\"\"] * total_examples  # Initialize with empty strings\n",
      "\n",
      "    for idx, output in enumerate(outputs):\n",
      "        original_idx = valid_indices[idx]\n",
      "        response = output.outputs[0].text.strip()\n",
      "        responses[original_idx] = response\n",
      "\n",
      "    # Add responses to dataset\n",
      "    logger.info(\"Adding responses to dataset...\")\n",
      "    dataset = dataset.add_column(output_column, responses)\n",
      "\n",
      "    # Create dataset card\n",
      "    logger.info(\"Creating dataset card...\")\n",
      "    card_content = create_dataset_card(\n",
      "        source_dataset=src_dataset_hub_id,\n",
      "        model_id=model_id,\n",
      "        messages_column=messages_column,\n",
      "        sampling_params=sampling_params,\n",
      "        tensor_parallel_size=tensor_parallel_size,\n",
      "        num_examples=total_examples,\n",
      "        generation_time=generation_start_time,\n",
      "        num_skipped=len(skipped_info) if skip_long_prompts else 0,\n",
      "        max_model_len_used=effective_max_len if skip_long_prompts else None,\n",
      "    )\n",
      "\n",
      "    # Push dataset to hub\n",
      "    logger.info(f\"Pushing dataset to: {output_dataset_hub_id}\")\n",
      "    dataset.push_to_hub(output_dataset_hub_id, token=HF_TOKEN)\n",
      "\n",
      "    # Push dataset card\n",
      "    card = DatasetCard(card_content)\n",
      "    card.push_to_hub(output_dataset_hub_id, token=HF_TOKEN)\n",
      "\n",
      "    logger.info(\"✅ Generation complete!\")\n",
      "    logger.info(\n",
      "        f\"Dataset available at: https://huggingface.co/datasets/{output_dataset_hub_id}\"\n",
      "    )\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    if len(sys.argv) > 1:\n",
      "        parser = argparse.ArgumentParser(\n",
      "            description=\"Generate responses for dataset prompts using vLLM\",\n",
      "            formatter_class=argparse.RawDescriptionHelpFormatter,\n",
      "            epilog=\"\"\"\n",
      "Examples:\n",
      "  # Basic usage with default Qwen model\n",
      "  uv run generate-responses.py input-dataset output-dataset\n",
      "  \n",
      "  # With custom model and parameters\n",
      "  uv run generate-responses.py input-dataset output-dataset \\\\\n",
      "    --model-id meta-llama/Llama-3.1-8B-Instruct \\\\\n",
      "    --temperature 0.9 \\\\\n",
      "    --max-tokens 2048\n",
      "  \n",
      "  # Force specific GPU configuration\n",
      "  uv run generate-responses.py input-dataset output-dataset \\\\\n",
      "    --tensor-parallel-size 2 \\\\\n",
      "    --gpu-memory-utilization 0.95\n",
      "  \n",
      "  # Using environment variable for token\n",
      "  HF_TOKEN=hf_xxx uv run generate-responses.py input-dataset output-dataset\n",
      "            \"\"\",\n",
      "        )\n",
      "\n",
      "        parser.add_argument(\n",
      "            \"src_dataset_hub_id\",\n",
      "            help=\"Input dataset on Hugging Face Hub (e.g., username/dataset-name)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"output_dataset_hub_id\", help=\"Output dataset name on Hugging Face Hub\"\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--model-id\",\n",
      "            type=str,\n",
      "            default=\"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
      "            help=\"Model to use for generation (default: Qwen3-30B-A3B-Instruct-2507)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--messages-column\",\n",
      "            type=str,\n",
      "            default=\"messages\",\n",
      "            help=\"Column containing chat messages (default: messages)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--output-column\",\n",
      "            type=str,\n",
      "            default=\"response\",\n",
      "            help=\"Column name for generated responses (default: response)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--temperature\",\n",
      "            type=float,\n",
      "            default=0.7,\n",
      "            help=\"Sampling temperature (default: 0.7)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--top-p\",\n",
      "            type=float,\n",
      "            default=0.8,\n",
      "            help=\"Top-p sampling parameter (default: 0.8)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--top-k\",\n",
      "            type=int,\n",
      "            default=20,\n",
      "            help=\"Top-k sampling parameter (default: 20)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--min-p\",\n",
      "            type=float,\n",
      "            default=0.0,\n",
      "            help=\"Minimum probability threshold (default: 0.0)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--max-tokens\",\n",
      "            type=int,\n",
      "            default=16384,\n",
      "            help=\"Maximum tokens to generate (default: 16384)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--repetition-penalty\",\n",
      "            type=float,\n",
      "            default=1.0,\n",
      "            help=\"Repetition penalty (default: 1.0)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--gpu-memory-utilization\",\n",
      "            type=float,\n",
      "            default=0.90,\n",
      "            help=\"GPU memory utilization factor (default: 0.90)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--max-model-len\",\n",
      "            type=int,\n",
      "            help=\"Maximum model context length (default: model's default)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--tensor-parallel-size\",\n",
      "            type=int,\n",
      "            help=\"Number of GPUs to use (default: auto-detect)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--hf-token\",\n",
      "            type=str,\n",
      "            help=\"Hugging Face token (can also use HF_TOKEN env var)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--skip-long-prompts\",\n",
      "            action=\"store_true\",\n",
      "            default=True,\n",
      "            help=\"Skip prompts that exceed max_model_len instead of failing (default: True)\",\n",
      "        )\n",
      "        parser.add_argument(\n",
      "            \"--no-skip-long-prompts\",\n",
      "            dest=\"skip_long_prompts\",\n",
      "            action=\"store_false\",\n",
      "            help=\"Fail on prompts that exceed max_model_len\",\n",
      "        )\n",
      "\n",
      "        args = parser.parse_args()\n",
      "\n",
      "        main(\n",
      "            src_dataset_hub_id=args.src_dataset_hub_id,\n",
      "            output_dataset_hub_id=args.output_dataset_hub_id,\n",
      "            model_id=args.model_id,\n",
      "            messages_column=args.messages_column,\n",
      "            output_column=args.output_column,\n",
      "            temperature=args.temperature,\n",
      "            top_p=args.top_p,\n",
      "            top_k=args.top_k,\n",
      "            min_p=args.min_p,\n",
      "            max_tokens=args.max_tokens,\n",
      "            repetition_penalty=args.repetition_penalty,\n",
      "            gpu_memory_utilization=args.gpu_memory_utilization,\n",
      "            max_model_len=args.max_model_len,\n",
      "            tensor_parallel_size=args.tensor_parallel_size,\n",
      "            skip_long_prompts=args.skip_long_prompts,\n",
      "            hf_token=args.hf_token,\n",
      "        )\n",
      "    else:\n",
      "        # Show HF Jobs example when run without arguments\n",
      "        print(\"\"\"\n",
      "vLLM Response Generation Script\n",
      "==============================\n",
      "\n",
      "This script requires arguments. For usage information:\n",
      "    uv run generate-responses.py --help\n",
      "\n",
      "Example HF Jobs command with multi-GPU:\n",
      "    # If you're logged in with huggingface-cli, token will be auto-detected\n",
      "    hf jobs uv run \\\\\n",
      "        --flavor l4x4 \\\\\n",
      "        https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py \\\\\n",
      "        username/input-dataset \\\\\n",
      "        username/output-dataset \\\\\n",
      "        --messages-column messages \\\\\n",
      "        --model-id Qwen/Qwen3-30B-A3B-Instruct-2507 \\\\\n",
      "        --temperature 0.7 \\\\\n",
      "        --max-tokens 16384\n",
      "        \"\"\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | code-fold: true\n",
    "import requests\n",
    "\n",
    "print(requests.get(\"https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f15265",
   "metadata": {},
   "source": [
    "## An example: Running Qwen3-30B-A3B-Instruct to generare summaries of datasets from 2025 \n",
    "\n",
    "As an example, let's run Qwen3-30B-A3B-Instruct to generate summaries of datasets from 2025. We'll use the `hf-jobs` Python API to create a job that runs a uv Script on 4 GPUs with vLLM. First we'll quickly prepare the dataset and prompts. We'll use Polars + datasets to load the dataset and filter it down to the 2025 datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76885ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8260027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083dc6f91ca8406bb3c123f5f55122cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee33afa054404029b3464b709aac92e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/139M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/davanstrien/Documents/daniel/blog/posts/2025/hf-jobs/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\n",
    "    repo_id=\"librarian-bots/dataset_cards_with_metadata\",\n",
    "    local_dir=\"data\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[\"*.parquet\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533abe9",
   "metadata": {},
   "source": [
    "We'll do some filtering to focus on datasets where the cards are not super long or super short. We'll also filter to focus on datasets with at least one like and ten downloads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f163aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ab4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_parquet(\"data/**/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da668a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('datasetId', String),\n",
       "        ('author', String),\n",
       "        ('last_modified', String),\n",
       "        ('downloads', Int64),\n",
       "        ('likes', Int64),\n",
       "        ('tags', List(String)),\n",
       "        ('task_categories', List(String)),\n",
       "        ('createdAt', String),\n",
       "        ('trending_score', Float64),\n",
       "        ('card', String)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6f4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(pl.col(\"card\").str.len_chars() > 200)\n",
    "df = df.filter(pl.col(\"downloads\") > 2)\n",
    "df = df.filter(pl.col(\"likes\") > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd8ed41",
   "metadata": {},
   "source": [
    "We make sure we have datetime for the createdAt column so we can filter by year. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5804547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col(\"createdAt\").str.to_datetime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8626f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "this_year = datetime.now().year\n",
    "this_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b11bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025 = df.filter(pl.col(\"createdAt\").dt.year() == this_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82dcab3",
   "metadata": {},
   "source": [
    "Since we're using the LazyFrame API, we can use the `collect` method to execute the query and get the results we get a nice optimized query plan. This is very nice since you can be quite lazy in how you filter and transform the data and Polars will optimize the query for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab2a1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"590pt\" height=\"210pt\" viewBox=\"0.00 0.00 590.00 210.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 205.5)\">\n",
       "<title>polars_query</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-205.5 585.5,-205.5 585.5,4 -4,4\"/>\n",
       "<!-- p1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>p1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"443.5,-201.5 138,-201.5 138,-165.5 443.5,-165.5 443.5,-201.5\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"290.75\" y=\"-178.45\" font-family=\"Times,serif\" font-size=\"14.00\">FILTER BY [(col(&quot;createdAt&quot;).dt.year()) == (2025)]</text>\n",
       "</g>\n",
       "<!-- p2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>p2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"463.75,-129.5 117.75,-129.5 117.75,-93.5 463.75,-93.5 463.75,-129.5\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"290.75\" y=\"-106.45\" font-family=\"Times,serif\" font-size=\"14.00\">WITH COLUMNS [col(&quot;createdAt&quot;).str.strptime([&quot;raise&quot;])]</text>\n",
       "</g>\n",
       "<!-- p1&#45;&#45;p2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>p1--p2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.75,-165.2C290.75,-154.35 290.75,-140.42 290.75,-129.6\"/>\n",
       "</g>\n",
       "<!-- p3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>p3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"581.5,-57.5 0,-57.5 0,0 581.5,0 581.5,-57.5\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"290.75\" y=\"-40.2\" font-family=\"Times,serif\" font-size=\"14.00\">Parquet SCAN [data/data/train-00000-of-00001.parquet]</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"290.75\" y=\"-23.7\" font-family=\"Times,serif\" font-size=\"14.00\">π */10;</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"290.75\" y=\"-7.2\" font-family=\"Times,serif\" font-size=\"14.00\">σ [([([(col(&quot;downloads&quot;)) &gt; (2)]) &amp; ([(col(&quot;likes&quot;)) &gt; (1)])]) &amp; ([(col(&quot;card&quot;).str.len_chars()) &gt; (200)])]</text>\n",
       "</g>\n",
       "<!-- p2&#45;&#45;p3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>p2--p3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.75,-93.38C290.75,-83.1 290.75,-69.77 290.75,-57.97\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_2025.show_graph(optimized=True, engine=\"streaming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080661d",
   "metadata": {},
   "source": [
    "Polars and datasets play nicely together so we can easily convert between the two. Since we've done all the filtering we want, we can convert the Polars DataFrame to a Datasets Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e91e55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_polars(df_2025.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12264565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['datasetId', 'author', 'last_modified', 'downloads', 'likes', 'tags', 'task_categories', 'createdAt', 'trending_score', 'card'],\n",
       "    num_rows: 2419\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42583729",
   "metadata": {},
   "source": [
    "We'll do one more filter to remove datasets that don't have a card. We could also do this in Polars but since the `huggingface_hub` library has a nice way of converting a string into a dataset card where we can seperate the YAML from the main content, we'll do it using the `datasets` library and the `filter` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f6406ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import DatasetCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040bddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_short_card(row, min_length=200):\n",
    "    card = DatasetCard(row['card']).text\n",
    "    return len(card) > min_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9da564b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d50e11f31a4a4d9e78a80dbdf9b2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.filter(is_short_card, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2463ad5",
   "metadata": {},
   "source": [
    "### Preparing the prompts\n",
    "\n",
    "Since the uv + vLLM script expecets as input a list of prompts, we'll convert the dataset to a list of prompts. We'll use the `map` function to create a list of prompts that we can use for inference. We'll use the `card` field of the dataset to create a prompt that asks the model to summarize the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dcf6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_for_card(row, max_length=8000):\n",
    "    card = DatasetCard(row['card']).text\n",
    "    datasetId = row['datasetId']\n",
    "    return f\"\"\"You are a helpful assistant that provides concise summaries of dataset cards for datasets on the Hugging Face Hub.\n",
    "The Hub ID of the dataset is: {datasetId}.\n",
    "The dataset card is as follows:\n",
    "{card[:max_length]}]\n",
    "Please write a one to two sentence summary of the dataset card.\n",
    "The summary should be concise and informative, capturing the essence of the dataset.\n",
    "The summary should be in English.\n",
    "The goal of the summary is to provide a quick overview of the dataset's content and purpose. \n",
    "This summary will be used to help users quickly understand the dataset and as input for creating embeddings for the dataset card.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25caba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that provides concise summaries of dataset cards for datasets on the Hugging Face Hub.\n",
      "The Hub ID of the dataset is: agentlans/high-quality-multilingual-sentences.\n",
      "The dataset card is as follows:\n",
      "# High Quality Multilingual Sentences\n",
      "\n",
      "- This dataset contains multilingual sentences derived from the [agentlans/LinguaNova](https://huggingface.co/datasets/agentlans/LinguaNova) dataset.\n",
      "- It includes 1.58 million rows across 51 different languages, each in its own configuration.\n",
      "\n",
      "Example row (from the `all` config):\n",
      "```json\n",
      "{\n",
      "    \"text\": \"امام جمعه اصفهان گفت: میزان نیاز آب شرب اصفهان ۱۱.۵ متر مکعب است که تمام استان اصفهان را پوشش میدهد و نسبت به قبل از انقلاب یکی از پیشرفتها در حوزه آب بوده است.\",\n",
      "    \"fasttext\": \"fa\",\n",
      "    \"gcld3\": \"fa\"\n",
      "}\n",
      "```\n",
      "\n",
      "Fields:\n",
      "- **text**: The sentence in the original language.\n",
      "- **fasttext**, **gcld3**: Language codes determined using fastText and gcld3 Python packages.\n",
      "\n",
      "## Configurations\n",
      "\n",
      "Each individual language is available as a separate configuration, such as `ar`, `en`. These configurations contain only sentences identified to be of that specific language by both the fastText and gcld3 models.\n",
      "\n",
      "Example row (from a language-specific config):\n",
      "```json\n",
      "{\n",
      "    \"text\": \"Ne vienas asmuo yra apsaugotas nuo parazitų atsiradimo organizme.\"\n",
      "}\n",
      "```\n",
      "\n",
      "## Methods\n",
      "\n",
      "### Data Loading and Processing\n",
      "\n",
      "The `all` split was downloaded from the [agentlans/LinguaNova](https://huggingface.co/datasets/agentlans/LinguaNova) dataset.\n",
      "1. **Text Cleaning**: Raw text was cleaned by removing HTML tags, emails, emojis, hashtags, user handles, and URLs. Unicode characters and whitespace were normalized, and hyphenated words were handled to ensure consistency.\n",
      "2. **Sentence Segmentation**: Text was segmented into individual sentences using ICU's `BreakIterator` class, which efficiently processed different languages and punctuation.\n",
      "3. **Deduplication**: Duplicate entries were removed to maintain uniqueness and prevent redundancy in the dataset.\n",
      "\n",
      "### Language Detection\n",
      "\n",
      "Two methods were used for language identification:\n",
      "1. **gcld3**: Google's Compact Language Detector 3 was used for fast and accurate language identification.\n",
      "2. **fastText**: Facebook’s fastText model was employed, which improved accuracy by considering subword information.\n",
      "\n",
      "### Quality Assessment\n",
      "\n",
      "Text quality was assessed through batch inference using the [agentlans/multilingual-e5-small-aligned-quality](https://huggingface.co/agentlans/multilingual-e5-small-aligned-quality) model.\n",
      "1. **Data Retrieval**: Entries with a quality score of 1 or higher and a minimum input length of 20 characters were retained.\n",
      "2. **Text Refinement**: Leading punctuation and spaces were removed, and balanced quotation marks were validated using regular expressions.\n",
      "\n",
      "### Dataset Configs\n",
      "\n",
      "The filtered sentences and their annotated languages were written to the `all.jsonl` file. The file was then split into language-specific JSONL files, containing only those sentences that matched consistently with both gcld3 and fasttext in terms of language identification. Only languages with at least 100 sentences after filtering were included in these configs.\n",
      "\n",
      "## Usage\n",
      "\n",
      "### Loading the Dataset\n",
      "```python\n",
      "from datasets import load_dataset\n",
      "\n",
      "dataset = load_dataset('agentlans/high-quality-multilingual-sentences', 'all')\n",
      "```\n",
      "\n",
      "For language-specific configurations:\n",
      "```python\n",
      "language_config = load_dataset('agentlans/high-quality-multilingual-sentences', 'en')  # Replace with desired language code.\n",
      "```\n",
      "\n",
      "### Example Usage in Python\n",
      "```python\n",
      "from datasets import load_dataset\n",
      "\n",
      "# Load the dataset for all languages or a specific one\n",
      "dataset_all = load_dataset(\"agentlans/high-quality-multilingual-sentences\", \"all\")\n",
      "print(dataset_all[\"train\"][0])\n",
      "\n",
      "language_config = load_dataset(\"agentlans/high-quality-multilingual-sentences\", \"en\")  # Replace 'en' with desired language code.\n",
      "print(language_config[\"train\"][:5])\n",
      "```\n",
      "\n",
      "## Limitations\n",
      "\n",
      "- **Multilingual content bias**: The quality classifier is biased towards educational and more formal content.\n",
      "- **Language coverage**: Limited to the 50 written languages from LinguaNova. There's a lack of African and indigenous languages.\n",
      "- **Short input issues**: Language identification accuracy can suffer when working with short inputs like single sentences.\n",
      "- **Sentence segmentation challenges**: Some languages' delimiters might not be handled correctly.\n",
      "- **Redundancy**: The filtering was only done on exact matches so some sentences may be similar (but not identical).\n",
      "\n",
      "Additionally:\n",
      "- **Thai data imbalance**: Fewer examples are available for `th` (Thai) than expected. Could be a sentence segmentation problem.\n",
      "- **Malay and Indonesian**: There are few examples for the `ms` (Malay) subset. Consider also using the `id` (Indonesian) subset when training models.\n",
      "- **Chinese written forms**: This dataset does not distinguish between different Chinese character variations.\n",
      "\n",
      "## Licence\n",
      "\n",
      "This dataset is released under a [Creative Commons Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/) licence, allowing for free use and distribution as long as proper attribution is given to the original source.]\n",
      "Please write a one to two sentence summary of the dataset card.\n",
      "The summary should be concise and informative, capturing the essence of the dataset.\n",
      "The summary should be in English.\n",
      "The goal of the summary is to provide a quick overview of the dataset's content and purpose. \n",
      "This summary will be used to help users quickly understand the dataset and as input for creating embeddings for the dataset card.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(format_prompt_for_card(ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "573099ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(row):\n",
    "    return {\"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": format_prompt_for_card(row),\n",
    "        },\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcfa0186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef8632b6625452c8ffb6b07c6d051f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2082 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(create_messages, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be4e760d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['datasetId', 'author', 'last_modified', 'downloads', 'likes', 'tags', 'task_categories', 'createdAt', 'trending_score', 'card', 'messages'],\n",
       "    num_rows: 2082\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8adacd",
   "metadata": {},
   "source": [
    "We remove columns we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe86956",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns([c for c in ds.column_names if c not in ['messages', 'datasetId']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad88490",
   "metadata": {},
   "source": [
    "And push to the Hub!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87636e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef08ac3391e46d3a6a7f0584d7290de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13063c219cfa4d5ab79f8646d39eb1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e567916bdef47128032038d3e10151f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f017f624e4174a0391a6c82796658f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdff298798d046ed890d35a44fe8f12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  13%|#2        |  526kB / 4.13MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9f8daf19f54a749f2cb3de8265614b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/davanstrien/cards_with_prompts/commit/8e32c041eba4fbf1729e3f5a4d1536365185f7d2', commit_message='Upload dataset', commit_description='', oid='8e32c041eba4fbf1729e3f5a4d1536365185f7d2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/davanstrien/cards_with_prompts', endpoint='https://huggingface.co', repo_type='dataset', repo_id='davanstrien/cards_with_prompts'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"davanstrien/cards_with_prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef4e7a",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "Hugging Face recently moved most of the backend storage to `Xet`. The tl;dr of this is that it means that datasets are deduplicated at a much more granular level, this makes working with datasets which change regularly much more efficient. See [](https://huggingface.co/docs/hub/en/storage-backends#xet) for more details. This combined with Jobs could make for a very powerful combination for running jobs on datasets that change frequently.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8d2b1",
   "metadata": {},
   "source": [
    "## Launching our job\n",
    "\n",
    "We now have the dataset with the prompts we want to use for inference. \n",
    "\n",
    "The interface for Jobs should look familiar if you've used Docker before. We can use Jobs via CLI or Python API. Via the CLI a basic command to run a job looks like this:\n",
    "\n",
    "```bash\n",
    "hf jobs run python:3.12 python -c \"print('Hello from the cloud!')\"\n",
    "```\n",
    "\n",
    "There is also an experimental uv command thaty allows us to run uv scripts directly:\n",
    "\n",
    "```bash\n",
    "hf jobs uv run script-url\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c33f03",
   "metadata": {},
   "source": [
    "As an example, we can run another simple script from the uv scripts org and just print the help for the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4259b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davanstrien/Library/Application Support/uv/tools/huggingface-hub/lib/python3.13/site-packages/huggingface_hub/utils/_experimental.py:60: UserWarning: 'HfApi.run_uv_job' is experimental and might be subject to breaking changes in the future without prior notice. You can disable this warning by setting `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` as environment variable.\n",
      "  warnings.warn(\n",
      "Job started with ID: 688a31096dcd97e42f8095e7\n",
      "View at: https://huggingface.co/jobs/davanstrien/688a31096dcd97e42f8095e7\n",
      "Downloading pygments (1.2MiB)\n",
      "Downloading hf-xet (3.0MiB)\n",
      "Downloading numpy (15.9MiB)\n",
      "Downloading tokenizers (3.0MiB)\n",
      "Downloading setuptools (1.1MiB)\n",
      "Downloading aiohttp (1.6MiB)\n",
      "Downloading pandas (11.4MiB)\n",
      "Downloading pyarrow (40.8MiB)\n",
      "Downloading usearch (2.0MiB)\n",
      "Downloading hf-transfer (3.4MiB)\n",
      "Downloading simsimd (1.0MiB)\n",
      " Downloading simsimd\n",
      " Downloading usearch\n",
      " Downloading tokenizers\n",
      " Downloading hf-xet\n",
      " Downloading hf-transfer\n",
      " Downloading aiohttp\n",
      " Downloading pygments\n",
      " Downloading setuptools\n",
      " Downloading numpy\n",
      " Downloading pyarrow\n",
      " Downloading pandas\n",
      "Installed 50 packages in 116ms\n",
      "usage: semantic-dedupekDOpug.py [-h] [--split SPLIT]\n",
      "                                [--method {duplicates,outliers,representatives}]\n",
      "                                [--threshold THRESHOLD]\n",
      "                                [--batch-size BATCH_SIZE]\n",
      "                                [--max-samples MAX_SAMPLES] [--private]\n",
      "                                [--hf-token HF_TOKEN]\n",
      "                                dataset column output_repo\n",
      "\n",
      "Deduplicate a dataset using semantic similarity\n",
      "\n",
      "positional arguments:\n",
      "  dataset               Input dataset ID (e.g., 'imdb' or 'username/dataset')\n",
      "  column                Text column to deduplicate on\n",
      "  output_repo           Output dataset repository name\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --split SPLIT         Dataset split to process (default: train)\n",
      "  --method {duplicates,outliers,representatives}\n",
      "                        Deduplication method (default: duplicates)\n",
      "  --threshold THRESHOLD\n",
      "                        Similarity threshold for duplicates (default: 0.9)\n",
      "  --batch-size BATCH_SIZE\n",
      "                        Batch size for processing (default: 64)\n",
      "  --max-samples MAX_SAMPLES\n",
      "                        Maximum number of samples to process (for testing)\n",
      "  --private             Create private dataset repository\n",
      "  --hf-token HF_TOKEN   Hugging Face API token (defaults to HF_TOKEN env var)\n",
      "\n",
      "Examples:\n",
      "  # Basic usage\n",
      "  uv run semantic-dedupe.py imdb text imdb-deduped\n",
      "\n",
      "  # With options\n",
      "  uv run semantic-dedupe.py squad question squad-deduped --threshold 0.85 --method duplicates\n",
      "\n",
      "  # Test with small sample\n",
      "  uv run semantic-dedupe.py large-dataset text test-dedup --max-samples 100\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "hf jobs uv run https://huggingface.co/datasets/uv-scripts/deduplication/raw/main/semantic-dedupe.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02324659",
   "metadata": {},
   "source": [
    "You'll see that uv takes care of installing the dependencies and running the script. This is very convenient since we don't have to worry about setting up a virtual environment or installing dependencies manually. This can also be very nice if you want to share a script with others and want to help them avoid getting stuck in dependency hell.\n",
    "\n",
    "We can also run hf jobs via the Python API. This is very convenient if you want to run jobs programmatically or if you want to integrate Jobs into your existing Python code (i.e. to run one step that requires a GPU and another step that doesn't)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec526c6",
   "metadata": {},
   "source": [
    "## Running our inference job via huggingface_hub library\n",
    "\n",
    "We can use the `huggingface_hub` library to run our inference job using `run_uv_job`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360eb5ef",
   "metadata": {},
   "source": [
    "We'll grab a token to pass to our job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fd551e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, get_token\n",
    "\n",
    "\n",
    "HF_TOKEN = get_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba51054",
   "metadata": {},
   "source": [
    "We'll create an instance of the `HfApi` class and use the `run_uv_job` method to run our job. We'll pass the URL of the script we want to run, the dataset we want to use, and the parameters we want to use for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e209c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbeb958",
   "metadata": {},
   "source": [
    "Let's see what the `run_uv_job` method looks like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaaf5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "api.run_uv_job(\n",
      "    script: \u001b[33m'str'\u001b[39m,\n",
      "    *,\n",
      "    script_args: \u001b[33m'Optional[List[str]]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    dependencies: \u001b[33m'Optional[List[str]]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    python: \u001b[33m'Optional[str]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    image: \u001b[33m'Optional[str]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    env: \u001b[33m'Optional[Dict[str, Any]]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    secrets: \u001b[33m'Optional[Dict[str, Any]]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    flavor: \u001b[33m'Optional[SpaceHardware]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    timeout: \u001b[33m'Optional[Union[int, float, str]]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    namespace: \u001b[33m'Optional[str]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    token: \u001b[33m'Union[bool, str, None]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    _repo: \u001b[33m'Optional[str]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> \u001b[33m'JobInfo'\u001b[39m\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Run a UV script Job on Hugging Face infrastructure.\n",
      "\n",
      "Args:\n",
      "    script (`str`):\n",
      "        Path or URL of the UV script.\n",
      "\n",
      "    script_args (`List[str]`, *optional*)\n",
      "        Arguments to pass to the script.\n",
      "\n",
      "    dependencies (`List[str]`, *optional*)\n",
      "        Dependencies to use to run the UV script.\n",
      "\n",
      "    python (`str`, *optional*)\n",
      "        Use a specific Python version. Default is 3.12.\n",
      "\n",
      "    image (`str`, *optional*, defaults to \"ghcr.io/astral-sh/uv:python3.12-bookworm\"):\n",
      "        Use a custom Docker image with `uv` installed.\n",
      "\n",
      "    env (`Dict[str, Any]`, *optional*):\n",
      "        Defines the environment variables for the Job.\n",
      "\n",
      "    secrets (`Dict[str, Any]`, *optional*):\n",
      "        Defines the secret environment variables for the Job.\n",
      "\n",
      "    flavor (`str`, *optional*):\n",
      "        Flavor for the hardware, as in Hugging Face Spaces. See [`SpaceHardware`] for possible values.\n",
      "        Defaults to `\"cpu-basic\"`.\n",
      "\n",
      "    timeout (`Union[int, float, str]`, *optional*):\n",
      "        Max duration for the Job: int/float with s (seconds, default), m (minutes), h (hours) or d (days).\n",
      "        Example: `300` or `\"5m\"` for 5 minutes.\n",
      "\n",
      "    namespace (`str`, *optional*):\n",
      "        The namespace where the Job will be created. Defaults to the current user's namespace.\n",
      "\n",
      "    token `(Union[bool, str, None]`, *optional*):\n",
      "        A valid user access token. If not provided, the locally saved token will be used, which is the\n",
      "        recommended authentication method. Set to `False` to disable authentication.\n",
      "        Refer to: https://huggingface.co/docs/huggingface_hub/quick-start#authentication.\n",
      "\n",
      "Example:\n",
      "\n",
      "    ```python\n",
      "    >>> from huggingface_hub import run_uv_job\n",
      "    >>> script = \"https://raw.githubusercontent.com/huggingface/trl/refs/heads/main/trl/scripts/sft.py\"\n",
      "    >>> run_uv_job(script, dependencies=[\"trl\"], flavor=\"a10g-small\")\n",
      "    ```\n",
      "\u001b[31mFile:\u001b[39m      ~/Documents/daniel/blog/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "?api.run_uv_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2ee4f",
   "metadata": {},
   "source": [
    "we can use the `run_uv_job` method to run our job. We'll pass the URL of the script we want to run, the dataset we want to use, and the parameters we want to use for the job. These parameters will be passed to the script as command line arguments. Since we're using vLLM, we'll pass the vllm Docker image. This will mean that our job is run in this Docker container. \n",
    "\n",
    "::: {.callout-note}\n",
    "This Docker image already has uv installed but if you want to use an image + uv for an image without uv insalled you'll need to make sure uv is installed first. You can also not specify any image and hf jobs will use the default UV image which has uv installed. This will work well in many cases but for LLM inference libraries which can have quite specific requirements, it can be useful to use a specific image that has the library installed.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0fcbb3",
   "metadata": {},
   "source": [
    "We can now run our job using the `run_uv_job` method. This will start the job and return a job object that we can use to monitor the job's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be8e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davanstrien/Documents/daniel/blog/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_experimental.py:60: UserWarning: 'HfApi.run_uv_job' is experimental and might be subject to breaking changes in the future without prior notice. You can disable this warning by setting `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` as environment variable.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "job = api.run_uv_job(\n",
    "    script=\"https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py\",\n",
    "    script_args=[\n",
    "        \"davanstrien/cards_with_prompts\",  # Dataset with prompts\n",
    "        \"davanstrien/test-generated-responses\",  # Where to store the generated responses\n",
    "        \"--model-id\",  # Model to use for inference\n",
    "        \"Qwen/Qwen3-30B-A3B-Instruct-2507\",  # Model to use for inference\n",
    "        \"--gpu-memory-utilization\",  # GPU memory utilization\n",
    "        \"0.9\",\n",
    "        \"--max-tokens\",  # Maximum number of tokens\n",
    "        \"900\",\n",
    "        \"--max-model-len\",  # Maximum model length\n",
    "        \"8000\",\n",
    "    ],\n",
    "    flavor=\"l4x4\",  # What hardware to use\n",
    "    image=\"vllm/vllm-openai:latest\",  # Docker image to use\n",
    "    secrets={\"HF_TOKEN\": HF_TOKEN},  # Pass as secret``\n",
    "    env={\"UV_PRERELEASE\": \"if-necessary\"},  # Pass as env var\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ab096",
   "metadata": {},
   "source": [
    "We can get a url for our job, this will give us a page where we can monitor the job's progress and see the logs (**note** this won't URL won't work for you unless you run this job yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e1c0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job URL: https://huggingface.co/jobs/davanstrien/688a33391c97bc486de2a232\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job URL: {job.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7e187",
   "metadata": {},
   "source": [
    "![](https://github.com/davanstrien/blog/blob/1338b6009e211353489fcc50267a4d9e5d4632a9/posts/2025/hf-jobs/assets/jobs-dashboard.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7168907",
   "metadata": {},
   "source": [
    "We can also print the status of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4799e136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JobStatus(stage='RUNNING', message=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85534613",
   "metadata": {},
   "source": [
    "There are also a bunch of other attributes from the job that can be useful when running jobs as part of a larger workflow. For example, we can get the job's creation time, the job's status etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac102e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 7, 30, 14, 59, 5, 648000, tzinfo=datetime.timezone.utc)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b75ccee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l4x4'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9e4ca1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uv',\n",
       " 'run',\n",
       " 'https://huggingface.co/datasets/uv-scripts/vllm/raw/main/generate-responses.py',\n",
       " 'davanstrien/cards_with_prompts',\n",
       " 'davanstrien/test-generated-responses',\n",
       " '--model-id',\n",
       " 'Qwen/Qwen3-30B-A3B-Instruct-2507',\n",
       " '--gpu-memory-utilization',\n",
       " '0.9',\n",
       " '--max-tokens',\n",
       " '900',\n",
       " '--max-model-len',\n",
       " '8000']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1869499",
   "metadata": {},
   "source": [
    "We can also grab the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eecfea48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object HfApi.fetch_job_logs at 0x1612df4c0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.fetch_job_logs(\n",
    "    job_id=job.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15fcdd",
   "metadata": {},
   "source": [
    "This returns a generator, let's turn it into a list so we can print out a few examples of the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9464687",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    list(\n",
    "        api.fetch_job_logs(\n",
    "            job_id=job.id,\n",
    "        )\n",
    "    )[:-10]\n",
    ")  # Print the last 10 lines of logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b26128",
   "metadata": {},
   "source": [
    "We can also see the resulting dataset for the job [here](https://huggingface.co/datasets/davanstrien/test-generated-responses) or below. You can see we have the original prompts and the generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38001a5d",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://huggingface.co/datasets/davanstrien/test-generated-responses/embed/viewer/default/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"560px\"\n",
    "></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
