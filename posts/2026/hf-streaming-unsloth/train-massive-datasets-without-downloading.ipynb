{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqNx3876CUOV"
   },
   "source": [
    "---\n",
    "title: \"Train on Massive Datasets Without Downloading with Hugging Face Streaming and Unsloth\"\n",
    "description: \"Stream datasets directly from Hugging Face Hub without downloading. Combine streaming with Unsloth for disk-free LLM training on Colab, Kaggle, or HF Jobs.\"\n",
    "author: \"Daniel van Strien\"\n",
    "categories: [\"huggingface\", \"unsloth\", \"streaming-datasets\", \"fineweb\"]\n",
    "date: 2026-01-07\n",
    "image: https://raw.githubusercontent.com/huggingface/blog/refs/heads/main/assets/hf_unsloth/thumbnail.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zei4QQ1pCUOW"
   },
   "source": [
    "## GPU poor and disk poor?\n",
    "\n",
    "[Unsloth](https://huggingface.co/unsloth) has massively lowered the barriers to training and fine-tuning models by reducing the GPU resources required.\n",
    "\n",
    "However, many AI datasets are very large ‚Äî often multiple TBs.\n",
    "What if you want to train on a dataset larger than your disk?\n",
    "\n",
    "Using ü§ó datasets + streaming means you can train directly from a Hugging Face hosted dataset without needing to download and store the whole dataset locally. This means even GPU and disk poor people can do things like continued pretraining!\n",
    "\n",
    "### Can we make Qwen speak Latin?\n",
    "\n",
    "FineWeb-2 has **1.47 million Latin texts** (~1.7GB). Let's use them to try to teach a small LLM some Latin - no disk space required.\n",
    "\n",
    "This is perfect for \"GPU poor AND disk poor\" setups - Colab, Kaggle, or any constrained environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Ka31pZ_KCUOW"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXRYWNwWCUOX",
    "outputId": "5fc66b31-02e8-4142-a993-7c477e3ea853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GS_z0BMCUOX"
   },
   "source": [
    "## Vibe Check: Before Training\n",
    "\n",
    "Let's see what this base model generates from a Latin prompt *before* any Latin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsf7-sv3CUOX",
    "outputId": "2e6ea6b6-f020-4ba6-9c9d-b64eca9630f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: Lingua Latina est√©mao\n",
      "\n",
      "# 04555 - P√≥rticos, conyugues y mujeres\n",
      "\n",
      "Hasta ahora, el libro de esta edici√≥n ha tratado de la historia del p√∫rpico en todo el mundo, pero sobre la mujer. No se trata, por lo tanto, de un\n"
     ]
    }
   ],
   "source": [
    "# Before any Latin training - what does the base model produce?\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\"Lingua Latina est\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(\"BEFORE:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xXsGJD9D7qS"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mesemjryCUOX"
   },
   "source": [
    "## The Key: Streaming Dataset\n",
    "\n",
    "The magic is `streaming=True` - data flows directly from the Hugging Face Hub without you first needing to download data locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtZaQDajCUOX",
    "outputId": "8416677c-e36a-4979-f731-1825ce6282d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Ita est in oratione senex mente confusus, eo quod illam imaginem Deitatis, quam proponere sibi in oratione consueverat, aboleri de suo corde sentiret, ut in amarissimos fletus, crebrosque singultus repente prorumpens, in terram prostratus, cum ejulatu validissimo proclamaret; \"Heu me miserum! tulerunt a me Deum meum, et quem nunc teneam non habeo, vel quem adorem, aut interpallam am nescio.\" Cassian, Collat. x. 2.',\n",
       " 'id': '<urn:uuid:318d65fb-88ea-43cd-8687-8a8d802317a5>',\n",
       " 'dump': 'CC-MAIN-2013-20',\n",
       " 'url': 'http://www.ourcivilisation.com/smartboard/shop/gibbone/rome/volume2/nt470/013.htm',\n",
       " 'date': '2013-05-18T13:41:33Z',\n",
       " 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696382398/warc/CC-MAIN-20130516092622-00034-ip-10-60-113-184.ec2.internal.warc.gz',\n",
       " 'language': 'lat',\n",
       " 'language_score': 0.9927687644958496,\n",
       " 'language_script': 'Latn',\n",
       " 'minhash_cluster_size': 10,\n",
       " 'top_langs': '{}'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb-2\",\n",
    "    name=\"lat_Latn\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Peek at the data\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uxrdANQxCUOX"
   },
   "outputs": [],
   "source": [
    "def format_text(example):\n",
    "    return {\"text\": example[\"text\"] + tokenizer.eos_token}\n",
    "\n",
    "formatted_dataset = dataset.map(format_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNWWellhEoim",
    "outputId": "4596381a-ae16-44e5-a47c-08b55491c3d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    num_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset.map(format_text)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "L_mxGiHLEsPV",
    "outputId": "98519fa2-2984-48cc-e94a-ff3baaae734c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Ita est in oratione senex mente confusus, eo quod illam imaginem Deitatis, quam proponere sibi in oratione consueverat, aboleri de suo corde sentiret, ut in amarissimos fletus, crebrosque singultus repente prorumpens, in terram prostratus, cum ejulatu validissimo proclamaret; \"Heu me miserum! tulerunt a me Deum meum, et quem nunc teneam non habeo, vel quem adorem, aut interpallam am nescio.\" Cassian, Collat. x. 2.<|endoftext|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(example))['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlpA37a4CUOX"
   },
   "source": [
    "## Training\n",
    "\n",
    "For streaming datasets, use `max_steps` instead of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "NdG8OS--CUOX",
    "outputId": "fef9dfe7-3aa9-483e-94de-3ea576dff732"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 800 | Num Epochs = 9,223,372,036,854,775,807 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 10,092,544 of 606,142,464 (1.67% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:51, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.716300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=3.5483124160766604, metrics={'train_runtime': 185.9221, 'train_samples_per_second': 4.303, 'train_steps_per_second': 0.538, 'total_flos': 2425465405440000.0, 'train_loss': 3.5483124160766604, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=100,  # Use max_steps, not epochs!\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "        packing=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7_nfy49CUOX"
   },
   "source": [
    "## Vibe Check: After Training\n",
    "\n",
    "Same prompt, same settings - let's see if 100 steps of Latin made a difference (probably not...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMKxI-cnCUOX",
    "outputId": "8d92d348-0dc8-456a-e9fe-056106a37c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER: Lingua Latina est invenire, ut invenire est. Ego, invenire, invenire, ut, ut, ut, ut, ut, ut, ut ut, ut, ut, ut, ut. Invenire, invenire, invenire, ut, ut, ut, ut,\n"
     ]
    }
   ],
   "source": [
    "# After Latin training - same prompt, same settings\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\"Lingua Latina est\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(\"AFTER:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTWGSnAbCUOY"
   },
   "source": [
    "## Scaling Up with HF Jobs\n",
    "\n",
    "The notebook above works great for quick experiments. But streaming in Colab isn't as fast as it could be since the network speed in Colab can be quite slow.\n",
    "\n",
    "**What if we ran training directly on Hugging Face?**\n",
    "\n",
    "With [HF Jobs](https://huggingface.co/docs/huggingface_hub/guides/jobs), compute is co-located with the data and Jobs have very fast connection so it can be much faster!\n",
    "\n",
    "| Environment | Speed | Bottleneck |\n",
    "|-------------|-------|------------|\n",
    "| Colab A100 | 0.36 it/s | Network latency |\n",
    "| HF Jobs A100 | 0.74 it/s | GPU |\n",
    "\n",
    "This streaming approach was fairly naive so there are likely some extra tweaks to make it even faster!\n",
    "\n",
    "I trained a larger model (Qwen3 4B) for 1000 steps using a [UV script](https://docs.astral.sh/uv/guides/scripts/) on HF Jobs. The result: [davanstrien/qwen3-4b-latin](https://huggingface.co/davanstrien/qwen3-4b-latin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8vS1fG8CUOY"
   },
   "source": [
    "### Try the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGdgFnTiCUOY",
    "outputId": "e364880a-d9dd-413e-bc38-dadd3ddad7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Lingua Latina est etum, nec.,usis. atit, et.,.. n.um sedam,em ei.,,que\n",
      ",.am a. quisus et.,,, et a inus in e. aliqu.is,,....\n",
      ",.,at ipsum\n"
     ]
    }
   ],
   "source": [
    "# Load the Latin-trained 4B model\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"davanstrien/qwen3-4b-latin\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Generate some Latin\n",
    "inputs = tokenizer(\"Lingua Latina est\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNbpikxHGSbE"
   },
   "source": [
    "Still bad, but more Latin for sure!\n",
    "\n",
    "The goal in this post wasn't really to show how to continued fine tuning effectively, but to show how you can avoid needing huge amounts of disk space to train models effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL99d_KDCUOY"
   },
   "source": [
    "## Why This Matters\n",
    "\n",
    "\n",
    "- **Minimal disk space needed** - train on massive datasets without downloading\n",
    "- **Works everywhere** - Colab, Kaggle, HF Jobs, any constrained environment\n",
    "- **Scales up** - combine with HF Jobs for training without needing local disk or GPU!\n",
    "\n",
    "The full UV script is available [here](https://huggingface.co/datasets/uv-scripts/training/blob/main/latin-llm-streaming.py) if you want to train your own.\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [Streaming Datasets for ML Training](https://huggingface.co/blog/streaming-datasets) - HF blog post on streaming\n",
    "- [Datasets Streaming Documentation](https://huggingface.co/docs/datasets/stream) - full docs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
