{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "title:  Dynamically updating a Hugging Face hub organization README\n",
    "description: Using the huggingface_hub library and Jinja to update a README dynamically\n",
    "author: \"Daniel van Strien\"\n",
    "date: 2023-03-07\"\n",
    "image: preview.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tl;dr we can use the `huggingface_hub` library to auto generate a model card readme for the [BigLAM organization](https://huggingface.co/biglam)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## What are we aiming to do?\n",
    "\n",
    "The Hugging Face hub allows organizations to create a README card to describe their organization.\n",
    "\n",
    "![](https://github.com/davanstrien/blog/raw/master/images/_readme_auto_generate/before_readme.png)\n",
    "\n",
    "Whilst you can manually create this there might be some content that would be nice to auto populate. For example, for the BigLAM organization, we're mainly focused on collecting datasets. Since we have many tasks supported by these datasets we might want to create a list of datasets organized by task. Ideally we don't want to have to manually update this. Let's see how we can do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First we'll install the `huggingface_hub` library which allows us to interact with the hub. We'll install `Jinja2` for templating and `toolz` because `toolz` makes Python infinitely more delightful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIYdn1woOS1n",
    "outputId": "b856754e-887d-452e-d65d-152a92d39c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (0.11.1)\r\n",
      "Requirement already satisfied: toolz in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (0.12.0)\r\n",
      "Requirement already satisfied: Jinja2 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (3.1.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (6.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (23.0)\r\n",
      "Requirement already satisfied: requests in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (2.28.2)\r\n",
      "Requirement already satisfied: filelock in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (4.4.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from Jinja2) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/davanstrien/Documents/daniel/blog/venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.0.1)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub toolz Jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhW-DrjU1xH9"
   },
   "outputs": [],
   "source": [
    "import toolz\n",
    "from huggingface_hub import list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We list all the datasets under this organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mM40uk0FeH1M"
   },
   "outputs": [],
   "source": [
    "big_lam_datasets = list(iter(list_datasets(author=\"biglam\", limit=None, full=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We want to check which tasks our organization currently has. If we look at an example of one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo: {\n",
       "\tid: biglam/illustrated_ads\n",
       "\tsha: 688e7d96e99cd5730a17a5c55b0964d27a486904\n",
       "\tlastModified: 2023-01-18T20:38:15.000Z\n",
       "\ttags: ['task_categories:image-classification', 'task_ids:multi-class-image-classification', 'annotations_creators:expert-generated', 'size_categories:n<1K', 'license:cc0-1.0', 'lam', 'historic newspapers']\n",
       "\tprivate: False\n",
       "\tauthor: biglam\n",
       "\tdescription: The Dataset contains images derived from the Newspaper Navigator (news-navigator.labs.loc.gov/), a dataset of images drawn from the Library of Congress Chronicling America collection.\n",
       "\tcitation: @dataset{van_strien_daniel_2021_5838410,\n",
       "  author       = {van Strien, Daniel},\n",
       "  title        = {{19th Century United States Newspaper Advert images \n",
       "                   with 'illustrated' or 'non illustrated' labels}},\n",
       "  month        = oct,\n",
       "  year         = 2021,\n",
       "  publisher    = {Zenodo},\n",
       "  version      = {0.0.1},\n",
       "  doi          = {10.5281/zenodo.5838410},\n",
       "  url          = {https://doi.org/10.5281/zenodo.5838410}}\n",
       "\tcardData: {'annotations_creators': ['expert-generated'], 'language': [], 'language_creators': [], 'license': ['cc0-1.0'], 'multilinguality': [], 'pretty_name': \"19th Century United States Newspaper Advert images with 'illustrated' or 'non illustrated' labels\", 'size_categories': ['n<1K'], 'source_datasets': [], 'tags': ['lam', 'historic newspapers'], 'task_categories': ['image-classification'], 'task_ids': ['multi-class-image-classification']}\n",
       "\tsiblings: []\n",
       "\t_id: 62b9bb453b3301c319d5b53e\n",
       "\tdisabled: False\n",
       "\tgated: False\n",
       "\tgitalyUid: 4a051da032bb27da0bc286b288384bb3362f56546a387b130121cd279db336e1\n",
       "\tlikes: 3\n",
       "\tdownloads: 11\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_lam_datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see the `cardData` attribute contains an item containing the tasks supported by a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image-classification']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_lam_datasets[0].cardData['task_categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTLeW_LPeWgZ"
   },
   "outputs": [],
   "source": [
    "def get_task_categories(dataset):\n",
    "    try:\n",
    "        yield from dataset.cardData['task_categories']\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use the `toolz.frequencies` function to get counts of these tasks in our org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vY62Tu9neuiS",
    "outputId": "87475cb1-e278-4078-e76a-d3c5a4986d58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image-classification': 8,\n",
       " 'text-classification': 6,\n",
       " 'image-to-text': 2,\n",
       " 'text-generation': 7,\n",
       " 'object-detection': 5,\n",
       " 'fill-mask': 2,\n",
       " 'text-to-image': 1,\n",
       " 'image-to-image': 1,\n",
       " 'token-classification': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_frequencies = toolz.frequencies(\n",
    "    toolz.concat(map(get_task_categories, big_lam_datasets))\n",
    ")\n",
    "task_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since we want to organize by task type, let's grab the names of all the tasks in the BigLAM organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGhCXxVpktEL",
    "outputId": "ff2e4a3c-e3ce-4023-b348-0378df89bab4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image-classification', 'text-classification', 'image-to-text', 'text-generation', 'object-detection', 'fill-mask', 'text-to-image', 'image-to-image', 'token-classification'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = task_frequencies.keys()\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We now want to group together datasets by the task(s) they support. We can use a default dict to create a dictionary where the keys are the task and the values are a list of datasets supporting that task. **Note** some datasets support multiple tasks so may appear under more than one task key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wFHxBi9eNtJ"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIcRmeVzeQkr"
   },
   "outputs": [],
   "source": [
    "datasets_by_task = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cY0z884gfJfY"
   },
   "outputs": [],
   "source": [
    "for dataset in big_lam_datasets:\n",
    "    tasks = get_task_categories(dataset)\n",
    "    for task in tasks:\n",
    "        datasets_by_task[task].append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We now have a dictionary which allows us to get all datasets supporting a task, for example `fill-mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMYXHeF2lBAB",
    "outputId": "939fbb63-21c0-4aaf-fb70-c71cf962ce5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetInfo: {\n",
       " \tid: biglam/berlin_state_library_ocr\n",
       " \tsha: a890935d5bd754ddc5b85f56b6f34f6d2bb4abba\n",
       " \tlastModified: 2022-08-05T09:36:24.000Z\n",
       " \ttags: ['task_categories:fill-mask', 'task_categories:text-generation', 'task_ids:masked-language-modeling', 'task_ids:language-modeling', 'annotations_creators:machine-generated', 'language_creators:expert-generated', 'multilinguality:multilingual', 'size_categories:1M<n<10M', 'language:de', 'language:nl', 'language:en', 'language:fr', 'language:es', 'license:cc-by-4.0', 'ocr', 'library']\n",
       " \tprivate: False\n",
       " \tauthor: biglam\n",
       " \tdescription: None\n",
       " \tcitation: None\n",
       " \tcardData: {'annotations_creators': ['machine-generated'], 'language': ['de', 'nl', 'en', 'fr', 'es'], 'language_creators': ['expert-generated'], 'license': ['cc-by-4.0'], 'multilinguality': ['multilingual'], 'pretty_name': 'Berlin State Library OCR', 'size_categories': ['1M<n<10M'], 'source_datasets': [], 'tags': ['ocr', 'library'], 'task_categories': ['fill-mask', 'text-generation'], 'task_ids': ['masked-language-modeling', 'language-modeling']}\n",
       " \tsiblings: []\n",
       " \t_id: 62e0431281d9ca6484efac31\n",
       " \tdisabled: False\n",
       " \tgated: False\n",
       " \tgitalyUid: 3818ba9c8b624d79f1fcfb0c79bd197fb5b3a3f9de2452aed5028e8b6435f56a\n",
       " \tlikes: 3\n",
       " \tdownloads: 5\n",
       " },\n",
       " DatasetInfo: {\n",
       " \tid: biglam/bnl_newspapers1841-1879\n",
       " \tsha: 588db6c242ecae417b92830d5646121c15726fea\n",
       " \tlastModified: 2022-11-15T09:25:43.000Z\n",
       " \ttags: ['task_categories:text-generation', 'task_categories:fill-mask', 'task_ids:language-modeling', 'task_ids:masked-language-modeling', 'annotations_creators:no-annotation', 'language_creators:expert-generated', 'multilinguality:multilingual', 'size_categories:100K<n<1M', 'source_datasets:original', 'language:de', 'language:fr', 'language:lb', 'language:nl', 'language:la', 'language:en', 'license:cc0-1.0', 'newspapers', '1800-1900']\n",
       " \tprivate: False\n",
       " \tauthor: biglam\n",
       " \tdescription: None\n",
       " \tcitation: None\n",
       " \tcardData: {'annotations_creators': ['no-annotation'], 'language': ['de', 'fr', 'lb', 'nl', 'la', 'en'], 'language_creators': ['expert-generated'], 'license': ['cc0-1.0'], 'multilinguality': ['multilingual'], 'pretty_name': 'BnL Newspapers 1841-1879', 'size_categories': ['100K<n<1M'], 'source_datasets': ['original'], 'tags': ['newspapers', '1800-1900'], 'task_categories': ['text-generation', 'fill-mask'], 'task_ids': ['language-modeling', 'masked-language-modeling']}\n",
       " \tsiblings: []\n",
       " \t_id: 6372286ce8891da06b2a5d2f\n",
       " \tdisabled: False\n",
       " \tgated: False\n",
       " \tgitalyUid: 039f217af964cfa1317f03d58c367ba6f0e415721b107a298cd4e75cbad50e8b\n",
       " \tlikes: 2\n",
       " \tdownloads: 3\n",
       " }]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_by_task[\"fill-mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## How can we create a README that dynamically updates\n",
    "\n",
    "We now have our datasets organized by task. However, at the moment, this is in the form of a Python dictionary. It would be much nicer to render it a more pleasing format. This is where a [templating engine](https://www.fullstackpython.com/template-engines.html) can help. In this case we'll use [Jinja](https://jinja.palletsprojects.com/en/3.0.x/templates/).\n",
    "\n",
    "A templating engine allows us to create a template which can dynamically be updated based on values we pass in. We won't go in depth to templating engines/Jinja in this blog post because I'm not an expert in templating engines. This [Real Python article](https://realpython.com/primer-on-jinja-templating/) is a nice introduction to Jinja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vruzBySj7HIt"
   },
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can start by taking a look at our template. Since a lot of the template I created doesn't update, we'll use `tail` to look at the bottom of the template which is dynamically updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An overview of datasets currently made available via BigLam organised by task type.\r\n",
      "\r\n",
      "{% for task_type, datasets in task_dictionary.items() %}\r\n",
      "\r\n",
      "<details>\r\n",
      "  <summary>{{ task_type }}</summary>\r\n",
      "    {% for dataset in datasets %}\r\n",
      "  - [{{dataset.cardData['pretty_name']}}](https://huggingface.co/datasets/biglam/{{ dataset.id }})\r\n",
      "  {%- endfor %}\r\n",
      "\r\n",
      "</details>\r\n",
      "{% endfor %}"
     ]
    }
   ],
   "source": [
    "!tail -n 12 templates/readme.jinja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Even if you aren't familiar with templating engines, you can probably see roughly what this does. We look through all the keys and values in our dictionary, create a section for that task based on the dictionary key. We next loop through the dictionary values (which in this case is a list) and create a link for that dataset. Since we're looping through `DatasetInfo` objects in the list we can grab things like the `pretty_name` for the dataset and dynamically create a URL link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can load this template as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d9_63yDnWEj"
   },
   "outputs": [],
   "source": [
    "environment = Environment(loader=FileSystemLoader(\"templates/\"))\n",
    "template = environment.get_template(\"readme.jinja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a context dictionary which we use to pass through our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUSi0b4Qoew_"
   },
   "outputs": [],
   "source": [
    "context = {\n",
    "    \"task_dictionary\": datasets_by_task,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now render this and see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ls0FRd7LmwyH",
    "outputId": "5b98c9f5-c152-4012-9f19-aee61b695aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: README\n",
      "emoji: ðŸ“š\n",
      "colorFrom: pink\n",
      "colorTo: gray\n",
      "sdk: static\n",
      "pinned: false\n",
      "---\n",
      "\n",
      "BigScience ðŸŒ¸ is an open scientific collaboration of nearly 600 researchers from 50 countries and 250 institutions who collaborate on various projects within the natural language processing (NLP) space to broaden the accessibility of language datasets while working on challenging scientific questions around training language models.\n",
      "\n",
      "\n",
      "BigLAM started as a [datasets hackathon](https://github.com/bigscience-workshop/lam) focused on making data from Libraries, Archives, and Museums (LAMS) with potential machine-learning applications accessible via the Hugging Face Hub.\n",
      "We are continuing to work on making more datasets available via the Hugging Face hub to help make these datasets more discoverable, open them up to new audiences, and help ensure that machine-learning datasets more closely reflect the richness of human culture.\n",
      "\n",
      "\n",
      "## Dataset Overview\n",
      "\n",
      "An overview of datasets currently made available via BigLam organised by task type.\n",
      "\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>image-classification</summary>\n",
      "    \n",
      "  - [19th Century United States Newspaper Advert images with 'illustrated' or 'non illustrated' labels](https://huggingface.co/datasets/biglam/biglam/illustrated_ads)\n",
      "  - [Brill Iconclass AI Test Set ](https://huggingface.co/datasets/biglam/biglam/brill_iconclass)\n",
      "  - [National Library of Scotland Chapbook Illustrations](https://huggingface.co/datasets/biglam/biglam/nls_chapbook_illustrations)\n",
      "  - [Encyclopaedia Britannica Illustrated](https://huggingface.co/datasets/biglam/biglam/encyclopaedia_britannica_illustrated)\n",
      "  - [V4Design Europeana style dataset](https://huggingface.co/datasets/biglam/biglam/v4design_europeana_style_dataset)\n",
      "  - [Early Printed Books Font Detection Dataset](https://huggingface.co/datasets/biglam/biglam/early_printed_books_font_detection)\n",
      "  - [Dataset of Pages from Early Printed Books with Multiple Font Groups](https://huggingface.co/datasets/biglam/biglam/early_printed_books_with_multiple_font_groups)\n",
      "  - [DEArt: Dataset of European Art](https://huggingface.co/datasets/biglam/biglam/european_art)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>text-classification</summary>\n",
      "    \n",
      "  - [Annotated dataset to assess the accuracy of the textual description of cultural heritage records](https://huggingface.co/datasets/biglam/biglam/cultural_heritage_metadata_accuracy)\n",
      "  - [Atypical Animacy](https://huggingface.co/datasets/biglam/biglam/atypical_animacy)\n",
      "  - [Old Bailey Proceedings](https://huggingface.co/datasets/biglam/biglam/old_bailey_proceedings)\n",
      "  - [Lampeter Corpus](https://huggingface.co/datasets/biglam/biglam/lampeter_corpus)\n",
      "  - [Hansard Speeches](https://huggingface.co/datasets/biglam/biglam/hansard_speech)\n",
      "  - [Contentious Contexts Corpus](https://huggingface.co/datasets/biglam/biglam/contentious_contexts)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>image-to-text</summary>\n",
      "    \n",
      "  - [Brill Iconclass AI Test Set ](https://huggingface.co/datasets/biglam/biglam/brill_iconclass)\n",
      "  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>text-generation</summary>\n",
      "    \n",
      "  - [Old Bailey Proceedings](https://huggingface.co/datasets/biglam/biglam/old_bailey_proceedings)\n",
      "  - [Hansard Speeches](https://huggingface.co/datasets/biglam/biglam/hansard_speech)\n",
      "  - [Berlin State Library OCR](https://huggingface.co/datasets/biglam/biglam/berlin_state_library_ocr)\n",
      "  - [Literary fictions of Gallica](https://huggingface.co/datasets/biglam/biglam/gallica_literary_fictions)\n",
      "  - [Europeana Newspapers ](https://huggingface.co/datasets/biglam/biglam/europeana_newspapers)\n",
      "  - [Gutenberg Poetry Corpus](https://huggingface.co/datasets/biglam/biglam/gutenberg-poetry-corpus)\n",
      "  - [BnL Newspapers 1841-1879](https://huggingface.co/datasets/biglam/biglam/bnl_newspapers1841-1879)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>object-detection</summary>\n",
      "    \n",
      "  - [National Library of Scotland Chapbook Illustrations](https://huggingface.co/datasets/biglam/biglam/nls_chapbook_illustrations)\n",
      "  - [YALTAi Tabular Dataset](https://huggingface.co/datasets/biglam/biglam/yalta_ai_tabular_dataset)\n",
      "  - [YALTAi Tabular Dataset](https://huggingface.co/datasets/biglam/biglam/yalta_ai_segmonto_manuscript_dataset)\n",
      "  - [Beyond Words](https://huggingface.co/datasets/biglam/biglam/loc_beyond_words)\n",
      "  - [DEArt: Dataset of European Art](https://huggingface.co/datasets/biglam/biglam/european_art)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>fill-mask</summary>\n",
      "    \n",
      "  - [Berlin State Library OCR](https://huggingface.co/datasets/biglam/biglam/berlin_state_library_ocr)\n",
      "  - [BnL Newspapers 1841-1879](https://huggingface.co/datasets/biglam/biglam/bnl_newspapers1841-1879)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>text-to-image</summary>\n",
      "    \n",
      "  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>image-to-image</summary>\n",
      "    \n",
      "  - [Old Book Illustrations](https://huggingface.co/datasets/biglam/biglam/oldbookillustrations)\n",
      "\n",
      "</details>\n",
      "\n",
      "\n",
      "<details>\n",
      "  <summary>token-classification</summary>\n",
      "    \n",
      "  - [Unsilencing Colonial Archives via Automated Entity Recognition](https://huggingface.co/datasets/biglam/biglam/unsilence_voc)\n",
      "\n",
      "</details>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(template.render(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/tmp/README.md','w') as f:\n",
    "    f.write(template.render(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Updating the README on the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This looks pretty good! It would be nice to also update the org README without having to manually edit the file. The `huggingface_hub` library helps us out here once again. Since the organization README is actually a special type of Hugging Face Space, we can interact with it in the same way we could for models or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXddO8_2fZv9"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We'll create a `HFApi` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since we're planning to write to a repo we'll need to login to the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40591d8ec7df49b0aacc1210b1f81af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now upload the rendered README file we created above to our `biglam/README` space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/spaces/biglam/README/blob/main/README.md'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"/tmp/readme.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=\"biglam/README\",\n",
    "    repo_type=\"space\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we look at our updated README, we'll see we now have some nice collapsible sections for each task type containing the datasets for that task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![After README](after_readme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next steps, whilst this was already quite useful, at the moment we still have to run this code when we want to regenerate our README. [Webhooks](https://huggingface.co/docs/hub/webhooks) make it possible to make this fully automated by creating a webhook that monitors any changes to repos under the BigLAM org. Would love to hear from anyone who tries this out!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "scratchpad",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
