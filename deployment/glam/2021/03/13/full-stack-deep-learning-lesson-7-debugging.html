<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Full Stack Deep Learning notes on troubleshooting and debugging | Daniel van Strien</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Full Stack Deep Learning notes on troubleshooting and debugging" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="full-stack deep learning notes" />
<meta property="og:description" content="full-stack deep learning notes" />
<link rel="canonical" href="https://danielvanstrien.xyz/deployment/glam/2021/03/13/full-stack-deep-learning-lesson-7-debugging.html" />
<meta property="og:url" content="https://danielvanstrien.xyz/deployment/glam/2021/03/13/full-stack-deep-learning-lesson-7-debugging.html" />
<meta property="og:site_name" content="Daniel van Strien" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-13T00:00:00-06:00" />
<script type="application/ld+json">
{"headline":"Full Stack Deep Learning notes on troubleshooting and debugging","url":"https://danielvanstrien.xyz/deployment/glam/2021/03/13/full-stack-deep-learning-lesson-7-debugging.html","dateModified":"2021-03-13T00:00:00-06:00","datePublished":"2021-03-13T00:00:00-06:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://danielvanstrien.xyz/deployment/glam/2021/03/13/full-stack-deep-learning-lesson-7-debugging.html"},"description":"full-stack deep learning notes","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://danielvanstrien.xyz/feed.xml" title="Daniel van Strien" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-59247814-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-59247814-2');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Daniel van Strien</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Full Stack Deep Learning notes on troubleshooting and debugging</h1><p class="page-description">full-stack deep learning notes</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-13T00:00:00-06:00" itemprop="datePublished">
        Mar 13, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#deployment">deployment</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#glam">glam</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction-to-my-notes">Introduction to my notes</h1>

<p>These are my notes from the <a href="https://fullstackdeeplearning.com/">full-stack deep learning course</a>.</p>

<p>They have a  focus on the GLAM setting, which might have different requirements, incentives and resources compared to business deployments of ML projects. These notes are for my use primarily. I am posting them to make myself more accountable for making semi-coherent notes.</p>

<p><strong>These are my notes, written in a personal capacity - my employer denounces all my views</strong></p>

<h2 id="why-talk-about-debugging">Why talk about debugging?</h2>

<p>80-90% of time spent on debugging? Useful to have some approaches for doing this work.</p>

<h3 id="why-is-deep-learning-debugging-hard">Why is deep learning debugging hard?</h3>

<h4 id="small-implementation-details">Small implementation details</h4>
<p>Often you may get an error that results in weird results but the code still runs. You don’t have an exception to help you debug. Often need to manually dig around to find the cause of the error. Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">images</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="n">glob</span><span class="p">(</span><span class="s">'path/to/images/*'</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="n">glob</span><span class="p">(</span><span class="s">'path/to/labels/*'</span><span class="p">)</span>
</code></pre></div></div>
<p>Will not return images and labels in the correct order because of how glob is implemented in python. This doesn’t throw an error but the model won’t learn anything.</p>

<h4 id="hyperparamers">Hyperparamers</h4>
<p>Small changes to hyperparameters can make the difference between model training well and not training at all.</p>

<h4 id="datamodel-fit">Data/Model fit</h4>

<p>Performance on model on one set of images i.e. imagenet might not translate to the data you are working with. Transfer learning will often help but it might be unclear exactly how the model/new tasks translate to new types of data/problems. 
#</p>

<h4 id="models-vs-dataset-construction">Models vs Dataset construction</h4>

<p>In an academic setting, a lot of thought is given to choosing models/algorithms. Often this is less of a focus when deploying machine learning in a production setting where there will be many more challenges around constructing a dataset.</p>

<p>Notes: I think there is a shift here is going from training n a validation/training set to having in the back of your head that predictions will be made ‘in the wild’.</p>

<p>There can be a fair bit of variation in GLAM datasets which also potentially makes data drift hard to track</p>

<h3 id="architecture-selection">Architecture Selection</h3>

<p>It can be overwhelming to pick between all the different types of model architectures. Suggestion to start out with a few different flavour and change if needed:</p>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Model</th>
      <th>Maybe move to?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Images</td>
      <td>LeNet-like model</td>
      <td>ResNet</td>
    </tr>
    <tr>
      <td>Sequences</td>
      <td>LSTM with one hidden layer</td>
      <td>Attention</td>
    </tr>
    <tr>
      <td>other</td>
      <td>Fully connected neural net with one hidden layer</td>
      <td>Problem dependent</td>
    </tr>
  </tbody>
</table>

<h3 id="sensible-defaults">Sensible defaults</h3>
<p>version zero of your model could start with:</p>

<ul>
  <li>optimizer: Adam with learning rate 3e-4</li>
  <li>activations: relu  (FC and Convultional models), tanh (LSTMS)</li>
  <li>Intitilization: He et al. normal(relu), GLorot normal (tanh)</li>
  <li>Regulirization: none</li>
  <li>data normalization: none</li>
</ul>

<h3 id="consider-simplifying-the-problem">Consider simplifying the problem</h3>

<ul>
  <li>start with a subset of the training data</li>
  <li>use a fixed number of objects, classes etc.</li>
  <li>create a simpler synthetic training set</li>
</ul>

<h3 id="model-evaluation">Model evaluation</h3>

<p>tl;dr apply bias-variance decomposition</p>

<p><img src="../images/bias-variance.png" alt="" /></p>

<blockquote>
  <p>In statistics and machine learning, the bias-variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters. The bias-variance dilemma or bias-variance problem is the conflict in trying to simultaneously minimise these two sources of error that prevent supervised learning algorithms from generalising beyond their training set:[1][2]
 The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
   The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). <br />
<a href="">https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff</a></p>
</blockquote>

<ul>
  <li>Test error = irreducible error + bias + variance + val overfitting</li>
</ul>

<p>Here we try and see the difference in error between our training data and some kind of baseline, i.e. human performance, to see how much we are underfitting. We also compare validation to training to see how much overfitting.</p>

<p>This assumes training, validation, and test come from the same distribution.</p>

<p>Notes: tracking the impact of distribution shift on model performance helpful here.</p>

<p>Example</p>

<table>
  <thead>
    <tr>
      <th>Error source</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Goal</td>
      <td>1%</td>
    </tr>
    <tr>
      <td>Train error</td>
      <td>20%</td>
    </tr>
    <tr>
      <td>Validation error</td>
      <td>27%</td>
    </tr>
    <tr>
      <td>Test error</td>
      <td>28%</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>The difference between our goal and our train error shows underfitting</li>
  <li>The difference between train and validation also shows we’re also overfitting</li>
  <li>test and validation error difference is okay</li>
</ul>

<p>🤷‍♂️ Both overfitting and underfitting - how to decide what to do next…</p>

<h3 id="prioritizing-improvements">prioritizing improvements</h3>

<p>It seems hard to both deal with under and overfitting. The suggested process is to follow the following steps:</p>

<p>1) address underfitting
2) address overfitting
3) address distribution shift
4) re-balance data (if applicable)</p>

<h3 id="how-to-address-under-fitting-ie-reduce-bias">How to address under-fitting (i.e. reduce bias)</h3>

<p>These suggestions are listed in order of what to try first.</p>

<ul>
  <li>Make the model bigger (i.e. resnet34, to resnet50)</li>
  <li>reduce regulirization</li>
  <li>error analysis</li>
  <li>choose different model architecture (i.e. bump from LeNet to ResNet). This can introduce new problems, so be careful here</li>
  <li>add features 😬 sometimes this can really help even in deep-learning</li>
</ul>

<h3 id="addressing-over-fitting">Addressing over-fitting</h3>

<ul>
  <li>add more training data (if possible)</li>
  <li>add normalisation</li>
  <li>data augmentation</li>
  <li>regularisation</li>
  <li>error analysis</li>
  <li>choose better model architecture</li>
  <li>tune hyperparameters</li>
  <li>early stopping</li>
  <li>remove features</li>
  <li>reduce model size</li>
</ul>

<h3 id="addressing-distribution-shift">Addressing distribution shift</h3>

<p>a) analyse test-val set errors - why might these errors be there in the validation set. 
b) analyse test-val set errors and synthesise more data 
c) domain adaptation</p>

<h3 id="error-analysis">Error analysis</h3>

<p>Try and find the different sources of error in test/val and see how easily they can be fixed + how much they contribute to the error.</p>

<p>This could be particularly important when domain expertise might help track down why some of these errors occur. This might also be used to reevaluate labels being targeted if they often cause confusion. Fastai has some nice methods for this <code class="language-plaintext highlighter-rouge">most_confused</code> to show which labels are often confused. If this confusion is reasonable, you may a) either not care too much about these mistakes b) collapse two labels into one label.</p>

<h3 id="domain-adaptation">Domain adaptation</h3>
<ul>
  <li>supervised: fine-tune pre-trained model</li>
  <li>un-supervised: more in the research domain at the moment?</li>
</ul>

<h2 id="tuning-hypermaters">Tuning Hypermaters</h2>

<p>Model and optimiser choices?</p>
<ul>
  <li>how many layers</li>
  <li>kernel size</li>
  <li>etc.</li>
</ul>

<p>How to choose what to tune? some hyperparameters are more essential but its often hard to know which are going to be more important</p>

<p>rules of thumb of what is worth tuning</p>

<table>
  <thead>
    <tr>
      <th>Hyperparamter</th>
      <th>likely senstivity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Learning rate</td>
      <td>high</td>
    </tr>
    <tr>
      <td>Learning rate schedule</td>
      <td>high</td>
    </tr>
    <tr>
      <td>Optimiser choice</td>
      <td>low</td>
    </tr>
    <tr>
      <td>other optimiser parameters</td>
      <td>low</td>
    </tr>
    <tr>
      <td>batch size</td>
      <td>low (what fits onto GPU)</td>
    </tr>
    <tr>
      <td>weight initialisation</td>
      <td>medium</td>
    </tr>
    <tr>
      <td>loss function</td>
      <td>high</td>
    </tr>
    <tr>
      <td>model depth</td>
      <td>medium</td>
    </tr>
    <tr>
      <td>layer size</td>
      <td>high</td>
    </tr>
    <tr>
      <td>layer params</td>
      <td>medium</td>
    </tr>
    <tr>
      <td>Weight of regularisation</td>
      <td>medium</td>
    </tr>
    <tr>
      <td>nonlinearity</td>
      <td>low</td>
    </tr>
  </tbody>
</table>

<h3 id="approaches-to-hyperparameters">Approaches to hyperparameters</h3>

<h4 id="manual">Manual:</h4>
<ul>
  <li>focus on what is likely to make a difference conceptually</li>
  <li>train and evaluate model</li>
  <li>guess a better parameters</li>
  <li>can be good as a starting point + combined with other approaches</li>
</ul>

<h4 id="grid-search">Grid search</h4>
<ul>
  <li>easy to implement</li>
  <li>but expensive and you need to have a good sense of sensible range points</li>
</ul>

<h4 id="random-search">Random search</h4>
<ul>
  <li>often better than grid search for the same number of runs</li>
  <li>not very easy to interpret</li>
</ul>

<h4 id="bayesian-approaches">Bayesian approaches</h4>

<ul>
  <li>There are nice frameworks for doing this</li>
  <li>generally most efficient t</li>
  <li>can be hard to implement</li>
</ul>

<h2 id="overarching-notes">Overarching notes</h2>
<p>personally, I think some of these suggestions might be better suited in settings where ml will be the product/service. In GLAMS, this might sometimes be the case, but there is also low hanging fruit using ml as a ‘tool’. In this case, I think it might make more sense to start with the proper implementation of ResNet as a starting point rather than coding your own net from scratch.</p>

<p>The discussion on error analysis was excellent and is a valuable framework for working out how to prioritise improvements.</p>

  </div><a class="u-url" href="/deployment/glam/2021/03/13/full-stack-deep-learning-lesson-7-debugging.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>digital collections + computational methods</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/davanstrien" title="davanstrien"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/vanstriendaniel" title="vanstriendaniel"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
